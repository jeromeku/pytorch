# Session Log

pytorch — model: not_provided — profile: Codex CLI coding agent — sandbox: workspace-write (network: enabled, approvals: on-request)

## Transcript

### User
```
<user_instructions>

# Global operating rules for Codex

**Non-negotiable logging policy**

- For every session:
  1. Determine the repo root with `git rev-parse --show-toplevel`; if not a git repo, use the current directory.
  2. Create (if missing) `<repo-root>/codex-logs/>`.
  3. Write a Markdown transcript to `codex-logs/<YYYYMMDD-HHMMSS>-session.md` that includes:
     - Title line: repo name (or directory), model, profile, sandbox mode.
     - All user messages and Codex replies, verbatim (fenced Markdown blocks).
     - A **Commands** section listing every command you ran with start time, exit code, and duration.
     - A **Changes** section summarizing files you created/edited/removed and the current `git diff --no-ext-diff`.
  4. Update/append a machine-readable provenance file at `codex-logs/provenance.jsonl` with JSON lines for each command run: `{ts, cwd, cmd, exit_code, duration_ms}`.
- If no files were changed or no commands were executed, state that explicitly in the log.
- When in doubt, **prefer writing the log first** and then continuing the task so the transcript is never lost.

--- project-doc ---

- This is the only AGENTS.md, there are no recursive AGENTS.md
- When you are working on a bug, first create a standalone file that
  reproduces the bug and verify it fails in the expected way.  Use this to
  test if your changes work.  Once the change is passing, find an appropriate
  test file to add the test to and make sure to follow local conventions on
  the test file.
- If you are running the real test suite, DO NOT run the entire test suite.
  Instead run only a single test case, e.g., 'python test/test_torch.py TestTorch.test_dir'
- Do NOT run setup.py, you do not have a working build environment
- Do NOT run pre-commit, it is not setup
- To run lint, run 'lintrunner -a' (which will autoapply changes)
- Do NOT attempt to install dependencies, you do not have Internet access
- When you are ready to make a PR, do exactly these steps:
  - git stash -u
  - git reset --hard $(cat /tmp/orig_work.txt) # NB: reset to the LOCAL branch, do NOT fetch
  - git stash pop
  - Resolve conflicts if necessary

</user_instructions>

<environment_context>
  <cwd>/home/jeromeku/pytorch</cwd>
  <approval_policy>on-request</approval_policy>
  <sandbox_mode>workspace-write</sandbox_mode>
  <network_access>enabled</network_access>
</environment_context>

# Context from my IDE setup:

## Active file: torch_compile_sandbox/scripts/run_inductor_ir_trace.py

## Open tabs:
- pipeline_inductor.md: torch_compile_sandbox/pipeline_inductor.md
- examples.md: torch_compile_sandbox/examples.md
- run_fx_pass_trace.py: torch_compile_sandbox/scripts/run_fx_pass_trace.py
- run_inductor_ir_trace.py: torch_compile_sandbox/scripts/run_inductor_ir_trace.py

## My request for Codex:
Explain how higher order ops work in pytorch.

Specifically, provide a step by step trace of how flex attention uses higher order ops and how HOPs helps with compiling flex attention.

Provide full pathname / line number links as well as code snippets to illustrate.

Give a minimal example that exposes the various low-level pytorch (non-user facing) functions that are called when instantiating, building, and compiling a flex attention op.

I want to understand the full call path, starting from user level flex attention call all the way to torch.compile inductor kernel generation -- spare no details, every file / function touched / called.


Document all your findings in a folder "codex/HOPs-flex-attn"
```

### Assistant
```
I’ll set up session logging, then scan the repo for HOPs/FlexAttention to map the call path.
```

## Commands

(Commands will be appended as they run.)

## Changes

(Files created/edited/removed and git diff will appear here.)
- [2025-09-06T15:26:24+00:00] exit=0 dur=55ms: `rg -n --no-heading -S 'FlexAttention|flex_attention'`
  - stdout:
    codex-logs/20250906-152543-session.md:83:I’ll set up session logging, then scan the repo for HOPs/FlexAttention to map the call path.
    codex-logs/provenance.jsonl:2:{"ts":"-S","cwd":"'FlexAttention|flex_attention'"","cmd":0,"exit_code":62,"duration_ms":0}
    benchmarks/transformer/score_mod.py:18:from torch.nn.attention.flex_attention import (
    benchmarks/transformer/score_mod.py:22:    flex_attention,
    benchmarks/transformer/score_mod.py:381:            flex_attention, dynamic=dynamic, mode="max-autotune-no-cudagraphs"
    benchmarks/transformer/score_mod.py:384:        compiled_sdpa = torch.compile(flex_attention, dynamic=dynamic)
    torchgen/_autoheuristic/README.md:125:- flex_attention: https://github.com/pytorch/pytorch/pull/130398
    test/test_nestedtensor.py:29:from torch.nn.attention.flex_attention import create_nested_block_mask, flex_attention
    test/test_nestedtensor.py:39:    flex_attention_supported_platform,
    test/test_nestedtensor.py:7289:    @flex_attention_supported_platform
    test/test_nestedtensor.py:7296:    def test_flex_attention(self, device, dtype, noncontig_with_holes, cross_attention):
    test/test_nestedtensor.py:7301:        # Run FlexAttention with a causal mask
    test/test_nestedtensor.py:7314:        out_flex = flex_attention(query, key, value, block_mask=block_mask)
    test/test_nestedtensor.py:7321:        # Run FlexAttention with a score_mod that represents causal attention
    test/test_nestedtensor.py:7325:        out_flex2 = flex_attention(query, key, value, score_mod=causal_score_mod)
    test/test_nestedtensor.py:7345:    @flex_attention_supported_platform
    test/test_nestedtensor.py:7347:    def test_flex_attention_converts_stacked_seq_indices(self, device, dtype):
    test/test_nestedtensor.py:7350:        # validates that FlexAttention properly converts indices within the
    test/test_nestedtensor.py:7360:        flex_attention(query, key, value, score_mod=my_score_mod)
    test/test_nestedtensor.py:7374:        output = flex_attention(query, key, value, score_mod=batch_specific_score_mod)
    test/test_nestedtensor.py:7375:        output_identity = flex_attention(
    test/test_nestedtensor.py:7394:        output = flex_attention(query, key, value, block_mask=block_mask)
    test/test_nestedtensor.py:7397:        output2 = flex_attention(query, key, value, block_mask=block_mask2)
    torch/_inductor/choices.py:89:    def get_flex_attention_fwd_configs(
    torch/_inductor/choices.py:95:    def get_flex_attention_bwd_configs(
    test/run_test.py:312:    "inductor/test_flex_attention",  # OOM
    torch/_inductor/ir.py:4896:                torch.ops.higher_order.flex_attention,
    torch/_inductor/ir.py:4897:                torch.ops.higher_order.flex_attention_backward,
    test/inductor/test_perf.py:580:        from torch.nn.attention.flex_attention import create_block_mask
    test/dynamo/test_modes.py:50:        if func == torch._higher_order_ops.flex_attention:
    test/dynamo/test_modes.py:692:    def test_flex_attention(self):
    test/dynamo/test_modes.py:694:        from torch.nn.attention.flex_attention import create_block_mask, flex_attention
    test/dynamo/test_modes.py:698:        flex_attention = torch.compile(flex_attention, dynamic=False)
    test/dynamo/test_modes.py:736:        from torch.nn.attention.flex_attention import (
    test/dynamo/test_modes.py:737:            flex_attention as flex_attention_eager,
    test/dynamo/test_modes.py:741:            flex_attention = torch.compile(flex_attention_eager, dynamic=False)
    test/dynamo/test_modes.py:749:                    flex_attention(
    test/dynamo/test_modes.py:760:        from torch.nn.attention.flex_attention import (
    test/dynamo/test_modes.py:761:            flex_attention as flex_attention_eager,
    test/dynamo/test_modes.py:770:                    flex_attention_eager(
    test/inductor/test_flex_attention.py:25:from torch.nn.attention.flex_attention import (
    test/inductor/test_flex_attention.py:37:    flex_attention,
    test/inductor/test_flex_attention.py:38:    flex_attention_hop,
    test/inductor/test_flex_attention.py:49:    flex_attention_supported_platform as supported_platform,
    test/inductor/test_flex_attention.py:154:        flex_attention,
    test/inductor/test_flex_attention.py:440:class TestFlexAttention(InductorTestCase):
    test/inductor/test_flex_attention.py:745:        compiled_sdpa = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:1261:            flex_attention,
    test/inductor/test_flex_attention.py:1282:            flex_attention,
    test/inductor/test_flex_attention.py:1463:            flex_attention, block_mask=block_mask, enable_gqa=(not Hq == Hkv)
    test/inductor/test_flex_attention.py:1741:                return flex_attention(q, k, v, score_mod=score_mod)
    test/inductor/test_flex_attention.py:1864:        attention = functools.partial(flex_attention, score_mod=score_mod_func)
    test/inductor/test_flex_attention.py:1984:        out_only = flex_attention(query, key, value, score_mod)
    test/inductor/test_flex_attention.py:1985:        out_max, aux_max = flex_attention(
    test/inductor/test_flex_attention.py:1992:        out_both, aux_both = flex_attention(
    test/inductor/test_flex_attention.py:2000:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:2078:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:2079:        flex_compile_partial = torch.compile(flex_attention, fullgraph=False)
    test/inductor/test_flex_attention.py:2166:                flex_attention(query, key, value, return_lse=True)
    test/inductor/test_flex_attention.py:2179:                flex_attention(
    test/inductor/test_flex_attention.py:2258:            q2 = flex_attention(q, k1, v1, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2259:            return flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_attention.py:2301:            out1 = flex_attention(q, k, v, block_mask=mask1)
    test/inductor/test_flex_attention.py:2302:            out2 = flex_attention(q, k, v, block_mask=mask2)
    test/inductor/test_flex_attention.py:2336:        attention1 = functools.partial(flex_attention, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2340:            q3 = flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_attention.py:2341:            return flex_attention(q3, k3, v3, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2366:            q2 = flex_attention(q, k1, v1, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2367:            return flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_attention.py:2403:            q2 = flex_attention(
    test/inductor/test_flex_attention.py:2410:            return flex_attention(
    test/inductor/test_flex_attention.py:2444:        attention1 = functools.partial(flex_attention, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2448:            q3 = flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_attention.py:2449:            return flex_attention(q3, k3, v3, score_mod=scoremod_1)
    test/inductor/test_flex_attention.py:2498:            flex_attention,
    test/inductor/test_flex_attention.py:2505:            q3 = flex_attention(
    test/inductor/test_flex_attention.py:2512:            return flex_attention(
    test/inductor/test_flex_attention.py:2538:            return flex_attention(q.sin(), k, v, score_mod=func).cos()
    test/inductor/test_flex_attention.py:2589:        attention = functools.partial(flex_attention, block_mask=block_mask)
    test/inductor/test_flex_attention.py:2625:            out = flex_attention(q, k, v)
    test/inductor/test_flex_attention.py:2676:            flex_attention(query, key, value, _identity)
    test/inductor/test_flex_attention.py:2746:        func = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:2766:        attention = functools.partial(flex_attention, block_mask=block_mask)
    test/inductor/test_flex_attention.py:2801:        torch.compile(flex_attention, fullgraph=True)(
    test/inductor/test_flex_attention.py:2820:            flex_attention, block_mask=block_mask, enable_gqa=True
    test/inductor/test_flex_attention.py:2888:            return flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_attention.py:2892:            return flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_attention.py:2933:            _, lse = flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_attention.py:2958:        func = torch.compile(flex_attention, backend="aot_eager", fullgraph=True)
    test/inductor/test_flex_attention.py:2987:                x = torch.nn.attention.flex_attention.flex_attention(q, k, v)
    test/inductor/test_flex_attention.py:3007:        def flex_attention_lse_only(q, k, v):
    test/inductor/test_flex_attention.py:3008:            return flex_attention(q, k, v, return_lse=True)[1]
    test/inductor/test_flex_attention.py:3010:        func = torch.compile(flex_attention_lse_only, backend="aot_eager")
    test/inductor/test_flex_attention.py:3029:        out, lse = flex_attention(q, k, v, return_lse=True)
    test/inductor/test_flex_attention.py:3036:        out2, lse2 = torch.compile(flex_attention)(q, k, v, return_lse=True)
    test/inductor/test_flex_attention.py:3070:        torch.compile(flex_attention, dynamic=True, fullgraph=True)(
    test/inductor/test_flex_attention.py:3085:        out = torch.compile(flex_attention, dynamic=True, fullgraph=True)(
    test/inductor/test_flex_attention.py:3096:        out2 = torch.compile(flex_attention, dynamic=True, fullgraph=True)(
    test/inductor/test_flex_attention.py:3137:            out_eager = flex_attention(query, key, value, score_mod)
    test/inductor/test_flex_attention.py:3138:            flex_compiled = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:3162:        func = torch.compile(flex_attention, backend=mode, fullgraph=True)
    test/inductor/test_flex_attention.py:3191:        func = torch.compile(flex_attention, backend=mode, fullgraph=True)
    test/inductor/test_flex_attention.py:3205:        func = torch.compile(flex_attention, dynamic=True, fullgraph=True)
    test/inductor/test_flex_attention.py:3210:        out_eager = flex_attention(Q, K_sliced, V_sliced)
    test/inductor/test_flex_attention.py:3235:    def test_flex_attention_stride_ordering(self, device, mode, permute_order, shape):
    test/inductor/test_flex_attention.py:3261:            func = torch.compile(flex_attention, backend=mode, fullgraph=True)
    test/inductor/test_flex_attention.py:3268:            func = flex_attention
    test/inductor/test_flex_attention.py:3288:    def test_flex_attention_backward_stride_ordering(
    test/inductor/test_flex_attention.py:3308:            torch.compile(flex_attention, backend=mode, fullgraph=True)
    test/inductor/test_flex_attention.py:3310:            else flex_attention
    test/inductor/test_flex_attention.py:3331:        """Test flex_attention with tensors having non contiguous last dimension."""
    test/inductor/test_flex_attention.py:3362:            golden_out = flex_attention(q_gold, k_gold, v_gold)
    test/inductor/test_flex_attention.py:3363:            ref_out = flex_attention(q_ref, k_ref, v_ref)
    test/inductor/test_flex_attention.py:3365:            flex_compiled = torch.compile(flex_attention, fullgraph=True, dynamic=True)
    test/inductor/test_flex_attention.py:3424:            torch.compile(flex_attention, dynamic=False) if compile else flex_attention
    test/inductor/test_flex_attention.py:3469:            torch.compile(flex_attention, fullgraph=True),
    test/inductor/test_flex_attention.py:3487:            lambda q, k, v: flex_attention(q, k, v, block_mask=block_mask),
    test/inductor/test_flex_attention.py:3500:        attention = functools.partial(flex_attention, block_mask=block_mask)
    test/inductor/test_flex_attention.py:3520:                o = flex_attention(q, k, v, score_mod=score_mod)
    test/inductor/test_flex_attention.py:3553:        out_eager, lse_eager = flex_attention(query, key, value, return_lse=True)
    test/inductor/test_flex_attention.py:3555:        flex_compile = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:3569:    @common_utils.parametrize("backend", ["flex_attention", "flex_decode", "eager"])
    test/inductor/test_flex_attention.py:3573:            flex_call = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:3575:        elif backend == "flex_attention":
    test/inductor/test_flex_attention.py:3577:            flex_call = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:3581:            flex_call = flex_attention
    test/inductor/test_flex_attention.py:3603:        sliding_window_causal = torch.nn.attention.flex_attention.create_block_mask(
    test/inductor/test_flex_attention.py:3611:        global_causal = torch.nn.attention.flex_attention.create_block_mask(
    test/inductor/test_flex_attention.py:3672:            flex_attention(query, key, value)
    test/inductor/test_flex_attention.py:3692:            torch.compile(flex_attention)(q, k, v, score_mod=euclidean_dist_pos_embed)
    test/inductor/test_flex_attention.py:3713:            torch.compile(flex_attention)(q, k, v, score_mod=euclidean_dist_pos_embed)
    test/inductor/test_flex_attention.py:3746:            torch.compile(flex_attention)(q, k, v, block_mask=block_mask)
    test/inductor/test_flex_attention.py:3760:        out_eager, lse_eager = flex_attention(
    test/inductor/test_flex_attention.py:3764:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:3780:        def _flex_attention_mask(b, h, q_idx, kv_idx, input_lengths):
    test/inductor/test_flex_attention.py:3803:                        _flex_attention_mask,
    test/inductor/test_flex_attention.py:3814:                    flex_attention, dynamic=True, fullgraph=True, backend=counter
    test/inductor/test_flex_attention.py:3845:        out_eager = flex_attention(query, key, value, kernel_options=kernel_options)
    test/inductor/test_flex_attention.py:3847:        flex_compile = torch.compile(flex_attention, fullgraph=True, dynamic=True)
    test/inductor/test_flex_attention.py:3865:        out_eager = flex_attention(query, key, value, block_mask=block_mask)
    test/inductor/test_flex_attention.py:3868:            flex_attention, fullgraph=True, dynamic=True, mode="max-autotune"
    test/inductor/test_flex_attention.py:3886:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:3910:        attention = functools.partial(flex_attention, block_mask=block_mask)
    test/inductor/test_flex_attention.py:3927:            flex_attention, score_mod=apply_multiplicative_bias
    test/inductor/test_flex_attention.py:3936:        attention = functools.partial(flex_attention, score_mod=_identity)
    test/inductor/test_flex_attention.py:3966:            torch.compile(flex_attention)(query, key, value, block_mask=block_mask)
    test/inductor/test_flex_attention.py:3986:                y = flex_attention(
    test/inductor/test_flex_attention.py:4036:                return flex_attention(
    test/inductor/test_flex_attention.py:4082:        func = torch.compile(flex_attention, backend=cnt, fullgraph=True)
    test/inductor/test_flex_attention.py:4108:        flex_attention = torch.ops.higher_order.flex_attention(l_query_, l_key_, l_value_, score_mod_0, (128, 128, l_block_mask_kv_num_blocks, l_block_mask_kv_indices, l_block_mask_full_kv_num_blocks, l_block_mask_full_kv_indices, l_block_mask_q_num_blocks, l_block_mask_q_indices, l_block_mask_full_q_num_blocks, l_block_mask_full_q_indices, 128, 128, mask_fn_0), 0.5, {'PRESCALE_QK': False, 'ROWS_GUARANTEED_SAFE': False, 'BLOCKS_ARE_CONTIGUOUS': False, 'WRITE_DQ': True, 'OUTPUT_LOGSUMEXP': True, 'OUTPUT_MAX': False}, (), ());  l_query_ = l_key_ = l_value_ = score_mod_0 = l_block_mask_kv_num_blocks = l_block_mask_kv_indices = l_block_mask_full_kv_num_blocks = l_block_mask_full_kv_indices = l_block_mask_q_num_blocks = l_block_mask_q_indices = l_block_mask_full_q_num_blocks = l_block_mask_full_q_indices = mask_fn_0 = None
    test/inductor/test_flex_attention.py:4109:        out: "f64[2, 2, 128, 4]" = flex_attention[0];  flex_attention = None
    test/inductor/test_flex_attention.py:4148:        flex_attention_backward = torch.ops.higher_order.flex_attention_backward(primals_1, primals_2, primals_3, getitem_2, getitem_3, tangents_1, full_default_4, fw_graph0, joint_graph0, (1, 1, full, full_default, None, None, convert_element_type, convert_element_type_1, None, None, 1073741824, 1073741824, mask_graph0), 0.5, {'PRESCALE_QK': False, 'ROWS_GUARANTEED_SAFE': False, 'BLOCKS_ARE_CONTIGUOUS': False, 'WRITE_DQ': True, 'OUTPUT_LOGSUMEXP': True, 'OUTPUT_MAX': False}, (), ());  primals_1 = primals_2 = primals_3 = getitem_2 = getitem_3 = tangents_1 = full_default_4 = fw_graph0 = joint_graph0 = full = full_default = convert_element_type = convert_element_type_1 = mask_graph0 = None
    test/inductor/test_flex_attention.py:4149:        getitem_5: "f64[2, 2, 128, 4]" = flex_attention_backward[0]
    test/inductor/test_flex_attention.py:4150:        getitem_6: "f64[2, 2, 128, 4]" = flex_attention_backward[1]
    test/inductor/test_flex_attention.py:4151:        getitem_7: "f64[2, 2, 128, 4]" = flex_attention_backward[2];  flex_attention_backward = None
    test/inductor/test_flex_attention.py:4181:        where subclasses that error on as_strided would fail in flex_attention.
    test/inductor/test_flex_attention.py:4241:        from torch._higher_order_ops.flex_attention import (
    test/inductor/test_flex_attention.py:4242:            flex_attention as flex_attention_hop,
    test/inductor/test_flex_attention.py:4245:        @flex_attention_hop.py_impl(AsStridedErrorTensor)
    test/inductor/test_flex_attention.py:4246:        def flex_attention_as_strided_error_tensor(
    test/inductor/test_flex_attention.py:4258:            out, lse, max_scores = flex_attention_hop(
    test/inductor/test_flex_attention.py:4293:        # Test 2: Run flex_attention with normal tensors first
    test/inductor/test_flex_attention.py:4294:        compiled_fn = torch.compile(flex_attention, backend="aot_eager")
    test/inductor/test_flex_attention.py:4306:        # call flex_attention_fake_impl which uses as_strided
    test/inductor/test_flex_attention.py:4329:        attention = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:4340:                return flex_attention(q, k, v, block_mask=block_mask)
    test/inductor/test_flex_attention.py:4366:                flex_attention_hop,
    test/inductor/test_flex_attention.py:4368:            [torch.ops.aten.mm.default, flex_attention_hop],
    test/inductor/test_flex_attention.py:4372:        class FlexAttentionModule(nn.Module):
    test/inductor/test_flex_attention.py:4408:                attn_output = flex_attention(
    test/inductor/test_flex_attention.py:4435:        # Define a model that uses FlexAttention with selective activation checkpointing
    test/inductor/test_flex_attention.py:4439:                self.flex_attn = FlexAttentionModule(hidden_size, num_heads)
    test/inductor/test_flex_attention.py:4488:        flex_attention(q, k, v)
    test/inductor/test_flex_attention.py:4492:        flex_attention(q, k, v)
    test/inductor/test_flex_attention.py:4496:        compiled_fa = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:4507:        compiled_fa = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:4518:        flex_compiled = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:4545:        C2 = flex_attention(q, k, v)
    test/inductor/test_flex_attention.py:4570:        flex_compile = torch.compile(flex_attention, fullgraph=True, dynamic=True)
    test/inductor/test_flex_attention.py:4594:        flex_compile = torch.compile(flex_attention, fullgraph=True, dynamic=True)
    test/inductor/test_flex_attention.py:4612:        import torch.nn.attention.flex_attention as fa
    test/inductor/test_flex_attention.py:4631:                out_compiled = fa.flex_attention(
    test/inductor/test_flex_attention.py:4636:                "flex_attention called without torch.compile", str(cm.warning)
    test/inductor/test_flex_attention.py:4645:                out_debug = fa.flex_attention(
    test/inductor/test_flex_attention.py:4986:        torch.compile(flex_attention)(q, k, v, score_mod=score_mod)
    test/inductor/test_flex_attention.py:5109:            torch.compile(flex_attention)(q, k, v, block_mask=block_mask)
    test/inductor/test_flex_attention.py:5125:        def flex_attention_fn():
    test/inductor/test_flex_attention.py:5126:            out = flex_attention(q, k, v, enable_gqa=True)
    test/inductor/test_flex_attention.py:5130:        compiled_fn = torch.compile(flex_attention_fn, fullgraph=True)
    test/inductor/test_flex_attention.py:5158:            flex_attention_call = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:5160:            flex_attention_call = flex_attention
    test/inductor/test_flex_attention.py:5175:        flex_attention_call(*create_inputs(1024), block_mask=block_mask)
    test/inductor/test_flex_attention.py:5177:            flex_attention_call(*create_inputs(2048), block_mask=block_mask)
    test/inductor/test_flex_attention.py:5181:            flex_attention_call(*create_inputs(1024), block_mask=block_mask)
    test/inductor/test_flex_attention.py:5241:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:5294:        flex_compile = torch.compile(flex_attention, fullgraph=True)
    test/inductor/test_flex_attention.py:5815:        # Note: This has been carefully tested that FlexAttention is within
    test/inductor/test_flex_attention.py:5818:        # of FlexAttention!
    test/inductor/test_flex_attention.py:5830:            self.skipTest("Compiled FlexAttention is less accurate than eager in fp32")
    test/inductor/test_flex_attention.py:5879:        flex_compiled = torch.compile(flex_attention, mode=mode)
    test/inductor/test_flex_attention.py:5880:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:5882:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:5913:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:5914:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:5916:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:5948:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:5949:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:5951:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:5984:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:5985:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:5987:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6017:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6018:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6020:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6052:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6053:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6055:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6085:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6086:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6088:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6122:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6123:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6125:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6162:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6163:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6165:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6196:        flex_compiled = torch.compile(flex_attention, mode=mode)
    test/inductor/test_flex_attention.py:6197:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6199:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6234:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6235:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6237:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6268:        flex_compiled = torch.compile(flex_attention, mode=mode)
    test/inductor/test_flex_attention.py:6269:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6271:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6308:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6309:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6311:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6368:        out = flex_attention(
    test/inductor/test_flex_attention.py:6405:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_attention.py:6406:        out_eager = flex_attention(query, key, value, score_mod=bias_func)
    test/inductor/test_flex_attention.py:6409:        out_gold = flex_attention(
    test/inductor/test_flex_attention.py:6421:    def _test_flex_attention_with_dynamic_max_autotune(self, device):
    test/inductor/test_flex_attention.py:6443:            flex_attention, dynamic=True, mode="max-autotune-no-cudagraphs"
    test/inductor/test_flex_attention.py:6462:    def test_flex_attention_with_dynamic_max_autotune(self, device):
    test/inductor/test_flex_attention.py:6463:        self._test_flex_attention_with_dynamic_max_autotune(device)
    test/inductor/test_flex_attention.py:6467:    def test_flex_attention_with_dynamic_max_autotune_graph_partition(self, device):
    test/inductor/test_flex_attention.py:6468:        self._test_flex_attention_with_dynamic_max_autotune(device)
    test/inductor/test_flex_attention.py:6711:        out_flex = flex_attention(q_flex, k_flex, v_flex, score_mod=score_mod)
    test/inductor/test_flex_attention.py:6731:    TestFlexAttention, globals(), only_for=test_device, allow_xpu=True
    test/inductor/test_codecache.py:1014:    def test_flex_attention_caching(self):
    test/inductor/test_codecache.py:1015:        from torch.nn.attention.flex_attention import create_block_mask, flex_attention
    test/inductor/test_codecache.py:1025:            return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)
    test/inductor/test_codecache.py:1031:            return flex_attention(q, k, v, score_mod=score_mod2, block_mask=block_mask)
    test/functorch/test_aotdispatch.py:66:from torch.nn.attention.flex_attention import flex_attention
    test/functorch/test_aotdispatch.py:7204:                y = flex_attention(query=q, key=k, value=v)
    test/dynamo/test_higher_order_ops.py:7202:    "flex_attention",  # LoweringException: AssertionError
    test/dynamo/test_higher_order_ops.py:7203:    "flex_attention_backward",  # AssertionError: Input shapes should have M >= 16, N >= 16 and K >= 16
    test/inductor/test_flex_decoding.py:16:from torch.nn.attention.flex_attention import (
    test/inductor/test_flex_decoding.py:21:    flex_attention,
    test/inductor/test_flex_decoding.py:28:    flex_attention_supported_platform as supported_platform,
    test/inductor/test_flex_decoding.py:107:        flex_attention,
    test/inductor/test_flex_decoding.py:601:        compiled_sdpa = torch.compile(flex_attention)
    test/inductor/test_flex_decoding.py:777:            return flex_attention(q, k, v, score_mod)
    test/inductor/test_flex_decoding.py:1196:        attention = functools.partial(flex_attention, score_mod=score_mod_func)
    test/inductor/test_flex_decoding.py:1318:            q2 = flex_attention(q, k1, v1, score_mod=scoremod_1)
    test/inductor/test_flex_decoding.py:1319:            return flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_decoding.py:1344:        attention1 = functools.partial(flex_attention, score_mod=scoremod_1)
    test/inductor/test_flex_decoding.py:1348:            q3 = flex_attention(q2, k2, v2, score_mod=scoremod_2)
    test/inductor/test_flex_decoding.py:1349:            return flex_attention(q3, k3, v3, score_mod=scoremod_1)
    test/inductor/test_flex_decoding.py:1376:            q2 = flex_attention(q, k1, v1, score_mod=scoremod_1, block_mask=block_mask)
    test/inductor/test_flex_decoding.py:1377:            return flex_attention(
    test/inductor/test_flex_decoding.py:1413:            q2 = flex_attention(
    test/inductor/test_flex_decoding.py:1420:            return flex_attention(
    test/inductor/test_flex_decoding.py:1457:            flex_attention, score_mod=scoremod_1, block_mask=block_mask
    test/inductor/test_flex_decoding.py:1462:            q3 = flex_attention(q2, k2, v2, score_mod=scoremod_2, block_mask=block_mask)
    test/inductor/test_flex_decoding.py:1463:            return flex_attention(
    test/inductor/test_flex_decoding.py:1513:            flex_attention,
    test/inductor/test_flex_decoding.py:1520:            q3 = flex_attention(
    test/inductor/test_flex_decoding.py:1527:            return flex_attention(
    test/inductor/test_flex_decoding.py:1574:            flex_attention(query, key, value, _identity)
    test/inductor/test_flex_decoding.py:1645:        flex = torch.compile(flex_attention, dynamic=False)
    test/inductor/test_flex_decoding.py:1674:        attention = functools.partial(flex_attention, score_mod=score_mod)
    test/inductor/test_flex_decoding.py:1695:            flex_attention, block_mask=block_mask, score_mod=score_mod
    test/inductor/test_flex_decoding.py:1713:        attention = functools.partial(flex_attention, block_mask=block_mask)
    test/inductor/test_flex_decoding.py:1779:            return flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_decoding.py:1783:            return flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_decoding.py:1817:        flex_compiled = torch.compile(flex_attention)
    test/inductor/test_flex_decoding.py:1842:            _, lse = flex_attention(q, k, v, score_mod, return_lse=True)
    test/inductor/test_flex_decoding.py:1906:            compiled_flex_attention = torch.compile(flex_attention)
    test/inductor/test_flex_decoding.py:1907:            ref = flex_attention(q, k, v)
    test/inductor/test_flex_decoding.py:1908:            res = compiled_flex_attention(q, k, v)
    test/inductor/test_flex_decoding.py:1935:        flex_attention_compiled = torch.compile(flex_attention, dynamic=False)
    test/inductor/test_flex_decoding.py:1942:        eager = flex_attention(q, k, v, block_mask=mask_2)
    test/inductor/test_flex_decoding.py:1943:        out = flex_attention_compiled(q, k, v, block_mask=mask_2)
    test/inductor/test_flex_decoding.py:2011:            ref_out = flex_attention(
    test/inductor/test_flex_decoding.py:2014:            golden_out = flex_attention(
    test/inductor/test_compiled_autograd.py:34:from torch.nn.attention.flex_attention import flex_attention
    test/inductor/test_compiled_autograd.py:3752:    def test_flex_attention(self):
    test/inductor/test_compiled_autograd.py:3760:                flex_attention(x, x, x, score_mod=_squared).sum().backward()
    torch/_inductor/template_heuristics/triton.py:74:# FlexAttention Configs
    test/test_fake_tensor.py:381:        from torch.nn.attention.flex_attention import _identity, flex_attention
    test/test_fake_tensor.py:385:                flex_attention,
    test/distributed/test_dynamo_distributed.py:35:from torch.nn.attention.flex_attention import flex_attention
    test/distributed/test_dynamo_distributed.py:1457:    @skipIfXpu  # XPU device doesn't support flex_attention yet.
    test/distributed/test_dynamo_distributed.py:1459:    def test_compiled_flex_attention_full_model_ddp(self):
    test/distributed/test_dynamo_distributed.py:1470:                self.attention = flex_attention
    test/distributed/test_dynamo_distributed.py:1513:    @skipIfXpu  # XPU device doesn't support flex_attention yet.
    test/distributed/test_dynamo_distributed.py:1515:    def test_compiled_flex_attention_local_ddp(self):
    test/distributed/test_dynamo_distributed.py:1526:                self.attention = torch.compile(flex_attention)
    test/distributed/tensor/test_attention.py:29:from torch.nn.attention.flex_attention import (
    test/distributed/tensor/test_attention.py:32:    flex_attention,
    test/distributed/tensor/test_attention.py:450:# Compile the flex_attention function
    test/distributed/tensor/test_attention.py:451:compiled_flex_attention = torch.compile(flex_attention, dynamic=False, fullgraph=True)
    test/distributed/tensor/test_attention.py:534:class RingFlexAttentionTest(DTensorTestBase):
    test/distributed/tensor/test_attention.py:539:    def _test_ring_flex_attention(
    test/distributed/tensor/test_attention.py:577:        expect_out, expect_lse = compiled_flex_attention(
    test/distributed/tensor/test_attention.py:601:        # set CP context dispatch mode to use TORCH_FUNCTION for flex_attention
    test/distributed/tensor/test_attention.py:638:            cp_out, cp_lse = compiled_flex_attention(
    test/distributed/tensor/test_attention.py:684:    def test_ring_flex_attention(self) -> None:
    test/distributed/tensor/test_attention.py:687:            self._test_ring_flex_attention,
    test/distributed/tensor/test_attention.py:694:                self._test_ring_flex_attention,
    test/distributed/tensor/test_attention.py:703:    def test_ring_flex_attention_document_mask(self) -> None:
    test/distributed/tensor/test_attention.py:707:        # compilations: 1 for single-rank flex_attention and 1 for CP flex_attention.
    test/distributed/tensor/test_attention.py:724:        # for each subtest. This can be baked into self._test_ring_flex_attention as
    test/distributed/tensor/test_attention.py:738:                self._test_ring_flex_attention,
    torch/_inductor/fx_passes/b2b_gemm.py:462:    This function is adapted from ../kernel/flex_attention.py.
    torch/_inductor/select_algorithm.py:329:        This is used by flex_attention's backwards grad for captured buffers, see
    torch/_higher_order_ops/__init__.py:17:from torch._higher_order_ops.flex_attention import (
    torch/_higher_order_ops/__init__.py:18:    flex_attention,
    torch/_higher_order_ops/__init__.py:19:    flex_attention_backward,
    torch/_higher_order_ops/__init__.py:46:    "flex_attention",
    torch/_higher_order_ops/__init__.py:47:    "flex_attention_backward",
    torch/_higher_order_ops/flex_attention.py:34:# Duplicate of _inductor/kernel/flex_attention.py to avoid circular import
    torch/_higher_order_ops/flex_attention.py:80:class FlexAttentionHOP(HigherOrderOperator):
    torch/_higher_order_ops/flex_attention.py:82:        super().__init__("flex_attention", cacheable=True)
    torch/_higher_order_ops/flex_attention.py:110:flex_attention = FlexAttentionHOP()
    torch/_higher_order_ops/flex_attention.py:113:class FlexAttentionBackwardHOP(HigherOrderOperator):
    torch/_higher_order_ops/flex_attention.py:115:        super().__init__("flex_attention_backward")
    torch/_higher_order_ops/flex_attention.py:156:flex_attention_backward = FlexAttentionBackwardHOP()
    torch/_higher_order_ops/flex_attention.py:182:    from torch.nn.attention.flex_attention import _vmap_for_bhqkv
    torch/_higher_order_ops/flex_attention.py:270:@flex_attention.py_impl(DispatchKey.CompositeExplicitAutograd)
    torch/_higher_order_ops/flex_attention.py:297:def trace_flex_attention(
    torch/_higher_order_ops/flex_attention.py:309:    """Traces the flex_attention operator with the given score_mod function and other_buffers.
    torch/_higher_order_ops/flex_attention.py:317:    example_out = flex_attention(
    torch/_higher_order_ops/flex_attention.py:359:        "call_function", flex_attention, proxy_args, {}
    torch/_higher_order_ops/flex_attention.py:366:@flex_attention.py_impl(ProxyTorchDispatchMode)
    torch/_higher_order_ops/flex_attention.py:367:def flex_attention_proxy_torch_dispatch_mode(
    torch/_higher_order_ops/flex_attention.py:380:    return trace_flex_attention(
    torch/_higher_order_ops/flex_attention.py:394:@flex_attention.py_functionalize_impl
    torch/_higher_order_ops/flex_attention.py:395:def flex_attention_functionalize(
    torch/_higher_order_ops/flex_attention.py:407:    """Defines the functionalization rules for the flex_attention operator.
    torch/_higher_order_ops/flex_attention.py:466:        out = flex_attention(
    torch/_higher_order_ops/flex_attention.py:480:@register_fake(flex_attention)
    torch/_higher_order_ops/flex_attention.py:481:def flex_attention_fake_impl(
    torch/_higher_order_ops/flex_attention.py:526:redirect_to_mode(flex_attention, _CachingTorchDispatchMode)
    torch/_higher_order_ops/flex_attention.py:527:redirect_to_mode(flex_attention, _CachedTorchDispatchMode)
    torch/_higher_order_ops/flex_attention.py:629:class FlexAttentionAutogradOp(torch.autograd.Function):
    torch/_higher_order_ops/flex_attention.py:659:            out, logsumexp, max_scores = flex_attention(
    torch/_higher_order_ops/flex_attention.py:736:        ) = flex_attention_backward(
    torch/_higher_order_ops/flex_attention.py:770:@flex_attention.py_impl(DispatchKey.Autograd)
    torch/_higher_order_ops/flex_attention.py:771:def flex_attention_autograd(
    torch/_higher_order_ops/flex_attention.py:807:        out, logsumexp, max_scores = FlexAttentionAutogradOp.apply(
    torch/_higher_order_ops/flex_attention.py:825:@flex_attention_backward.py_impl(DispatchKey.CompositeExplicitAutograd)
    torch/_higher_order_ops/flex_attention.py:922:    from torch.nn.attention.flex_attention import _vmap_for_bhqkv
    torch/_higher_order_ops/flex_attention.py:990:def trace_flex_attention_backward(
    torch/_higher_order_ops/flex_attention.py:1012:    example_out = flex_attention_backward(
    torch/_higher_order_ops/flex_attention.py:1076:        flex_attention_backward,
    torch/_higher_order_ops/flex_attention.py:1079:        name="flex_attention_backward",
    torch/_higher_order_ops/flex_attention.py:1086:@flex_attention_backward.py_impl(ProxyTorchDispatchMode)
    torch/_higher_order_ops/flex_attention.py:1087:def flex_attention_backward_proxy_torch_dispatch_mode(
    torch/_higher_order_ops/flex_attention.py:1107:    return trace_flex_attention_backward(
    torch/_higher_order_ops/flex_attention.py:1126:@flex_attention_backward.py_functionalize_impl
    torch/_higher_order_ops/flex_attention.py:1127:def flex_attention_backward_functionalize(
    torch/_higher_order_ops/flex_attention.py:1146:    """Defines the functionalization rules for the flex_attention operator.
    torch/_higher_order_ops/flex_attention.py:1203:        ) = flex_attention_backward(
    torch/_higher_order_ops/flex_attention.py:1223:@register_fake(flex_attention_backward)
    torch/_higher_order_ops/flex_attention.py:1224:def flex_attention_backward_fake_tensor_mode(
    torch/_higher_order_ops/flex_attention.py:1292:flex_attention_backward.py_autograd_impl(
    torch/_higher_order_ops/flex_attention.py:1293:    autograd_not_implemented(flex_attention_backward, deferred_error=True)
    torch/_dynamo/trace_rules.py:3367:    "torch.nn.attention.flex_attention",
    torch/_dynamo/variables/higher_order_ops.py:130:# For example, in FlexAttentionHigherOrderVariable, we want to create several scalars
    torch/_dynamo/variables/higher_order_ops.py:2719:class FlexAttentionBackwardHighOrderVariable(TorchHigherOrderOperatorVariable):
    torch/_dynamo/variables/higher_order_ops.py:2747:                "Missing Dynamo support for FlexAttentionBackward HOP argument. Please file an issue."
    torch/_dynamo/variables/higher_order_ops.py:2780:class FlexAttentionHigherOrderVariable(TorchHigherOrderOperatorVariable):
    torch/_dynamo/variables/higher_order_ops.py:2883:            mask_fn = UserFunctionVariable(torch.nn.attention._flex_attention._no_mask)
    torch/_dynamo/variables/higher_order_ops.py:3489:    "flex_attention": FlexAttentionHigherOrderVariable,
    torch/_dynamo/variables/higher_order_ops.py:3490:    "flex_attention_backward": FlexAttentionBackwardHighOrderVariable,
    torch/_dynamo/variables/builder.py:3643:        elif isinstance(value, type(torch._higher_order_ops.flex_attention_backward)):
    torch/_dynamo/variables/builder.py:3644:            return torch._dynamo.variables.higher_order_ops.FlexAttentionBackwardHighOrderVariable(
    torch/_inductor/codegen/cpp_flex_attention_template.py:687:class CppFlexAttentionTemplate(CppTemplate):
    torch/_inductor/codegen/cpp_flex_attention_template.py:706:        super().__init__("flex_attention", input_nodes, layout, parallel_num_threads())
    torch/_inductor/codegen/cpp_flex_attention_template.py:939:            CppFlexAttentionTemplate,
    torch/_inductor/codegen/cpp_flex_attention_template.py:972:                "Unsupported for `epilogue_nodes` in CppFlexAttentionTemplate."
    torch/_inductor/kernel/flex/__init__.py:3:from . import flex_attention, flex_decoding
    torch/_inductor/kernel/flex/flex_decoding.py:2:"""Triton Implementation of the flex_attention Kernel for short query length (FlexDecoding)"""
    torch/_inductor/kernel/flex/flex_decoding.py:39:       use the main flex_attention kernel.
    torch/_inductor/kernel/flex/flex_decoding.py:51:        # kv num blocks for grouped query heads. We just fall back to main flex_attention
    torch/_inductor/kernel/flex/flex_cpu.py:17:from ...codegen.cpp_flex_attention_template import CppFlexAttentionTemplate
    torch/_inductor/kernel/flex/flex_cpu.py:86:    # the modification function of the CppFlexAttentionTemplate class.
    torch/_inductor/kernel/flex/flex_cpu.py:243:            "`torch.float` , `torch.float16` and `torch.bfloat16` are supported in FlexAttention for CPU device. "
    torch/_inductor/kernel/flex/flex_cpu.py:302:    CppFlexAttentionTemplate.add_choices(
    torch/_inductor/kernel/flex/flex_cpu.py:325:        "flex_attention",
    torch/_inductor/kernel/flex/flex_attention.py:2:"""Triton Implementation of the flex_attention Kernel"""
    torch/_inductor/kernel/flex/flex_attention.py:45:def flex_attention_grid(batch_size, q_heads, num_queries, d_model, meta, *, cdiv):
    torch/_inductor/kernel/flex/flex_attention.py:69:flex_attention_template = TritonTemplate(
    torch/_inductor/kernel/flex/flex_attention.py:70:    name="flex_attention",
    torch/_inductor/kernel/flex/flex_attention.py:71:    grid=flex_attention_grid,
    torch/_inductor/kernel/flex/flex_attention.py:72:    source=load_template("flex_attention")
    torch/_inductor/kernel/flex/flex_attention.py:78:@register_lowering(torch.ops.higher_order.flex_attention, type_promotion_kind=None)
    torch/_inductor/kernel/flex/flex_attention.py:79:def flex_attention(
    torch/_inductor/kernel/flex/flex_attention.py:90:    """The main lowering for the flex_attention hop
    torch/_inductor/kernel/flex/flex_attention.py:283:    configs = V.choices.get_flex_attention_fwd_configs(
    torch/_inductor/kernel/flex/flex_attention.py:348:        error = flex_attention_template.maybe_append_choice(
    torch/_inductor/kernel/flex/flex_attention.py:398:        "flex_attention",
    torch/_inductor/kernel/flex/flex_attention.py:422:def flex_attention_backward_grid(
    torch/_inductor/kernel/flex/flex_attention.py:439:flex_attention_backward_template = TritonTemplate(
    torch/_inductor/kernel/flex/flex_attention.py:440:    name="flex_attention_backward",
    torch/_inductor/kernel/flex/flex_attention.py:441:    grid=flex_attention_backward_grid,
    torch/_inductor/kernel/flex/flex_attention.py:524:    torch.ops.higher_order.flex_attention_backward, type_promotion_kind=None
    torch/_inductor/kernel/flex/flex_attention.py:526:def flex_attention_backward(*args, **kwargs):
    torch/_inductor/kernel/flex/flex_attention.py:527:    """Lowering for the flex_attention_backward op in triton"""
    torch/_inductor/kernel/flex/flex_attention.py:726:    configs = V.choices.get_flex_attention_bwd_configs(
    torch/_inductor/kernel/flex/flex_attention.py:776:        flex_attention_backward_template.maybe_append_choice(
    torch/_inductor/kernel/flex/flex_attention.py:846:        "flex_attention_backward",
    torch/nested/_internal/ops.py:2645:from torch._higher_order_ops.flex_attention import (
    torch/nested/_internal/ops.py:2646:    flex_attention as flex_attention_hop,
    torch/nested/_internal/ops.py:2647:    flex_attention_backward as flex_attention_backward_hop,
    torch/nested/_internal/ops.py:2652:@flex_attention_hop.py_impl(NestedTensor)  # type: ignore[misc]
    torch/nested/_internal/ops.py:2672:            "flex_attention(): Nested tensor score_mod / mask_mod buffers are not "
    torch/nested/_internal/ops.py:2677:    output = flex_attention_hop(
    torch/nested/_internal/ops.py:2709:@flex_attention_backward_hop.py_impl(NestedTensor)  # type: ignore[misc]
    torch/nested/_internal/ops.py:2728:    output = flex_attention_backward_hop(
    torch/distributed/tensor/examples/flex_attention_cp.py:3:torchrun --standalone --nnodes=1 --nproc-per-node=4 flex_attention_cp.py
    torch/distributed/tensor/examples/flex_attention_cp.py:15:from torch.nn.attention.flex_attention import (
    torch/distributed/tensor/examples/flex_attention_cp.py:19:    flex_attention,
    torch/distributed/tensor/examples/flex_attention_cp.py:58:    # Compile the flex_attention function
    torch/distributed/tensor/examples/flex_attention_cp.py:59:    compiled_flex_attention = torch.compile(flex_attention, dynamic=False)
    torch/distributed/tensor/examples/flex_attention_cp.py:99:    out = compiled_flex_attention(q, k, v, score_mod=None, block_mask=block_mask)
    torch/distributed/tensor/examples/flex_attention_cp.py:134:    cp_out = compiled_flex_attention(
    torch/distributed/tensor/examples/flex_attention_cp.py:145:    # compare with the flex_attention output
    torch/distributed/tensor/experimental/_attention.py:26:from torch.nn.attention.flex_attention import (
    torch/distributed/tensor/experimental/_attention.py:1128:    This API creates a special BlockMask for Context Parallel FlexAttention:
    torch/distributed/tensor/experimental/_attention.py:1134:    used in flex_attention once the shape check passes.
    torch/distributed/tensor/experimental/_attention.py:1145:        :class:`BlockMask`: the block_mask to be used in flex_attention() within the
    torch/distributed/tensor/experimental/_attention.py:1152:        FlexAttention size is small.
    torch/distributed/tensor/experimental/_attention.py:1154:    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE
    torch/distributed/tensor/experimental/_attention.py:1198:    # flex_attention function checks the following shape so we need to rewrite:
    torch/distributed/tensor/experimental/_attention.py:1264:            # special handler for flex_attention
    torch/distributed/tensor/experimental/_attention.py:1265:            if func == torch._higher_order_ops.flex_attention:
    torch/distributed/tensor/experimental/_attention.py:1279:                # shape rewrite: because torch.nn.flex_attention() checks
    torch/testing/_internal/hop_db.py:8:from torch.nn.attention.flex_attention import _create_empty_block_mask, flex_attention
    torch/testing/_internal/hop_db.py:173:def sample_inputs_flex_attention(opinfo, device, dtype, requires_grad, **kwargs):
    torch/testing/_internal/hop_db.py:391:        name="flex_attention",
    torch/testing/_internal/hop_db.py:393:        op=flex_attention,
    torch/testing/_internal/hop_db.py:394:        sample_inputs_func=sample_inputs_flex_attention,
    torch/testing/_internal/hop_db.py:412:        name="flex_attention_backward",
    torch/testing/_internal/hop_db.py:414:        op=flex_attention,
    torch/testing/_internal/hop_db.py:415:        sample_inputs_func=sample_inputs_flex_attention,
    torch/testing/_internal/common_device_type.py:1984:flex_attention_supported_platform = unittest.skipUnless(
    torch/nn/attention/flex_attention.py:3:"""This module implements the user facing API for flex_attention in PyTorch."""
    torch/nn/attention/flex_attention.py:28:from torch._higher_order_ops.flex_attention import flex_attention as flex_attention_hop
    torch/nn/attention/flex_attention.py:45:# This flag only affects the internal compilation when flex_attention is called directly.
    torch/nn/attention/flex_attention.py:46:# If you have already wrapped flex_attention in torch.compile(), this flag has no effect
    torch/nn/attention/flex_attention.py:50:#   import torch.nn.attention.flex_attention as fa
    torch/nn/attention/flex_attention.py:53:#   output = fa.flex_attention(q, k, v, score_mod=my_score_mod)
    torch/nn/attention/flex_attention.py:71:    "flex_attention",
    torch/nn/attention/flex_attention.py:88:    """Options for controlling the behavior of FlexAttention kernels.
    torch/nn/attention/flex_attention.py:108:            output = flex_attention(q, k, v, kernel_options=kernel_opts)
    torch/nn/attention/flex_attention.py:111:            from torch.nn.attention.flex_attention import FlexKernelOptions
    torch/nn/attention/flex_attention.py:118:            output = flex_attention(q, k, v, kernel_options=kernel_opts)
    torch/nn/attention/flex_attention.py:126:            output = flex_attention(q, k, v, kernel_options=kernel_opts)
    torch/nn/attention/flex_attention.py:205:    """Request which auxiliary outputs to compute from flex_attention.
    torch/nn/attention/flex_attention.py:215:    """Auxiliary outputs from flex_attention operation.
    torch/nn/attention/flex_attention.py:1057:            output = flex_attention(query, key, value, block_mask=block_mask)
    torch/nn/attention/flex_attention.py:1246:            output = flex_attention(query, key, value, block_mask=block_mask)
    torch/nn/attention/flex_attention.py:1264:            output = flex_attention(query, key, value, block_mask=block_mask)
    torch/nn/attention/flex_attention.py:1357:            "FlexAttention is only supported on CUDA, CPU or HPU devices. "
    torch/nn/attention/flex_attention.py:1366:            "FlexAttention does not support mixed nested tensor / non-nested tensor inputs. "
    torch/nn/attention/flex_attention.py:1376:            "FlexAttention does not support nested tensors that are non-contiguous with holes. "
    torch/nn/attention/flex_attention.py:1440:def flex_attention(
    torch/nn/attention/flex_attention.py:1511:        `torch.nn.attention.flex_attention` is a prototype feature in PyTorch.
    torch/nn/attention/flex_attention.py:1664:        out, lse, max_scores = flex_attention_hop(
    torch/nn/attention/flex_attention.py:1679:            warning_id="flex_attention_performance",
    torch/nn/attention/flex_attention.py:1681:                "flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n\n"
    torch/nn/attention/flex_attention.py:1682:                "SOLUTION: Use torch.compile(flex_attention)(...)\n\n"
    torch/nn/attention/flex_attention.py:1684:                "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n\n"
    torch/nn/attention/flex_attention.py:1690:        raise RuntimeError("flex_attention requires dynamo support")
    torch/nn/attention/flex_attention.py:1698:    def _flex_attention_hop_wrapper(*args, **kwargs):
    torch/nn/attention/flex_attention.py:1699:        return flex_attention_hop(*args, **kwargs)
    torch/nn/attention/flex_attention.py:1713:                        flex_fn = _flex_attention_hop_wrapper
    torch/nn/attention/flex_attention.py:1716:                            _flex_attention_hop_wrapper, backend=backend, fullgraph=True
    torch/nn/attention/experimental/_paged_attention.py:3:This module implements Paged Attention on top of flex_attention.
    torch/nn/attention/experimental/_paged_attention.py:10:from torch.nn.attention.flex_attention import (
    tools/linter/adapters/docstring_linter-grandfather.json:47:  "torch/_inductor/codegen/cpp_flex_attention_template.py": {
    tools/linter/adapters/docstring_linter-grandfather.json:48:    "class CppFlexAttentionTemplate": 403,
    tools/linter/adapters/docstring_linter-grandfather.json:49:    "def CppFlexAttentionTemplate.modification()": 102
    tools/testing/target_determination/heuristics/filepath.py:29:    "aot": ["flex_attention", "autograd"],
    docs/source/nn.attention.rst:23:    flex_attention
    docs/source/nn.attention.rst:30:    nn.attention.flex_attention
    docs/source/nested.md:325:[FlexAttention](https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention).
    docs/source/nested.md:330:for usage examples of NJT with FlexAttention.
    docs/source/nn.attention.flex_attention.md:6:# torch.nn.attention.flex_attention
    docs/source/nn.attention.flex_attention.md:9:.. currentmodule:: torch.nn.attention.flex_attention
    docs/source/nn.attention.flex_attention.md:12:.. py:module:: torch.nn.attention.flex_attention
    docs/source/nn.attention.flex_attention.md:15:.. autofunction:: flex_attention
- [2025-09-06T15:26:37+00:00] exit=0 dur=59ms: `rg -n --no-heading -S 'torch\._higher_order_ops\.flex_attention|_higher_order_ops/flex_attention'`
  - stdout:
    codex-logs/20250906-152543-session.md:124:    test/dynamo/test_modes.py:50:        if func == torch._higher_order_ops.flex_attention:
    codex-logs/20250906-152543-session.md:251:    test/inductor/test_flex_attention.py:4241:        from torch._higher_order_ops.flex_attention import (
    codex-logs/20250906-152543-session.md:423:    torch/_higher_order_ops/__init__.py:17:from torch._higher_order_ops.flex_attention import (
    codex-logs/20250906-152543-session.md:428:    torch/_higher_order_ops/flex_attention.py:34:# Duplicate of _inductor/kernel/flex_attention.py to avoid circular import
    codex-logs/20250906-152543-session.md:429:    torch/_higher_order_ops/flex_attention.py:80:class FlexAttentionHOP(HigherOrderOperator):
    codex-logs/20250906-152543-session.md:430:    torch/_higher_order_ops/flex_attention.py:82:        super().__init__("flex_attention", cacheable=True)
    codex-logs/20250906-152543-session.md:431:    torch/_higher_order_ops/flex_attention.py:110:flex_attention = FlexAttentionHOP()
    codex-logs/20250906-152543-session.md:432:    torch/_higher_order_ops/flex_attention.py:113:class FlexAttentionBackwardHOP(HigherOrderOperator):
    codex-logs/20250906-152543-session.md:433:    torch/_higher_order_ops/flex_attention.py:115:        super().__init__("flex_attention_backward")
    codex-logs/20250906-152543-session.md:434:    torch/_higher_order_ops/flex_attention.py:156:flex_attention_backward = FlexAttentionBackwardHOP()
    codex-logs/20250906-152543-session.md:435:    torch/_higher_order_ops/flex_attention.py:182:    from torch.nn.attention.flex_attention import _vmap_for_bhqkv
    codex-logs/20250906-152543-session.md:436:    torch/_higher_order_ops/flex_attention.py:270:@flex_attention.py_impl(DispatchKey.CompositeExplicitAutograd)
    codex-logs/20250906-152543-session.md:437:    torch/_higher_order_ops/flex_attention.py:297:def trace_flex_attention(
    codex-logs/20250906-152543-session.md:438:    torch/_higher_order_ops/flex_attention.py:309:    """Traces the flex_attention operator with the given score_mod function and other_buffers.
    codex-logs/20250906-152543-session.md:439:    torch/_higher_order_ops/flex_attention.py:317:    example_out = flex_attention(
    codex-logs/20250906-152543-session.md:440:    torch/_higher_order_ops/flex_attention.py:359:        "call_function", flex_attention, proxy_args, {}
    codex-logs/20250906-152543-session.md:441:    torch/_higher_order_ops/flex_attention.py:366:@flex_attention.py_impl(ProxyTorchDispatchMode)
    codex-logs/20250906-152543-session.md:442:    torch/_higher_order_ops/flex_attention.py:367:def flex_attention_proxy_torch_dispatch_mode(
    codex-logs/20250906-152543-session.md:443:    torch/_higher_order_ops/flex_attention.py:380:    return trace_flex_attention(
    codex-logs/20250906-152543-session.md:444:    torch/_higher_order_ops/flex_attention.py:394:@flex_attention.py_functionalize_impl
    codex-logs/20250906-152543-session.md:445:    torch/_higher_order_ops/flex_attention.py:395:def flex_attention_functionalize(
    codex-logs/20250906-152543-session.md:446:    torch/_higher_order_ops/flex_attention.py:407:    """Defines the functionalization rules for the flex_attention operator.
    codex-logs/20250906-152543-session.md:447:    torch/_higher_order_ops/flex_attention.py:466:        out = flex_attention(
    codex-logs/20250906-152543-session.md:448:    torch/_higher_order_ops/flex_attention.py:480:@register_fake(flex_attention)
    codex-logs/20250906-152543-session.md:449:    torch/_higher_order_ops/flex_attention.py:481:def flex_attention_fake_impl(
    codex-logs/20250906-152543-session.md:450:    torch/_higher_order_ops/flex_attention.py:526:redirect_to_mode(flex_attention, _CachingTorchDispatchMode)
    codex-logs/20250906-152543-session.md:451:    torch/_higher_order_ops/flex_attention.py:527:redirect_to_mode(flex_attention, _CachedTorchDispatchMode)
    codex-logs/20250906-152543-session.md:452:    torch/_higher_order_ops/flex_attention.py:629:class FlexAttentionAutogradOp(torch.autograd.Function):
    codex-logs/20250906-152543-session.md:453:    torch/_higher_order_ops/flex_attention.py:659:            out, logsumexp, max_scores = flex_attention(
    codex-logs/20250906-152543-session.md:454:    torch/_higher_order_ops/flex_attention.py:736:        ) = flex_attention_backward(
    codex-logs/20250906-152543-session.md:455:    torch/_higher_order_ops/flex_attention.py:770:@flex_attention.py_impl(DispatchKey.Autograd)
    codex-logs/20250906-152543-session.md:456:    torch/_higher_order_ops/flex_attention.py:771:def flex_attention_autograd(
    codex-logs/20250906-152543-session.md:457:    torch/_higher_order_ops/flex_attention.py:807:        out, logsumexp, max_scores = FlexAttentionAutogradOp.apply(
    codex-logs/20250906-152543-session.md:458:    torch/_higher_order_ops/flex_attention.py:825:@flex_attention_backward.py_impl(DispatchKey.CompositeExplicitAutograd)
    codex-logs/20250906-152543-session.md:459:    torch/_higher_order_ops/flex_attention.py:922:    from torch.nn.attention.flex_attention import _vmap_for_bhqkv
    codex-logs/20250906-152543-session.md:460:    torch/_higher_order_ops/flex_attention.py:990:def trace_flex_attention_backward(
    codex-logs/20250906-152543-session.md:461:    torch/_higher_order_ops/flex_attention.py:1012:    example_out = flex_attention_backward(
    codex-logs/20250906-152543-session.md:462:    torch/_higher_order_ops/flex_attention.py:1076:        flex_attention_backward,
    codex-logs/20250906-152543-session.md:463:    torch/_higher_order_ops/flex_attention.py:1079:        name="flex_attention_backward",
    codex-logs/20250906-152543-session.md:464:    torch/_higher_order_ops/flex_attention.py:1086:@flex_attention_backward.py_impl(ProxyTorchDispatchMode)
    codex-logs/20250906-152543-session.md:465:    torch/_higher_order_ops/flex_attention.py:1087:def flex_attention_backward_proxy_torch_dispatch_mode(
    codex-logs/20250906-152543-session.md:466:    torch/_higher_order_ops/flex_attention.py:1107:    return trace_flex_attention_backward(
    codex-logs/20250906-152543-session.md:467:    torch/_higher_order_ops/flex_attention.py:1126:@flex_attention_backward.py_functionalize_impl
    codex-logs/20250906-152543-session.md:468:    torch/_higher_order_ops/flex_attention.py:1127:def flex_attention_backward_functionalize(
    codex-logs/20250906-152543-session.md:469:    torch/_higher_order_ops/flex_attention.py:1146:    """Defines the functionalization rules for the flex_attention operator.
    codex-logs/20250906-152543-session.md:470:    torch/_higher_order_ops/flex_attention.py:1203:        ) = flex_attention_backward(
    codex-logs/20250906-152543-session.md:471:    torch/_higher_order_ops/flex_attention.py:1223:@register_fake(flex_attention_backward)
    codex-logs/20250906-152543-session.md:472:    torch/_higher_order_ops/flex_attention.py:1224:def flex_attention_backward_fake_tensor_mode(
    codex-logs/20250906-152543-session.md:473:    torch/_higher_order_ops/flex_attention.py:1292:flex_attention_backward.py_autograd_impl(
    codex-logs/20250906-152543-session.md:474:    torch/_higher_order_ops/flex_attention.py:1293:    autograd_not_implemented(flex_attention_backward, deferred_error=True)
    codex-logs/20250906-152543-session.md:483:    torch/_dynamo/variables/builder.py:3643:        elif isinstance(value, type(torch._higher_order_ops.flex_attention_backward)):
    codex-logs/20250906-152543-session.md:520:    torch/nested/_internal/ops.py:2645:from torch._higher_order_ops.flex_attention import (
    codex-logs/20250906-152543-session.md:544:    torch/distributed/tensor/experimental/_attention.py:1265:            if func == torch._higher_order_ops.flex_attention:
    codex-logs/20250906-152543-session.md:556:    torch/nn/attention/flex_attention.py:28:from torch._higher_order_ops.flex_attention import flex_attention as flex_attention_hop
    torch/distributed/tensor/experimental/_attention.py:1265:            if func == torch._higher_order_ops.flex_attention:
    torch/_higher_order_ops/__init__.py:17:from torch._higher_order_ops.flex_attention import (
    torch/nested/_internal/ops.py:2645:from torch._higher_order_ops.flex_attention import (
    torch/_dynamo/variables/builder.py:3643:        elif isinstance(value, type(torch._higher_order_ops.flex_attention_backward)):
    torch/nn/attention/flex_attention.py:28:from torch._higher_order_ops.flex_attention import flex_attention as flex_attention_hop
    test/dynamo/test_modes.py:50:        if func == torch._higher_order_ops.flex_attention:
    test/inductor/test_flex_attention.py:4241:        from torch._higher_order_ops.flex_attention import (
- [2025-09-06T15:26:54+00:00] exit=0 dur=25ms: `rg -n --no-heading -S '_inductor/.+flex_attention' torch/_inductor | sed -n '1,200p'`
- [2025-09-06T15:26:59+00:00] exit=0 dur=19ms: `find torch/_inductor -type f -iname '*flex*att*' -maxdepth 5 | sort`
  - stdout:
    torch/_inductor/codegen/cpp_flex_attention_template.py
    torch/_inductor/kernel/flex/flex_attention.py
    torch/_inductor/kernel/flex/templates/flex_attention.py.jinja
- [2025-09-06T15:27:11+00:00] exit=0 dur=19ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '1,130p'`
  - stdout:
         1	import math
         2	from collections.abc import Sequence
         3	from typing import Any, Callable, Optional, Union
         4	
         5	import torch
         6	import torch.utils._pytree as pytree
         7	from torch import Tensor
         8	from torch._C import DispatchKey
         9	from torch._higher_order_ops.utils import (
        10	    _has_potential_branch_input_mutation,
        11	    _maybe_reenter_make_fx,
        12	    autograd_not_implemented,
        13	    has_user_subclass,
        14	    redirect_to_mode,
        15	    reenter_make_fx,
        16	    register_fake,
        17	    save_tensors_and_symints_for_backward,
        18	    saved_tensors_and_symints,
        19	    UnsupportedAliasMutationException,
        20	    validate_subgraph_args_types,
        21	)
        22	from torch._ops import HigherOrderOperator
        23	from torch._subclasses import FakeTensor
        24	from torch._subclasses.functional_tensor import FunctionalTensor
        25	from torch.fx.experimental.proxy_tensor import (
        26	    make_fx,
        27	    ProxyTorchDispatchMode,
        28	    track_tensor_tree,
        29	)
        30	from torch.fx.graph_module import GraphModule
        31	from torch.utils.checkpoint import _CachedTorchDispatchMode, _CachingTorchDispatchMode
        32	
        33	
        34	# Duplicate of _inductor/kernel/flex_attention.py to avoid circular import
        35	def _construct_strides(
        36	    sizes: Sequence[int],
        37	    fill_order: Sequence[int],
        38	) -> Sequence[int]:
        39	    """From a list of sizes and a fill order, construct the strides of the permuted tensor."""
        40	    # Initialize strides
        41	    assert len(sizes) == len(fill_order), (
        42	        "Length of sizes must match the length of the fill order"
        43	    )
        44	    strides = [0] * len(sizes)
        45	
        46	    # Start with stride 1 for the innermost dimension
        47	    current_stride = 1
        48	
        49	    # Iterate through the fill order populating strides
        50	    for dim in fill_order:
        51	        strides[dim] = current_stride
        52	        current_stride *= sizes[dim]
        53	
        54	    return strides
        55	
        56	
        57	def _permute_strides(out: torch.Tensor, query_strides: tuple[int, ...]) -> torch.Tensor:
        58	    """
        59	    Create a new tensor with the same data and shape as the input,
        60	    but with strides permuted based on the input tensor's stride order.
        61	
        62	    Args:
        63	        out (torch.Tensor): The output tensor of attention.
        64	        query_strides (List[int]): The stride order of the input query tensor
        65	
        66	    Returns:
        67	        torch.Tensor: A new tensor with same shape and data as the input,
        68	        but with strides permuted based on the query tensor's stride order.
        69	    """
        70	    from torch._inductor.ir import get_fill_order
        71	
        72	    fill_order = get_fill_order(query_strides)
        73	    assert out.storage_offset() == 0, "Only support storage_offset == 0"
        74	    out_strides = _construct_strides(out.shape, fill_order)
        75	    new_out = out.new_empty(out.shape).as_strided(out.shape, out_strides)
        76	    new_out.copy_(out)
        77	    return new_out
        78	
        79	
        80	class FlexAttentionHOP(HigherOrderOperator):
        81	    def __init__(self) -> None:
        82	        super().__init__("flex_attention", cacheable=True)
        83	
        84	    def __call__(
        85	        self,
        86	        query: torch.Tensor,
        87	        key: torch.Tensor,
        88	        value: torch.Tensor,
        89	        score_mod: Callable,
        90	        block_mask: tuple,
        91	        scale: float,
        92	        kernel_options: dict[str, Any],
        93	        score_mod_other_buffers: tuple = (),
        94	        mask_mod_other_buffers: tuple = (),
        95	    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        96	        validate_subgraph_args_types(score_mod_other_buffers + mask_mod_other_buffers)
        97	        return super().__call__(
        98	            query,
        99	            key,
       100	            value,
       101	            score_mod,
       102	            block_mask,
       103	            scale,
       104	            kernel_options,
       105	            score_mod_other_buffers,
       106	            mask_mod_other_buffers,
       107	        )
       108	
       109	
       110	flex_attention = FlexAttentionHOP()
       111	
       112	
       113	class FlexAttentionBackwardHOP(HigherOrderOperator):
       114	    def __init__(self) -> None:
       115	        super().__init__("flex_attention_backward")
       116	
       117	    def __call__(
       118	        self,
       119	        query: torch.Tensor,
       120	        key: torch.Tensor,
       121	        value: torch.Tensor,
       122	        out: torch.Tensor,
       123	        logsumexp: torch.Tensor,
       124	        grad_out: torch.Tensor,
       125	        grad_logsumexp: torch.Tensor,
       126	        fw_graph: Union[Callable, GraphModule],
       127	        joint_graph: GraphModule,
       128	        block_mask: tuple,
       129	        scale: float,
       130	        kernel_options: dict[str, Any],
- [2025-09-06T15:27:11+00:00] exit=0 dur=18ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '250,430p'`
  - stdout:
       250	    # Set fully masked rows' sumexp to 0.0
       251	    logsumexp = post_mod_scores.logsumexp(dim=-1)
       252	    masked_rows = torch.all(post_mod_scores == -float("inf"), dim=-1)
       253	    logsumexp = torch.where(masked_rows, -float("inf"), logsumexp)
       254	
       255	    # working precision will be used so no need to cast to fp32
       256	    max_scores = torch.max(post_mod_scores, dim=-1)[0]
       257	
       258	    post_mod_scores = torch._safe_softmax(post_mod_scores, dim=-1)
       259	
       260	    # NB: kernel computes in ln2 space, we always convert back at the top level op, so
       261	    # for math impl we divide by log(2) because we will multiply by log(2)
       262	
       263	    return (
       264	        post_mod_scores.to(query.dtype) @ value,
       265	        logsumexp / math.log(2),
       266	        max_scores / math.log(2),
       267	    )
       268	
       269	
       270	@flex_attention.py_impl(DispatchKey.CompositeExplicitAutograd)
       271	def sdpa_dense(
       272	    query: torch.Tensor,
       273	    key: torch.Tensor,
       274	    value: torch.Tensor,
       275	    score_mod: Callable,
       276	    block_mask: tuple,
       277	    scale: float,
       278	    kernel_options: dict[str, Any],
       279	    score_mod_other_buffers: tuple = (),
       280	    mask_mod_other_buffers: tuple = (),
       281	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       282	    out, lse, max_scores = math_attention(
       283	        query,
       284	        key,
       285	        value,
       286	        score_mod,
       287	        block_mask,
       288	        scale,
       289	        kernel_options,
       290	        score_mod_other_buffers,
       291	        mask_mod_other_buffers,
       292	    )
       293	    out = _permute_strides(out, query.stride())
       294	    return out, lse, max_scores
       295	
       296	
       297	def trace_flex_attention(
       298	    proxy_mode: ProxyTorchDispatchMode,
       299	    query: torch.Tensor,
       300	    key: torch.Tensor,
       301	    value: torch.Tensor,
       302	    score_mod: Callable,
       303	    block_mask: tuple,
       304	    scale: float,
       305	    kernel_options: dict[str, Any],
       306	    score_mod_other_buffers: tuple = (),
       307	    mask_mod_other_buffers: tuple = (),
       308	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       309	    """Traces the flex_attention operator with the given score_mod function and other_buffers.
       310	
       311	    Trace SDPA will call make_fx with "fake" example vals and then trace the score_mod function
       312	    This will produce a GraphModule that will be stored on the root tracer as "sdpa_score". We
       313	    access this graph module in inductor to inline the score_mod function to the triton template.
       314	    """
       315	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       316	
       317	    example_out = flex_attention(
       318	        query,
       319	        key,
       320	        value,
       321	        score_mod,
       322	        block_mask,
       323	        scale,
       324	        kernel_options,
       325	        score_mod_other_buffers,
       326	        mask_mod_other_buffers,
       327	    )
       328	    example_vals = [query.new_zeros((), requires_grad=query.requires_grad)] + [
       329	        query.new_zeros((), dtype=torch.int) for _ in range(4)
       330	    ]
       331	    mask_example_vals = [query.new_zeros((), dtype=torch.int) for _ in range(4)]
       332	    mask_mod = block_mask[-1]
       333	    with TransformGetItemToIndex():
       334	        score_graph = reenter_make_fx(score_mod)(
       335	            *example_vals, *score_mod_other_buffers
       336	        )
       337	        mask_graph = reenter_make_fx(mask_mod)(
       338	            *mask_example_vals, *mask_mod_other_buffers
       339	        )
       340	    assert isinstance(proxy_mode.tracer, torch.fx.Tracer)
       341	    block_mask = block_mask[:-1] + (mask_graph,)
       342	    qualname = proxy_mode.tracer.get_fresh_qualname("sdpa_score")
       343	    proxy_mode.tracer.root.register_module(qualname, score_graph)
       344	    mask_qualname = proxy_mode.tracer.get_fresh_qualname("sdpa_mask")
       345	    proxy_mode.tracer.root.register_module(mask_qualname, mask_graph)
       346	    node_args = (
       347	        query,
       348	        key,
       349	        value,
       350	        score_graph,
       351	        block_mask,
       352	        scale,
       353	        kernel_options,
       354	        score_mod_other_buffers,
       355	        mask_mod_other_buffers,
       356	    )
       357	    proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, node_args)
       358	    out_proxy = proxy_mode.tracer.create_proxy(
       359	        "call_function", flex_attention, proxy_args, {}
       360	    )
       361	    return track_tensor_tree(
       362	        example_out, out_proxy, constant=None, tracer=proxy_mode.tracer
       363	    )
       364	
       365	
       366	@flex_attention.py_impl(ProxyTorchDispatchMode)
       367	def flex_attention_proxy_torch_dispatch_mode(
       368	    mode: ProxyTorchDispatchMode,
       369	    query: torch.Tensor,
       370	    key: torch.Tensor,
       371	    value: torch.Tensor,
       372	    score_mod: Callable,
       373	    block_mask: tuple,
       374	    scale: float,
       375	    kernel_options: dict[str, Any],
       376	    score_mod_other_buffers: tuple = (),
       377	    mask_mod_other_buffers: tuple = (),
       378	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       379	    assert mode is not None, "Mode should always be enabled for python fallback key"
       380	    return trace_flex_attention(
       381	        mode,
       382	        query,
       383	        key,
       384	        value,
       385	        score_mod,
       386	        block_mask,
       387	        scale,
       388	        kernel_options,
       389	        score_mod_other_buffers,
       390	        mask_mod_other_buffers,
       391	    )
       392	
       393	
       394	@flex_attention.py_functionalize_impl
       395	def flex_attention_functionalize(
       396	    ctx: torch._subclasses.functional_tensor.BaseFunctionalizeAPI,
       397	    query: torch.Tensor,
       398	    key: torch.Tensor,
       399	    value: torch.Tensor,
       400	    score_mod: Callable,
       401	    block_mask: tuple,
       402	    scale: float,
       403	    kernel_options: dict[str, Any],
       404	    score_mod_other_buffers: tuple = (),
       405	    mask_mod_other_buffers: tuple = (),
       406	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       407	    """Defines the functionalization rules for the flex_attention operator.
       408	
       409	    Write now we are unwrapping each tensor and then redispatching to the next, however we want to
       410	    guard against any mutations in the score_mod function, to the other_buffers since those
       411	    are free variables.
       412	    """
       413	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       414	
       415	    if has_user_subclass(
       416	        (
       417	            query,
       418	            key,
       419	            value,
       420	            score_mod,
       421	            block_mask,
       422	            scale,
       423	            kernel_options,
       424	            score_mod_other_buffers,
       425	            mask_mod_other_buffers,
       426	        ),
       427	        allowed_subclasses=(FakeTensor, FunctionalTensor),
       428	    ):
       429	        return NotImplemented
       430	
- [2025-09-06T15:27:12+00:00] exit=0 dur=17ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '470,560p'`
  - stdout:
       470	            functional_score_mod,
       471	            block_mask_unwrapped,
       472	            scale,
       473	            kernel_options,
       474	            score_mod_other_buffers_unwrapped,
       475	            mask_mod_other_buffers_unwrapped,
       476	        )
       477	    return ctx.wrap_tensors(out)  # type: ignore[return-value, arg-type]
       478	
       479	
       480	@register_fake(flex_attention)
       481	def flex_attention_fake_impl(
       482	    query: torch.Tensor,
       483	    key: torch.Tensor,
       484	    value: torch.Tensor,
       485	    score_mod: Callable,
       486	    block_mask: tuple,
       487	    scale: float,
       488	    kernel_options: dict[str, Any],
       489	    score_mod_other_buffers: tuple = (),
       490	    mask_mod_other_buffers: tuple = (),
       491	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       492	    if has_user_subclass(
       493	        (
       494	            query,
       495	            key,
       496	            value,
       497	            score_mod,
       498	            block_mask,
       499	            scale,
       500	            kernel_options,
       501	            score_mod_other_buffers,
       502	            mask_mod_other_buffers,
       503	        ),
       504	        allowed_subclasses=(FakeTensor,),
       505	    ):
       506	        return NotImplemented
       507	
       508	    # TODO: Figure out a better way to handle this for NJT than using sum()
       509	    if query.is_nested:
       510	        out = torch.empty_like(query, memory_format=torch.contiguous_format)
       511	        logsumexp = query.sum(dim=-1)
       512	        max_scores = query.max(dim=-1)[0]
       513	        return out, logsumexp, max_scores
       514	
       515	    v_head_dim = value.size(-1)
       516	    batch_size, num_heads, seq_len_q, _q_head_dim = query.shape
       517	    logsumexp = query.new_empty(batch_size, num_heads, seq_len_q, dtype=torch.float32)
       518	    max_scores = query.new_empty(batch_size, num_heads, seq_len_q, dtype=torch.float32)
       519	    out_shape = (batch_size, num_heads, seq_len_q, v_head_dim)
       520	    out = query.new_empty(out_shape)
       521	    out = _permute_strides(out, query.stride())
       522	    return out, logsumexp, max_scores
       523	
       524	
       525	# Registers dispatches for SAC
       526	redirect_to_mode(flex_attention, _CachingTorchDispatchMode)
       527	redirect_to_mode(flex_attention, _CachedTorchDispatchMode)
       528	
       529	
       530	# ---------------------------- Autograd Implementation ----------------------------
       531	def create_fw_bw_graph(
       532	    score_mod: Callable,
       533	    index_values: tuple[Tensor, Tensor, Tensor, Tensor, Tensor],
       534	    other_buffers: tuple[Tensor, ...],
       535	) -> tuple[Callable, Callable]:
       536	    # See Note:[HOP create fw_bw graph]
       537	
       538	    # All of these imports need to be here in order to avoid circular dependencies
       539	    from torch._dispatch.python import suspend_functionalization
       540	    from torch._functorch.aot_autograd import AOTConfig, create_joint
       541	    from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode
       542	    from torch._subclasses.functional_tensor import disable_functional_mode
       543	    from torch.fx.experimental.proxy_tensor import disable_proxy_modes_tracing
       544	
       545	    dummy_aot_config = AOTConfig(
       546	        fw_compiler=None,  # type: ignore[arg-type]
       547	        bw_compiler=None,  # type: ignore[arg-type]
       548	        partition_fn=None,  # type: ignore[arg-type]
       549	        decompositions={},
       550	        num_params_buffers=0,
       551	        aot_id=0,
       552	        keep_inference_input_mutations=False,
       553	    )
       554	
       555	    with suspend_functionalization(), disable_functional_mode():
       556	        with disable_proxy_modes_tracing():
       557	
       558	            def _from_fun(
       559	                t: Union[Tensor, torch.SymInt, int],
       560	            ) -> Union[Tensor, torch.SymInt, int]:
- [2025-09-06T15:27:12+00:00] exit=0 dur=17ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '620,860p'`
  - stdout:
       620	
       621	            return grads
       622	
       623	        joint_graph = make_fx(joint_f)(
       624	            *unwrapped_score_mod_indexes, example_grad, *unwrapped_other_buffers
       625	        )
       626	        return score_mod, joint_graph
       627	
       628	
       629	class FlexAttentionAutogradOp(torch.autograd.Function):
       630	    @staticmethod
       631	    def forward(
       632	        ctx: Any,
       633	        query: Tensor,
       634	        key: Tensor,
       635	        value: Tensor,
       636	        fw_graph: Callable,
       637	        joint_graph: Callable,
       638	        block_mask: tuple[Any, ...],
       639	        scale: float,
       640	        kernel_options: dict[str, Any],
       641	        mask_mod_other_buffers: tuple[Any, ...],
       642	        *score_mod_other_buffers: tuple[Any, ...],
       643	    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       644	        any_buffer_requires_grad = any(
       645	            buffer.requires_grad
       646	            for buffer in mask_mod_other_buffers
       647	            if isinstance(buffer, torch.Tensor)
       648	        )
       649	        assert not any_buffer_requires_grad, (
       650	            "Captured buffers from mask mod that require grad are not supported."
       651	        )
       652	        ctx._fw_graph = fw_graph
       653	        ctx._joint_graph = joint_graph
       654	        ctx._mask_graph = block_mask[-1]
       655	        ctx.scale = scale
       656	        ctx.kernel_options = kernel_options
       657	        ctx._score_mod_other_buffers_len = len(score_mod_other_buffers)
       658	        with torch._C._AutoDispatchBelowAutograd():
       659	            out, logsumexp, max_scores = flex_attention(
       660	                query,
       661	                key,
       662	                value,
       663	                fw_graph,
       664	                block_mask,
       665	                scale,
       666	                kernel_options,
       667	                score_mod_other_buffers,
       668	                mask_mod_other_buffers,
       669	            )
       670	        # no grads for you sir
       671	        ctx.mark_non_differentiable(max_scores)
       672	        save_tensors_and_symints_for_backward(
       673	            ctx,
       674	            (
       675	                query,
       676	                key,
       677	                value,
       678	                out,
       679	                logsumexp,
       680	                max_scores,
       681	                *block_mask[:-1],
       682	                *score_mod_other_buffers,
       683	                *mask_mod_other_buffers,
       684	            ),
       685	        )
       686	        return out, logsumexp, max_scores
       687	
       688	    @staticmethod
       689	    def backward(  # type: ignore[override]
       690	        ctx: Any,
       691	        grad_out: Tensor,
       692	        grad_logsumexp: Tensor,
       693	        grad_max_scores: Tensor,
       694	    ) -> tuple[Optional[Tensor], ...]:
       695	        fw_args = saved_tensors_and_symints(ctx)
       696	        (
       697	            query,
       698	            key,
       699	            value,
       700	            out,
       701	            logsumexp,
       702	            max_scores,
       703	            query_lengths,
       704	            kv_lengths,
       705	            kv_num_blocks,
       706	            kv_indices,
       707	            full_kv_num_blocks,
       708	            full_kv_indices,
       709	            q_num_blocks,
       710	            q_indices,
       711	            full_q_num_blocks,
       712	            full_q_indices,
       713	            Q_BLOCK_SIZE,
       714	            KV_BLOCK_SIZE,
       715	            *other_buffers,
       716	        ) = fw_args
       717	        fw_graph = ctx._fw_graph
       718	        joint_graph = ctx._joint_graph
       719	        mask_graph = ctx._mask_graph
       720	        scale = ctx.scale
       721	        kernel_options = ctx.kernel_options
       722	        score_mod_other_buffers = tuple(
       723	            other_buffers[: ctx._score_mod_other_buffers_len]
       724	        )
       725	        mask_mod_other_buffers = tuple(
       726	            other_buffers[ctx._score_mod_other_buffers_len :]
       727	        )
       728	        # We have asserted that mask_mod_other_buffers do not require grad,
       729	        # but score_mod_other_buffers can require grad.
       730	        none_grads = [None] * 6
       731	        (
       732	            grad_query,
       733	            grad_key,
       734	            grad_value,
       735	            grad_score_mod_captured,
       736	        ) = flex_attention_backward(
       737	            query,
       738	            key,
       739	            value,
       740	            out,
       741	            logsumexp,
       742	            grad_out,
       743	            grad_logsumexp,
       744	            fw_graph,
       745	            joint_graph,
       746	            (
       747	                query_lengths,
       748	                kv_lengths,
       749	                kv_num_blocks,
       750	                kv_indices,
       751	                full_kv_num_blocks,
       752	                full_kv_indices,
       753	                q_num_blocks,
       754	                q_indices,
       755	                full_q_num_blocks,
       756	                full_q_indices,
       757	                Q_BLOCK_SIZE,
       758	                KV_BLOCK_SIZE,
       759	                mask_graph,
       760	            ),
       761	            scale,
       762	            kernel_options,
       763	            score_mod_other_buffers,
       764	            mask_mod_other_buffers,
       765	        )
       766	        return grad_query, grad_key, grad_value, *none_grads, *grad_score_mod_captured
       767	
       768	
       769	# TODO: Rework DispatchKey.Autograd to py_autograd_impl
       770	@flex_attention.py_impl(DispatchKey.Autograd)
       771	def flex_attention_autograd(
       772	    query: torch.Tensor,
       773	    key: torch.Tensor,
       774	    value: torch.Tensor,
       775	    score_mod: Callable,
       776	    block_mask: tuple,
       777	    scale: float,
       778	    kernel_options: dict[str, Any],
       779	    score_mod_other_buffers: tuple[Tensor, ...] = (),
       780	    mask_mod_other_buffers: tuple[Tensor, ...] = (),
       781	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       782	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       783	
       784	    with TransformGetItemToIndex():
       785	        input_requires_grad = any(
       786	            isinstance(t, torch.Tensor) and t.requires_grad
       787	            for t in (query, key, value, *score_mod_other_buffers)
       788	        )
       789	        if torch.is_grad_enabled() and input_requires_grad:
       790	            if block_mask[7] is None:
       791	                raise RuntimeError(
       792	                    "BlockMask q_indices is None. Backward pass requires q_indices to be computed. "
       793	                    "Please create the BlockMask with compute_q_blocks=True"
       794	                )
       795	            example_vals = (
       796	                query.new_zeros((), requires_grad=input_requires_grad),
       797	                query.new_zeros((), dtype=torch.int),
       798	                query.new_zeros((), dtype=torch.int),
       799	                query.new_zeros((), dtype=torch.int),
       800	                query.new_zeros((), dtype=torch.int),
       801	            )
       802	            fw_graph, bw_graph = create_fw_bw_graph(
       803	                score_mod, example_vals, score_mod_other_buffers
       804	            )
       805	        else:
       806	            fw_graph, bw_graph = score_mod, None
       807	        out, logsumexp, max_scores = FlexAttentionAutogradOp.apply(
       808	            query,
       809	            key,
       810	            value,
       811	            fw_graph,
       812	            bw_graph,
       813	            block_mask,
       814	            scale,
       815	            kernel_options,
       816	            mask_mod_other_buffers,
       817	            *score_mod_other_buffers,
       818	        )
       819	    return out, logsumexp, max_scores
       820	
       821	
       822	# ---------------------------- Backward HOP Implementation ----------------------------
       823	
       824	
       825	@flex_attention_backward.py_impl(DispatchKey.CompositeExplicitAutograd)
       826	def sdpa_dense_backward(
       827	    query: torch.Tensor,
       828	    key: torch.Tensor,
       829	    value: torch.Tensor,
       830	    out: torch.Tensor,
       831	    logsumexp: torch.Tensor,
       832	    grad_out: torch.Tensor,
       833	    grad_logsumexp: torch.Tensor,
       834	    fw_graph: Callable,  # GraphModule type hint?
       835	    joint_graph: Callable,
       836	    block_mask: tuple,
       837	    scale: float,
       838	    kernel_options: dict[str, Any],
       839	    score_mod_other_buffers: tuple,
       840	    mask_mod_other_buffers: tuple,
       841	) -> tuple[
       842	    torch.Tensor, torch.Tensor, torch.Tensor, tuple[Optional[torch.Tensor], ...]
       843	]:
       844	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       845	
       846	    Bq, Hq, seq_len_q, qk_head_dim = query.shape
       847	    Bkv, Hkv, seq_len_kv, v_head_dim = value.shape
       848	
       849	    # Get outputs before calling repeat interleave and permute to input stride orders
       850	    actual_grad_query = query.new_empty((Bq, Hq, seq_len_q, qk_head_dim))
       851	    actual_grad_query = _permute_strides(actual_grad_query, query.stride())
       852	
       853	    actual_grad_key = key.new_empty((Bq, Hkv, seq_len_kv, qk_head_dim))
       854	    actual_grad_key = _permute_strides(actual_grad_key, key.stride())
       855	
       856	    actual_grad_value = value.new_empty((Bq, Hkv, seq_len_kv, v_head_dim))
       857	    actual_grad_value = _permute_strides(actual_grad_value, value.stride())
       858	
       859	    def _maybe_new_buffer(
       860	        buffer: Union[torch.Tensor, torch.SymInt, int],
- [2025-09-06T15:27:12+00:00] exit=0 dur=17ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '980,1130p'`
  - stdout:
       980	    ]
       981	
       982	    return (
       983	        actual_grad_query,
       984	        actual_grad_key,
       985	        actual_grad_value,
       986	        tuple(score_mod_other_buffer_grads),
       987	    )
       988	
       989	
       990	def trace_flex_attention_backward(
       991	    proxy_mode: ProxyTorchDispatchMode,
       992	    query: torch.Tensor,
       993	    key: torch.Tensor,
       994	    value: torch.Tensor,
       995	    out: torch.Tensor,
       996	    logsumexp: torch.Tensor,
       997	    grad_out: torch.Tensor,
       998	    grad_logsumexp: torch.Tensor,
       999	    fw_graph: Union[Callable, GraphModule],
      1000	    joint_graph: GraphModule,
      1001	    block_mask: tuple,
      1002	    scale: float,
      1003	    kernel_options: dict[str, Any],
      1004	    score_mod_other_buffers: tuple = (),
      1005	    mask_mod_other_buffers: tuple = (),
      1006	) -> tuple[
      1007	    torch.Tensor, torch.Tensor, torch.Tensor, tuple[Optional[torch.Tensor], ...]
      1008	]:
      1009	    """We already have the forward graph and joint graph from the forward pass, so we create a proxy attach both graphs"""
      1010	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
      1011	
      1012	    example_out = flex_attention_backward(
      1013	        query,
      1014	        key,
      1015	        value,
      1016	        out,
      1017	        logsumexp,
      1018	        grad_out,
      1019	        grad_logsumexp,
      1020	        fw_graph,
      1021	        joint_graph,
      1022	        block_mask,
      1023	        scale,
      1024	        kernel_options,
      1025	        score_mod_other_buffers,
      1026	        mask_mod_other_buffers,
      1027	    )
      1028	
      1029	    requires_grad = any(pytree.tree_map(lambda x: x.requires_grad, (query, key)))
      1030	    fw_example_vals = [query.new_zeros((), requires_grad=requires_grad)] + [
      1031	        query.new_zeros((), dtype=torch.int) for _ in range(4)
      1032	    ]
      1033	    bw_example_vals = fw_example_vals + [query.new_zeros(())]
      1034	    mask_example_vals = [query.new_zeros((), dtype=torch.int) for _ in range(4)]
      1035	    mask_graph = block_mask[-1]
      1036	    with TransformGetItemToIndex():
      1037	        # There's no active make_fx during the compiled autograd graph's initial capture
      1038	        fw_graph = _maybe_reenter_make_fx(fw_graph)(
      1039	            *fw_example_vals, *score_mod_other_buffers
      1040	        )
      1041	        joint_graph = _maybe_reenter_make_fx(joint_graph)(
      1042	            *bw_example_vals, *score_mod_other_buffers
      1043	        )
      1044	        mask_graph = _maybe_reenter_make_fx(mask_graph)(
      1045	            *mask_example_vals, *mask_mod_other_buffers
      1046	        )
      1047	    assert isinstance(proxy_mode.tracer, torch.fx.Tracer)
      1048	    block_mask = block_mask[:-1] + (mask_graph,)
      1049	
      1050	    qualname = proxy_mode.tracer.get_fresh_qualname("fw_graph")
      1051	    proxy_mode.tracer.root.register_module(qualname, fw_graph)  # type: ignore[arg-type]
      1052	    qualname = proxy_mode.tracer.get_fresh_qualname("joint_graph")
      1053	    proxy_mode.tracer.root.register_module(qualname, joint_graph)
      1054	    qualname = proxy_mode.tracer.get_fresh_qualname("mask_graph")
      1055	    proxy_mode.tracer.root.register_module(qualname, mask_graph)
      1056	
      1057	    node_args = (
      1058	        query,
      1059	        key,
      1060	        value,
      1061	        out,
      1062	        logsumexp,
      1063	        grad_out,
      1064	        grad_logsumexp,
      1065	        fw_graph,
      1066	        joint_graph,
      1067	        block_mask,
      1068	        scale,
      1069	        kernel_options,
      1070	        score_mod_other_buffers,
      1071	        mask_mod_other_buffers,
      1072	    )
      1073	    proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, node_args)
      1074	    out_proxy = proxy_mode.tracer.create_proxy(
      1075	        "call_function",
      1076	        flex_attention_backward,
      1077	        proxy_args,
      1078	        {},
      1079	        name="flex_attention_backward",
      1080	    )
      1081	    return track_tensor_tree(
      1082	        example_out, out_proxy, constant=None, tracer=proxy_mode.tracer
      1083	    )
      1084	
      1085	
      1086	@flex_attention_backward.py_impl(ProxyTorchDispatchMode)
      1087	def flex_attention_backward_proxy_torch_dispatch_mode(
      1088	    mode: ProxyTorchDispatchMode,
      1089	    query: torch.Tensor,
      1090	    key: torch.Tensor,
      1091	    value: torch.Tensor,
      1092	    out: torch.Tensor,
      1093	    logsumexp: torch.Tensor,
      1094	    grad_out: torch.Tensor,
      1095	    grad_logsumexp: torch.Tensor,
      1096	    fw_graph: Union[Callable, GraphModule],
      1097	    joint_graph: GraphModule,
      1098	    block_mask: tuple,
      1099	    scale: float,
      1100	    kernel_options: dict[str, Any],
      1101	    score_mod_other_buffers: tuple = (),
      1102	    mask_mod_other_buffers: tuple = (),
      1103	) -> tuple[
      1104	    torch.Tensor, torch.Tensor, torch.Tensor, tuple[Optional[torch.Tensor], ...]
      1105	]:
      1106	    assert mode is not None, "Mode should always be enabled for python fallback key"
      1107	    return trace_flex_attention_backward(
      1108	        mode,
      1109	        query,
      1110	        key,
      1111	        value,
      1112	        out,
      1113	        logsumexp,
      1114	        grad_out,
      1115	        grad_logsumexp,
      1116	        fw_graph,
      1117	        joint_graph,
      1118	        block_mask,
      1119	        scale,
      1120	        kernel_options,
      1121	        score_mod_other_buffers,
      1122	        mask_mod_other_buffers,
      1123	    )
      1124	
      1125	
      1126	@flex_attention_backward.py_functionalize_impl
      1127	def flex_attention_backward_functionalize(
      1128	    ctx: torch._subclasses.functional_tensor.BaseFunctionalizeAPI,
      1129	    query: torch.Tensor,
      1130	    key: torch.Tensor,
- [2025-09-06T15:27:12+00:00] exit=0 dur=17ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '1200,1320p'`
  - stdout:
      1200	            grad_key,
      1201	            grad_value,
      1202	            grad_score_mod_captured,
      1203	        ) = flex_attention_backward(
      1204	            query_unwrapped,
      1205	            key_unwrapped,
      1206	            value_unwrapped,
      1207	            out_unwrapped,
      1208	            logsumexp_unwrapped,
      1209	            grad_out_unwrapped,
      1210	            grad_logsumexp_unwrapped,
      1211	            functional_fw_graph,  # type: ignore[arg-type]
      1212	            functional_joint_graph,  # type: ignore[arg-type]
      1213	            block_mask_unwrapped,
      1214	            scale,
      1215	            kernel_options,
      1216	            score_mod_other_buffers_unwrapped,
      1217	            mask_mod_other_buffers_unwrapped,
      1218	        )
      1219	
      1220	    return ctx.wrap_tensors((grad_query, grad_key, grad_value, grad_score_mod_captured))  # type: ignore[return-value,arg-type]
      1221	
      1222	
      1223	@register_fake(flex_attention_backward)
      1224	def flex_attention_backward_fake_tensor_mode(
      1225	    query: torch.Tensor,
      1226	    key: torch.Tensor,
      1227	    value: torch.Tensor,
      1228	    out: torch.Tensor,
      1229	    logsumexp: torch.Tensor,
      1230	    grad_out: torch.Tensor,
      1231	    grad_logsumexp: torch.Tensor,
      1232	    fw_graph: Union[Callable, GraphModule],
      1233	    joint_graph: GraphModule,
      1234	    block_mask: tuple,
      1235	    scale: float,
      1236	    kernel_options: dict[str, Any],
      1237	    score_mod_other_buffers: tuple = (),
      1238	    mask_mod_other_buffers: tuple = (),
      1239	) -> tuple[
      1240	    torch.Tensor, torch.Tensor, torch.Tensor, tuple[Optional[torch.Tensor], ...]
      1241	]:
      1242	    if has_user_subclass(
      1243	        (
      1244	            query,
      1245	            key,
      1246	            value,
      1247	            out,
      1248	            logsumexp,
      1249	            grad_out,
      1250	            grad_logsumexp,
      1251	            block_mask,
      1252	            scale,
      1253	            kernel_options,
      1254	            score_mod_other_buffers,
      1255	            mask_mod_other_buffers,
      1256	        ),
      1257	        allowed_subclasses=(FakeTensor,),
      1258	    ):
      1259	        return NotImplemented
      1260	    Bq, _, _, qk_head_dim = query.shape
      1261	    Bkv, Hkv, seq_len_kv, v_head_dim = value.shape
      1262	
      1263	    grad_query = torch.empty_like(query)
      1264	    # zeros_and_scatter creates a contiguous zeros tensor -> contiguous_format
      1265	    grad_score_mod_captured = tuple(
      1266	        [
      1267	            (
      1268	                torch.empty_like(buffer, memory_format=torch.contiguous_format)
      1269	                if isinstance(buffer, torch.Tensor) and buffer.requires_grad
      1270	                else None
      1271	            )
      1272	            for buffer in score_mod_other_buffers
      1273	        ]
      1274	    )
      1275	
      1276	    broadcasted_grad_key = key.new_empty((Bq, Hkv, seq_len_kv, qk_head_dim))
      1277	    broadcasted_grad_key = _permute_strides(broadcasted_grad_key, key.stride())
      1278	
      1279	    broadcasted_grad_value = value.new_empty((Bq, Hkv, seq_len_kv, v_head_dim))
      1280	    broadcasted_grad_value = _permute_strides(broadcasted_grad_value, value.stride())
      1281	
      1282	    if Bq > 1 and Bkv == 1:
      1283	        grad_key = torch.sum(broadcasted_grad_key, dim=0, keepdim=True)
      1284	        grad_value = torch.sum(broadcasted_grad_value, dim=0, keepdim=True)
      1285	    else:
      1286	        grad_key = broadcasted_grad_key
      1287	        grad_value = broadcasted_grad_value
      1288	
      1289	    return grad_query, grad_key, grad_value, grad_score_mod_captured
      1290	
      1291	
      1292	flex_attention_backward.py_autograd_impl(
      1293	    autograd_not_implemented(flex_attention_backward, deferred_error=True)
      1294	)
- [2025-09-06T15:27:25+00:00] exit=0 dur=18ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '260,330p'`
  - stdout:
       260	    # NB: kernel computes in ln2 space, we always convert back at the top level op, so
       261	    # for math impl we divide by log(2) because we will multiply by log(2)
       262	
       263	    return (
       264	        post_mod_scores.to(query.dtype) @ value,
       265	        logsumexp / math.log(2),
       266	        max_scores / math.log(2),
       267	    )
       268	
       269	
       270	@flex_attention.py_impl(DispatchKey.CompositeExplicitAutograd)
       271	def sdpa_dense(
       272	    query: torch.Tensor,
       273	    key: torch.Tensor,
       274	    value: torch.Tensor,
       275	    score_mod: Callable,
       276	    block_mask: tuple,
       277	    scale: float,
       278	    kernel_options: dict[str, Any],
       279	    score_mod_other_buffers: tuple = (),
       280	    mask_mod_other_buffers: tuple = (),
       281	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       282	    out, lse, max_scores = math_attention(
       283	        query,
       284	        key,
       285	        value,
       286	        score_mod,
       287	        block_mask,
       288	        scale,
       289	        kernel_options,
       290	        score_mod_other_buffers,
       291	        mask_mod_other_buffers,
       292	    )
       293	    out = _permute_strides(out, query.stride())
       294	    return out, lse, max_scores
       295	
       296	
       297	def trace_flex_attention(
       298	    proxy_mode: ProxyTorchDispatchMode,
       299	    query: torch.Tensor,
       300	    key: torch.Tensor,
       301	    value: torch.Tensor,
       302	    score_mod: Callable,
       303	    block_mask: tuple,
       304	    scale: float,
       305	    kernel_options: dict[str, Any],
       306	    score_mod_other_buffers: tuple = (),
       307	    mask_mod_other_buffers: tuple = (),
       308	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       309	    """Traces the flex_attention operator with the given score_mod function and other_buffers.
       310	
       311	    Trace SDPA will call make_fx with "fake" example vals and then trace the score_mod function
       312	    This will produce a GraphModule that will be stored on the root tracer as "sdpa_score". We
       313	    access this graph module in inductor to inline the score_mod function to the triton template.
       314	    """
       315	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       316	
       317	    example_out = flex_attention(
       318	        query,
       319	        key,
       320	        value,
       321	        score_mod,
       322	        block_mask,
       323	        scale,
       324	        kernel_options,
       325	        score_mod_other_buffers,
       326	        mask_mod_other_buffers,
       327	    )
       328	    example_vals = [query.new_zeros((), requires_grad=query.requires_grad)] + [
       329	        query.new_zeros((), dtype=torch.int) for _ in range(4)
       330	    ]
- [2025-09-06T15:27:25+00:00] exit=0 dur=18ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '330,420p'`
  - stdout:
       330	    ]
       331	    mask_example_vals = [query.new_zeros((), dtype=torch.int) for _ in range(4)]
       332	    mask_mod = block_mask[-1]
       333	    with TransformGetItemToIndex():
       334	        score_graph = reenter_make_fx(score_mod)(
       335	            *example_vals, *score_mod_other_buffers
       336	        )
       337	        mask_graph = reenter_make_fx(mask_mod)(
       338	            *mask_example_vals, *mask_mod_other_buffers
       339	        )
       340	    assert isinstance(proxy_mode.tracer, torch.fx.Tracer)
       341	    block_mask = block_mask[:-1] + (mask_graph,)
       342	    qualname = proxy_mode.tracer.get_fresh_qualname("sdpa_score")
       343	    proxy_mode.tracer.root.register_module(qualname, score_graph)
       344	    mask_qualname = proxy_mode.tracer.get_fresh_qualname("sdpa_mask")
       345	    proxy_mode.tracer.root.register_module(mask_qualname, mask_graph)
       346	    node_args = (
       347	        query,
       348	        key,
       349	        value,
       350	        score_graph,
       351	        block_mask,
       352	        scale,
       353	        kernel_options,
       354	        score_mod_other_buffers,
       355	        mask_mod_other_buffers,
       356	    )
       357	    proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, node_args)
       358	    out_proxy = proxy_mode.tracer.create_proxy(
       359	        "call_function", flex_attention, proxy_args, {}
       360	    )
       361	    return track_tensor_tree(
       362	        example_out, out_proxy, constant=None, tracer=proxy_mode.tracer
       363	    )
       364	
       365	
       366	@flex_attention.py_impl(ProxyTorchDispatchMode)
       367	def flex_attention_proxy_torch_dispatch_mode(
       368	    mode: ProxyTorchDispatchMode,
       369	    query: torch.Tensor,
       370	    key: torch.Tensor,
       371	    value: torch.Tensor,
       372	    score_mod: Callable,
       373	    block_mask: tuple,
       374	    scale: float,
       375	    kernel_options: dict[str, Any],
       376	    score_mod_other_buffers: tuple = (),
       377	    mask_mod_other_buffers: tuple = (),
       378	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       379	    assert mode is not None, "Mode should always be enabled for python fallback key"
       380	    return trace_flex_attention(
       381	        mode,
       382	        query,
       383	        key,
       384	        value,
       385	        score_mod,
       386	        block_mask,
       387	        scale,
       388	        kernel_options,
       389	        score_mod_other_buffers,
       390	        mask_mod_other_buffers,
       391	    )
       392	
       393	
       394	@flex_attention.py_functionalize_impl
       395	def flex_attention_functionalize(
       396	    ctx: torch._subclasses.functional_tensor.BaseFunctionalizeAPI,
       397	    query: torch.Tensor,
       398	    key: torch.Tensor,
       399	    value: torch.Tensor,
       400	    score_mod: Callable,
       401	    block_mask: tuple,
       402	    scale: float,
       403	    kernel_options: dict[str, Any],
       404	    score_mod_other_buffers: tuple = (),
       405	    mask_mod_other_buffers: tuple = (),
       406	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       407	    """Defines the functionalization rules for the flex_attention operator.
       408	
       409	    Write now we are unwrapping each tensor and then redispatching to the next, however we want to
       410	    guard against any mutations in the score_mod function, to the other_buffers since those
       411	    are free variables.
       412	    """
       413	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
       414	
       415	    if has_user_subclass(
       416	        (
       417	            query,
       418	            key,
       419	            value,
       420	            score_mod,
- [2025-09-06T15:27:25+00:00] exit=0 dur=18ms: `nl -ba torch/_higher_order_ops/flex_attention.py | sed -n '420,520p'`
  - stdout:
       420	            score_mod,
       421	            block_mask,
       422	            scale,
       423	            kernel_options,
       424	            score_mod_other_buffers,
       425	            mask_mod_other_buffers,
       426	        ),
       427	        allowed_subclasses=(FakeTensor, FunctionalTensor),
       428	    ):
       429	        return NotImplemented
       430	
       431	    query_unwrapped = ctx.unwrap_tensors(query)
       432	    key_unwrapped = ctx.unwrap_tensors(key)
       433	    value_unwrapped = ctx.unwrap_tensors(value)
       434	    block_mask_unwrapped = ctx.unwrap_tensors(block_mask)
       435	    score_mod_other_buffers_unwrapped = ctx.unwrap_tensors(score_mod_other_buffers)
       436	    mask_mod_other_buffers_unwrapped = ctx.unwrap_tensors(mask_mod_other_buffers)
       437	
       438	    # Appease the mypy overlords
       439	    assert isinstance(query_unwrapped, torch.Tensor)
       440	    assert isinstance(key_unwrapped, torch.Tensor)
       441	    assert isinstance(value_unwrapped, torch.Tensor)
       442	    assert isinstance(block_mask_unwrapped, tuple)
       443	    assert isinstance(score_mod_other_buffers_unwrapped, tuple)
       444	    assert isinstance(mask_mod_other_buffers_unwrapped, tuple)
       445	
       446	    example_vals = (
       447	        [query_unwrapped.new_zeros(())]
       448	        + [query_unwrapped.new_zeros((), dtype=torch.int) for _ in range(4)]
       449	        + list(score_mod_other_buffers_unwrapped)
       450	    )
       451	    with ctx.redispatch_to_next():
       452	        functional_score_mod = ctx.functionalize(score_mod)
       453	        pre_dispatch = hasattr(ctx, "mode") and ctx.mode.pre_dispatch
       454	        with TransformGetItemToIndex():
       455	            # TODO: So far only the input mutations are checked
       456	            # In the other HOPs, also aliases are checked which is
       457	            # omitted here
       458	            mutates = _has_potential_branch_input_mutation(
       459	                score_mod, example_vals, pre_dispatch
       460	            )
       461	        # The only care about mutations of existing buffers since we can't replay these.
       462	        # However, we can just error if anything is detected
       463	        if mutates:
       464	            raise UnsupportedAliasMutationException("Mutations detected in score_mod")
       465	
       466	        out = flex_attention(
       467	            query_unwrapped,
       468	            key_unwrapped,
       469	            value_unwrapped,
       470	            functional_score_mod,
       471	            block_mask_unwrapped,
       472	            scale,
       473	            kernel_options,
       474	            score_mod_other_buffers_unwrapped,
       475	            mask_mod_other_buffers_unwrapped,
       476	        )
       477	    return ctx.wrap_tensors(out)  # type: ignore[return-value, arg-type]
       478	
       479	
       480	@register_fake(flex_attention)
       481	def flex_attention_fake_impl(
       482	    query: torch.Tensor,
       483	    key: torch.Tensor,
       484	    value: torch.Tensor,
       485	    score_mod: Callable,
       486	    block_mask: tuple,
       487	    scale: float,
       488	    kernel_options: dict[str, Any],
       489	    score_mod_other_buffers: tuple = (),
       490	    mask_mod_other_buffers: tuple = (),
       491	) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
       492	    if has_user_subclass(
       493	        (
       494	            query,
       495	            key,
       496	            value,
       497	            score_mod,
       498	            block_mask,
       499	            scale,
       500	            kernel_options,
       501	            score_mod_other_buffers,
       502	            mask_mod_other_buffers,
       503	        ),
       504	        allowed_subclasses=(FakeTensor,),
       505	    ):
       506	        return NotImplemented
       507	
       508	    # TODO: Figure out a better way to handle this for NJT than using sum()
       509	    if query.is_nested:
       510	        out = torch.empty_like(query, memory_format=torch.contiguous_format)
       511	        logsumexp = query.sum(dim=-1)
       512	        max_scores = query.max(dim=-1)[0]
       513	        return out, logsumexp, max_scores
       514	
       515	    v_head_dim = value.size(-1)
       516	    batch_size, num_heads, seq_len_q, _q_head_dim = query.shape
       517	    logsumexp = query.new_empty(batch_size, num_heads, seq_len_q, dtype=torch.float32)
       518	    max_scores = query.new_empty(batch_size, num_heads, seq_len_q, dtype=torch.float32)
       519	    out_shape = (batch_size, num_heads, seq_len_q, v_head_dim)
       520	    out = query.new_empty(out_shape)
- [2025-09-06T15:27:36+00:00] exit=0 dur=19ms: `nl -ba torch/_inductor/ir.py | sed -n '4870,4920p'`
  - stdout:
      4870	
      4871	
      4872	class TritonTemplateBuffer(TemplateBuffer):
      4873	    def __init__(
      4874	        self,
      4875	        layout: Layout,
      4876	        inputs: Sequence[IRNode],
      4877	        make_kernel_render: Optional[Callable[_P, _T]],
      4878	        mutated_inputs: Optional[Iterable[IRNode]] = None,
      4879	        allowed_prologue_inps: Optional[OrderedSet[str]] = None,
      4880	    ) -> None:
      4881	        """
      4882	        NOTE:[TritonTemplates with multiple outputs]
      4883	        We want the ability for TritonTemplates to output multiple tensors. Triton
      4884	        kernels have no notion of outputs and this is done by creating tensors that
      4885	        are then mutated by the kernel. Currently our STORE_OUTPUT codegen doesn't
      4886	        support creating multinode outputs for triton templates.
      4887	        We work around this by creating an extra input buffer during the lowering
      4888	        and we mark them as mutated inputs.
      4889	        """
      4890	        super().__init__(layout, inputs, make_kernel_render)
      4891	        self.mutated_inputs = mutated_inputs
      4892	        self.outputs: list[Buffer] = [self]
      4893	        if mutated_inputs is not None:
      4894	            # Ensure that the mutated inputs are only allowed for certain nodes
      4895	            allowed_set = (
      4896	                torch.ops.higher_order.flex_attention,
      4897	                torch.ops.higher_order.flex_attention_backward,
      4898	            )
      4899	            current_node = V.graph.current_node.target
      4900	            assert current_node in allowed_set, (
      4901	                f"Mutated inputs are only allowed for {allowed_set} but got {current_node}"
      4902	            )
      4903	            assert isinstance(self.inputs[0], IRNode), type(self.inputs[0])
      4904	            device = self.inputs[0].get_device()
      4905	            self.outputs += [
      4906	                MutationOutput(NoneLayout(device=device), buf, self)
      4907	                for buf in mutated_inputs
      4908	            ]
      4909	
      4910	        self.allowed_prologue_inps = (
      4911	            allowed_prologue_inps if allowed_prologue_inps else OrderedSet()
      4912	        )
      4913	
      4914	        self.subgraph_inps: Optional[list[Optional[Union[IRNode, sympy.Expr]]]] = None
      4915	        self.subgraph_outs: Optional[list[Optional[IRNode]]] = None
      4916	
      4917	    def get_free_symbol_uses(
      4918	        self, unbacked_only: bool = False
      4919	    ) -> OrderedSet[sympy.Symbol]:
      4920	        res = super().get_free_symbol_uses(unbacked_only)
- [2025-09-06T15:27:41+00:00] exit=0 dur=18ms: `nl -ba torch/_inductor/codegen/cpp_flex_attention_template.py | sed -n '1,220p'`
  - stdout:
         1	# mypy: allow-untyped-defs
         2	import contextlib
         3	import logging
         4	import re
         5	from typing import Optional
         6	from unittest.mock import patch
         7	
         8	import sympy
         9	
        10	import torch
        11	import torch.utils
        12	
        13	from ...utils._ordered_set import OrderedSet
        14	from .. import ir
        15	from ..ir import TensorBox
        16	from ..select_algorithm import DataProcessorTemplateWrapper
        17	from ..utils import parallel_num_threads
        18	from ..virtualized import V
        19	from .cpp_template import CppTemplate
        20	from .cpp_utils import GemmBlocking
        21	
        22	
        23	log = logging.getLogger(__name__)
        24	
        25	# TODO: reuse cpp codegen to generate below pointwise/reduction kernels
        26	SOFTMAX_FUSIONS = r"""
        27	// 1) out = exp(a - val)
        28	// 2) val = sum(out)
        29	template <typename T1, typename T2>
        30	inline void {{kernel_name}}_exp_reduce_sum_fusion_kernel(
        31	    T1* a,
        32	    const int& size,
        33	    T2* out,
        34	    T1& val) {
        35	  auto vec_size = at::vec::Vectorized<T1>::size();
        36	  auto vec_max = at::vec::Vectorized<T1>(val);
        37	  T1 tmp_sum = 0;
        38	  auto vec_tmp_sum = at::vec::Vectorized<T1>(tmp_sum);
        39	  for (long i = 0; i < vec_size * (size / vec_size); i += vec_size) {
        40	    auto tmp0 = at::vec::Vectorized<T1>::loadu(a + i);
        41	    auto tmp1 = tmp0 - vec_max;
        42	    auto tmp2 = tmp1.exp_u20();
        43	    vec_tmp_sum += tmp2;
        44	    at::native::_store(out + i, tmp2);
        45	  }
        46	  tmp_sum = at::vec::vec_reduce_all<T1>(
        47	      [](at::vec::Vectorized<T1>& x, at::vec::Vectorized<T1>& y) {
        48	        return x + y;
        49	      },
        50	      vec_tmp_sum);
        51	  for (long i = vec_size * (size / vec_size); i < size; i++) {
        52	    auto tmp0 = a[i];
        53	    auto tmp1 = tmp0 - val;
        54	    auto tmp2 = exp(tmp1);
        55	    tmp_sum += tmp2;
        56	    out[i] = tmp2;
        57	  }
        58	  val = tmp_sum;
        59	}
        60	
        61	// 1) out = a * scale
        62	// 2) max = max(out)
        63	template <typename scalar_t>
        64	inline void {{kernel_name}}_mul_reduce_max_fusion_kernel(
        65	    const scalar_t* a,
        66	    const scalar_t& scale,
        67	    const int& size,
        68	    scalar_t* out,
        69	    scalar_t& max) {
        70	  auto vec_size = at::vec::Vectorized<scalar_t>::size();
        71	  auto vec_scale = at::vec::Vectorized<scalar_t>(scale);
        72	  scalar_t tmp_max = -std::numeric_limits<scalar_t>::infinity();
        73	  auto vec_tmp_max = at::vec::Vectorized<scalar_t>(tmp_max);
        74	  for (long i = 0; i < vec_size * (size / vec_size); i += vec_size) {
        75	    auto tmp0 = at::vec::Vectorized<scalar_t>::loadu(a + i);
        76	    auto tmp1 = tmp0 * vec_scale;
        77	    vec_tmp_max = at::vec::maximum(vec_tmp_max, tmp1);
        78	    at::native::_store(out + i, tmp1);
        79	  }
        80	  for (long i = vec_size * (size / vec_size); i < size; i++) {
        81	    auto tmp0 = a[i];
        82	    auto tmp1 = tmp0 * scale;
        83	    tmp_max = std::max(tmp_max, tmp1);
        84	    out[i] = tmp1;
        85	  }
        86	  max = std::max(
        87	      tmp_max,
        88	      at::vec::vec_reduce_all<scalar_t>(
        89	          [](at::vec::Vectorized<scalar_t>& x, at::vec::Vectorized<scalar_t>& y) {
        90	            return at::vec::maximum(x, y);
        91	          },
        92	          vec_tmp_max));
        93	}
        94	
        95	template <typename scalar_t>
        96	static inline scalar_t* {{kernel_name}}_conditional_data_ptr(scalar_t* ptr, scalar_t* ptr2) {
        97	  TORCH_CHECK(ptr2 == nullptr);
        98	  return ptr;
        99	}
       100	
       101	template <typename scalar_t,
       102	          typename std::enable_if_t<c10::is_reduced_floating_point_v<scalar_t>, int> = 0>
       103	static inline scalar_t* {{kernel_name}}_conditional_data_ptr(float* ptr, scalar_t* ptr2) {
       104	  return ptr2;
       105	}
       106	
       107	template <typename scalar_t>
       108	inline void {{kernel_name}}_fill_stub(scalar_t* data, scalar_t val, int64_t size) {
       109	  using Vec = at::vec::Vectorized<scalar_t>;
       110	  Vec data_vec = Vec(val);
       111	  int64_t d = 0;
       112	  for (; d < size - (size % Vec::size()); d += Vec::size()) {
       113	    data_vec.store(data + d);
       114	  }
       115	  #if !defined(_MSC_VER) && !defined(COMPILING_FOR_MIN_SIZE)
       116	  # pragma unroll
       117	  #endif
       118	  for (; d < size; d++) {
       119	    data[d] = val;
       120	  }
       121	}
       122	
       123	// out = a * scale
       124	template <typename scalar_t>
       125	inline void {{kernel_name}}_mul_scale_kernel(
       126	    scalar_t* a,
       127	    scalar_t scale,
       128	    int64_t size) {
       129	  auto vec_size = at::vec::Vectorized<scalar_t>::size();
       130	  auto vec_scale = at::vec::Vectorized<scalar_t>(scale);
       131	  for (int64_t i = 0; i < vec_size * (size / vec_size); i += vec_size) {
       132	    auto tmp0 = at::vec::Vectorized<scalar_t>::loadu(a + i);
       133	    auto tmp1 = tmp0 * vec_scale;
       134	    at::native::_store(a + i, tmp1);
       135	  }
       136	  for (int64_t i = vec_size * (size / vec_size); i < size; i++) {
       137	    auto tmp0 = a[i];
       138	    auto tmp1 = tmp0 * scale;
       139	    a[i] = tmp1;
       140	  }
       141	}
       142	
       143	"""
       144	
       145	BRGEMM_PACK_FUNCTIONS = r"""
       146	template <typename scalar_t>
       147	inline void {{kernel_name}}_copy_value_with_pad(
       148	    const scalar_t* value_ptr,
       149	    scalar_t* dst_ptr,
       150	    int64_t rows,
       151	    int64_t cols,
       152	    int64_t prows,
       153	    int64_t pcols,
       154	    int64_t ldi) {
       155	  auto vec_size = at::vec::Vectorized<scalar_t>::size();
       156	  int64_t i = 0;
       157	  for (; i < rows; i++) {
       158	    int64_t j = 0;
       159	    for (; j < cols - (cols % vec_size); j += vec_size) {
       160	      auto vec_v =
       161	          at::vec::Vectorized<scalar_t>::loadu(value_ptr + i * ldi + j);
       162	      vec_v.store(dst_ptr + i * pcols + j);
       163	    }
       164	
       165	    if (j < cols) {
       166	      auto vec_v = at::vec::Vectorized<scalar_t>::loadu(
       167	          value_ptr + i * ldi + j, cols - j);
       168	      vec_v.store(dst_ptr + i * pcols + j, cols - j);
       169	    }
       170	
       171	    // col padding
       172	    auto psize = pcols - cols;
       173	    if (psize > 0) {
       174	      auto zero_vec = at::vec::Vectorized<scalar_t>(0);
       175	      int64_t pj = 0;
       176	      for (; pj < psize - (psize % vec_size); pj += vec_size) {
       177	        zero_vec.store(dst_ptr + i * pcols + cols + pj);
       178	      }
       179	      if (pj < psize) {
       180	        zero_vec.store(dst_ptr + i * pcols + cols + pj, psize - pj);
       181	      }
       182	    }
       183	  }
       184	  // row padding
       185	  for (; i < prows; i++) {
       186	    auto zero_vec = at::vec::Vectorized<scalar_t>(0);
       187	    int64_t j = 0;
       188	    for (; j < pcols - (pcols % vec_size); j += vec_size) {
       189	      zero_vec.store(dst_ptr + i * pcols + j);
       190	    }
       191	    if (j < pcols) {
       192	      zero_vec.store(dst_ptr + i * pcols + j, pcols - j);
       193	    }
       194	
       195	  }
       196	}
       197	"""
       198	
       199	MICRO_GEMM_TEMPLATE = r"""
       200	GEMM_DEFINE
       201	"""
       202	
       203	ALLOCATE_BUFFER = r"""
       204	  int64_t {{buffer_name}}_dtype_itemsize = c10::is_reduced_floating_point_v<{{buffer_dtype}}> ? 2 : 4;
       205	  auto& {{buffer_name}}_allocator = *at::getCPUAllocator();
       206	  auto {{buffer_name}}_work_data = {{buffer_name}}_allocator.allocate({{buffer_size}}*{{buffer_name}}_dtype_itemsize);
       207	  void* {{buffer_name}}_data_ptr = {{buffer_name}}_work_data.get();
       208	  {{buffer_dtype}}* {{buffer_name}} = ({{buffer_dtype}}*){{buffer_name}}_data_ptr;
       209	"""
       210	
       211	FLEX_ATTENTION_TEMPLATE = r"""
       212	{{template.header().getvalue()}}
       213	#include <ATen/native/cpu/utils.h>
       214	#include <ATen/native/CPUBlas.h>
       215	#include <ATen/Context.h>
       216	{{template.codegen_micro_gemm(kernel.kernel_name)}}
       217	{{template.codegen_softmax_fusion(kernel.kernel_name)}}
       218	{{template.codegen_brgemm_pack_function(kernel.kernel_name)}}
       219	{%- set kernel_args = {"query": query, "key": key, "value": value,
       220	                       "kv_num_blocks": kv_num_blocks, "kv_indices": kv_indices,
- [2025-09-06T15:27:52+00:00] exit=0 dur=19ms: `nl -ba torch/_inductor/kernel/flex/flex_attention.py | sed -n '1,200p'`
  - stdout:
         1	# mypy: allow-untyped-defs
         2	"""Triton Implementation of the flex_attention Kernel"""
         3	
         4	import logging
         5	import math
         6	from collections.abc import Sequence
         7	from dataclasses import dataclass
         8	from typing import Any, Optional, Union
         9	
        10	import sympy
        11	
        12	import torch
        13	from torch._inductor.utils import can_use_tma
        14	from torch._inductor.virtualized import V
        15	
        16	from ...ir import ComputedBuffer, ExternKernel, FixedLayout, TensorBox
        17	from ...lowering import empty, empty_strided, lowerings, register_lowering
        18	from ...select_algorithm import (
        19	    autotune_select_algorithm,
        20	    SymbolicGridFn,
        21	    TritonTemplate,
        22	)
        23	from .common import (
        24	    build_subgraph_buffer,
        25	    create_indices_fake,
        26	    create_num_blocks_fake_generator,
        27	    create_placeholder,
        28	    get_fwd_subgraph_outputs,
        29	    infer_dense_strides,
        30	    load_template,
        31	    maybe_realize,
        32	    set_head_dim_values,
        33	    SubgraphResults,
        34	)
        35	from .flex_cpu import lower_cpu
        36	from .flex_decoding import _use_flex_decoding, create_flex_decoding_kernel
        37	
        38	
        39	log = logging.getLogger(__name__)
        40	aten = torch.ops.aten
        41	Expr = sympy.Expr
        42	
        43	
        44	@SymbolicGridFn
        45	def flex_attention_grid(batch_size, q_heads, num_queries, d_model, meta, *, cdiv):
        46	    """How is this kernel parallelized?
        47	    We create a grid of (ceil_div(n_queries, query_block_size), batch_size, num_heads)
        48	    Each block is responsible for iterating over blocks of keys and values calculating
        49	    the final attention output.
        50	    """
        51	    return (cdiv(num_queries, meta["BLOCK_M"]), batch_size, q_heads)
        52	
        53	
        54	def get_float32_precision():
        55	    if (
        56	        (
        57	            torch.backends.cuda.matmul.fp32_precision == "ieee"
        58	            if torch.backends.cuda.matmul.fp32_precision != "none"
        59	            else torch.get_float32_matmul_precision() == "highest"
        60	        )
        61	        or torch.version.hip
        62	        or torch.mtia.is_available()
        63	    ):
        64	        return "'ieee'"
        65	    else:
        66	        return "'tf32'"
        67	
        68	
        69	flex_attention_template = TritonTemplate(
        70	    name="flex_attention",
        71	    grid=flex_attention_grid,
        72	    source=load_template("flex_attention")
        73	    + load_template("utilities")
        74	    + load_template("common"),
        75	)
        76	
        77	
        78	@register_lowering(torch.ops.higher_order.flex_attention, type_promotion_kind=None)
        79	def flex_attention(
        80	    query,
        81	    key,
        82	    value,
        83	    subgraph,
        84	    block_mask,
        85	    scale,
        86	    kernel_options,
        87	    score_mod_other_buffers,
        88	    mask_mod_other_buffers,
        89	):
        90	    """The main lowering for the flex_attention hop
        91	    This can currently lower to one of 3 templates:
        92	    1. Base Triton Template
        93	    2. Flex Decode Triton Template
        94	    3. Cpu specific CPP template
        95	    """
        96	    if query.get_device().type == "cpu":
        97	        return lower_cpu(
        98	            query,
        99	            key,
       100	            value,
       101	            subgraph,
       102	            block_mask,
       103	            scale,
       104	            kernel_options,
       105	            score_mod_other_buffers,
       106	            mask_mod_other_buffers,
       107	        )
       108	    # below is cuda path if device is not cpu
       109	    # tl.dot does not support embedding size less than 16
       110	    small_dqk = V.graph.sizevars.evaluate_expr(sympy.Lt(query.get_size()[-1], 16))
       111	    small_dv = V.graph.sizevars.evaluate_expr(sympy.Lt(value.get_size()[-1], 16))
       112	    if small_dqk or small_dv:
       113	        raise NotImplementedError(
       114	            f"NYI: embedding dimension of the query, key, and value must be "
       115	            f"at least 16 but got E={query.get_size()[-1]} and Ev={value.get_size()[-1]}"
       116	        )
       117	
       118	    (
       119	        _,  # q_length
       120	        _,  # kv_length
       121	        kv_num_blocks,
       122	        kv_indices,
       123	        full_kv_num_blocks,
       124	        full_kv_indices,
       125	        q_num_blocks,
       126	        q_indices,
       127	        full_q_num_blocks,
       128	        full_q_indices,
       129	        SPARSE_Q_BLOCK_SIZE,
       130	        SPARSE_KV_BLOCK_SIZE,
       131	        mask_graph,
       132	    ) = block_mask
       133	
       134	    placeholder_inps = [
       135	        create_placeholder(name, dtype, query.get_device())
       136	        for name, dtype in [
       137	            ("score", query.get_dtype()),
       138	            ("b", torch.int32),
       139	            ("h", torch.int32),
       140	            ("m", torch.int32),
       141	            ("n", torch.int32),
       142	        ]
       143	    ]
       144	    subgraph_buffer = build_subgraph_buffer(
       145	        placeholder_inps + list(score_mod_other_buffers), subgraph
       146	    )
       147	
       148	    mask_graph_placeholder_inps = [
       149	        create_placeholder(name, dtype, query.get_device())
       150	        for name, dtype in [
       151	            ("b", torch.int32),
       152	            ("h", torch.int32),
       153	            ("m", torch.int32),
       154	            ("n", torch.int32),
       155	        ]
       156	    ]
       157	    mask_graph_buffer = build_subgraph_buffer(
       158	        mask_graph_placeholder_inps + list(mask_mod_other_buffers), mask_graph
       159	    )
       160	
       161	    kernel_options = dict(kernel_options)
       162	    # Mark symbols in custom kernel options as static shapes and add guards.
       163	    kernel_options = {
       164	        k: V.graph.sizevars.guard_int(v) if isinstance(v, sympy.Symbol) else v
       165	        for k, v in kernel_options.items()
       166	    }
       167	    kernel_options.setdefault("FLOAT32_PRECISION", get_float32_precision())
       168	    enable_gqa = V.graph.sizevars.evaluate_expr(
       169	        sympy.Ne(query.get_size()[1], key.get_size()[1]),
       170	    )
       171	    if _use_flex_decoding(query, kv_indices, value, kernel_options, enable_gqa):
       172	        return create_flex_decoding_kernel(
       173	            query,
       174	            key,
       175	            value,
       176	            block_mask,
       177	            scale,
       178	            kernel_options,
       179	            subgraph_buffer,
       180	            mask_graph_buffer,
       181	            score_mod_other_buffers,
       182	            mask_mod_other_buffers,
       183	        )
       184	
       185	    (
       186	        query,
       187	        key,
       188	        value,
       189	        kv_num_blocks,
       190	        kv_indices,
       191	        full_kv_num_blocks,
       192	        full_kv_indices,
       193	        q_num_blocks,
       194	        q_indices,
       195	        full_q_num_blocks,
       196	        full_q_indices,
       197	    ) = maybe_realize(
       198	        [
       199	            query,
       200	            key,
- [2025-09-06T15:28:12+00:00] exit=0 dur=18ms: `nl -ba torch/nn/attention/flex_attention.py | sed -n '1,120p'`
  - stdout:
         1	# mypy: allow-untyped-defs
         2	# flake8: noqa: B950
         3	"""This module implements the user facing API for flex_attention in PyTorch."""
         4	
         5	import functools
         6	import inspect
         7	import itertools
         8	import math
         9	import operator
        10	import warnings
        11	from enum import Enum
        12	from typing import Any, Callable, NamedTuple, Optional, Union
        13	
        14	import torch
        15	from torch import Tensor
        16	
        17	
        18	try:
        19	    from typing import TypedDict
        20	except ImportError:
        21	    from typing_extensions import TypedDict
        22	
        23	try:
        24	    from typing import NotRequired
        25	except ImportError:
        26	    from typing_extensions import NotRequired
        27	
        28	from torch._higher_order_ops.flex_attention import flex_attention as flex_attention_hop
        29	from torch._higher_order_ops.utils import _set_compilation_env
        30	from torch._prims_common import DeviceLikeType
        31	from torch.fx.experimental.proxy_tensor import (
        32	    _temp_remove_metadata_torch_function_mode,
        33	    _temp_remove_pre_dispatch_torch_function_mode,
        34	)
        35	from torch.nn.attention._utils import _validate_sdpa_input
        36	from torch.utils._pytree import tree_map_only
        37	
        38	
        39	# Private debug flag to disable internal compilation wrapping for debugging purposes.
        40	# WARNING: This is intended ONLY for debugging score_mod and mask_mod functions.
        41	# When enabled, this bypasses the required internal compilation that ensures correctness
        42	# and performance. Only use this temporarily when you need to set breakpoints
        43	# in your score_mod/mask_mod functions during development.
        44	#
        45	# This flag only affects the internal compilation when flex_attention is called directly.
        46	# If you have already wrapped flex_attention in torch.compile(), this flag has no effect
        47	# and the user's compilation will still occur.
        48	#
        49	# Usage:
        50	#   import torch.nn.attention.flex_attention as fa
        51	#   fa._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True
        52	#   # Now you can set breakpoints in your score_mod/mask_mod
        53	#   output = fa.flex_attention(q, k, v, score_mod=my_score_mod)
        54	#
        55	_FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = False
        56	
        57	_WARNINGS_SHOWN: set[str] = set()
        58	
        59	
        60	def _warn_once(
        61	    warning_id: str, message: str, category: type[Warning] = UserWarning
        62	) -> None:
        63	    """Helper to ensure each warning is shown only once per process."""
        64	    if warning_id not in _WARNINGS_SHOWN:
        65	        warnings.warn(message, category, stacklevel=2)
        66	        _WARNINGS_SHOWN.add(warning_id)
        67	
        68	
        69	__all__ = [
        70	    "BlockMask",
        71	    "flex_attention",
        72	    "AuxOutput",
        73	    "AuxRequest",
        74	    "FlexKernelOptions",
        75	    "create_block_mask",
        76	    "create_mask",
        77	    "create_nested_block_mask",
        78	    "or_masks",
        79	    "and_masks",
        80	    "noop_mask",
        81	]
        82	
        83	_score_mod_signature = Callable[[Tensor, Tensor, Tensor, Tensor, Tensor], Tensor]
        84	_mask_mod_signature = Callable[[Tensor, Tensor, Tensor, Tensor], Tensor]
        85	
        86	
        87	class FlexKernelOptions(TypedDict, total=False):
        88	    """Options for controlling the behavior of FlexAttention kernels.
        89	
        90	    These options are passed to the underlying Triton kernels to control performance
        91	    and numerical behavior. Most users will not need to specify these options as the
        92	    default autotuning provides good performance.
        93	
        94	    The options can be prefixed with ``fwd_`` or ``bwd_`` to apply only to forward or
        95	    backward pass respectively. For example: ``fwd_BLOCK_M`` and ``bwd_BLOCK_M1``.
        96	
        97	    Note:
        98	      We currently do not provide any backward compatibility guarantees for these options.
        99	      That being said most of these have remained pretty stable since their introduction. But
       100	      We do not consider this part of the public API just yet. We think that some documentation
       101	      Is better than secret hidden flags, but we may change these options in the future.
       102	
       103	    Example Usage:
       104	        .. code-block:: python
       105	
       106	            # Using dictionary (backward compatible)
       107	            kernel_opts = {"BLOCK_M": 64, "BLOCK_N": 64, "PRESCALE_QK": True}
       108	            output = flex_attention(q, k, v, kernel_options=kernel_opts)
       109	
       110	            # Using TypedDict (recommended for type safety)
       111	            from torch.nn.attention.flex_attention import FlexKernelOptions
       112	
       113	            kernel_opts: FlexKernelOptions = {
       114	                "BLOCK_M": 64,
       115	                "BLOCK_N": 64,
       116	                "PRESCALE_QK": True,
       117	            }
       118	            output = flex_attention(q, k, v, kernel_options=kernel_opts)
       119	
       120	            # Forward/backward specific options
- [2025-09-06T15:28:13+00:00] exit=0 dur=18ms: `nl -ba torch/nn/attention/flex_attention.py | sed -n '1400,1730p'`
  - stdout:
      1400	
      1401	    def is_col_major(tensor: Tensor) -> bool:
      1402	        return tensor.stride()[-2] == 1
      1403	
      1404	    # These memory layout constraint are only for FP8 GEMMs on NVIDIA GPU architectures >= SM89 and < SM100.
      1405	    # This is because GPU arch < SM89 does not not support FP8 GEMMs, and
      1406	    # SM100 has support for TN, NT, TT, NN layouts for FP8 GEMMs
      1407	    # (i.e., left and right operands can be in row or column major layouts)
      1408	    # so this check is only needed for older architectures.
      1409	    # See: https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md
      1410	    fp8_dtypes = (
      1411	        torch.float8_e4m3fn,
      1412	        torch.float8_e5m2,
      1413	    )
      1414	    gemm_precision = query.dtype
      1415	
      1416	    should_enforce_mem_layout = (
      1417	        gemm_precision in fp8_dtypes
      1418	        and torch.version.cuda is not None
      1419	        and torch.cuda.get_device_capability("cuda") >= (8, 9)
      1420	        and torch.cuda.get_device_capability("cuda") < (10, 0)
      1421	    )
      1422	    if not should_enforce_mem_layout:
      1423	        return query, key, value
      1424	
      1425	    # Query must be in row-major memory layout as the left-operand in the FP8 GEMM `q @ k.T`
      1426	    if not is_row_major(query):
      1427	        query = query.contiguous()
      1428	
      1429	    # Key must be in row-major memory layout as it will be transposed when used as the right-operand
      1430	    # in the FP8 GEMM `q @ k.T`, meaning it will correctly be in column-major memory layout for the GEMM.
      1431	    if not is_row_major(key):
      1432	        key = key.contiguous()
      1433	
      1434	    # Value must be in column-major memory layout as the right-operand in the FP8 GEMM `softmax_scores @ v`
      1435	    if not is_col_major(value):
      1436	        value = value.transpose(-2, -1).contiguous().transpose(-2, -1)
      1437	    return query, key, value
      1438	
      1439	
      1440	def flex_attention(
      1441	    query: Tensor,
      1442	    key: Tensor,
      1443	    value: Tensor,
      1444	    score_mod: Optional[_score_mod_signature] = None,
      1445	    block_mask: Optional[BlockMask] = None,
      1446	    scale: Optional[float] = None,
      1447	    enable_gqa: bool = False,
      1448	    return_lse: bool = False,
      1449	    kernel_options: Optional[FlexKernelOptions] = None,
      1450	    *,
      1451	    return_aux: Optional[AuxRequest] = None,
      1452	) -> Union[Tensor, tuple[Tensor, Tensor], tuple[Tensor, AuxOutput]]:
      1453	    r"""This function implements scaled dot product attention with an arbitrary attention score modification function.
      1454	
      1455	    This function computes the scaled dot product attention between query, key, and value tensors with a user-defined
      1456	    attention score modification function. The attention score modification function will be applied after the attention
      1457	    scores have been calculated between the query and key tensors. The attention scores are calculated as follows:
      1458	
      1459	    The ``score_mod`` function should have the following signature:
      1460	
      1461	    .. code-block:: python
      1462	
      1463	        def score_mod(
      1464	            score: Tensor,
      1465	            batch: Tensor,
      1466	            head: Tensor,
      1467	            q_idx: Tensor,
      1468	            k_idx: Tensor
      1469	        ) -> Tensor:
      1470	
      1471	    Where:
      1472	        - ``score``: A scalar tensor representing the attention score,
      1473	          with the same data type and device as the query, key, and value tensors.
      1474	        - ``batch``, ``head``, ``q_idx``, ``k_idx``: Scalar tensors indicating
      1475	          the batch index, query head index, query index, and key/value index, respectively.
      1476	          These should have the ``torch.int`` data type and be located on the same device as the score tensor.
      1477	
      1478	    Args:
      1479	        query (Tensor): Query tensor; shape :math:`(B, Hq, L, E)`. For FP8 dtypes, should be in row-major memory layout for optimal performance.
      1480	        key (Tensor): Key tensor; shape :math:`(B, Hkv, S, E)`. For FP8 dtypes, should be in row-major memory layout for optimal performance.
      1481	        value (Tensor): Value tensor; shape :math:`(B, Hkv, S, Ev)`. For FP8 dtypes, should be in column-major memory layout for optimal performance.
      1482	        score_mod (Optional[Callable]): Function to modify attention scores. By default no score_mod is applied.
      1483	        block_mask (Optional[BlockMask]): BlockMask object that controls the blocksparsity pattern of the attention.
      1484	        scale (Optional[float]): Scaling factor applied prior to softmax. If none, the default value is set to :math:`\frac{1}{\sqrt{E}}`.
      1485	        enable_gqa (bool): If set to True, enables Grouped Query Attention (GQA) and broadcasts key/value heads to query heads.
      1486	        return_lse (bool): Whether to return the logsumexp of the attention scores. Default is False. **Deprecated**: Use ``return_aux=AuxRequest(lse=True)`` instead.
      1487	        kernel_options (Optional[FlexKernelOptions]):
      1488	            Options to control the behavior of the underlying Triton kernels.
      1489	            See :class:`FlexKernelOptions` for available options and usage examples.
      1490	        return_aux (Optional[AuxRequest]): Specifies which auxiliary outputs to compute and return.
      1491	            If None, only the attention output is returned. Use ``AuxRequest(lse=True, max_scores=True)``
      1492	            to request both auxiliary outputs.
      1493	
      1494	    Returns:
      1495	        output (Tensor): Attention output; shape :math:`(B, Hq, L, Ev)`.
      1496	
      1497	        When ``return_aux`` is not None:
      1498	            aux (AuxOutput): Auxiliary outputs with requested fields populated.
      1499	
      1500	        When ``return_aux`` is None (deprecated paths):
      1501	            lse (Tensor): Log-sum-exp of attention scores; shape :math:`(B, Hq, L)`. Only returned if ``return_lse=True``.
      1502	
      1503	    Shape legend:
      1504	        - :math:`N: \text{Batch size} ... : \text{Any number of other batch dimensions (optional)}`
      1505	        - :math:`S: \text{Source sequence length}`
      1506	        - :math:`L: \text{Target sequence length}`
      1507	        - :math:`E: \text{Embedding dimension of the query and key}`
      1508	        - :math:`Ev: \text{Embedding dimension of the value}`
      1509	
      1510	    .. warning::
      1511	        `torch.nn.attention.flex_attention` is a prototype feature in PyTorch.
      1512	        Please look forward to a more stable implementation in a future version of PyTorch.
      1513	        Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype
      1514	
      1515	    """
      1516	    # Some basic input validation
      1517	    _validate_sdpa_input(query, key, value)
      1518	    _validate_embed_dim(query, key, value)
      1519	    _validate_device(query, key, value)
      1520	    _validate_nestedness(query, key, value)
      1521	    query, key, value = _enforce_mem_layouts(query, key, value)
      1522	    if query.dim() != 4 or key.dim() != 4 or value.dim() != 4:
      1523	        raise NotImplementedError("NYI: query, key, and value must be 4D tensors")
      1524	    if (not enable_gqa) and query.size(-3) != key.size(-3):
      1525	        raise ValueError(
      1526	            f"Expect query and key/value to have the same number of heads "
      1527	            f"but got Hq={query.size(-3)} and Hkv={key.size(-3)}. "
      1528	            f"Try setting enable_gqa=True for GQA."
      1529	        )
      1530	    if enable_gqa:
      1531	        Hq = query.size(1)
      1532	        Hkv = key.size(1)
      1533	        if Hq % Hkv != 0:
      1534	            raise ValueError(
      1535	                f"Expect number of query heads to be a multiple of kv heads for GQA "
      1536	                f"but got Hq={Hq} and Hkv={Hkv}."
      1537	            )
      1538	    if query.size(0) != key.size(0):
      1539	        if block_mask is None:
      1540	            raise ValueError(
      1541	                f"Expect query and key/value to have the same batch size, "
      1542	                f"or non-none block_mask, "
      1543	                f"but got block_mask=None, Bq={query.size(0)}, and Bkv={key.size(0)}."
      1544	            )
      1545	
      1546	        if block_mask.kv_num_blocks.size(0) != query.size(0):
      1547	            raise ValueError(
      1548	                f"Expect query and key/value to have the same batch size, "
      1549	                f"or block_mask and query to have the same batch size, "
      1550	                f"but got Bq={query.size(0)}, Bkv={key.size(0)}, B_block_mask={block_mask.kv_num_blocks.size(0)}."
      1551	            )
      1552	
      1553	    if score_mod is None:
      1554	        score_mod = _identity
      1555	    elif query.is_nested:
      1556	        # use same NJT if the ragged structures for sequence lengths match between q and kv
      1557	        kv = (
      1558	            query
      1559	            if query.size(query._ragged_idx) == key.size(query._ragged_idx)  # type: ignore[attr-defined]
      1560	            else key
      1561	        )
      1562	        score_mod = _nested_mod_func_adapter(score_mod, query, kv, is_score_mod=True)  # type: ignore[assignment]
      1563	
      1564	    if block_mask is None:
      1565	        block_mask = _create_empty_block_mask(query, key)
      1566	
      1567	    if (
      1568	        block_mask.BLOCK_SIZE[0] == _LARGE_SPARSE_BLOCK_SIZE
      1569	        and block_mask.BLOCK_SIZE[1] == _LARGE_SPARSE_BLOCK_SIZE
      1570	    ):
      1571	        # This corresponds to the case where we essentially have a "no-op" block mask.
      1572	        pass
      1573	    elif query.is_nested:
      1574	        if block_mask.shape[-2] != query._values.size(query._ragged_idx - 1):  # type: ignore[attr-defined]
      1575	            raise RuntimeError(
      1576	                f"block_mask of shape {block_mask.shape} is not compatible with nested tensor input "
      1577	                f"with total sequence length of {query._values.size(query._ragged_idx - 1)}"  # type: ignore[attr-defined]
      1578	            )
      1579	    else:
      1580	        block_mask_q_len = block_mask.shape[-2]
      1581	        block_mask_kv_len = block_mask.shape[-1]
      1582	        if query.size(-2) > block_mask_q_len or key.size(-2) > block_mask_kv_len:
      1583	            raise ValueError(
      1584	                f"block_mask was created for block_mask.shape={block_mask.shape} but got q_len={query.size(-2)} and kv_len={key.size(-2)}. "
      1585	                "As the block mask was created for a smaller length than you're using it for, you likely need to create a new block mask."
      1586	            )
      1587	        elif (
      1588	            query.size(-2) < block_mask_q_len and key.size(-2) <= block_mask_kv_len
      1589	        ) or (query.size(-2) <= block_mask_q_len and key.size(-2) < block_mask_kv_len):
      1590	            raise ValueError(
      1591	                f"block_mask was created for block_mask.shape={block_mask.shape} but got q_len={query.size(-2)} and kv_len={key.size(-2)}. "
      1592	                "As the block mask was created for a larger length than you're using it for, you can either 1. create a new block mask with the correct length, or 2. 'adjust' the existing block mask to the correct length by calling block_mask._adjust(q_len, kv_len). This essentially 'crops' the block mask to the upper left corner, which does not work for all mask_mods!"
      1593	            )
      1594	        assert query.size(-2) == block_mask_q_len
      1595	        assert key.size(-2) == block_mask_kv_len
      1596	
      1597	    if scale is None:
      1598	        scale = 1.0 / math.sqrt(query.size(-1))
      1599	
      1600	    if query.device != block_mask.kv_num_blocks.device:  # type: ignore[union-attr]
      1601	        raise RuntimeError(
      1602	            f"Expect q/k/v and block_mask to be on the same device "
      1603	            f"but got {query.device} and {block_mask.kv_num_blocks.device}."  # type: ignore[union-attr]
      1604	        )
      1605	
      1606	    # Handle deprecation warnings for old parameters
      1607	    if return_lse and return_aux is not None:
      1608	        raise ValueError(
      1609	            "Cannot specify both return_lse and return_aux. "
      1610	            "return_lse is deprecated, please use return_aux=AuxRequest(lse=True) instead."
      1611	        )
      1612	    elif return_lse and return_aux is None:
      1613	        _warn_once(
      1614	            "deprecated_return_lse",
      1615	            "return_lse is deprecated and will be removed in v2.7. "
      1616	            "Please use return_aux=AuxRequest(lse=True) instead.",
      1617	            category=FutureWarning,
      1618	        )
      1619	
      1620	    kernel_options = _apply_kernel_options(
      1621	        query,
      1622	        key,
      1623	        value,
      1624	        return_lse,
      1625	        kernel_options,
      1626	        return_aux,
      1627	    )
      1628	
      1629	    def _finalize_outputs(
      1630	        out,
      1631	        lse,
      1632	        max_scores,
      1633	        *,
      1634	        return_aux: Optional[AuxRequest],
      1635	        return_lse: bool,
      1636	    ):
      1637	        """Normalize stats and build return value (aux-aware, legacy-compatible)."""
      1638	        ln2 = math.log(2.0)
      1639	        return_lse = return_lse or return_aux is not None and return_aux.lse
      1640	        return_max = return_aux is not None and return_aux.max_scores
      1641	
      1642	        lse_scaled = lse * ln2 if (return_lse and lse.numel() > 0) else None
      1643	        max_scaled = (
      1644	            max_scores * ln2 if (return_max and max_scores.numel() > 0) else None
      1645	        )
      1646	
      1647	        if return_aux is not None:
      1648	            return out, AuxOutput(
      1649	                lse=lse_scaled,
      1650	                max_scores=max_scaled,
      1651	            )
      1652	
      1653	        if return_lse:
      1654	            return out, lse_scaled
      1655	
      1656	        return out
      1657	
      1658	    if torch.compiler.is_dynamo_compiling():
      1659	        # mark head_dim and number of heads to be static
      1660	        for x in [query, key, value]:
      1661	            torch._dynamo.mark_static(x, -3)
      1662	            torch._dynamo.mark_static(x, -1)
      1663	
      1664	        out, lse, max_scores = flex_attention_hop(
      1665	            query,
      1666	            key,
      1667	            value,
      1668	            score_mod,
      1669	            block_mask.as_tuple(),
      1670	            scale,
      1671	            kernel_options,  # type: ignore[union-attr]
      1672	        )
      1673	        return _finalize_outputs(
      1674	            out, lse, max_scores, return_aux=return_aux, return_lse=return_lse
      1675	        )
      1676	
      1677	    if not _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG:
      1678	        _warn_once(
      1679	            warning_id="flex_attention_performance",
      1680	            message=(
      1681	                "flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n\n"
      1682	                "SOLUTION: Use torch.compile(flex_attention)(...)\n\n"
      1683	                "If you want to debug your score_mod/mask_mod, you can set:\n"
      1684	                "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n\n"
      1685	                "This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results."
      1686	            ),
      1687	        )
      1688	
      1689	    if not torch._dynamo.is_dynamo_supported():
      1690	        raise RuntimeError("flex_attention requires dynamo support")
      1691	
      1692	    from torch._dynamo.backends.debugging import (
      1693	        make_eager_backend_with_torch_function_mode,
      1694	    )
      1695	
      1696	    # Dynamo is expecting a callable with "__code__" attribute.
      1697	    # We cannot directly pass hop to it. So we wrap it in a dummy function.
      1698	    def _flex_attention_hop_wrapper(*args, **kwargs):
      1699	        return flex_attention_hop(*args, **kwargs)
      1700	
      1701	    with _set_compilation_env():
      1702	        with torch._dynamo.utils.disable_cache_limit():
      1703	            with _temp_remove_pre_dispatch_torch_function_mode():
      1704	                with _temp_remove_metadata_torch_function_mode() as metadata_mode:
      1705	                    if metadata_mode:
      1706	                        backend: Union[str, Callable[..., Any]] = (
      1707	                            make_eager_backend_with_torch_function_mode(metadata_mode)
      1708	                        )
      1709	                    else:
      1710	                        backend = "eager"
      1711	
      1712	                    if _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG:
      1713	                        flex_fn = _flex_attention_hop_wrapper
      1714	                    else:
      1715	                        flex_fn = torch.compile(
      1716	                            _flex_attention_hop_wrapper, backend=backend, fullgraph=True
      1717	                        )
      1718	
      1719	                    out, lse, max_scores = flex_fn(
      1720	                        query,
      1721	                        key,
      1722	                        value,
      1723	                        score_mod,
      1724	                        block_mask.as_tuple(),  # type: ignore[union-attr]
      1725	                        scale,
      1726	                        kernel_options,
      1727	                    )
      1728	    return _finalize_outputs(
      1729	        out, lse, max_scores, return_aux=return_aux, return_lse=return_lse
      1730	    )
- [2025-09-06T15:28:25+00:00] exit=0 dur=20ms: `nl -ba torch/_higher_order_ops/__init__.py | sed -n '1,120p'`
  - stdout:
         1	from torch._higher_order_ops._invoke_quant import (
         2	    invoke_quant,
         3	    invoke_quant_packed,
         4	    InvokeQuant,
         5	)
         6	from torch._higher_order_ops.aoti_call_delegate import aoti_call_delegate
         7	from torch._higher_order_ops.associative_scan import associative_scan
         8	from torch._higher_order_ops.auto_functionalize import (
         9	    auto_functionalized,
        10	    auto_functionalized_v2,
        11	)
        12	from torch._higher_order_ops.base_hop import BaseHOP
        13	from torch._higher_order_ops.cond import cond
        14	from torch._higher_order_ops.effects import with_effects
        15	from torch._higher_order_ops.executorch_call_delegate import executorch_call_delegate
        16	from torch._higher_order_ops.flat_apply import flat_apply
        17	from torch._higher_order_ops.flex_attention import (
        18	    flex_attention,
        19	    flex_attention_backward,
        20	)
        21	from torch._higher_order_ops.foreach_map import _foreach_map, foreach_map
        22	from torch._higher_order_ops.hints_wrap import hints_wrapper
        23	from torch._higher_order_ops.invoke_subgraph import invoke_subgraph
        24	from torch._higher_order_ops.map import map
        25	from torch._higher_order_ops.out_dtype import out_dtype
        26	from torch._higher_order_ops.run_const_graph import run_const_graph
        27	from torch._higher_order_ops.scan import scan
        28	from torch._higher_order_ops.strict_mode import strict_mode
        29	from torch._higher_order_ops.torchbind import call_torchbind
        30	from torch._higher_order_ops.while_loop import while_loop
        31	from torch._higher_order_ops.wrap import (
        32	    dynamo_bypassing_wrapper,
        33	    tag_activation_checkpoint,
        34	    wrap_activation_checkpoint,
        35	    wrap_with_autocast,
        36	    wrap_with_set_grad_enabled,
        37	)
        38	
        39	
        40	__all__ = [
        41	    "cond",
        42	    "while_loop",
        43	    "invoke_subgraph",
        44	    "scan",
        45	    "map",
        46	    "flex_attention",
        47	    "flex_attention_backward",
        48	    "hints_wrapper",
        49	    "BaseHOP",
        50	    "flat_apply",
        51	    "foreach_map",
        52	    "_foreach_map",
        53	    "with_effects",
        54	    "tag_activation_checkpoint",
        55	    "auto_functionalized",
        56	    "auto_functionalized_v2",
        57	    "associative_scan",
        58	    "out_dtype",
        59	    "executorch_call_delegate",
        60	    "call_torchbind",
        61	    "run_const_graph",
        62	    "InvokeQuant",
        63	    "invoke_quant",
        64	    "invoke_quant_packed",
        65	    "wrap_with_set_grad_enabled",
        66	    "wrap_with_autocast",
        67	    "wrap_activation_checkpoint",
        68	    "dynamo_bypassing_wrapper",
        69	    "strict_mode",
        70	    "aoti_call_delegate",
        71	    "map",
        72	]
- [2025-09-06T15:28:33+00:00] exit=0 dur=56ms: `rg -n --no-heading -S 'TransformGetItemToIndex'`
  - stdout:
    codex-logs/20250906-152543-session.md:869:       315	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:887:       333	    with TransformGetItemToIndex():
    codex-logs/20250906-152543-session.md:967:       413	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1242:       782	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1244:       784	    with TransformGetItemToIndex():
    codex-logs/20250906-152543-session.md:1304:       844	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1353:      1010	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1379:      1036	    with TransformGetItemToIndex():
    codex-logs/20250906-152543-session.md:1628:       315	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1649:       333	    with TransformGetItemToIndex():
    codex-logs/20250906-152543-session.md:1729:       413	    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    codex-logs/20250906-152543-session.md:1773:       454	        with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:170:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:191:    with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:315:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:333:    with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:413:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:454:        with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:782:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:784:    with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:844:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:932:    with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:942:    with TransformGetItemToIndex():
    torch/_higher_order_ops/flex_attention.py:1010:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_higher_order_ops/flex_attention.py:1036:    with TransformGetItemToIndex():
    torch/_dynamo/_trace_wrapped_higher_order_op.py:123:class TransformGetItemToIndex(TorchFunctionMode):
    torch/_dynamo/variables/higher_order_ops.py:2799:        from .._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/_dynamo/variables/higher_order_ops.py:2828:        with TransformGetItemToIndex():
    torch/nn/attention/flex_attention.py:1000:    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex
    torch/nn/attention/flex_attention.py:1002:    with TransformGetItemToIndex():
- [2025-09-06T15:28:40+00:00] exit=0 dur=18ms: `nl -ba torch/_dynamo/_trace_wrapped_higher_order_op.py | sed -n '110,190p'`
  - stdout:
       110	        return (
       111	            torch.ops.flex_lib.zeros_and_scatter(
       112	                ctx.input_shape,
       113	                indices,
       114	                gradOut,
       115	            ),
       116	            None,
       117	        )
       118	
       119	
       120	mod_index = ModIndex.apply
       121	
       122	
       123	class TransformGetItemToIndex(TorchFunctionMode):
       124	    # This is needed since we want to support calling
       125	    # A[q_idx], where q_idx is a scalar tensor in score_mod.
       126	    # Today, when q_idx is a scalar tensor, we implicitly convert it to a python
       127	    # scalar and create a view. We do not want that behavior in this case, so we
       128	    # use this torchfunctionmode to override that behavior for score_mod
       129	    # wherever we're running it.
       130	    def __torch_function__(
       131	        self,
       132	        func: OpOverload,
       133	        types: tuple[torch._C._TensorMeta, ...],
       134	        args: tuple[object, ...] = (),
       135	        kwargs: Optional[dict[str, object]] = None,
       136	    ) -> object:
       137	        if func == torch.Tensor.__getitem__:
       138	            index_args = pytree.tree_leaves(args[1])
       139	            if all(isinstance(x, torch.Tensor) for x in index_args):
       140	                return mod_index(args[0], index_args)
       141	        return func(*args, **(kwargs or {}))
       142	
       143	
       144	def trace_wrapped(*args: Any, **kwargs: Any) -> Any:
       145	    with torch.no_grad():
       146	        return _trace_wrapped_op(*args, **kwargs)
       147	
       148	
       149	class TraceWrapped(HigherOrderOperator):
       150	    def __init__(self) -> None:
       151	        super().__init__("trace_wrapped")
       152	
       153	    def __call__(self, *args: Any, **kwargs: Any) -> Any:
       154	        return super().__call__(*args, **kwargs)
       155	
       156	
       157	# TODO(jansel): need to ensure this does not get DCEed
       158	_trace_wrapped_op = TraceWrapped()
       159	
       160	
       161	def _assert_meta(
       162	    grad: torch.Tensor,
       163	    size: tuple[int, ...],
       164	    stride: tuple[int, ...],
       165	    dtype: torch.dtype,
       166	) -> torch.Tensor:
       167	    assert grad.size() == size, "size mismatch"
       168	    assert grad.stride() == stride, "stride mismatch"
       169	    assert grad.dtype == dtype, "dtype mismatch"
       170	    return grad
       171	
       172	
       173	@_trace_wrapped_op.py_impl(ProxyTorchDispatchMode)
       174	def inner_trace(
       175	    mode: ProxyTorchDispatchMode,
       176	    *args: Any,
       177	    bw_state: Optional[BackwardState] = None,
       178	    **kwargs: Any,
       179	) -> Any:
       180	    def self_invoke(*args: Any, **dyn_kwargs: Any) -> Any:
       181	        with torch.no_grad():
       182	            return _trace_wrapped_op(*args, **dyn_kwargs, **kwargs)
       183	
       184	    def unwrap_proxies(x: Any) -> Any:
       185	        if isinstance(x, torch.Tensor):
       186	            return mode.tracer.unwrap_proxy(x)  # type: ignore[union-attr]
       187	        if isinstance(x, (list, tuple)):
       188	            return type(x)(map(unwrap_proxies, x))
       189	        if x is None:
       190	            return None
- [2025-09-06T15:28:47+00:00] exit=0 dur=26ms: `rg -n --no-heading -S 'register_lowering\(torch\.ops\.higher_order\.flex_attention_backward' torch/_inductor | sed -n '1,120p'`
- [2025-09-06T15:28:57+00:00] exit=0 dur=25ms: `rg -n --no-heading -S 'flex_attention_backward' torch/_inductor | sed -n '1,200p'`
  - stdout:
    torch/_inductor/ir.py:4897:                torch.ops.higher_order.flex_attention_backward,
    torch/_inductor/kernel/flex/flex_attention.py:422:def flex_attention_backward_grid(
    torch/_inductor/kernel/flex/flex_attention.py:439:flex_attention_backward_template = TritonTemplate(
    torch/_inductor/kernel/flex/flex_attention.py:440:    name="flex_attention_backward",
    torch/_inductor/kernel/flex/flex_attention.py:441:    grid=flex_attention_backward_grid,
    torch/_inductor/kernel/flex/flex_attention.py:524:    torch.ops.higher_order.flex_attention_backward, type_promotion_kind=None
    torch/_inductor/kernel/flex/flex_attention.py:526:def flex_attention_backward(*args, **kwargs):
    torch/_inductor/kernel/flex/flex_attention.py:527:    """Lowering for the flex_attention_backward op in triton"""
    torch/_inductor/kernel/flex/flex_attention.py:776:        flex_attention_backward_template.maybe_append_choice(
    torch/_inductor/kernel/flex/flex_attention.py:846:        "flex_attention_backward",
- [2025-09-06T15:29:08+00:00] exit=0 dur=18ms: `nl -ba torch/_inductor/choices.py | sed -n '1,140p'`
  - stdout:
         1	from __future__ import annotations
         2	
         3	import typing
         4	from typing import Any, Optional, TYPE_CHECKING, Union
         5	
         6	import sympy
         7	
         8	import torch
         9	
        10	from . import config
        11	from .codecache import write_text
        12	from .kernel_inputs import KernelInputs  # noqa: TC001
        13	from .kernel_template_choice import make_ktc_generator
        14	from .metrics import get_metric_table, is_metric_table_enabled
        15	from .runtime.hints import DeviceProperties, ReductionHint
        16	from .scheduler import BaseSchedulerNode, Scheduler, WhyNoFuse
        17	from .template_heuristics import get_template_heuristic
        18	from .template_heuristics.triton import (
        19	    BaseConfigHeuristic,
        20	    CPUConfigHeuristic,
        21	    CUDAConfigHeuristic,
        22	    MTIAConfigHeuristic,
        23	    ROCmConfigHeuristic,
        24	    XPUConfigHeuristic,
        25	)
        26	from .virtualized import V
        27	
        28	
        29	if TYPE_CHECKING:
        30	    from collections.abc import Generator
        31	    from functools import partial
        32	
        33	    from triton import Config as TritonConfig
        34	
        35	    from torch.utils._ordered_set import OrderedSet
        36	
        37	    from .codegen.common import KernelTemplate
        38	    from .codegen.simd_kernel_features import SIMDKernelFeatures
        39	    from .codegen.triton import TritonKernel
        40	    from .ir import ChoiceCaller
        41	    from .select_algorithm import ExternKernelChoice
        42	
        43	
        44	class Sortable(typing.Protocol):
        45	    """Anything that can be used as a list.sort() key (int/tuple/etc)"""
        46	
        47	    def __lt__(self, other: typing.Self) -> bool: ...
        48	
        49	
        50	class InductorChoices:
        51	    """
        52	    This class contains a collection of default heuristics that effect performance of our generated
        53	    code.  We try to not put correctness requirements in this file.
        54	
        55	    You can override the choices made here by doing:
        56	
        57	            class MyHeuristics(InductorChoices):
        58	                ...
        59	
        60	            torch._inductor.virtualized.V.set_choices_handler(MyHeuristics())
        61	    """
        62	
        63	    def get_config_heuristics(
        64	        self, device_type: Optional[str] = "cuda"
        65	    ) -> BaseConfigHeuristic:
        66	        if device_type == "cuda":
        67	            if torch.version.hip is None:
        68	                return CUDAConfigHeuristic()
        69	            else:
        70	                return ROCmConfigHeuristic()
        71	        elif device_type == "xpu":
        72	            return XPUConfigHeuristic()
        73	        elif device_type == "cpu":
        74	            return CPUConfigHeuristic()
        75	        elif device_type == "mtia":
        76	            return MTIAConfigHeuristic()
        77	        else:
        78	            return BaseConfigHeuristic()
        79	
        80	    # Conv configs
        81	    def get_conv_configs(
        82	        self, device_type: Optional[str] = "cuda"
        83	    ) -> partial[Generator[TritonConfig, None, None]]:
        84	        conv_heuristics = self.get_config_heuristics(device_type)
        85	        return conv_heuristics.get_conv_configs()
        86	
        87	    # Flex attention configs
        88	    # TODO(coconutruben): break out flexattention/decode configs into the new retrieval mechanism
        89	    def get_flex_attention_fwd_configs(
        90	        self, head_dim: int, dtype: torch.dtype, device_type: Optional[str] = "cuda"
        91	    ) -> list[Any]:
        92	        flex_heuristics = self.get_config_heuristics(device_type)
        93	        return flex_heuristics.get_flex_attn_fwd_configs(head_dim, dtype)
        94	
        95	    def get_flex_attention_bwd_configs(
        96	        self, head_dim: int, dtype: torch.dtype, device_type: Optional[str] = "cuda"
        97	    ) -> list[Any]:
        98	        flex_heuristics = self.get_config_heuristics(device_type)
        99	        return flex_heuristics.get_flex_attn_bwd_configs(head_dim, dtype)
       100	
       101	    def get_flex_decode_configs(
       102	        self, head_dim: int, dtype: torch.dtype, device_type: Optional[str] = "cuda"
       103	    ) -> list[Any]:
       104	        flex_heuristics = self.get_config_heuristics(device_type)
       105	        return flex_heuristics.get_flex_decode_configs(head_dim, dtype)
       106	
       107	    def get_mm_configs(
       108	        self,
       109	        kernel_inputs: KernelInputs,
       110	        layout: Any,
       111	        templates: list[Union[KernelTemplate, ExternKernelChoice]],
       112	        op_name: str,
       113	        kwarg_overrides: Optional[dict[str, dict[str, Any]]] = None,
       114	    ) -> list[ChoiceCaller]:
       115	        """
       116	        Get list of ChoiceCallers for MM templates using template-specific heuristics.
       117	
       118	        Args:
       119	            kernel_inputs: MMKernelInputs containing input tensor nodes and matrix indices
       120	            layout: Output layout
       121	            templates: List of template objects (KernelTemplate or ExternKernelChoice)
       122	            op_name: Operation name (e.g., "bmm", "baddbmm", "addmm", "mm_plus_mm")
       123	            kwarg_overrides: Optional dict of kwargs to override for each template heuristic,
       124	                             indexed by template.uid. These only override the per config kwargs, not the extra kwargs
       125	        Returns:
       126	            List of ChoiceCaller objects from the templates
       127	        """
       128	        if kwarg_overrides is None:
       129	            kwarg_overrides = {}
       130	        input_tensors = kernel_inputs.nodes()
       131	        if len(input_tensors) < 2:
       132	            raise ValueError(f"Need at least 2 input tensors, got {len(input_tensors)}")
       133	
       134	        # Extract device_type from kernel_inputs
       135	        device_type = kernel_inputs.device_type
       136	        assert device_type is not None, "get_mm_configs requires a valid device type"
       137	
       138	        # First pass: Create dict of template.uid to generator of KernelTemplateChoice objects
       139	        template_choices = {}
       140	        for template in templates:

## Changes (final)

### git status --porcelain
?? codex-logs/
?? codex/
?? custom_ops/
?? symmetric_memory_playground/
?? }

### git diff --no-ext-diff
