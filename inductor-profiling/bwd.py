# AOT ID: ['0_backward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
assert_alignment = torch._C._dynamo.guards.assert_alignment
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_jeromeku/bu/cbu7276zvubqrtizufo5d3ztdjabdmbm4znrhdxiiyr5n2ykbcq7.py
# Topologically Sorted Source Nodes: [x_183, x_184], Original ATen: [aten.view, aten.expand, aten.div, aten._native_batch_norm_legit_functional, aten.relu, aten.threshold_backward, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_183 => add_303, convert_element_type_399, mul_463, mul_469, sub_57, unsqueeze_228, unsqueeze_229, unsqueeze_230, unsqueeze_231
#   x_184 => relu_6
# Graph fragment:
#   %convolution_154 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convolution_154]
#   %getitem_435 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=getitem_435]
#   %rsqrt_57 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=rsqrt_57]
#   %primals_477 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=primals_477]
#   %primals_478 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=primals_478]
#   %mm : Tensor "f16[128, 1536][1536, 1]cuda:0" = PlaceHolder[target=mm]
#   %view_2 : Tensor "f16[128, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [128, 1536, 1, 1]), kwargs = {})
#   %expand : Tensor "f16[128, 1536, 7, 7][1536, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_2, [128, 1536, 7, 7]), kwargs = {})
#   %div : Tensor "f16[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand, 49), kwargs = {})
#   %sub_57 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_154, %getitem_435), kwargs = {})
#   %mul_463 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_57, %rsqrt_57), kwargs = {})
#   %unsqueeze_228 : Tensor "f32[1536, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_477, -1), kwargs = {})
#   %unsqueeze_229 : Tensor "f32[1536, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_228, -1), kwargs = {})
#   %mul_469 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_463, %unsqueeze_229), kwargs = {})
#   %unsqueeze_230 : Tensor "f32[1536, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_478, -1), kwargs = {})
#   %unsqueeze_231 : Tensor "f32[1536, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_230, -1), kwargs = {})
#   %add_303 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_469, %unsqueeze_231), kwargs = {})
#   %convert_element_type_399 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_303, torch.float16), kwargs = {})
#   %relu_6 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_399,), kwargs = {})
#   %le : Tensor "b8[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_6, 0), kwargs = {})
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %where : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le, %full_default, %div), kwargs = {})
#   %convert_element_type_411 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where, torch.float32), kwargs = {})
#   return %convert_element_type_411
triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96755712}, 'kernel_num_gb': 0.058220544, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9633792
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 1536)
    x2 = xindex // 75264
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr5 + (x0 + 1536*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tmp13 = 0.0
    tmp14 = tmp12 <= tmp13
    tmp16 = 0.02040816326530612
    tmp17 = tmp15 * tmp16
    tmp18 = tl.where(tmp14, tmp13, tmp17)
    tmp19 = tmp18.to(tl.float32)
    tl.store(out_ptr0 + (x3), tmp19, None)


def get_args():
    arg_0 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 9633792,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.058220544
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ib/cibgh7ay7cbflclqrup7jfaikk4qfqjifscxtur4vcim6yuvzsxl.py
# Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_183 => convert_element_type_398, squeeze_171
# Graph fragment:
#   %convert_element_type_411 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convert_element_type_411]
#   %convolution_154 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convolution_154]
#   %getitem_435 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=getitem_435]
#   %squeeze_171 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_435, [0, 2, 3]), kwargs = {})
#   %unsqueeze_232 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_171, 0), kwargs = {})
#   %unsqueeze_233 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_232, 2), kwargs = {})
#   %unsqueeze_234 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_233, 3), kwargs = {})
#   %sum_2 : Tensor "f32[1536][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_411, [0, 2, 3]), kwargs = {})
#   %convert_element_type_398 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_154, torch.float32), kwargs = {})
#   %sub_58 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_398, %unsqueeze_234), kwargs = {})
#   %mul_470 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_411, %sub_58), kwargs = {})
#   %sum_3 : Tensor "f32[1536][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_470, [0, 2, 3]), kwargs = {})
#   return %buf6,%buf8
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 59013120, 'r0_': 0}, 'kernel_num_gb': 0.058411008, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 75264
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 1536)
    x1 = xindex // 1536
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp6 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp10 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_2 + 196608*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + 1536*r0_2 + 196608*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp5 - tmp6
        tmp8 = tmp0 * tmp7
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(r0_mask & xmask, tmp11, _tmp10)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536, 49), (1, 1536), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1536, 49), (1, 1536), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 75264, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.058411008
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/o4/co4l7dxdbqmyq2tz3smxih3rixpy2ne2wnri7xcr6jjsl7htfgen.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf6 : Tensor "f32[1536, 49][1, 1536]cuda:0" = PlaceHolder[target=buf6]
#   %sum_2 : Tensor "f32[1536][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_411, [0, 2, 3]), kwargs = {})
#   return %sum_2
triton_per_fused_native_batch_norm_backward_2 = async_compile.triton('triton_per_fused_native_batch_norm_backward_2', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_2', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 313344, 'r0_': 0}, 'kernel_num_gb': 0.0003072, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_native_batch_norm_backward_2(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1536
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((1536, 49), (1, 1536), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1536, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_native_batch_norm_backward_2.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_native_batch_norm_backward_2.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0003072
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/md/cmdcd2w6jvbi7hb4avbaab5pnwwvk6ubriaq3imoj42knpnhof6k.py
# Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_183 => convert_element_type_398, squeeze_171, squeeze_172
# Graph fragment:
#   %buf8 : Tensor "f32[1536, 49][1, 1536]cuda:0" = PlaceHolder[target=buf8]
#   %sum_3 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=sum_3]
#   %rsqrt_57 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=rsqrt_57]
#   %squeeze_171 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_435, [0, 2, 3]), kwargs = {})
#   %unsqueeze_232 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_171, 0), kwargs = {})
#   %unsqueeze_233 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_232, 2), kwargs = {})
#   %unsqueeze_234 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_233, 3), kwargs = {})
#   %convert_element_type_398 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_154, torch.float32), kwargs = {})
#   %sub_58 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_398, %unsqueeze_234), kwargs = {})
#   %mul_470 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_411, %sub_58), kwargs = {})
#   %sum_3 : Tensor "f32[1536][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_470, [0, 2, 3]), kwargs = {})
#   %squeeze_172 : Tensor "f32[1536][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_57, [0, 2, 3]), kwargs = {})
#   %mul_478 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_3, %squeeze_172), kwargs = {})
#   return %sum_3,%mul_478
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 331776, 'r0_': 0}, 'kernel_num_gb': 0.000319488, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1536
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_1), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((1536, 49), (1, 1536), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 1536, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000319488
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vg/cvgo7cjmpddyzoxc7whmt57quwlkefyvzv4imkbmasck35467s7b.py
# Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_183 => convert_element_type_398, squeeze_171, squeeze_172
# Graph fragment:
#   %convert_element_type_411 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convert_element_type_411]
#   %convolution_154 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convolution_154]
#   %getitem_435 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=getitem_435]
#   %sum_3 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=sum_3]
#   %rsqrt_57 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=rsqrt_57]
#   %sum_2 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=sum_2]
#   %primals_477 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=primals_477]
#   %squeeze_171 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_435, [0, 2, 3]), kwargs = {})
#   %unsqueeze_232 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_171, 0), kwargs = {})
#   %unsqueeze_233 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_232, 2), kwargs = {})
#   %unsqueeze_234 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_233, 3), kwargs = {})
#   %convert_element_type_398 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_154, torch.float32), kwargs = {})
#   %sub_58 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_398, %unsqueeze_234), kwargs = {})
#   %mul_471 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_2, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_235 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_471, 0), kwargs = {})
#   %unsqueeze_236 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_235, 2), kwargs = {})
#   %unsqueeze_237 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_236, 3), kwargs = {})
#   %mul_472 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_3, 0.00015943877551020407), kwargs = {})
#   %squeeze_172 : Tensor "f32[1536][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_57, [0, 2, 3]), kwargs = {})
#   %mul_473 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_172, %squeeze_172), kwargs = {})
#   %mul_474 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_472, %mul_473), kwargs = {})
#   %unsqueeze_238 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_474, 0), kwargs = {})
#   %unsqueeze_239 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_238, 2), kwargs = {})
#   %unsqueeze_240 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_239, 3), kwargs = {})
#   %mul_475 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_172, %primals_477), kwargs = {})
#   %unsqueeze_241 : Tensor "f32[1, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_475, 0), kwargs = {})
#   %unsqueeze_242 : Tensor "f32[1, 1536, 1][1536, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_241, 2), kwargs = {})
#   %unsqueeze_243 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_242, 3), kwargs = {})
#   %mul_476 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_58, %unsqueeze_240), kwargs = {})
#   %sub_60 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_411, %mul_476), kwargs = {})
#   %sub_61 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_60, %unsqueeze_237), kwargs = {})
#   %mul_477 : Tensor "f32[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_61, %unsqueeze_243), kwargs = {})
#   %convert_element_type_413 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_477, torch.float16), kwargs = {})
#   %convolution_backward : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_413, %add_298, %convert_element_type_397, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf11
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96368640}, 'kernel_num_gb': 0.077101056, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9633792
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 1536)
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp1 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp3 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 0.00015943877551020407
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp19, None)


def get_args():
    arg_0 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 9633792,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.077101056
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/l6/cl6626gnqwx3wqfvhhqomemjy65edxupzzlzzbgvi7km2g2i76ws.py
# Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_180 => convert_element_type_395
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %cat_40 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_40]
#   %unsqueeze_246 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_246]
#   %convert_element_type_415 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_436, torch.float32), kwargs = {})
#   %sum_4 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_415, [0, 2, 3]), kwargs = {})
#   %convert_element_type_395 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_40, torch.float32), kwargs = {})
#   %sub_62 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_395, %unsqueeze_246), kwargs = {})
#   %mul_479 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_415, %sub_62), kwargs = {})
#   %sum_5 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_479, [0, 2, 3]), kwargs = {})
#   return %buf16,%buf18
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6831264, 'r0_': 0}, 'kernel_num_gb': 0.006727776, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 12936
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 264)
    x1 = xindex // 264
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 12936, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.006727776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ws/cwsndg3ijniuool2jj6rant3dv6cb364bci33dd4z5eqy4s4grtm.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf16 : Tensor "f32[264, 49][1, 264]cuda:0" = PlaceHolder[target=buf16]
#   %convert_element_type_415 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_436, torch.float32), kwargs = {})
#   %sum_4 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_415, [0, 2, 3]), kwargs = {})
#   return %sum_4
triton_per_fused_native_batch_norm_backward_6 = async_compile.triton('triton_per_fused_native_batch_norm_backward_6', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_6', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 53856, 'r0_': 0}, 'kernel_num_gb': 5.28e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_native_batch_norm_backward_6(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 264
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 264, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_native_batch_norm_backward_6.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_native_batch_norm_backward_6.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.28e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ri/cri4u6kuprxuzxgtpeqtb636nqksp33qddwuivlez76tuv5ur2nq.py
# Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_180 => convert_element_type_395
# Graph fragment:
#   %buf18 : Tensor "f32[264, 49][1, 264]cuda:0" = PlaceHolder[target=buf18]
#   %sum_5 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_5]
#   %squeeze_169 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=squeeze_169]
#   %convert_element_type_415 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_436, torch.float32), kwargs = {})
#   %convert_element_type_395 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_40, torch.float32), kwargs = {})
#   %sub_62 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_395, %unsqueeze_246), kwargs = {})
#   %mul_479 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_415, %sub_62), kwargs = {})
#   %sum_5 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_479, [0, 2, 3]), kwargs = {})
#   %mul_487 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_5, %squeeze_169), kwargs = {})
#   return %sum_5,%mul_487
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 57024, 'r0_': 0}, 'kernel_num_gb': 5.4912e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 264
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_1), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 264, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.4912e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5c/c5cxvzswq5bkfg3ricwka5e4xa6wte477moqesusmu7nnh3ne7a4.py
# Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_180 => convert_element_type_395
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %cat_40 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_40]
#   %unsqueeze_246 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_246]
#   %sum_5 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_5]
#   %squeeze_169 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=squeeze_169]
#   %sum_4 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_4]
#   %primals_471 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_471]
#   %convert_element_type_415 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_436, torch.float32), kwargs = {})
#   %convert_element_type_395 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_40, torch.float32), kwargs = {})
#   %sub_62 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_395, %unsqueeze_246), kwargs = {})
#   %mul_480 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_4, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_247 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_480, 0), kwargs = {})
#   %unsqueeze_248 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_247, 2), kwargs = {})
#   %unsqueeze_249 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_248, 3), kwargs = {})
#   %mul_481 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_5, 0.00015943877551020407), kwargs = {})
#   %mul_482 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_169, %squeeze_169), kwargs = {})
#   %mul_483 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_481, %mul_482), kwargs = {})
#   %unsqueeze_250 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_483, 0), kwargs = {})
#   %unsqueeze_251 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_250, 2), kwargs = {})
#   %unsqueeze_252 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_251, 3), kwargs = {})
#   %mul_484 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_169, %primals_471), kwargs = {})
#   %unsqueeze_253 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_484, 0), kwargs = {})
#   %unsqueeze_254 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_253, 2), kwargs = {})
#   %unsqueeze_255 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_254, 3), kwargs = {})
#   %mul_485 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_62, %unsqueeze_252), kwargs = {})
#   %sub_64 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_415, %mul_485), kwargs = {})
#   %sub_65 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_64, %unsqueeze_249), kwargs = {})
#   %mul_486 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_65, %unsqueeze_255), kwargs = {})
#   %convert_element_type_417 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_486, torch.float16), kwargs = {})
#   return %convert_element_type_417
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13251744}, 'kernel_num_gb': 0.009940128, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 0.00015943877551020407
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009940128
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/eo/ceozlqgavtxgzexythintgccemxpjm72nd4ty33zof3mzwa5dpqg.py
# Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_176 => add_292, convert_element_type_384, mul_446, mul_452, sub_55, unsqueeze_220, unsqueeze_221, unsqueeze_222, unsqueeze_223
# Graph fragment:
#   %cat_39 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=cat_39]
#   %getitem_429 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_429]
#   %rsqrt_55 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_55]
#   %primals_460 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_460]
#   %primals_461 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_461]
#   %sub_55 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_39, %getitem_429), kwargs = {})
#   %mul_446 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_55, %rsqrt_55), kwargs = {})
#   %unsqueeze_220 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_460, -1), kwargs = {})
#   %unsqueeze_221 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_220, -1), kwargs = {})
#   %mul_452 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_446, %unsqueeze_221), kwargs = {})
#   %unsqueeze_222 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_461, -1), kwargs = {})
#   %unsqueeze_223 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_222, -1), kwargs = {})
#   %add_292 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_452, %unsqueeze_223), kwargs = {})
#   %convert_element_type_384 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_292, torch.float16), kwargs = {})
#   return %convert_element_type_384
triton_poi_fused__native_batch_norm_legit_functional_9 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_9', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_9', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 59634432}, 'kernel_num_gb': 0.039764736, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_9(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9934848
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 1584)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 9934848,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_9.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_9.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.039764736
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ad/cadtzh6hkedwvf6ky56a5m5wab5opxxrfsvh7gsgqetkxrgz62pg.py
# Topologically Sorted Source Nodes: [x_177, sigmoid_15], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_15 => sigmoid_63
#   x_177 => convert_element_type_385, convert_element_type_386, mul_453, sigmoid_61
# Graph fragment:
#   %getitem_442 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_442]
#   %getitem_439 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_439]
#   %convert_element_type_384 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_384]
#   %sum_6 : Tensor "f16[128, 1584, 1, 1][1584, 1, 202752, 202752]cuda:0" = PlaceHolder[target=sum_6]
#   %convolution_151 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=convolution_151]
#   %cat_41 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_442, %getitem_439], 1), kwargs = {})
#   %convert_element_type_385 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_384, torch.float32), kwargs = {})
#   %sigmoid_61 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_385,), kwargs = {})
#   %mul_453 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_385, %sigmoid_61), kwargs = {})
#   %convert_element_type_386 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_453, torch.float16), kwargs = {})
#   %mul_488 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_41, %convert_element_type_386), kwargs = {})
#   %sigmoid_63 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_151,), kwargs = {})
#   %sum_6 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_488, [2, 3], True), kwargs = {})
#   %convert_element_type_420 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_6, torch.float32), kwargs = {})
#   %convert_element_type_421 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_63, torch.float32), kwargs = {})
#   %sub_66 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_421), kwargs = {})
#   %mul_490 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_421, %sub_66), kwargs = {})
#   %mul_491 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_420, %mul_490), kwargs = {})
#   %convert_element_type_422 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_491, torch.float16), kwargs = {})
#   return %sum_6,%convert_element_type_422
triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10 = async_compile.triton('triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 262144, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 60825600, 'r0_': 0}, 'kernel_num_gb': 0.040955904, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 202752
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    x0 = (xindex % 1584)
    r0_2 = r0_index
    x1 = xindex // 1584
    x3 = xindex
    tmp11 = tl.load(in_ptr2 + (x0 + 1584*r0_2 + 77616*x1), r0_mask & xmask, other=0.0).to(tl.float32)
    tmp22 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x0
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 792, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (792*r0_2 + 38808*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 1584, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (792*r0_2 + 38808*x1 + ((-792) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tl.sigmoid(tmp12)
    tmp14 = tmp12 * tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp16 = tmp10 * tmp15
    tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
    tmp19 = tl.where(r0_mask & xmask, tmp17, 0)
    tmp20 = tl.sum(tmp19, 1)[:, None].to(tl.float32)
    tmp21 = tmp20.to(tl.float32)
    tmp23 = tl.sigmoid(tmp22)
    tmp24 = tmp23.to(tl.float32)
    tmp25 = 1.0
    tmp26 = tmp25 - tmp24
    tmp27 = tmp24 * tmp26
    tmp28 = tmp21 * tmp27
    tmp29 = tmp28.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp29, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 202752, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.040955904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wn/cwnvw54h5adda6renmjm6rkhxrepsywu245w4wfbgdmgqpyvntad.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_445 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0" = PlaceHolder[target=getitem_445]
#   %convolution_150 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0" = PlaceHolder[target=convolution_150]
#   %sigmoid_64 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_150,), kwargs = {})
#   %full_default_1 : Tensor "f16[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.full.default](args = ([128, 132, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_67 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_1, %sigmoid_64), kwargs = {})
#   %mul_492 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_150, %sub_67), kwargs = {})
#   %add_304 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_492, 1), kwargs = {})
#   %mul_493 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_64, %add_304), kwargs = {})
#   %mul_494 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_445, %mul_493), kwargs = {})
#   return %mul_494
triton_poi_fused_add_fill_mul_sigmoid_sub_11 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_11', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_11', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 135168}, 'kernel_num_gb': 0.000101376, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_11(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 16896,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_11.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_11.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000101376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bb/cbbnmtvz5zcbiv6eaujlphfifihcdzkls65k32rfnvsmb4xabgfz.py
# Topologically Sorted Source Nodes: [sigmoid_15], Original ATen: [aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_15 => sigmoid_63
# Graph fragment:
#   %getitem_442 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_442]
#   %getitem_439 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_439]
#   %convolution_151 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=convolution_151]
#   %getitem_448 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_448]
#   %convert_element_type_384 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_384]
#   %cat_41 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_442, %getitem_439], 1), kwargs = {})
#   %sigmoid_63 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_151,), kwargs = {})
#   %mul_489 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_41, %sigmoid_63), kwargs = {})
#   %expand_1 : Tensor "f16[128, 1584, 7, 7][1584, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_448, [128, 1584, 7, 7]), kwargs = {})
#   %div_1 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_1, 49), kwargs = {})
#   %add_305 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_489, %div_1), kwargs = {})
#   %sigmoid_65 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_384,), kwargs = {})
#   %full_default_2 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=6] = call_function[target=torch.ops.aten.full.default](args = ([128, 1584, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_68 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_2, %sigmoid_65), kwargs = {})
#   %mul_495 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_384, %sub_68), kwargs = {})
#   %add_306 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_495, 1), kwargs = {})
#   %mul_496 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_65, %add_306), kwargs = {})
#   %mul_497 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_305, %mul_496), kwargs = {})
#   %convert_element_type_427 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_497, torch.float32), kwargs = {})
#   return %convert_element_type_427
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 79478784, 'x': 60420096}, 'kernel_num_gb': 0.080289792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 49
    y0 = (yindex % 49)
    tmp11 = tl.load(in_ptr2 + (x2 + 1584*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 1584*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 1584*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 792, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (792*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 1584, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (792*y3 + ((-792) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.02040816326530612
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 49*x2 + 77616*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.080289792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wy/cwydpfmxjejqoptjgvsymtg5f6qc4mgdqudhsn655h6rf6wgl6xt.py
# Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_176 => convert_element_type_383, squeeze_165
# Graph fragment:
#   %convert_element_type_427 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_427]
#   %cat_39 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=cat_39]
#   %getitem_429 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_429]
#   %squeeze_165 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_429, [0, 2, 3]), kwargs = {})
#   %unsqueeze_256 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_165, 0), kwargs = {})
#   %unsqueeze_257 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_256, 2), kwargs = {})
#   %unsqueeze_258 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_257, 3), kwargs = {})
#   %convert_element_type_383 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_39, torch.float32), kwargs = {})
#   %sub_69 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_383, %unsqueeze_258), kwargs = {})
#   %mul_498 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_427, %sub_69), kwargs = {})
#   %sum_10 : Tensor "f32[1584][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_498, [0, 2, 3]), kwargs = {})
#   return %buf48
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20496960, 'r0_': 39739392}, 'kernel_num_gb': 0.059925888, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 77616
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 1584)
    x1 = xindex // 1584
    tmp3 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp7 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (49*x0 + 77616*((r0_2 + 128*x1) // 49) + (((r0_2 + 128*x1) % 49))), r0_mask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 1584*r0_2 + 202752*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp4 = tmp2 - tmp3
        tmp5 = tmp0 * tmp4
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0 + 1600*x1), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584, 49), (1, 1600), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 77616, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.059925888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/32/c32kvr3qca763xs4usxqaziwav75bxmm6cpaudr24sogj75lrwj6.py
# Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_176 => convert_element_type_383, squeeze_165, squeeze_166
# Graph fragment:
#   %buf48 : Tensor "f32[1584, 49][1, 1600]cuda:0" = PlaceHolder[target=buf48]
#   %sum_10 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=sum_10]
#   %rsqrt_55 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_55]
#   %squeeze_165 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_429, [0, 2, 3]), kwargs = {})
#   %unsqueeze_256 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_165, 0), kwargs = {})
#   %unsqueeze_257 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_256, 2), kwargs = {})
#   %unsqueeze_258 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_257, 3), kwargs = {})
#   %convert_element_type_383 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_39, torch.float32), kwargs = {})
#   %sub_69 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_383, %unsqueeze_258), kwargs = {})
#   %mul_498 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_427, %sub_69), kwargs = {})
#   %sum_10 : Tensor "f32[1584][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_498, [0, 2, 3]), kwargs = {})
#   %squeeze_166 : Tensor "f32[1584][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_55, [0, 2, 3]), kwargs = {})
#   %mul_506 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_10, %squeeze_166), kwargs = {})
#   return %sum_10,%mul_506
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 342144, 'r0_': 0}, 'kernel_num_gb': 0.000329472, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1584
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1600*r0_1), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((1584, 49), (1, 1600), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 1584, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000329472
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/x2/cx25k36vrsbobloaozfkk4inqzph5nmxnfjjpmurwsop4fjioips.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_427 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_427]
#   %sum_9 : Tensor "f32[1584][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_427, [0, 2, 3]), kwargs = {})
#   return %sum_9
triton_red_fused_native_batch_norm_backward_15 = async_compile.triton('triton_red_fused_native_batch_norm_backward_15', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 2048, 'r0_': 8192},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_15', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12672, 'r0_': 39739392}, 'kernel_num_gb': 0.039745728, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_15(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1584
    r0_numel = 6272
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 49)
        r0_2 = r0_index // 49
        tmp0 = tl.load(in_ptr0 + (r0_1 + 49*x0 + 77616*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1584, 6272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_15.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_15.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.039745728
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zi/czi3k2ei7l2oa3kvqoznlniwegbsy5ssdmev55f46wogedic2hkc.py
# Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_176 => convert_element_type_383, squeeze_165, squeeze_166
# Graph fragment:
#   %convert_element_type_427 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_427]
#   %cat_39 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=cat_39]
#   %getitem_429 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_429]
#   %sum_10 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=sum_10]
#   %rsqrt_55 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_55]
#   %sum_9 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=sum_9]
#   %primals_460 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_460]
#   %squeeze_165 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_429, [0, 2, 3]), kwargs = {})
#   %unsqueeze_256 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_165, 0), kwargs = {})
#   %unsqueeze_257 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_256, 2), kwargs = {})
#   %unsqueeze_258 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_257, 3), kwargs = {})
#   %convert_element_type_383 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_39, torch.float32), kwargs = {})
#   %sub_69 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_383, %unsqueeze_258), kwargs = {})
#   %mul_499 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_9, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_259 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_499, 0), kwargs = {})
#   %unsqueeze_260 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_259, 2), kwargs = {})
#   %unsqueeze_261 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_260, 3), kwargs = {})
#   %mul_500 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_10, 0.00015943877551020407), kwargs = {})
#   %squeeze_166 : Tensor "f32[1584][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_55, [0, 2, 3]), kwargs = {})
#   %mul_501 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_166, %squeeze_166), kwargs = {})
#   %mul_502 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_500, %mul_501), kwargs = {})
#   %unsqueeze_262 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_502, 0), kwargs = {})
#   %unsqueeze_263 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_262, 2), kwargs = {})
#   %unsqueeze_264 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_263, 3), kwargs = {})
#   %mul_503 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_166, %primals_460), kwargs = {})
#   %unsqueeze_265 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_503, 0), kwargs = {})
#   %unsqueeze_266 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_265, 2), kwargs = {})
#   %unsqueeze_267 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_266, 3), kwargs = {})
#   %mul_504 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_69, %unsqueeze_264), kwargs = {})
#   %sub_71 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_427, %mul_504), kwargs = {})
#   %sub_72 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_71, %unsqueeze_261), kwargs = {})
#   %mul_505 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_72, %unsqueeze_267), kwargs = {})
#   %convert_element_type_429 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_505, torch.float16), kwargs = {})
#   return %convert_element_type_429
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 262144, 'x': 64}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 19901376, 'x': 79478784}, 'kernel_num_gb': 0.079510464, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 202752
    xnumel = 49
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 1584)
    y1 = yindex // 1584
    tmp0 = tl.load(in_ptr0 + (x2 + 49*y3), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + 1584*x2 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (y0), ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), ymask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (y0), ymask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 0.00015943877551020407
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(out_ptr0 + (x2 + 49*y3), tmp19, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 202752, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.079510464
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/di/cdikczqk22wclocmbkc5istm2keqbuaf56m22ihcl4zhk4icg6hw.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_429 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_429]
#   %slice_6 : Tensor "f16[128, 396, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_429, 1, 1188, 1584), kwargs = {})
#   %convolution_backward_5 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_6, %getitem_427, %convert_element_type_382, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False]), kwargs = {})
#   return %buf52
triton_poi_fused_convolution_backward_slice_17 = async_compile.triton('triton_poi_fused_convolution_backward_slice_17', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_17', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_17(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (58212 + x2 + 49*y0 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_17.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_17.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ra/cra43eguqvgnvdsibcai5vi5x2qsuwwxacjum7qdjrkm3oze7bcb.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_429 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_429]
#   %slice_5 : Tensor "f16[128, 396, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_429, 1, 792, 1188), kwargs = {})
#   %convolution_backward_6 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_5, %getitem_422, %convert_element_type_381, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False]), kwargs = {})
#   return %buf57
triton_poi_fused_convolution_backward_slice_18 = async_compile.triton('triton_poi_fused_convolution_backward_slice_18', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_18', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_18(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (38808 + x2 + 49*y0 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_18.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_18.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cl/cclk34hpuk65f5cumgvpiwg2oau6ck63opznx7nbm2nhou6su2bt.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_429 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_429]
#   %slice_4 : Tensor "f16[128, 396, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_429, 1, 396, 792), kwargs = {})
#   %convolution_backward_7 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_4, %getitem_417, %convert_element_type_380, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False]), kwargs = {})
#   return %buf62
triton_poi_fused_convolution_backward_slice_19 = async_compile.triton('triton_poi_fused_convolution_backward_slice_19', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_19', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_19(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (19404 + x2 + 49*y0 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_19.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_19.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7f/c7fjwdvajf2y43gldyj2rcsz4sbob5shy3xqev4vk7mxf4ygnmnz.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_429 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_429]
#   %slice_3 : Tensor "f16[128, 396, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_429, 1, 0, 396), kwargs = {})
#   %convolution_backward_8 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_3, %getitem_412, %convert_element_type_379, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False]), kwargs = {})
#   return %buf67
triton_poi_fused_convolution_backward_slice_20 = async_compile.triton('triton_poi_fused_convolution_backward_slice_20', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_20', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_20(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (x2 + 49*y0 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_20.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_20.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w3/cw37xcevjo7mdassrswvvetquaar47zrie6zymxehqcltm6jzgne.py
# Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
#   x_173 => add_287, convert_element_type_376, mul_438, mul_444, sub_54, unsqueeze_216, unsqueeze_217, unsqueeze_218, unsqueeze_219
# Graph fragment:
#   %convolution_145 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convolution_145]
#   %getitem_407 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_407]
#   %rsqrt_54 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_54]
#   %primals_451 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_451]
#   %primals_452 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_452]
#   %getitem_460 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=getitem_460]
#   %getitem_457 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=getitem_457]
#   %getitem_454 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=getitem_454]
#   %getitem_451 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=getitem_451]
#   %convert_element_type_376 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_376]
#   %full_default_2 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=6] = call_function[target=torch.ops.aten.full.default](args = ([128, 1584, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_42 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_460, %getitem_457, %getitem_454, %getitem_451], 1), kwargs = {})
#   %sub_54 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_145, %getitem_407), kwargs = {})
#   %mul_438 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_54, %rsqrt_54), kwargs = {})
#   %unsqueeze_216 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_451, -1), kwargs = {})
#   %unsqueeze_217 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_216, -1), kwargs = {})
#   %mul_444 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_438, %unsqueeze_217), kwargs = {})
#   %unsqueeze_218 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_452, -1), kwargs = {})
#   %unsqueeze_219 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_218, -1), kwargs = {})
#   %add_287 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_444, %unsqueeze_219), kwargs = {})
#   %convert_element_type_376 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_287, torch.float16), kwargs = {})
#   %sigmoid_66 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_376,), kwargs = {})
#   %sub_73 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_2, %sigmoid_66), kwargs = {})
#   %mul_507 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_376, %sub_73), kwargs = {})
#   %add_307 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_507, 1), kwargs = {})
#   %mul_508 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_66, %add_307), kwargs = {})
#   %mul_509 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_42, %mul_508), kwargs = {})
#   return %convert_element_type_376,%mul_509
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 39739392, 'x': 99373824}, 'kernel_num_gb': 0.059634432, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 49)
    y3 = yindex // 49
    tmp0 = tl.load(in_ptr0 + (x1 + 1584*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x1
    tmp12 = tl.full([1, 1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1, 1], 396, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (396*y0 + (x1)), tmp15 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1, 1], 792, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (396*y0 + ((-396) + x1)), tmp20 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1, 1], 1188, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tmp22 & tmp24
    tmp26 = tl.load(in_ptr7 + (396*y0 + ((-792) + x1)), tmp25 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tmp11 >= tmp23
    tmp28 = tl.full([1, 1], 1584, tl.int64)
    tmp29 = tmp11 < tmp28
    tmp30 = tl.load(in_ptr8 + (396*y0 + ((-1188) + x1)), tmp27 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tl.where(tmp25, tmp26, tmp30)
    tmp32 = tl.where(tmp20, tmp21, tmp31)
    tmp33 = tl.where(tmp15, tmp16, tmp32)
    tmp34 = tl.sigmoid(tmp10)
    tmp35 = 1.0
    tmp36 = tmp35 - tmp34
    tmp37 = tmp10 * tmp36
    tmp38 = tmp37 + tmp35
    tmp39 = tmp34 * tmp38
    tmp40 = tmp33 * tmp39
    tl.store(out_ptr1 + (y2 + 49*x1 + 77616*y3), tmp40, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.059634432
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kz/ckzl72e4esw65fkk5glxezut6hqxocohtmfqovcqzu6inlrmhbd2.py
# Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_173 => convert_element_type_375, squeeze_162
# Graph fragment:
#   %mul_509 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=mul_509]
#   %convolution_145 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convolution_145]
#   %getitem_407 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_407]
#   %convert_element_type_434 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_509, torch.float32), kwargs = {})
#   %squeeze_162 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_407, [0, 2, 3]), kwargs = {})
#   %unsqueeze_268 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_162, 0), kwargs = {})
#   %unsqueeze_269 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_268, 2), kwargs = {})
#   %unsqueeze_270 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_269, 3), kwargs = {})
#   %convert_element_type_375 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_145, torch.float32), kwargs = {})
#   %sub_74 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_375, %unsqueeze_270), kwargs = {})
#   %mul_510 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_434, %sub_74), kwargs = {})
#   %sum_12 : Tensor "f32[1584][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_510, [0, 2, 3]), kwargs = {})
#   return %buf75
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20496960, 'r0_': 19869696}, 'kernel_num_gb': 0.040056192, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 77616
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 1584)
    x1 = xindex // 1584
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (49*x0 + 77616*((r0_2 + 128*x1) // 49) + (((r0_2 + 128*x1) % 49))), r0_mask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (x0 + 1584*r0_2 + 202752*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp5 = tmp3 - tmp4
        tmp6 = tmp1 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x0 + 1600*x1), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584, 49), (1, 1600), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 77616, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.040056192
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yx/cyxld52eqlqfqlvx4c2ziokvkpjepbtw2rtlpmdeihz4tqldm437.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_509 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=mul_509]
#   %convert_element_type_434 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_509, torch.float32), kwargs = {})
#   %sum_11 : Tensor "f32[1584][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_434, [0, 2, 3]), kwargs = {})
#   return %sum_11
triton_red_fused_native_batch_norm_backward_23 = async_compile.triton('triton_red_fused_native_batch_norm_backward_23', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 2048, 'r0_': 8192},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_23', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12672, 'r0_': 19869696}, 'kernel_num_gb': 0.019876032, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_23(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1584
    r0_numel = 6272
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 49)
        r0_2 = r0_index // 49
        tmp0 = tl.load(in_ptr0 + (r0_1 + 49*x0 + 77616*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1584, 6272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_23.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_23.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019876032
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zu/czulxdu7dbo4kydq5qt7i642mffehur5x5uzkltan5pyom6gwsvw.py
# Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_173 => convert_element_type_375, squeeze_162, squeeze_163
# Graph fragment:
#   %mul_509 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0" = PlaceHolder[target=mul_509]
#   %convolution_145 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convolution_145]
#   %getitem_407 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_407]
#   %sum_12 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=sum_12]
#   %rsqrt_54 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_54]
#   %sum_11 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=sum_11]
#   %primals_451 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_451]
#   %convert_element_type_434 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_509, torch.float32), kwargs = {})
#   %squeeze_162 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_407, [0, 2, 3]), kwargs = {})
#   %unsqueeze_268 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_162, 0), kwargs = {})
#   %unsqueeze_269 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_268, 2), kwargs = {})
#   %unsqueeze_270 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_269, 3), kwargs = {})
#   %convert_element_type_375 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_145, torch.float32), kwargs = {})
#   %sub_74 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_375, %unsqueeze_270), kwargs = {})
#   %mul_511 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_11, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_271 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_511, 0), kwargs = {})
#   %unsqueeze_272 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_271, 2), kwargs = {})
#   %unsqueeze_273 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_272, 3), kwargs = {})
#   %mul_512 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_12, 0.00015943877551020407), kwargs = {})
#   %squeeze_163 : Tensor "f32[1584][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_54, [0, 2, 3]), kwargs = {})
#   %mul_513 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_163, %squeeze_163), kwargs = {})
#   %mul_514 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_512, %mul_513), kwargs = {})
#   %unsqueeze_274 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_514, 0), kwargs = {})
#   %unsqueeze_275 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_274, 2), kwargs = {})
#   %unsqueeze_276 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_275, 3), kwargs = {})
#   %mul_515 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_163, %primals_451), kwargs = {})
#   %unsqueeze_277 : Tensor "f32[1, 1584][1584, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_515, 0), kwargs = {})
#   %unsqueeze_278 : Tensor "f32[1, 1584, 1][1584, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_277, 2), kwargs = {})
#   %unsqueeze_279 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_278, 3), kwargs = {})
#   %mul_516 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_74, %unsqueeze_276), kwargs = {})
#   %sub_76 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_434, %mul_516), kwargs = {})
#   %sub_77 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_76, %unsqueeze_273), kwargs = {})
#   %mul_517 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_77, %unsqueeze_279), kwargs = {})
#   %convert_element_type_436 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_517, torch.float16), kwargs = {})
#   %convolution_backward_9 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_436, %add_282, %convert_element_type_374, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf78
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 19869696, 'x': 59640768}, 'kernel_num_gb': 0.059640768, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 49)
    y1 = yindex // 49
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 49*x2 + 77616*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2 + 1584*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 0.00015943877551020407
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + 1584*y3), tmp20, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.059640768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sj/csj5ap5eg4kwld4lm5is5btlz3uliqqga7tnt3cf2oe5yrm474wk.py
# Topologically Sorted Source Nodes: [x_170], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_170 => convert_element_type_372
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %cat_38 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_38]
#   %unsqueeze_282 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_282]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %convert_element_type_438 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_308, torch.float32), kwargs = {})
#   %sum_13 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_438, [0, 2, 3]), kwargs = {})
#   %convert_element_type_372 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_38, torch.float32), kwargs = {})
#   %sub_78 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_372, %unsqueeze_282), kwargs = {})
#   %mul_519 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_438, %sub_78), kwargs = {})
#   %sum_14 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_519, [0, 2, 3]), kwargs = {})
#   return %buf83,%buf85
triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10142880, 'r0_': 0}, 'kernel_num_gb': 0.010039392, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 12936
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 264)
    x1 = xindex // 264
    _tmp5 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp9 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp7 = tl.load(in_ptr2 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, R0_BLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(r0_mask & xmask, tmp6, _tmp5)
        tmp8 = tmp7.to(tl.float32)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp3 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, R0_BLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(r0_mask & xmask, tmp14, _tmp13)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tl.store(out_ptr1 + (x3), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 12936, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.010039392
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/t7/ct7bt25hvqppue2m4tyiscrphsvmvz5nzcpylnxzwtuus3f5tcqe.py
# Topologically Sorted Source Nodes: [x_170], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_170 => convert_element_type_372
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %cat_38 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_38]
#   %unsqueeze_282 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_282]
#   %sum_14 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_14]
#   %squeeze_160 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=squeeze_160]
#   %sum_13 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_13]
#   %primals_445 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_445]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %convert_element_type_438 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_308, torch.float32), kwargs = {})
#   %convert_element_type_372 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_38, torch.float32), kwargs = {})
#   %sub_78 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_372, %unsqueeze_282), kwargs = {})
#   %mul_520 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_13, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_283 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_520, 0), kwargs = {})
#   %unsqueeze_284 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_283, 2), kwargs = {})
#   %unsqueeze_285 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_284, 3), kwargs = {})
#   %mul_521 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_14, 0.00015943877551020407), kwargs = {})
#   %mul_522 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_160, %squeeze_160), kwargs = {})
#   %mul_523 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_521, %mul_522), kwargs = {})
#   %unsqueeze_286 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_523, 0), kwargs = {})
#   %unsqueeze_287 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_286, 2), kwargs = {})
#   %unsqueeze_288 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_287, 3), kwargs = {})
#   %mul_524 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_160, %primals_445), kwargs = {})
#   %unsqueeze_289 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_524, 0), kwargs = {})
#   %unsqueeze_290 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_289, 2), kwargs = {})
#   %unsqueeze_291 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_290, 3), kwargs = {})
#   %mul_525 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_78, %unsqueeze_288), kwargs = {})
#   %sub_80 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_438, %mul_525), kwargs = {})
#   %sub_81 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_80, %unsqueeze_285), kwargs = {})
#   %mul_526 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_81, %unsqueeze_291), kwargs = {})
#   %convert_element_type_440 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_526, torch.float16), kwargs = {})
#   return %convert_element_type_440
triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16563360}, 'kernel_num_gb': 0.013251744, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2), xmask).to(tl.float32)
    tmp4 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp6 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tmp7 = tmp5 - tmp6
    tmp9 = 0.00015943877551020407
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp3 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tmp22 = tmp21.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp22, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.013251744
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wd/cwdttwdpym42i33jtul4frhg4v7xnh6gflk4g4edwbuxiew52smt.py
# Topologically Sorted Source Nodes: [x_160], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_160 => convert_element_type_349
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %getitem_490 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_490]
#   %cat_36 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_36]
#   %unsqueeze_318 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_318]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %add_313 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_308, %getitem_490), kwargs = {})
#   %convert_element_type_461 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_313, torch.float32), kwargs = {})
#   %sum_22 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_461, [0, 2, 3]), kwargs = {})
#   %convert_element_type_349 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_36, torch.float32), kwargs = {})
#   %sub_94 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_349, %unsqueeze_318), kwargs = {})
#   %mul_559 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_461, %sub_94), kwargs = {})
#   %sum_23 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_559, [0, 2, 3]), kwargs = {})
#   return %buf150,%buf152
triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13454496, 'r0_': 0}, 'kernel_num_gb': 0.013351008, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 12936
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 264)
    x1 = xindex // 264
    _tmp7 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp11 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp3 = tl.load(in_ptr2 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr3 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask & xmask, tmp8, _tmp7)
        tmp10 = tmp9.to(tl.float32)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp5 * tmp12
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask & xmask, tmp16, _tmp15)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
    tl.store(out_ptr1 + (x3), tmp15, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 12936, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.013351008
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cv/ccvrkg6yjawndwrqr3cgo4sokzxvic2se2d6eaowu4zslldqsz6z.py
# Topologically Sorted Source Nodes: [x_160], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_160 => convert_element_type_349
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %getitem_490 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_490]
#   %cat_36 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_36]
#   %unsqueeze_318 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_318]
#   %sum_23 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_23]
#   %squeeze_151 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=squeeze_151]
#   %sum_22 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_22]
#   %primals_419 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_419]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %add_313 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_308, %getitem_490), kwargs = {})
#   %convert_element_type_461 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_313, torch.float32), kwargs = {})
#   %convert_element_type_349 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_36, torch.float32), kwargs = {})
#   %sub_94 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_349, %unsqueeze_318), kwargs = {})
#   %mul_560 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_22, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_319 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_560, 0), kwargs = {})
#   %unsqueeze_320 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_319, 2), kwargs = {})
#   %unsqueeze_321 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_320, 3), kwargs = {})
#   %mul_561 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_23, 0.00015943877551020407), kwargs = {})
#   %mul_562 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_151, %squeeze_151), kwargs = {})
#   %mul_563 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_561, %mul_562), kwargs = {})
#   %unsqueeze_322 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_563, 0), kwargs = {})
#   %unsqueeze_323 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_322, 2), kwargs = {})
#   %unsqueeze_324 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_323, 3), kwargs = {})
#   %mul_564 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_151, %primals_419), kwargs = {})
#   %unsqueeze_325 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_564, 0), kwargs = {})
#   %unsqueeze_326 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_325, 2), kwargs = {})
#   %unsqueeze_327 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_326, 3), kwargs = {})
#   %mul_565 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_94, %unsqueeze_324), kwargs = {})
#   %sub_96 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_461, %mul_565), kwargs = {})
#   %sub_97 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_96, %unsqueeze_321), kwargs = {})
#   %mul_566 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_97, %unsqueeze_327), kwargs = {})
#   return %mul_566
triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 26498208}, 'kernel_num_gb': 0.019874976, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2), xmask).to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (x2), xmask).to(tl.float32)
    tmp6 = tl.load(in_ptr3 + (x2), xmask).to(tl.float32)
    tmp8 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x0), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp5 = tmp4.to(tl.float32)
    tmp7 = tmp6.to(tl.float32)
    tmp9 = tmp7 - tmp8
    tmp11 = 0.00015943877551020407
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp5 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tl.store(out_ptr0 + (x2), tmp23, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019874976
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/r7/cr7evf3lfjir3rjxqkjgm25donlym3loj2xzpr74hmkuipcrxdu4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_566 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=mul_566]
#   %convert_element_type_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_566, torch.float16), kwargs = {})
#   %slice_14 : Tensor "f16[128, 132, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_463, 1, 132, 264), kwargs = {})
#   %convolution_backward_19 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_14, %getitem_375, %convert_element_type_348, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf156
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1048576}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6623232}, 'kernel_num_gb': 0.004967424, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 827904
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 132)
    x1 = xindex // 132
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (132 + x0 + 264*x1), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 132, 7, 7), (6468, 1, 924, 132), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 827904,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.004967424
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/n2/cn2sahivrpinkfhziok4zqlisywvukigqxg4fcp2kocg6i2sk2xr.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_566 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=mul_566]
#   %convert_element_type_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_566, torch.float16), kwargs = {})
#   %slice_13 : Tensor "f16[128, 132, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_463, 1, 0, 132), kwargs = {})
#   %convolution_backward_20 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_13, %getitem_374, %convert_element_type_347, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf161
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1048576}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6623232}, 'kernel_num_gb': 0.004967424, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 827904
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 132)
    x1 = xindex // 132
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 264*x1), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 132, 7, 7), (6468, 1, 924, 132), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 827904,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.004967424
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vq/cvq66u3tuqnixbtrsradv5jejzap333ka6htcupk73dipzqnkg3t.py
# Topologically Sorted Source Nodes: [x_157, sigmoid_13], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_13 => sigmoid_55
#   x_157 => convert_element_type_339, convert_element_type_340, mul_403, sigmoid_53
# Graph fragment:
#   %getitem_496 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_496]
#   %getitem_493 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_493]
#   %convert_element_type_338 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_338]
#   %sum_24 : Tensor "f16[128, 1584, 1, 1][1584, 1, 202752, 202752]cuda:0" = PlaceHolder[target=sum_24]
#   %convolution_133 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=convolution_133]
#   %cat_45 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_496, %getitem_493], 1), kwargs = {})
#   %convert_element_type_339 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_338, torch.float32), kwargs = {})
#   %sigmoid_53 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_339,), kwargs = {})
#   %mul_403 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_339, %sigmoid_53), kwargs = {})
#   %convert_element_type_340 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_403, torch.float16), kwargs = {})
#   %mul_568 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_45, %convert_element_type_340), kwargs = {})
#   %sigmoid_55 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_133,), kwargs = {})
#   %sum_24 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_568, [2, 3], True), kwargs = {})
#   %convert_element_type_466 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_24, torch.float32), kwargs = {})
#   %convert_element_type_467 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_55, torch.float32), kwargs = {})
#   %sub_98 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_467), kwargs = {})
#   %mul_570 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_467, %sub_98), kwargs = {})
#   %mul_571 : Tensor "f32[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_466, %mul_570), kwargs = {})
#   %convert_element_type_468 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_571, torch.float16), kwargs = {})
#   return %sum_24,%convert_element_type_468
triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31 = async_compile.triton('triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 262144, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 60825600, 'r0_': 0}, 'kernel_num_gb': 0.040955904, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 202752
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    x0 = (xindex % 1584)
    r0_2 = r0_index
    x1 = xindex // 1584
    x3 = xindex
    tmp11 = tl.load(in_ptr2 + (x0 + 1584*r0_2 + 77616*x1), r0_mask & xmask, other=0.0).to(tl.float32)
    tmp22 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x0
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 792, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (792*r0_2 + 38808*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 1584, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (792*r0_2 + 38808*x1 + ((-792) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tl.sigmoid(tmp12)
    tmp14 = tmp12 * tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp16 = tmp10 * tmp15
    tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
    tmp19 = tl.where(r0_mask & xmask, tmp17, 0)
    tmp20 = tl.sum(tmp19, 1)[:, None].to(tl.float32)
    tmp21 = tmp20.to(tl.float32)
    tmp23 = tl.sigmoid(tmp22)
    tmp24 = tmp23.to(tl.float32)
    tmp25 = 1.0
    tmp26 = tmp25 - tmp24
    tmp27 = tmp24 * tmp26
    tmp28 = tmp21 * tmp27
    tmp29 = tmp28.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp29, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 202752, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.040955904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2s/c2soi4tvh4gb62756xwuucirqg3yf4tlhygc5lq4ngbktj2gbgaa.py
# Topologically Sorted Source Nodes: [sigmoid_13], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_13 => sigmoid_55
# Graph fragment:
#   %getitem_496 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_496]
#   %getitem_493 : Tensor "f16[128, 792, 7, 7][38808, 1, 5544, 792]cuda:0" = PlaceHolder[target=getitem_493]
#   %convolution_133 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=convolution_133]
#   %getitem_502 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_502]
#   %convert_element_type_338 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_338]
#   %full_default_2 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=6] = call_function[target=torch.ops.aten.full.default](args = ([128, 1584, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_45 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_496, %getitem_493], 1), kwargs = {})
#   %sigmoid_55 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_133,), kwargs = {})
#   %mul_569 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_45, %sigmoid_55), kwargs = {})
#   %expand_3 : Tensor "f16[128, 1584, 7, 7][1584, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_502, [128, 1584, 7, 7]), kwargs = {})
#   %div_3 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_3, 49), kwargs = {})
#   %add_315 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_569, %div_3), kwargs = {})
#   %sigmoid_71 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_338,), kwargs = {})
#   %sub_100 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_2, %sigmoid_71), kwargs = {})
#   %mul_575 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_338, %sub_100), kwargs = {})
#   %add_316 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_575, 1), kwargs = {})
#   %mul_576 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_71, %add_316), kwargs = {})
#   %mul_577 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_315, %mul_576), kwargs = {})
#   %convert_element_type_473 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_577, torch.float32), kwargs = {})
#   return %convert_element_type_473
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 79478784, 'x': 60420096}, 'kernel_num_gb': 0.080289792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 49
    y0 = (yindex % 49)
    tmp11 = tl.load(in_ptr2 + (x2 + 1584*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 1584*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 1584*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 792, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (792*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 1584, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (792*y3 + ((-792) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.02040816326530612
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 49*x2 + 77616*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 1584, 7, 7), (77616, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.080289792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hq/chqbugjjrw7racpn42mahcjo6iyqw7zrywqbexof3mq6vcepi7fh.py
# Topologically Sorted Source Nodes: [x_151], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_151 => convert_element_type_326
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %getitem_490 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_490]
#   %getitem_517 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_517]
#   %convolution_126 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=convolution_126]
#   %unsqueeze_354 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_354]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %add_313 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_308, %getitem_490), kwargs = {})
#   %add_318 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_313, %getitem_517), kwargs = {})
#   %convert_element_type_484 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_318, torch.float32), kwargs = {})
#   %sum_31 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_484, [0, 2, 3]), kwargs = {})
#   %convert_element_type_326 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_126, torch.float32), kwargs = {})
#   %sub_110 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_326, %unsqueeze_354), kwargs = {})
#   %mul_599 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_484, %sub_110), kwargs = {})
#   %sum_32 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_599, [0, 2, 3]), kwargs = {})
#   return %buf219,%buf221
triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16766112, 'r0_': 0}, 'kernel_num_gb': 0.016662624, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 12936
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 264)
    x1 = xindex // 264
    _tmp9 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp13 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp17 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp3 = tl.load(in_ptr2 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr3 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp11 = tl.load(in_ptr4 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 + tmp5
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, R0_BLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(r0_mask & xmask, tmp10, _tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp14 = tmp12 - tmp13
        tmp15 = tmp7 * tmp14
        tmp16 = tl.broadcast_to(tmp15, [XBLOCK, R0_BLOCK])
        tmp18 = _tmp17 + tmp16
        _tmp17 = tl.where(r0_mask & xmask, tmp18, _tmp17)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tmp17 = tl.sum(_tmp17, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
    tl.store(out_ptr1 + (x3), tmp17, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((264, 49), (1, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 12936, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016662624
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zl/czltmkgoj6f3atl6imhx4jepd4bp3b6ogyf5wasrb4cjnvd3mlbx.py
# Topologically Sorted Source Nodes: [x_151], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_151 => convert_element_type_326
# Graph fragment:
#   %getitem_436 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_436]
#   %getitem_463 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_463]
#   %getitem_490 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_490]
#   %getitem_517 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=getitem_517]
#   %convolution_126 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=convolution_126]
#   %unsqueeze_354 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_354]
#   %sum_32 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_32]
#   %squeeze_142 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=squeeze_142]
#   %sum_31 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=sum_31]
#   %sub_113 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=sub_113]
#   %primals_393 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_393]
#   %add_308 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_436, %getitem_463), kwargs = {})
#   %add_313 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_308, %getitem_490), kwargs = {})
#   %add_318 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_313, %getitem_517), kwargs = {})
#   %convert_element_type_484 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_318, torch.float32), kwargs = {})
#   %convert_element_type_326 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_126, torch.float32), kwargs = {})
#   %sub_110 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_326, %unsqueeze_354), kwargs = {})
#   %mul_600 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_31, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_355 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_600, 0), kwargs = {})
#   %unsqueeze_356 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_355, 2), kwargs = {})
#   %unsqueeze_357 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_356, 3), kwargs = {})
#   %mul_601 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_32, 0.00015943877551020407), kwargs = {})
#   %mul_602 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_142, %squeeze_142), kwargs = {})
#   %mul_603 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_601, %mul_602), kwargs = {})
#   %unsqueeze_358 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_603, 0), kwargs = {})
#   %unsqueeze_359 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_358, 2), kwargs = {})
#   %unsqueeze_360 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_359, 3), kwargs = {})
#   %mul_604 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_142, %primals_393), kwargs = {})
#   %unsqueeze_361 : Tensor "f32[1, 264][264, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_604, 0), kwargs = {})
#   %unsqueeze_362 : Tensor "f32[1, 264, 1][264, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_361, 2), kwargs = {})
#   %unsqueeze_363 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_362, 3), kwargs = {})
#   %mul_605 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_110, %unsqueeze_360), kwargs = {})
#   %sub_112 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_484, %mul_605), kwargs = {})
#   %sub_113 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_112, %unsqueeze_357), kwargs = {})
#   %mul_606 : Tensor "f32[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_113, %unsqueeze_363), kwargs = {})
#   %convert_element_type_486 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_606, torch.float16), kwargs = {})
#   %convolution_backward_28 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_486, %mul_380, %convert_element_type_325, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %sub_113,%buf225
triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'in_ptr9': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 23186592}, 'kernel_num_gb': 0.019874976, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2), xmask).to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (x2), xmask).to(tl.float32)
    tmp5 = tl.load(in_ptr3 + (x2), xmask).to(tl.float32)
    tmp8 = tl.load(in_ptr4 + (x2), xmask).to(tl.float32)
    tmp10 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr7 + (x0), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x0), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr9 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp7 = tmp6.to(tl.float32)
    tmp9 = tmp8.to(tl.float32)
    tmp11 = tmp9 - tmp10
    tmp13 = 0.00015943877551020407
    tmp14 = tmp12 * tmp13
    tmp16 = tmp15 * tmp15
    tmp17 = tmp14 * tmp16
    tmp18 = tmp11 * tmp17
    tmp19 = tmp7 - tmp18
    tmp21 = tmp20 * tmp13
    tmp22 = tmp19 - tmp21
    tmp24 = tmp15 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp26, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_10 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019874976
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uv/cuvp74mzmg2lcy7tl6grdakoxsql5qqyom2djkcjqttq7bo2a4yp.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_147 => add_245, convert_element_type_316, mul_371, mul_377, sub_46, unsqueeze_184, unsqueeze_185, unsqueeze_186, unsqueeze_187
# Graph fragment:
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=cat_34]
#   %getitem_347 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_347]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %primals_384 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_384]
#   %sub_46 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_34, %getitem_347), kwargs = {})
#   %mul_371 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_46, %rsqrt_46), kwargs = {})
#   %unsqueeze_184 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_383, -1), kwargs = {})
#   %unsqueeze_185 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_184, -1), kwargs = {})
#   %mul_377 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_371, %unsqueeze_185), kwargs = {})
#   %unsqueeze_186 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_384, -1), kwargs = {})
#   %unsqueeze_187 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_186, -1), kwargs = {})
#   %add_245 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_377, %unsqueeze_187), kwargs = {})
#   %convert_element_type_316 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_245, torch.float16), kwargs = {})
#   return %convert_element_type_316
triton_poi_fused__native_batch_norm_legit_functional_35 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_35', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_35', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 36142080}, 'kernel_num_gb': 0.02409984, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_35(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 960)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_35.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_35.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02409984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pm/cpmgsjnowlhv2kljdrfkrkyezccayrfiojrlznogqvatik3rxlz4.py
# Topologically Sorted Source Nodes: [x_148, sigmoid_12], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
#   x_148 => convert_element_type_317, convert_element_type_318, mul_378, sigmoid_49
# Graph fragment:
#   %getitem_520 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=getitem_520]
#   %convert_element_type_316 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=convert_element_type_316]
#   %sum_33 : Tensor "f16[128, 960, 1, 1][960, 1, 122880, 122880]cuda:0" = PlaceHolder[target=sum_33]
#   %convolution_125 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=convolution_125]
#   %convert_element_type_317 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_316, torch.float32), kwargs = {})
#   %sigmoid_49 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_317,), kwargs = {})
#   %mul_378 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_317, %sigmoid_49), kwargs = {})
#   %convert_element_type_318 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_378, torch.float16), kwargs = {})
#   %mul_608 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_520, %convert_element_type_318), kwargs = {})
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %sum_33 : Tensor "f16[128, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_608, [2, 3], True), kwargs = {})
#   %convert_element_type_488 : Tensor "f32[128, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_33, torch.float32), kwargs = {})
#   %convert_element_type_489 : Tensor "f32[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_51, torch.float32), kwargs = {})
#   %sub_114 : Tensor "f32[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_489), kwargs = {})
#   %mul_610 : Tensor "f32[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_489, %sub_114), kwargs = {})
#   %mul_611 : Tensor "f32[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_488, %mul_610), kwargs = {})
#   %convert_element_type_490 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_611, torch.float16), kwargs = {})
#   return %sum_33,%convert_element_type_490
triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36 = async_compile.triton('triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 131072, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 24821760, 'r0_': 0}, 'kernel_num_gb': 0.02482176, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_2 = r0_index
    x0 = (xindex % 960)
    x1 = xindex // 960
    x3 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_2 + 47040*x1), r0_mask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0 + 960*r0_2 + 47040*x1), r0_mask, other=0.0).to(tl.float32)
    tmp12 = tl.load(in_ptr2 + (x3), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tl.sigmoid(tmp2)
    tmp4 = tmp2 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tmp6 = tmp0 * tmp5
    tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
    tmp9 = tl.where(r0_mask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None].to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp13 = tl.sigmoid(tmp12)
    tmp14 = tmp13.to(tl.float32)
    tmp15 = 1.0
    tmp16 = tmp15 - tmp14
    tmp17 = tmp14 * tmp16
    tmp18 = tmp11 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp19, None)


def get_args():
    arg_0 = rand_strided((128, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 122880, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02482176
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/de/cdebcgssmilzbqr7jn4wjsa7yxcbg2jxbfmf2df6bzolmvi5hnfr.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_523 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=getitem_523]
#   %convolution_124 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=convolution_124]
#   %sigmoid_73 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_124,), kwargs = {})
#   %full_default_10 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=4] = call_function[target=torch.ops.aten.full.default](args = ([128, 80, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_115 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_10, %sigmoid_73), kwargs = {})
#   %mul_612 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_124, %sub_115), kwargs = {})
#   %add_319 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_612, 1), kwargs = {})
#   %mul_613 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_73, %add_319), kwargs = {})
#   %mul_614 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_523, %mul_613), kwargs = {})
#   return %mul_614
triton_poi_fused_add_fill_mul_sigmoid_sub_37 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_37', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_37', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 81920}, 'kernel_num_gb': 6.144e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_37(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 10240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 10240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_37.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_37.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.144e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/z5/cz55l64zo63lvm4vizzoi6mrdezvyu2q7lxlqtrg2vi3a3mh2dzx.py
# Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
#   x_147 => convert_element_type_315, squeeze_138
# Graph fragment:
#   %getitem_520 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=getitem_520]
#   %convolution_125 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=convolution_125]
#   %getitem_526 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_526]
#   %convert_element_type_316 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=convert_element_type_316]
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=cat_34]
#   %getitem_347 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_347]
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %mul_609 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_520, %sigmoid_51), kwargs = {})
#   %expand_4 : Tensor "f16[128, 960, 7, 7][960, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_526, [128, 960, 7, 7]), kwargs = {})
#   %div_4 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_4, 49), kwargs = {})
#   %add_320 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_609, %div_4), kwargs = {})
#   %sigmoid_74 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_316,), kwargs = {})
#   %full_default_11 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 960, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_116 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_11, %sigmoid_74), kwargs = {})
#   %mul_615 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_316, %sub_116), kwargs = {})
#   %add_321 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_615, 1), kwargs = {})
#   %mul_616 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_74, %add_321), kwargs = {})
#   %mul_617 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_320, %mul_616), kwargs = {})
#   %convert_element_type_495 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_617, torch.float32), kwargs = {})
#   %squeeze_138 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_347, [0, 2, 3]), kwargs = {})
#   %unsqueeze_364 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_138, 0), kwargs = {})
#   %unsqueeze_365 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_364, 2), kwargs = {})
#   %unsqueeze_366 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_365, 3), kwargs = {})
#   %sum_36 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_495, [0, 2, 3]), kwargs = {})
#   %convert_element_type_315 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_34, torch.float32), kwargs = {})
#   %sub_117 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_315, %unsqueeze_366), kwargs = {})
#   %mul_618 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_495, %sub_117), kwargs = {})
#   %sum_37 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_618, [0, 2, 3]), kwargs = {})
#   return %buf246,%buf248
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 60967680, 'r0_': 0}, 'kernel_num_gb': 0.0369984, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 47040
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 960)
    x1 = xindex // 960
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp22 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp26 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_2 + 122880*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 960*((r0_2 + 128*x1) // 49)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp4 = tl.load(in_ptr2 + (x0 + 960*((r0_2 + 128*x1) // 49)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp8 = tl.load(in_ptr3 + (x0 + 960*r0_2 + 122880*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp20 = tl.load(in_ptr4 + (x0 + 960*r0_2 + 122880*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = tmp0 * tmp2
        tmp5 = 0.02040816326530612
        tmp6 = tmp4 * tmp5
        tmp7 = tmp3 + tmp6
        tmp9 = tl.sigmoid(tmp8)
        tmp10 = 1.0
        tmp11 = tmp10 - tmp9
        tmp12 = tmp8 * tmp11
        tmp13 = tmp12 + tmp10
        tmp14 = tmp9 * tmp13
        tmp15 = tmp7 * tmp14
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
        tmp21 = tmp20.to(tl.float32)
        tmp23 = tmp21 - tmp22
        tmp24 = tmp16 * tmp23
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, R0_BLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(r0_mask & xmask, tmp27, _tmp26)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp18, xmask)
    tl.store(out_ptr1 + (x3), tmp26, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((960, 49), (1, 960), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((960, 49), (1, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 47040, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0369984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/a7/ca7q3xt3dono5n23fqzsf3ajorggwsxidi7htp3mmm2sjpouc5de.py
# Topologically Sorted Source Nodes: [sigmoid_12], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
# Graph fragment:
#   %buf246 : Tensor "f32[960, 49][1, 960]cuda:0" = PlaceHolder[target=buf246]
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %mul_609 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_520, %sigmoid_51), kwargs = {})
#   %expand_4 : Tensor "f16[128, 960, 7, 7][960, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_526, [128, 960, 7, 7]), kwargs = {})
#   %div_4 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_4, 49), kwargs = {})
#   %add_320 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_609, %div_4), kwargs = {})
#   %sigmoid_74 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_316,), kwargs = {})
#   %full_default_11 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 960, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_116 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_11, %sigmoid_74), kwargs = {})
#   %mul_615 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_316, %sub_116), kwargs = {})
#   %add_321 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_615, 1), kwargs = {})
#   %mul_616 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_74, %add_321), kwargs = {})
#   %mul_617 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_320, %mul_616), kwargs = {})
#   %convert_element_type_495 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_617, torch.float32), kwargs = {})
#   %sum_36 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_495, [0, 2, 3]), kwargs = {})
#   return %sum_36
triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39 = async_compile.triton('triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 195840, 'r0_': 0}, 'kernel_num_gb': 0.000192, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((960, 49), (1, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 960, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000192
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ux/cuxyrzle5ixofxnpgtjx3gjubczkl44pwvcfyznbrodyapgtbfh4.py
# Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
#   x_147 => convert_element_type_315, squeeze_138, squeeze_139
# Graph fragment:
#   %buf248 : Tensor "f32[960, 49][1, 960]cuda:0" = PlaceHolder[target=buf248]
#   %sum_37 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_37]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %mul_609 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_520, %sigmoid_51), kwargs = {})
#   %expand_4 : Tensor "f16[128, 960, 7, 7][960, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_526, [128, 960, 7, 7]), kwargs = {})
#   %div_4 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_4, 49), kwargs = {})
#   %add_320 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_609, %div_4), kwargs = {})
#   %sigmoid_74 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_316,), kwargs = {})
#   %full_default_11 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 960, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_116 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_11, %sigmoid_74), kwargs = {})
#   %mul_615 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_316, %sub_116), kwargs = {})
#   %add_321 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_615, 1), kwargs = {})
#   %mul_616 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_74, %add_321), kwargs = {})
#   %mul_617 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_320, %mul_616), kwargs = {})
#   %convert_element_type_495 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_617, torch.float32), kwargs = {})
#   %squeeze_138 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_347, [0, 2, 3]), kwargs = {})
#   %unsqueeze_364 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_138, 0), kwargs = {})
#   %unsqueeze_365 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_364, 2), kwargs = {})
#   %unsqueeze_366 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_365, 3), kwargs = {})
#   %convert_element_type_315 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_34, torch.float32), kwargs = {})
#   %sub_117 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_315, %unsqueeze_366), kwargs = {})
#   %mul_618 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_495, %sub_117), kwargs = {})
#   %sum_37 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_618, [0, 2, 3]), kwargs = {})
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_626 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_37, %squeeze_139), kwargs = {})
#   return %sum_37,%mul_626
triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 207360, 'r0_': 0}, 'kernel_num_gb': 0.00019968, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((960, 49), (1, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 960, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00019968
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bc/cbcd74qwulmf2g5xl3ygfrkvtx5sqisxflwehfibofpi3whgh3i5.py
# Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
#   x_147 => convert_element_type_315, squeeze_138, squeeze_139
# Graph fragment:
#   %getitem_520 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=getitem_520]
#   %convolution_125 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=convolution_125]
#   %getitem_526 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_526]
#   %convert_element_type_316 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=convert_element_type_316]
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=cat_34]
#   %getitem_347 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_347]
#   %sum_37 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_37]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %sum_36 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_36]
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %mul_609 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_520, %sigmoid_51), kwargs = {})
#   %expand_4 : Tensor "f16[128, 960, 7, 7][960, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_526, [128, 960, 7, 7]), kwargs = {})
#   %div_4 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_4, 49), kwargs = {})
#   %add_320 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_609, %div_4), kwargs = {})
#   %sigmoid_74 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_316,), kwargs = {})
#   %full_default_11 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 960, 7, 7], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_116 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_11, %sigmoid_74), kwargs = {})
#   %mul_615 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_316, %sub_116), kwargs = {})
#   %add_321 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_615, 1), kwargs = {})
#   %mul_616 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_74, %add_321), kwargs = {})
#   %mul_617 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_320, %mul_616), kwargs = {})
#   %convert_element_type_495 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_617, torch.float32), kwargs = {})
#   %squeeze_138 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_347, [0, 2, 3]), kwargs = {})
#   %unsqueeze_364 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_138, 0), kwargs = {})
#   %unsqueeze_365 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_364, 2), kwargs = {})
#   %unsqueeze_366 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_365, 3), kwargs = {})
#   %convert_element_type_315 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_34, torch.float32), kwargs = {})
#   %sub_117 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_315, %unsqueeze_366), kwargs = {})
#   %mul_619 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_36, 0.00015943877551020407), kwargs = {})
#   %unsqueeze_367 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_619, 0), kwargs = {})
#   %unsqueeze_368 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_367, 2), kwargs = {})
#   %unsqueeze_369 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_368, 3), kwargs = {})
#   %mul_620 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_37, 0.00015943877551020407), kwargs = {})
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_621 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_139, %squeeze_139), kwargs = {})
#   %mul_622 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_620, %mul_621), kwargs = {})
#   %unsqueeze_370 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_622, 0), kwargs = {})
#   %unsqueeze_371 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_370, 2), kwargs = {})
#   %unsqueeze_372 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_371, 3), kwargs = {})
#   %mul_624 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_117, %unsqueeze_372), kwargs = {})
#   %sub_119 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_495, %mul_624), kwargs = {})
#   %sub_120 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_119, %unsqueeze_369), kwargs = {})
#   return %sub_120
triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 84802560}, 'kernel_num_gb': 0.06071808, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 960)
    x2 = xindex // 47040
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0 + 960*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr2 + (x0 + 960*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tl.load(in_ptr3 + (x3), None).to(tl.float32)
    tmp17 = tl.load(in_ptr4 + (x3), None).to(tl.float32)
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp0 * tmp2
    tmp5 = 0.02040816326530612
    tmp6 = tmp4 * tmp5
    tmp7 = tmp3 + tmp6
    tmp9 = tl.sigmoid(tmp8)
    tmp10 = 1.0
    tmp11 = tmp10 - tmp9
    tmp12 = tmp8 * tmp11
    tmp13 = tmp12 + tmp10
    tmp14 = tmp9 * tmp13
    tmp15 = tmp7 * tmp14
    tmp16 = tmp15.to(tl.float32)
    tmp18 = tmp17.to(tl.float32)
    tmp20 = tmp18 - tmp19
    tmp22 = 0.00015943877551020407
    tmp23 = tmp21 * tmp22
    tmp25 = tmp24 * tmp24
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp16 - tmp27
    tmp30 = tmp29 * tmp22
    tmp31 = tmp28 - tmp30
    tl.store(out_ptr0 + (x3), tmp31, None)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06071808
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hp/chpgkzcvx6zqzkli37org3sym3akynppfy35gxtkxewtdqmujwyg.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_147 => squeeze_139
# Graph fragment:
#   %sub_120 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=sub_120]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_623 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_139, %primals_383), kwargs = {})
#   %unsqueeze_373 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_623, 0), kwargs = {})
#   %unsqueeze_374 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_373, 2), kwargs = {})
#   %unsqueeze_375 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_374, 3), kwargs = {})
#   %mul_625 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_120, %unsqueeze_375), kwargs = {})
#   %convert_element_type_497 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_625, torch.float16), kwargs = {})
#   %slice_22 : Tensor "f16[128, 240, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_497, 1, 720, 960), kwargs = {})
#   %convolution_backward_31 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_22, %getitem_345, %convert_element_type_314, [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 240, [True, True, False]), kwargs = {})
#   return %buf252
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12044160}, 'kernel_num_gb': 0.00903936, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1505280
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 240)
    x1 = xindex // 240
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (720 + x0 + 960*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (720 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (720 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1505280,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00903936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/x2/cx2qxsawe2v2zwbwskx3ocsoowszwnscg3nhhptp3mdldwledayh.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_147 => squeeze_139
# Graph fragment:
#   %sub_120 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=sub_120]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_623 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_139, %primals_383), kwargs = {})
#   %unsqueeze_373 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_623, 0), kwargs = {})
#   %unsqueeze_374 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_373, 2), kwargs = {})
#   %unsqueeze_375 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_374, 3), kwargs = {})
#   %mul_625 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_120, %unsqueeze_375), kwargs = {})
#   %convert_element_type_497 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_625, torch.float16), kwargs = {})
#   %slice_21 : Tensor "f16[128, 240, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_497, 1, 480, 720), kwargs = {})
#   %convolution_backward_32 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_21, %getitem_340, %convert_element_type_313, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 240, [True, True, False]), kwargs = {})
#   return %buf257
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12044160}, 'kernel_num_gb': 0.00903936, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1505280
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 240)
    x1 = xindex // 240
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (480 + x0 + 960*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (480 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (480 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1505280,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00903936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lg/clgbpx442gzbg3xzdxh6robtmohjkozm3y2ktrzio4fhes25ncrl.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_147 => squeeze_139
# Graph fragment:
#   %sub_120 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=sub_120]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_623 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_139, %primals_383), kwargs = {})
#   %unsqueeze_373 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_623, 0), kwargs = {})
#   %unsqueeze_374 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_373, 2), kwargs = {})
#   %unsqueeze_375 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_374, 3), kwargs = {})
#   %mul_625 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_120, %unsqueeze_375), kwargs = {})
#   %convert_element_type_497 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_625, torch.float16), kwargs = {})
#   %slice_20 : Tensor "f16[128, 240, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_497, 1, 240, 480), kwargs = {})
#   %convolution_backward_33 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_20, %getitem_335, %convert_element_type_312, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 240, [True, True, False]), kwargs = {})
#   return %buf262
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12044160}, 'kernel_num_gb': 0.00903936, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1505280
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 240)
    x1 = xindex // 240
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (240 + x0 + 960*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (240 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (240 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1505280,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00903936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pq/cpq4t6iims6zz34s6oz2gdjcdfu6ea3ej5swhhywvuibi5sw3rv3.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_147 => squeeze_139
# Graph fragment:
#   %sub_120 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=sub_120]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %squeeze_139 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_46, [0, 2, 3]), kwargs = {})
#   %mul_623 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_139, %primals_383), kwargs = {})
#   %unsqueeze_373 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_623, 0), kwargs = {})
#   %unsqueeze_374 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_373, 2), kwargs = {})
#   %unsqueeze_375 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_374, 3), kwargs = {})
#   %mul_625 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_120, %unsqueeze_375), kwargs = {})
#   %convert_element_type_497 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_625, torch.float16), kwargs = {})
#   %slice_19 : Tensor "f16[128, 240, 7, 7][47040, 1, 6720, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_497, 1, 0, 240), kwargs = {})
#   %convolution_backward_34 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_19, %getitem_330, %convert_element_type_311, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 240, [True, True, False]), kwargs = {})
#   return %buf267
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12044160}, 'kernel_num_gb': 0.00903936, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1505280
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 240)
    x1 = xindex // 240
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1505280,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00903936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/m6/cm6w6iyc7kxq5o52xusbumgwgklz24tfvbkmh5lpebv6mtnroojx.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
#   x_144 => add_240, convert_element_type_308, mul_363, mul_369, sub_45, unsqueeze_180, unsqueeze_181, unsqueeze_182, unsqueeze_183
# Graph fragment:
#   %convolution_119 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convolution_119]
#   %getitem_325 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_325]
#   %rsqrt_45 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_45]
#   %primals_374 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_374]
#   %primals_375 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_375]
#   %getitem_538 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_538]
#   %getitem_535 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_535]
#   %getitem_532 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_532]
#   %getitem_529 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_529]
#   %convert_element_type_308 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convert_element_type_308]
#   %cat_47 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_538, %getitem_535, %getitem_532, %getitem_529], 1), kwargs = {})
#   %sub_45 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_119, %getitem_325), kwargs = {})
#   %mul_363 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_45, %rsqrt_45), kwargs = {})
#   %unsqueeze_180 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_374, -1), kwargs = {})
#   %unsqueeze_181 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_180, -1), kwargs = {})
#   %mul_369 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_363, %unsqueeze_181), kwargs = {})
#   %unsqueeze_182 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_375, -1), kwargs = {})
#   %unsqueeze_183 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_182, -1), kwargs = {})
#   %add_240 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_369, %unsqueeze_183), kwargs = {})
#   %convert_element_type_308 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_240, torch.float16), kwargs = {})
#   %sigmoid_75 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_308,), kwargs = {})
#   %full_default_12 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 960, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_121 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_12, %sigmoid_75), kwargs = {})
#   %mul_627 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_308, %sub_121), kwargs = {})
#   %add_322 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_627, 1), kwargs = {})
#   %mul_628 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_75, %add_322), kwargs = {})
#   %mul_629 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_47, %mul_628), kwargs = {})
#   return %convert_element_type_308,%mul_629
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 240860160}, 'kernel_num_gb': 0.14452224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 960
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 960*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x1
    tmp12 = tl.full([1, 1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1, 1], 240, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (240*y0 + (x1)), tmp15 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1, 1], 480, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (240*y0 + ((-240) + x1)), tmp20 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1, 1], 720, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tmp22 & tmp24
    tmp26 = tl.load(in_ptr7 + (240*y0 + ((-480) + x1)), tmp25 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tmp11 >= tmp23
    tmp28 = tl.full([1, 1], 960, tl.int64)
    tmp29 = tmp11 < tmp28
    tmp30 = tl.load(in_ptr8 + (240*y0 + ((-720) + x1)), tmp27 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tl.where(tmp25, tmp26, tmp30)
    tmp32 = tl.where(tmp20, tmp21, tmp31)
    tmp33 = tl.where(tmp15, tmp16, tmp32)
    tmp34 = tl.sigmoid(tmp10)
    tmp35 = 1.0
    tmp36 = tmp35 - tmp34
    tmp37 = tmp10 * tmp36
    tmp38 = tmp37 + tmp35
    tmp39 = tmp34 * tmp38
    tmp40 = tmp33 * tmp39
    tl.store(out_ptr1 + (y2 + 196*x1 + 188160*y3), tmp40, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 25088, 960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.14452224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zi/czirmmiayca4tgvnjgehluaicud43gu72s7lfbtirruw4cjwayfi.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_144 => convert_element_type_307, squeeze_135
# Graph fragment:
#   %mul_629 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_629]
#   %convolution_119 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convolution_119]
#   %getitem_325 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_325]
#   %convert_element_type_502 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_629, torch.float32), kwargs = {})
#   %squeeze_135 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_325, [0, 2, 3]), kwargs = {})
#   %unsqueeze_376 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_135, 0), kwargs = {})
#   %unsqueeze_377 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_376, 2), kwargs = {})
#   %unsqueeze_378 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_377, 3), kwargs = {})
#   %convert_element_type_307 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_119, torch.float32), kwargs = {})
#   %sub_122 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_307, %unsqueeze_378), kwargs = {})
#   %mul_630 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_502, %sub_122), kwargs = {})
#   %sum_39 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_630, [0, 2, 3]), kwargs = {})
#   return %buf275
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 49155840, 'r0_': 48168960}, 'kernel_num_gb': 0.09683328, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x3 = xindex
    x0 = (xindex % 960)
    x1 = xindex // 960
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (r0_2 + 196*x3), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (x0 + 960*r0_2 + 188160*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp5 = tmp3 - tmp4
        tmp6 = tmp1 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960, 128), (1, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122880, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09683328
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lz/clzy5zs344yue2q6tpdx4tr2aartp5vetv6x2siopfen5c4zjzbj.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_144 => convert_element_type_307, squeeze_135, squeeze_136
# Graph fragment:
#   %buf275 : Tensor "f32[960, 128][1, 960]cuda:0" = PlaceHolder[target=buf275]
#   %sum_39 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_39]
#   %rsqrt_45 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_45]
#   %convert_element_type_502 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_629, torch.float32), kwargs = {})
#   %squeeze_135 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_325, [0, 2, 3]), kwargs = {})
#   %unsqueeze_376 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_135, 0), kwargs = {})
#   %unsqueeze_377 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_376, 2), kwargs = {})
#   %unsqueeze_378 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_377, 3), kwargs = {})
#   %convert_element_type_307 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_119, torch.float32), kwargs = {})
#   %sub_122 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_307, %unsqueeze_378), kwargs = {})
#   %mul_630 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_502, %sub_122), kwargs = {})
#   %sum_39 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_630, [0, 2, 3]), kwargs = {})
#   %squeeze_136 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_45, [0, 2, 3]), kwargs = {})
#   %mul_638 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_39, %squeeze_136), kwargs = {})
#   return %sum_39,%mul_638
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 510720, 'r0_': 0}, 'kernel_num_gb': 0.00050304, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((960, 128), (1, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 960, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00050304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ym/cymmmijqh4izwihvlrz5hii4xay3tc2qqnfvayju3li6picrw42t.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_629 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_629]
#   %convert_element_type_502 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_629, torch.float32), kwargs = {})
#   %sum_38 : Tensor "f32[960][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_502, [0, 2, 3]), kwargs = {})
#   return %sum_38
triton_red_fused_native_batch_norm_backward_49 = async_compile.triton('triton_red_fused_native_batch_norm_backward_49', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 32768},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_49', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 7680, 'r0_': 48168960}, 'kernel_num_gb': 0.0481728, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_49(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 25088
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 196)
        r0_2 = r0_index // 196
        tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0 + 188160*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 960, 25088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_49.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_49.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0481728
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kp/ckp3sh3l7k4ftnc5fuaz3lbkrzwrmmai6uwj6o33ysdybbyqevdl.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_144 => convert_element_type_307, squeeze_135, squeeze_136
# Graph fragment:
#   %mul_629 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_629]
#   %convolution_119 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convolution_119]
#   %getitem_325 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_325]
#   %sum_39 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_39]
#   %rsqrt_45 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_45]
#   %sum_38 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=sum_38]
#   %primals_374 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_374]
#   %convert_element_type_502 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_629, torch.float32), kwargs = {})
#   %squeeze_135 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_325, [0, 2, 3]), kwargs = {})
#   %unsqueeze_376 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_135, 0), kwargs = {})
#   %unsqueeze_377 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_376, 2), kwargs = {})
#   %unsqueeze_378 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_377, 3), kwargs = {})
#   %convert_element_type_307 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_119, torch.float32), kwargs = {})
#   %sub_122 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_307, %unsqueeze_378), kwargs = {})
#   %mul_631 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_38, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_379 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_631, 0), kwargs = {})
#   %unsqueeze_380 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_379, 2), kwargs = {})
#   %unsqueeze_381 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_380, 3), kwargs = {})
#   %mul_632 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_39, 3.985969387755102e-05), kwargs = {})
#   %squeeze_136 : Tensor "f32[960][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_45, [0, 2, 3]), kwargs = {})
#   %mul_633 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_136, %squeeze_136), kwargs = {})
#   %mul_634 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_632, %mul_633), kwargs = {})
#   %unsqueeze_382 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_634, 0), kwargs = {})
#   %unsqueeze_383 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_382, 2), kwargs = {})
#   %unsqueeze_384 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_383, 3), kwargs = {})
#   %mul_635 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_136, %primals_374), kwargs = {})
#   %unsqueeze_385 : Tensor "f32[1, 960][960, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_635, 0), kwargs = {})
#   %unsqueeze_386 : Tensor "f32[1, 960, 1][960, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_385, 2), kwargs = {})
#   %unsqueeze_387 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_386, 3), kwargs = {})
#   %mul_636 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_122, %unsqueeze_384), kwargs = {})
#   %sub_124 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_502, %mul_636), kwargs = {})
#   %sub_125 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_124, %unsqueeze_381), kwargs = {})
#   %mul_637 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_125, %unsqueeze_387), kwargs = {})
#   %convert_element_type_504 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_637, torch.float16), kwargs = {})
#   %convolution_backward_35 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_504, %add_235, %convert_element_type_306, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf278
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 144526080}, 'kernel_num_gb': 0.14452608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 960
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 196)
    y1 = yindex // 196
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 196*x2 + 188160*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2 + 960*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 3.985969387755102e-05
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + 960*y3), tmp20, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 25088, 960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.14452608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pe/cpez2uozjwjqbz7vsa2slmoyhwx2mwis3in4kexitxwir2liabms.py
# Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_141 => convert_element_type_304
# Graph fragment:
#   %getitem_541 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=getitem_541]
#   %cat_33 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=cat_33]
#   %unsqueeze_390 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_390]
#   %convert_element_type_506 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_541, torch.float32), kwargs = {})
#   %sum_40 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_506, [0, 2, 3]), kwargs = {})
#   %convert_element_type_304 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_33, torch.float32), kwargs = {})
#   %sub_126 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_304, %unsqueeze_390), kwargs = {})
#   %mul_639 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_506, %sub_126), kwargs = {})
#   %sum_41 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_639, [0, 2, 3]), kwargs = {})
#   return %buf283,%buf285
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16558720, 'r0_': 0}, 'kernel_num_gb': 0.01630784, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 31360
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 160)
    x1 = xindex // 160
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_2 + 20480*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 160*r0_2 + 20480*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 31360, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01630784
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wk/cwk5vsx4u2h2bkz77hasskngsgv5ecrv4ctv632utywmvpc2lray.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf283 : Tensor "f32[160, 196][1, 160]cuda:0" = PlaceHolder[target=buf283]
#   %convert_element_type_506 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_541, torch.float32), kwargs = {})
#   %sum_40 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_506, [0, 2, 3]), kwargs = {})
#   return %sum_40
triton_red_fused_native_batch_norm_backward_52 = async_compile.triton('triton_red_fused_native_batch_norm_backward_52', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_52', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 126720, 'r0_': 0}, 'kernel_num_gb': 0.00012608, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_52(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 160
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 160, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_52.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_52.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00012608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/na/cnabb6wy5t26kvxdyljckn3rgons6tlnxd355dfna4orf6wgv6lf.py
# Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_141 => convert_element_type_304
# Graph fragment:
#   %buf285 : Tensor "f32[160, 196][1, 160]cuda:0" = PlaceHolder[target=buf285]
#   %sum_41 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_41]
#   %squeeze_133 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_133]
#   %convert_element_type_506 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_541, torch.float32), kwargs = {})
#   %convert_element_type_304 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_33, torch.float32), kwargs = {})
#   %sub_126 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_304, %unsqueeze_390), kwargs = {})
#   %mul_639 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_506, %sub_126), kwargs = {})
#   %sum_41 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_639, [0, 2, 3]), kwargs = {})
#   %mul_647 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_41, %squeeze_133), kwargs = {})
#   return %sum_41,%mul_647
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 128640, 'r0_': 0}, 'kernel_num_gb': 0.00012736, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 160
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 160, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00012736
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jl/cjl6hf5nmlxcm7z3l6urnnf7pi7kculq3jschrohkjybww7rfdft.py
# Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_141 => convert_element_type_304
# Graph fragment:
#   %getitem_541 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=getitem_541]
#   %cat_33 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=cat_33]
#   %unsqueeze_390 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_390]
#   %sum_41 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_41]
#   %squeeze_133 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_133]
#   %sum_40 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_40]
#   %primals_368 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_368]
#   %convert_element_type_506 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_541, torch.float32), kwargs = {})
#   %convert_element_type_304 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_33, torch.float32), kwargs = {})
#   %sub_126 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_304, %unsqueeze_390), kwargs = {})
#   %mul_640 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_40, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_391 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_640, 0), kwargs = {})
#   %unsqueeze_392 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_391, 2), kwargs = {})
#   %unsqueeze_393 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_392, 3), kwargs = {})
#   %mul_641 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_41, 3.985969387755102e-05), kwargs = {})
#   %mul_642 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_133, %squeeze_133), kwargs = {})
#   %mul_643 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_641, %mul_642), kwargs = {})
#   %unsqueeze_394 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_643, 0), kwargs = {})
#   %unsqueeze_395 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_394, 2), kwargs = {})
#   %unsqueeze_396 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_395, 3), kwargs = {})
#   %mul_644 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_133, %primals_368), kwargs = {})
#   %unsqueeze_397 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_644, 0), kwargs = {})
#   %unsqueeze_398 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_397, 2), kwargs = {})
#   %unsqueeze_399 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_398, 3), kwargs = {})
#   %mul_645 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_126, %unsqueeze_396), kwargs = {})
#   %sub_128 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_506, %mul_645), kwargs = {})
#   %sub_129 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_128, %unsqueeze_393), kwargs = {})
#   %mul_646 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_129, %unsqueeze_399), kwargs = {})
#   %convert_element_type_508 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_646, torch.float16), kwargs = {})
#   return %convert_element_type_508
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 32115840}, 'kernel_num_gb': 0.02408768, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 3.985969387755102e-05
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3n/c3ngox3duodtpdqduf2n5dvbu3ohmfdpjw7hy5zy73m6nndv4dnz.py
# Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_137 => add_229, convert_element_type_293, mul_346, mul_352, sub_43, unsqueeze_172, unsqueeze_173, unsqueeze_174, unsqueeze_175
# Graph fragment:
#   %cat_32 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_32]
#   %getitem_319 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_319]
#   %rsqrt_43 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_43]
#   %primals_357 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_357]
#   %primals_358 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_358]
#   %sub_43 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_32, %getitem_319), kwargs = {})
#   %mul_346 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_43, %rsqrt_43), kwargs = {})
#   %unsqueeze_172 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_357, -1), kwargs = {})
#   %unsqueeze_173 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_172, -1), kwargs = {})
#   %mul_352 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_346, %unsqueeze_173), kwargs = {})
#   %unsqueeze_174 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_358, -1), kwargs = {})
#   %unsqueeze_175 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_174, -1), kwargs = {})
#   %add_229 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_352, %unsqueeze_175), kwargs = {})
#   %convert_element_type_293 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_229, torch.float16), kwargs = {})
#   return %convert_element_type_293
triton_poi_fused__native_batch_norm_legit_functional_55 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_55', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_55', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 72261120}, 'kernel_num_gb': 0.04817664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_55(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12042240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 480)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 12042240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_55.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_55.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04817664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/a7/ca7s6o562iiqu7ogzcacfo3juchnd2274r2srvrtvjujm5hqb7gr.py
# Topologically Sorted Source Nodes: [x_138, sigmoid_11], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_11 => sigmoid_47
#   x_138 => convert_element_type_294, convert_element_type_295, mul_353, sigmoid_45
# Graph fragment:
#   %getitem_547 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_547]
#   %getitem_544 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_544]
#   %convert_element_type_293 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_293]
#   %sum_42 : Tensor "f16[128, 480, 1, 1][480, 1, 61440, 61440]cuda:0" = PlaceHolder[target=sum_42]
#   %convolution_116 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=convolution_116]
#   %cat_48 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_547, %getitem_544], 1), kwargs = {})
#   %convert_element_type_294 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_293, torch.float32), kwargs = {})
#   %sigmoid_45 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_294,), kwargs = {})
#   %mul_353 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_294, %sigmoid_45), kwargs = {})
#   %convert_element_type_295 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_353, torch.float16), kwargs = {})
#   %mul_648 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_48, %convert_element_type_295), kwargs = {})
#   %sigmoid_47 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_116,), kwargs = {})
#   %sum_42 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_648, [2, 3], True), kwargs = {})
#   %convert_element_type_511 : Tensor "f32[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_42, torch.float32), kwargs = {})
#   %convert_element_type_512 : Tensor "f32[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_47, torch.float32), kwargs = {})
#   %sub_130 : Tensor "f32[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_512), kwargs = {})
#   %mul_650 : Tensor "f32[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_512, %sub_130), kwargs = {})
#   %mul_651 : Tensor "f32[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_511, %mul_650), kwargs = {})
#   %convert_element_type_513 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_651, torch.float16), kwargs = {})
#   return %sum_42,%convert_element_type_513
triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56 = async_compile.triton('triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 72622080, 'r0_': 0}, 'kernel_num_gb': 0.0485376, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 61440
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 480)
    x1 = xindex // 480
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 480*r0_2 + 94080*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 240, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (240*r0_2 + 47040*x1 + (x0)), r0_mask & tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 480, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (240*r0_2 + 47040*x1 + ((-240) + x0)), r0_mask & tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tl.sigmoid(tmp12)
        tmp14 = tmp12 * tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp10 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp21 = tl.load(in_ptr3 + (x3), None, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tmp18.to(tl.float32)
    tmp22 = tl.sigmoid(tmp21)
    tmp23 = tmp22.to(tl.float32)
    tmp24 = 1.0
    tmp25 = tmp24 - tmp23
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 61440, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0485376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hi/chi22bueuirlwujcccjc5ecguroaqkfbfxlyxjwvf7vzthfl25qy.py
# Topologically Sorted Source Nodes: [sigmoid_11], Original ATen: [aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_11 => sigmoid_47
# Graph fragment:
#   %getitem_547 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_547]
#   %getitem_544 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=getitem_544]
#   %convolution_116 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=convolution_116]
#   %getitem_553 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_553]
#   %convert_element_type_293 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_293]
#   %cat_48 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_547, %getitem_544], 1), kwargs = {})
#   %sigmoid_47 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_116,), kwargs = {})
#   %mul_649 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_48, %sigmoid_47), kwargs = {})
#   %expand_5 : Tensor "f16[128, 480, 14, 14][480, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_553, [128, 480, 14, 14]), kwargs = {})
#   %div_5 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_5, 196), kwargs = {})
#   %add_324 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_649, %div_5), kwargs = {})
#   %sigmoid_77 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_293,), kwargs = {})
#   %full_default_14 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=6] = call_function[target=torch.ops.aten.full.default](args = ([128, 480, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_132 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_14, %sigmoid_77), kwargs = {})
#   %mul_655 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_293, %sub_132), kwargs = {})
#   %add_325 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_655, 1), kwargs = {})
#   %mul_656 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_77, %add_325), kwargs = {})
#   %mul_657 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_324, %mul_656), kwargs = {})
#   %convert_element_type_518 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_657, torch.float32), kwargs = {})
#   return %convert_element_type_518
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 72499200}, 'kernel_num_gb': 0.09658368, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 480
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 196
    y0 = (yindex % 196)
    tmp11 = tl.load(in_ptr2 + (x2 + 480*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 480*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 480*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 240, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (240*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 480, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (240*y3 + ((-240) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.00510204081632653
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 196*x2 + 94080*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09658368
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/s2/cs26x4cdiavrcft5a76ja7k54uqw3b6utv4crzh3sl5e4ces2oja.py
# Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_137 => convert_element_type_292, squeeze_129
# Graph fragment:
#   %convert_element_type_518 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_518]
#   %cat_32 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_32]
#   %getitem_319 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_319]
#   %squeeze_129 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_319, [0, 2, 3]), kwargs = {})
#   %unsqueeze_400 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_129, 0), kwargs = {})
#   %unsqueeze_401 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_400, 2), kwargs = {})
#   %unsqueeze_402 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_401, 3), kwargs = {})
#   %convert_element_type_292 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_32, torch.float32), kwargs = {})
#   %sub_133 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_292, %unsqueeze_402), kwargs = {})
#   %mul_658 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_518, %sub_133), kwargs = {})
#   %sum_46 : Tensor "f32[480][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_658, [0, 2, 3]), kwargs = {})
#   return %buf315
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 512, 'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24086400, 'x': 752640, 'r0_': 48168960}, 'kernel_num_gb': 0.07263168, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 480
    xnumel = 196
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp3 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp7 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (196*y0 + 94080*((r0_2 + 128*x1) // 196) + (((r0_2 + 128*x1) % 196))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (y0 + 480*r0_2 + 61440*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp4 = tmp2 - tmp3
        tmp5 = tmp0 * tmp4
        tmp6 = tl.broadcast_to(tmp5, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask & xmask & ymask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 196*y0), tmp7, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 480, 196, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.07263168
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2q/c2qbxwnh64ajmq4drgxsuk2bsgjai2xf6nepylh7w66kotzerkzy.py
# Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_137 => convert_element_type_292, squeeze_129, squeeze_130
# Graph fragment:
#   %buf315 : Tensor "f32[480, 196][196, 1]cuda:0" = PlaceHolder[target=buf315]
#   %sum_46 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=sum_46]
#   %rsqrt_43 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_43]
#   %squeeze_129 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_319, [0, 2, 3]), kwargs = {})
#   %unsqueeze_400 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_129, 0), kwargs = {})
#   %unsqueeze_401 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_400, 2), kwargs = {})
#   %unsqueeze_402 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_401, 3), kwargs = {})
#   %convert_element_type_292 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_32, torch.float32), kwargs = {})
#   %sub_133 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_292, %unsqueeze_402), kwargs = {})
#   %mul_658 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_518, %sub_133), kwargs = {})
#   %sum_46 : Tensor "f32[480][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_658, [0, 2, 3]), kwargs = {})
#   %squeeze_130 : Tensor "f32[480][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_43, [0, 2, 3]), kwargs = {})
#   %mul_666 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_46, %squeeze_130), kwargs = {})
#   return %sum_46,%mul_666
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 256},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 9600, 'r0_': 376320}, 'kernel_num_gb': 0.00038208, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 480
    r0_numel = 196
    R0_BLOCK: tl.constexpr = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((480, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 480, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00038208
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mp/cmpye2hjt4qp4gyqsdzkxurn4j47m54wvkmnywfbatvjg4sx563e.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_518 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_518]
#   %sum_45 : Tensor "f32[480][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_518, [0, 2, 3]), kwargs = {})
#   return %sum_45
triton_red_fused_native_batch_norm_backward_60 = async_compile.triton('triton_red_fused_native_batch_norm_backward_60', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 32768},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_60', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3840, 'r0_': 48168960}, 'kernel_num_gb': 0.04817088, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_60(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 480
    r0_numel = 25088
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 196)
        r0_2 = r0_index // 196
        tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0 + 94080*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 480, 25088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_60.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_60.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04817088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kk/ckkkynojnu7a3yp3qwzsc5tn36iozbuk5lulksczqvvnsapumcx2.py
# Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_137 => convert_element_type_292, squeeze_129, squeeze_130
# Graph fragment:
#   %convert_element_type_518 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_518]
#   %cat_32 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_32]
#   %getitem_319 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_319]
#   %sum_46 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=sum_46]
#   %rsqrt_43 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_43]
#   %sum_45 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=sum_45]
#   %primals_357 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_357]
#   %squeeze_129 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_319, [0, 2, 3]), kwargs = {})
#   %unsqueeze_400 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_129, 0), kwargs = {})
#   %unsqueeze_401 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_400, 2), kwargs = {})
#   %unsqueeze_402 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_401, 3), kwargs = {})
#   %convert_element_type_292 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_32, torch.float32), kwargs = {})
#   %sub_133 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_292, %unsqueeze_402), kwargs = {})
#   %mul_659 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_45, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_403 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_659, 0), kwargs = {})
#   %unsqueeze_404 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_403, 2), kwargs = {})
#   %unsqueeze_405 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_404, 3), kwargs = {})
#   %mul_660 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_46, 3.985969387755102e-05), kwargs = {})
#   %squeeze_130 : Tensor "f32[480][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_43, [0, 2, 3]), kwargs = {})
#   %mul_661 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_130, %squeeze_130), kwargs = {})
#   %mul_662 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_660, %mul_661), kwargs = {})
#   %unsqueeze_406 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_662, 0), kwargs = {})
#   %unsqueeze_407 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_406, 2), kwargs = {})
#   %unsqueeze_408 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_407, 3), kwargs = {})
#   %mul_663 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_130, %primals_357), kwargs = {})
#   %unsqueeze_409 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_663, 0), kwargs = {})
#   %unsqueeze_410 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_409, 2), kwargs = {})
#   %unsqueeze_411 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_410, 3), kwargs = {})
#   %mul_664 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_133, %unsqueeze_408), kwargs = {})
#   %sub_135 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_518, %mul_664), kwargs = {})
#   %sub_136 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_135, %unsqueeze_405), kwargs = {})
#   %mul_665 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_136, %unsqueeze_411), kwargs = {})
#   %convert_element_type_520 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_665, torch.float16), kwargs = {})
#   return %convert_element_type_520
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24094080, 'x': 96337920}, 'kernel_num_gb': 0.09634752, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 61440
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 480)
    y1 = yindex // 480
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y3), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + 480*x2 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (y0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (y0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 3.985969387755102e-05
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y3), tmp19, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 61440, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09634752
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/k7/ck7g42ricx6g6iyzikfxvh2k6hg7oky4cevzwnxli3mhicagusmi.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_520 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_520]
#   %slice_28 : Tensor "f16[128, 120, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_520, 1, 360, 480), kwargs = {})
#   %convolution_backward_40 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_28, %getitem_317, %convert_element_type_291, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False]), kwargs = {})
#   return %buf319
triton_poi_fused_convolution_backward_slice_62 = async_compile.triton('triton_poi_fused_convolution_backward_slice_62', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_62', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_62(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (70560 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_62.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_62.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/av/cavvyjlgo2tvuohbrwal4zmw6uyn2ey3onh3bgsqvb4jgtsdckja.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_520 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_520]
#   %slice_27 : Tensor "f16[128, 120, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_520, 1, 240, 360), kwargs = {})
#   %convolution_backward_41 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_27, %getitem_312, %convert_element_type_290, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False]), kwargs = {})
#   return %buf324
triton_poi_fused_convolution_backward_slice_63 = async_compile.triton('triton_poi_fused_convolution_backward_slice_63', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_63', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_63(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (47040 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_63.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_63.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pv/cpvu65f7wrnfd6tzbe6sw7q5fubxyqn2lbjs3tiiy5umy5wxnib4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_520 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_520]
#   %slice_26 : Tensor "f16[128, 120, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_520, 1, 120, 240), kwargs = {})
#   %convolution_backward_42 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_26, %getitem_307, %convert_element_type_289, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False]), kwargs = {})
#   return %buf329
triton_poi_fused_convolution_backward_slice_64 = async_compile.triton('triton_poi_fused_convolution_backward_slice_64', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_64', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_64(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (23520 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_64.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_64.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3a/c3a664h2uoiwfscjftctcxxuvxozkeig7zj3tsvmqe2uovisd52c.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_520 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_520]
#   %slice_25 : Tensor "f16[128, 120, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_520, 1, 0, 120), kwargs = {})
#   %convolution_backward_43 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_25, %getitem_302, %convert_element_type_288, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False]), kwargs = {})
#   return %buf334
triton_poi_fused_convolution_backward_slice_65 = async_compile.triton('triton_poi_fused_convolution_backward_slice_65', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_65', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_65(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_65.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_65.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/v2/cv2gz6ap66oz56vfmivcnnjnwx7hpxjy5pnhud25agusnzo7zpd6.py
# Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
#   x_134 => add_224, convert_element_type_285, mul_338, mul_344, sub_42, unsqueeze_168, unsqueeze_169, unsqueeze_170, unsqueeze_171
# Graph fragment:
#   %cat_31 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_31]
#   %getitem_297 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_297]
#   %rsqrt_42 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_42]
#   %primals_348 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_348]
#   %primals_349 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_349]
#   %getitem_565 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=getitem_565]
#   %getitem_562 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=getitem_562]
#   %getitem_559 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=getitem_559]
#   %getitem_556 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=getitem_556]
#   %convert_element_type_285 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_285]
#   %full_default_14 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=6] = call_function[target=torch.ops.aten.full.default](args = ([128, 480, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_49 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_565, %getitem_562, %getitem_559, %getitem_556], 1), kwargs = {})
#   %sub_42 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_31, %getitem_297), kwargs = {})
#   %mul_338 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_42, %rsqrt_42), kwargs = {})
#   %unsqueeze_168 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_348, -1), kwargs = {})
#   %unsqueeze_169 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_168, -1), kwargs = {})
#   %mul_344 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_338, %unsqueeze_169), kwargs = {})
#   %unsqueeze_170 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_349, -1), kwargs = {})
#   %unsqueeze_171 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_170, -1), kwargs = {})
#   %add_224 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_344, %unsqueeze_171), kwargs = {})
#   %convert_element_type_285 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_224, torch.float16), kwargs = {})
#   %sigmoid_78 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_285,), kwargs = {})
#   %sub_137 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_14, %sigmoid_78), kwargs = {})
#   %mul_667 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_285, %sub_137), kwargs = {})
#   %add_326 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_667, 1), kwargs = {})
#   %mul_668 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_78, %add_326), kwargs = {})
#   %mul_669 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_49, %mul_668), kwargs = {})
#   return %convert_element_type_285,%mul_669
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 120430080}, 'kernel_num_gb': 0.07226112, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 480
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 480*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x1
    tmp12 = tl.full([1, 1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1, 1], 120, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (120*y0 + (x1)), tmp15 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1, 1], 240, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (120*y0 + ((-120) + x1)), tmp20 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1, 1], 360, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tmp22 & tmp24
    tmp26 = tl.load(in_ptr7 + (120*y0 + ((-240) + x1)), tmp25 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tmp11 >= tmp23
    tmp28 = tl.full([1, 1], 480, tl.int64)
    tmp29 = tmp11 < tmp28
    tmp30 = tl.load(in_ptr8 + (120*y0 + ((-360) + x1)), tmp27 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tl.where(tmp25, tmp26, tmp30)
    tmp32 = tl.where(tmp20, tmp21, tmp31)
    tmp33 = tl.where(tmp15, tmp16, tmp32)
    tmp34 = tl.sigmoid(tmp10)
    tmp35 = 1.0
    tmp36 = tmp35 - tmp34
    tmp37 = tmp10 * tmp36
    tmp38 = tmp37 + tmp35
    tmp39 = tmp34 * tmp38
    tmp40 = tmp33 * tmp39
    tl.store(out_ptr1 + (y2 + 196*x1 + 94080*y3), tmp40, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 25088, 480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.07226112
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2n/c2n7idfpjluene7meisozre3ka3x7dp6p7osccxsilwj2f33t2rg.py
# Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_134 => convert_element_type_284, squeeze_126
# Graph fragment:
#   %mul_669 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_669]
#   %cat_31 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_31]
#   %getitem_297 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_297]
#   %convert_element_type_525 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_669, torch.float32), kwargs = {})
#   %squeeze_126 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_297, [0, 2, 3]), kwargs = {})
#   %unsqueeze_412 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_126, 0), kwargs = {})
#   %unsqueeze_413 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_412, 2), kwargs = {})
#   %unsqueeze_414 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_413, 3), kwargs = {})
#   %convert_element_type_284 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_31, torch.float32), kwargs = {})
#   %sub_138 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_284, %unsqueeze_414), kwargs = {})
#   %mul_670 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_525, %sub_138), kwargs = {})
#   %sum_48 : Tensor "f32[480][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_670, [0, 2, 3]), kwargs = {})
#   return %buf342
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 512, 'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24086400, 'x': 752640, 'r0_': 24084480}, 'kernel_num_gb': 0.0485472, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 480
    xnumel = 196
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp4 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp8 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (196*y0 + 94080*((r0_2 + 128*x1) // 196) + (((r0_2 + 128*x1) % 196))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (y0 + 480*r0_2 + 61440*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp5 = tmp3 - tmp4
        tmp6 = tmp1 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask & ymask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 196*y0), tmp8, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 480, 196, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0485472
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/34/c34ok6rljyrpvdrsrzggioygqex54jxlplfcfpwfixempnfdkhim.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_669 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_669]
#   %convert_element_type_525 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_669, torch.float32), kwargs = {})
#   %sum_47 : Tensor "f32[480][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_525, [0, 2, 3]), kwargs = {})
#   return %sum_47
triton_red_fused_native_batch_norm_backward_68 = async_compile.triton('triton_red_fused_native_batch_norm_backward_68', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 32768},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_68', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3840, 'r0_': 24084480}, 'kernel_num_gb': 0.0240864, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_68(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 480
    r0_numel = 25088
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 196)
        r0_2 = r0_index // 196
        tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0 + 94080*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 480, 25088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_68.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_68.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0240864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qr/cqrahmatngvvrls2q2qqf6tkel7yeus3ihixevk23b5j6xe53etq.py
# Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_134 => convert_element_type_284, squeeze_126, squeeze_127
# Graph fragment:
#   %mul_669 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_669]
#   %cat_31 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_31]
#   %getitem_297 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_297]
#   %sum_48 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=sum_48]
#   %rsqrt_42 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_42]
#   %sum_47 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=sum_47]
#   %primals_348 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_348]
#   %convert_element_type_525 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_669, torch.float32), kwargs = {})
#   %squeeze_126 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_297, [0, 2, 3]), kwargs = {})
#   %unsqueeze_412 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_126, 0), kwargs = {})
#   %unsqueeze_413 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_412, 2), kwargs = {})
#   %unsqueeze_414 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_413, 3), kwargs = {})
#   %convert_element_type_284 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_31, torch.float32), kwargs = {})
#   %sub_138 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_284, %unsqueeze_414), kwargs = {})
#   %mul_671 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_47, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_415 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_671, 0), kwargs = {})
#   %unsqueeze_416 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_415, 2), kwargs = {})
#   %unsqueeze_417 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_416, 3), kwargs = {})
#   %mul_672 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_48, 3.985969387755102e-05), kwargs = {})
#   %squeeze_127 : Tensor "f32[480][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_42, [0, 2, 3]), kwargs = {})
#   %mul_673 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_127, %squeeze_127), kwargs = {})
#   %mul_674 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_672, %mul_673), kwargs = {})
#   %unsqueeze_418 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_674, 0), kwargs = {})
#   %unsqueeze_419 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_418, 2), kwargs = {})
#   %unsqueeze_420 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_419, 3), kwargs = {})
#   %mul_675 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_127, %primals_348), kwargs = {})
#   %unsqueeze_421 : Tensor "f32[1, 480][480, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_675, 0), kwargs = {})
#   %unsqueeze_422 : Tensor "f32[1, 480, 1][480, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_421, 2), kwargs = {})
#   %unsqueeze_423 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_422, 3), kwargs = {})
#   %mul_676 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_138, %unsqueeze_420), kwargs = {})
#   %sub_140 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_525, %mul_676), kwargs = {})
#   %sub_141 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_140, %unsqueeze_417), kwargs = {})
#   %mul_677 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_141, %unsqueeze_423), kwargs = {})
#   %convert_element_type_527 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_677, torch.float16), kwargs = {})
#   return %convert_element_type_527
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24094080, 'x': 72253440}, 'kernel_num_gb': 0.07226304, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 61440
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 480)
    y1 = yindex // 480
    tmp0 = tl.load(in_out_ptr0 + (x2 + 196*y3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr0 + (y0 + 480*x2 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (y0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (y0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 3.985969387755102e-05
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + 196*y3), tmp20, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 61440, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.07226304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pw/cpw2y5f532w75evc6bmks52zt7chbwux3uglnutyug64gyyhveyi.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_527 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_527]
#   %slice_30 : Tensor "f16[128, 240, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_527, 1, 240, 480), kwargs = {})
#   %convolution_backward_44 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_30, %getitem_295, %convert_element_type_283, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf346
triton_poi_fused_convolution_backward_slice_70 = async_compile.triton('triton_poi_fused_convolution_backward_slice_70', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_70', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_70(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (47040 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_70.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_70.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/q2/cq2apgyhkbrrgicbdrxvijrc6qrutsfgiruar4mt764t3carxy3b.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_527 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_527]
#   %slice_29 : Tensor "f16[128, 240, 14, 14][94080, 1, 6720, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_527, 1, 0, 240), kwargs = {})
#   %convolution_backward_45 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_29, %getitem_294, %convert_element_type_282, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf351
triton_poi_fused_convolution_backward_slice_71 = async_compile.triton('triton_poi_fused_convolution_backward_slice_71', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_71', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_71(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_71.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_71.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/x7/cx7kie5kev5abwnwnn7l4x7ts3h6ebntkdisk3mvaorw3bzxxvnh.py
# Topologically Sorted Source Nodes: [x_131], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_131 => convert_element_type_280
# Graph fragment:
#   %getitem_541 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=getitem_541]
#   %getitem_571 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_571]
#   %getitem_568 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_568]
#   %cat_30 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=cat_30]
#   %unsqueeze_426 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_426]
#   %cat_50 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_571, %getitem_568], 1), kwargs = {})
#   %add_327 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_541, %cat_50), kwargs = {})
#   %convert_element_type_530 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_327, torch.float32), kwargs = {})
#   %sum_49 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_530, [0, 2, 3]), kwargs = {})
#   %convert_element_type_280 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_30, torch.float32), kwargs = {})
#   %sub_142 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_280, %unsqueeze_426), kwargs = {})
#   %mul_679 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_530, %sub_142), kwargs = {})
#   %sum_50 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_679, [0, 2, 3]), kwargs = {})
#   return %buf356,%buf358
triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 32615040, 'r0_': 0}, 'kernel_num_gb': 0.024336, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 31360
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 160)
    x1 = xindex // 160
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp19 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp23 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_2 + 20480*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp17 = tl.load(in_ptr3 + (x0 + 160*r0_2 + 20480*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = x0
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 80, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = tl.load(in_ptr1 + (80*r0_2 + 10240*x1 + (x0)), r0_mask & tmp5 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp1 >= tmp4
        tmp8 = tl.full([1, 1], 160, tl.int64)
        tmp9 = tmp1 < tmp8
        tmp10 = tl.load(in_ptr2 + (80*r0_2 + 10240*x1 + ((-80) + x0)), r0_mask & tmp7 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tl.where(tmp5, tmp6, tmp10)
        tmp12 = tmp0 + tmp11
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask & xmask, tmp16, _tmp15)
        tmp18 = tmp17.to(tl.float32)
        tmp20 = tmp18 - tmp19
        tmp21 = tmp13 * tmp20
        tmp22 = tl.broadcast_to(tmp21, [XBLOCK, R0_BLOCK])
        tmp24 = _tmp23 + tmp22
        _tmp23 = tl.where(r0_mask & xmask, tmp24, _tmp23)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tmp23 = tl.sum(_tmp23, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
    tl.store(out_ptr1 + (x3), tmp23, xmask)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((160, 196), (1, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 31360, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.024336
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mz/cmz2redpzcb2x57hbqy2rifyf7qx2osx62e4ndsiv5m56sn35lpt.py
# Topologically Sorted Source Nodes: [x_131], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_131 => convert_element_type_280
# Graph fragment:
#   %getitem_541 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=getitem_541]
#   %getitem_571 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_571]
#   %getitem_568 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_568]
#   %cat_30 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=cat_30]
#   %unsqueeze_426 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_426]
#   %sum_50 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_50]
#   %squeeze_124 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_124]
#   %sum_49 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_49]
#   %cat_50 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_571, %getitem_568], 1), kwargs = {})
#   %add_327 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_541, %cat_50), kwargs = {})
#   %convert_element_type_530 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_327, torch.float32), kwargs = {})
#   %convert_element_type_280 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_30, torch.float32), kwargs = {})
#   %sub_142 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_280, %unsqueeze_426), kwargs = {})
#   %mul_680 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_49, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_427 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_680, 0), kwargs = {})
#   %unsqueeze_428 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_427, 2), kwargs = {})
#   %unsqueeze_429 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_428, 3), kwargs = {})
#   %mul_681 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_50, 3.985969387755102e-05), kwargs = {})
#   %mul_682 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_124, %squeeze_124), kwargs = {})
#   %mul_683 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_681, %mul_682), kwargs = {})
#   %unsqueeze_430 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_683, 0), kwargs = {})
#   %unsqueeze_431 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_430, 2), kwargs = {})
#   %unsqueeze_432 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_431, 3), kwargs = {})
#   %mul_685 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_142, %unsqueeze_432), kwargs = {})
#   %sub_144 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_530, %mul_685), kwargs = {})
#   %sub_145 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_144, %unsqueeze_429), kwargs = {})
#   return %sub_145
triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 64227840}, 'kernel_num_gb': 0.04014336, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    x1 = xindex // 160
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 80, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (80*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 160, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (80*x1 + ((-80) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 3.985969387755102e-05
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tl.store(out_ptr0 + (x2), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04014336
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xz/cxzi5evjzvnebul4lsy6qr43etk5bct6wn5yplra7ezwoaj7r3cf.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_145 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=sub_145]
#   %squeeze_124 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_124]
#   %primals_341 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_341]
#   %mul_684 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_124, %primals_341), kwargs = {})
#   %unsqueeze_433 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_684, 0), kwargs = {})
#   %unsqueeze_434 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_433, 2), kwargs = {})
#   %unsqueeze_435 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_434, 3), kwargs = {})
#   %mul_686 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_145, %unsqueeze_435), kwargs = {})
#   %convert_element_type_532 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_686, torch.float16), kwargs = {})
#   %slice_32 : Tensor "f16[128, 80, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_532, 1, 80, 160), kwargs = {})
#   %convolution_backward_46 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_32, %getitem_291, %convert_element_type_279, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf362
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16056960}, 'kernel_num_gb': 0.01204352, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2007040
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 80)
    x1 = xindex // 80
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (80 + x0 + 160*x1), None)
    tmp1 = tl.load(in_ptr1 + (80 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (80 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2007040,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204352
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hx/chxdwj6poo2actu7zeedawjbur2sn4t5ko56dlg72xfkqgevrr6t.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_145 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=sub_145]
#   %squeeze_124 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_124]
#   %primals_341 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_341]
#   %mul_684 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_124, %primals_341), kwargs = {})
#   %unsqueeze_433 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_684, 0), kwargs = {})
#   %unsqueeze_434 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_433, 2), kwargs = {})
#   %unsqueeze_435 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_434, 3), kwargs = {})
#   %mul_686 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_145, %unsqueeze_435), kwargs = {})
#   %convert_element_type_532 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_686, torch.float16), kwargs = {})
#   %slice_31 : Tensor "f16[128, 80, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_532, 1, 0, 80), kwargs = {})
#   %convolution_backward_47 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_31, %getitem_290, %convert_element_type_278, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf367
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16056960}, 'kernel_num_gb': 0.01204352, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2007040
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 80)
    x1 = xindex // 80
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 160*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2007040,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204352
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mt/cmtoafgh4pitd7n3yfpkmczbyvigu4jkepaevpqgp3spoqkk3wbi.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_541 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=getitem_541]
#   %getitem_571 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_571]
#   %getitem_568 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_568]
#   %getitem_601 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_601]
#   %getitem_598 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_598]
#   %cat_50 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_571, %getitem_568], 1), kwargs = {})
#   %add_327 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_541, %cat_50), kwargs = {})
#   %cat_53 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_601, %getitem_598], 1), kwargs = {})
#   %add_332 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_327, %cat_53), kwargs = {})
#   return %add_332
triton_poi_fused_add_cat_76 = async_compile.triton('triton_poi_fused_add_cat_76', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_76', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 56197120}, 'kernel_num_gb': 0.03211264, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_76(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    x1 = xindex // 160
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 80, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr0 + (80*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 160, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr1 + (80*x1 + ((-80) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tl.load(in_ptr2 + (80*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (80*x1 + ((-80) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp5, tmp13, tmp14)
    tmp16 = tmp12 + tmp15
    tl.store(in_out_ptr0 + (x2), tmp16, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_76.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_76.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03211264
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uf/cufhno23ghpcjuibafqf4xrvmeziirgg4v7c7bkpkicteoqi7lzq.py
# Topologically Sorted Source Nodes: [x_112], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_112 => convert_element_type_232
# Graph fragment:
#   %add_332 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=add_332]
#   %getitem_631 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_631]
#   %getitem_628 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=getitem_628]
#   %convolution_88 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convolution_88]
#   %unsqueeze_498 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_498]
#   %sum_68 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_68]
#   %squeeze_106 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=squeeze_106]
#   %sum_67 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=sum_67]
#   %sub_177 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=sub_177]
#   %primals_287 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_287]
#   %cat_56 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_631, %getitem_628], 1), kwargs = {})
#   %add_337 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_332, %cat_56), kwargs = {})
#   %convert_element_type_578 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_337, torch.float32), kwargs = {})
#   %convert_element_type_232 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_88, torch.float32), kwargs = {})
#   %sub_174 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_232, %unsqueeze_498), kwargs = {})
#   %mul_760 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_67, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_499 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_760, 0), kwargs = {})
#   %unsqueeze_500 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_499, 2), kwargs = {})
#   %unsqueeze_501 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_500, 3), kwargs = {})
#   %mul_761 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_68, 3.985969387755102e-05), kwargs = {})
#   %mul_762 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_106, %squeeze_106), kwargs = {})
#   %mul_763 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_761, %mul_762), kwargs = {})
#   %unsqueeze_502 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_763, 0), kwargs = {})
#   %unsqueeze_503 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_502, 2), kwargs = {})
#   %unsqueeze_504 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_503, 3), kwargs = {})
#   %mul_764 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_106, %primals_287), kwargs = {})
#   %unsqueeze_505 : Tensor "f32[1, 160][160, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_764, 0), kwargs = {})
#   %unsqueeze_506 : Tensor "f32[1, 160, 1][160, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_505, 2), kwargs = {})
#   %unsqueeze_507 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_506, 3), kwargs = {})
#   %mul_765 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_174, %unsqueeze_504), kwargs = {})
#   %sub_176 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_578, %mul_765), kwargs = {})
#   %sub_177 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_176, %unsqueeze_501), kwargs = {})
#   %mul_766 : Tensor "f32[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_177, %unsqueeze_507), kwargs = {})
#   %convert_element_type_580 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_766, torch.float16), kwargs = {})
#   %convolution_backward_66 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_580, %mul_280, %convert_element_type_231, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %sub_177,%buf511
triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48172160}, 'kernel_num_gb': 0.03211584, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    x1 = xindex // 160
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 80, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (80*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 160, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (80*x1 + ((-80) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 3.985969387755102e-05
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tmp30 = tmp21 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp31.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp32, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03211584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4o/c4orulm66zqsixibqjwffmln76ay34mx4fy6cyzvazb75da4exrj.py
# Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_108 => add_182, convert_element_type_222, mul_271, mul_277, sub_34, unsqueeze_136, unsqueeze_137, unsqueeze_138, unsqueeze_139
# Graph fragment:
#   %convolution_85 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_85]
#   %getitem_231 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_231]
#   %rsqrt_34 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_34]
#   %primals_277 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_277]
#   %primals_278 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_278]
#   %sub_34 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_85, %getitem_231), kwargs = {})
#   %mul_271 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_34, %rsqrt_34), kwargs = {})
#   %unsqueeze_136 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_277, -1), kwargs = {})
#   %unsqueeze_137 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_136, -1), kwargs = {})
#   %mul_277 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_271, %unsqueeze_137), kwargs = {})
#   %unsqueeze_138 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_278, -1), kwargs = {})
#   %unsqueeze_139 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_138, -1), kwargs = {})
#   %add_182 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_277, %unsqueeze_139), kwargs = {})
#   %convert_element_type_222 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_182, torch.float16), kwargs = {})
#   return %convert_element_type_222
triton_poi_fused__native_batch_norm_legit_functional_78 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_78', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_78', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 93939456}, 'kernel_num_gb': 0.062629632, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_78(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_78.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_78.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062629632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/c5/cc5itpukcveeiwai2hdb5u6fmi5qmarxxialclaj2dj4ujp54hxw.py
# Topologically Sorted Source Nodes: [x_109, sigmoid_8], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
#   x_109 => convert_element_type_223, convert_element_type_224, mul_278, sigmoid_33
# Graph fragment:
#   %getitem_634 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=getitem_634]
#   %convert_element_type_222 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_222]
#   %sum_69 : Tensor "f16[128, 624, 1, 1][624, 1, 79872, 79872]cuda:0" = PlaceHolder[target=sum_69]
#   %convolution_87 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_87]
#   %convert_element_type_223 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_222, torch.float32), kwargs = {})
#   %sigmoid_33 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_223,), kwargs = {})
#   %mul_278 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_223, %sigmoid_33), kwargs = {})
#   %convert_element_type_224 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_278, torch.float16), kwargs = {})
#   %mul_768 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_634, %convert_element_type_224), kwargs = {})
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %sum_69 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_768, [2, 3], True), kwargs = {})
#   %convert_element_type_582 : Tensor "f32[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_69, torch.float32), kwargs = {})
#   %convert_element_type_583 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_35, torch.float32), kwargs = {})
#   %sub_178 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_583), kwargs = {})
#   %mul_770 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_583, %sub_178), kwargs = {})
#   %mul_771 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_582, %mul_770), kwargs = {})
#   %convert_element_type_584 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_771, torch.float16), kwargs = {})
#   return %sum_69,%convert_element_type_584
triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79 = async_compile.triton('triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 63098880, 'r0_': 0}, 'kernel_num_gb': 0.06309888, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 79872
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 122304*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 624*r0_2 + 122304*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tmp0 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tmp11 = tl.load(in_ptr2 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp10 = tmp8.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp12.to(tl.float32)
    tmp14 = 1.0
    tmp15 = tmp14 - tmp13
    tmp16 = tmp13 * tmp15
    tmp17 = tmp10 * tmp16
    tmp18 = tmp17.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp18, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06309888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/la/clactf24rlcqooyoovdy2nhkgic2pkqry56wkow5kie2nrg7taoa.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_637 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=getitem_637]
#   %convolution_86 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=convolution_86]
#   %sigmoid_85 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_86,), kwargs = {})
#   %full_default_22 : Tensor "f16[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 52, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_179 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_22, %sigmoid_85), kwargs = {})
#   %mul_772 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_86, %sub_179), kwargs = {})
#   %add_338 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_772, 1), kwargs = {})
#   %mul_773 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_85, %add_338), kwargs = {})
#   %mul_774 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_637, %mul_773), kwargs = {})
#   return %mul_774
triton_poi_fused_add_fill_mul_sigmoid_sub_80 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_80', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_80', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 53248}, 'kernel_num_gb': 3.9936e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_80(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6656
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 6656,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_80.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_80.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.9936e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b7/cb7xoxssjnrduebhkq5cusc4tfhfucfaicyiby2nsmfbskdsxvj7.py
# Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
#   x_108 => convert_element_type_221, squeeze_102
# Graph fragment:
#   %getitem_634 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=getitem_634]
#   %convolution_87 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_87]
#   %getitem_640 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_640]
#   %convert_element_type_222 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_222]
#   %convolution_85 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_85]
#   %getitem_231 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_231]
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %mul_769 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_634, %sigmoid_35), kwargs = {})
#   %expand_8 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_640, [128, 624, 14, 14]), kwargs = {})
#   %div_8 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_8, 196), kwargs = {})
#   %add_339 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_769, %div_8), kwargs = {})
#   %sigmoid_86 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_222,), kwargs = {})
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_180 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_86), kwargs = {})
#   %mul_775 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_222, %sub_180), kwargs = {})
#   %add_340 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_775, 1), kwargs = {})
#   %mul_776 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_86, %add_340), kwargs = {})
#   %mul_777 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_339, %mul_776), kwargs = {})
#   %convert_element_type_589 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_777, torch.float32), kwargs = {})
#   %squeeze_102 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_231, [0, 2, 3]), kwargs = {})
#   %unsqueeze_508 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_102, 0), kwargs = {})
#   %unsqueeze_509 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_508, 2), kwargs = {})
#   %unsqueeze_510 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_509, 3), kwargs = {})
#   %sum_72 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_589, [0, 2, 3]), kwargs = {})
#   %convert_element_type_221 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_85, torch.float32), kwargs = {})
#   %sub_181 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_221, %unsqueeze_510), kwargs = {})
#   %mul_778 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_589, %sub_181), kwargs = {})
#   %sum_73 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_778, [0, 2, 3]), kwargs = {})
#   return %buf532,%buf534
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 158508480, 'r0_': 0}, 'kernel_num_gb': 0.095229888, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122304
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp22 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp26 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 624*((r0_2 + 128*x1) // 196)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp4 = tl.load(in_ptr2 + (x0 + 624*((r0_2 + 128*x1) // 196)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp8 = tl.load(in_ptr3 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp20 = tl.load(in_ptr4 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = tmp0 * tmp2
        tmp5 = 0.00510204081632653
        tmp6 = tmp4 * tmp5
        tmp7 = tmp3 + tmp6
        tmp9 = tl.sigmoid(tmp8)
        tmp10 = 1.0
        tmp11 = tmp10 - tmp9
        tmp12 = tmp8 * tmp11
        tmp13 = tmp12 + tmp10
        tmp14 = tmp9 * tmp13
        tmp15 = tmp7 * tmp14
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
        tmp21 = tmp20.to(tl.float32)
        tmp23 = tmp21 - tmp22
        tmp24 = tmp16 * tmp23
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, R0_BLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(r0_mask & xmask, tmp27, _tmp26)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp18, xmask)
    tl.store(out_ptr1 + (x3), tmp26, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 122304, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.095229888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/de/cde2ym3wkbfnl373muvjmsgvnko2c2zltz3djrlb3gvwhnf7uwbe.py
# Topologically Sorted Source Nodes: [sigmoid_8], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
# Graph fragment:
#   %buf532 : Tensor "f32[624, 196][1, 624]cuda:0" = PlaceHolder[target=buf532]
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %mul_769 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_634, %sigmoid_35), kwargs = {})
#   %expand_8 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_640, [128, 624, 14, 14]), kwargs = {})
#   %div_8 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_8, 196), kwargs = {})
#   %add_339 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_769, %div_8), kwargs = {})
#   %sigmoid_86 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_222,), kwargs = {})
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_180 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_86), kwargs = {})
#   %mul_775 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_222, %sub_180), kwargs = {})
#   %add_340 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_775, 1), kwargs = {})
#   %mul_776 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_86, %add_340), kwargs = {})
#   %mul_777 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_339, %mul_776), kwargs = {})
#   %convert_element_type_589 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_777, torch.float32), kwargs = {})
#   %sum_72 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_589, [0, 2, 3]), kwargs = {})
#   return %sum_72
triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82 = async_compile.triton('triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 494208, 'r0_': 0}, 'kernel_num_gb': 0.000491712, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 624, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000491712
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ek/cekaaaqw3pvndzfbwdrsf4elikurpf55fs73v6br6m4wrppwswyo.py
# Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
#   x_108 => convert_element_type_221, squeeze_102, squeeze_103
# Graph fragment:
#   %buf534 : Tensor "f32[624, 196][1, 624]cuda:0" = PlaceHolder[target=buf534]
#   %sum_73 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_73]
#   %rsqrt_34 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_34]
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %mul_769 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_634, %sigmoid_35), kwargs = {})
#   %expand_8 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_640, [128, 624, 14, 14]), kwargs = {})
#   %div_8 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_8, 196), kwargs = {})
#   %add_339 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_769, %div_8), kwargs = {})
#   %sigmoid_86 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_222,), kwargs = {})
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_180 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_86), kwargs = {})
#   %mul_775 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_222, %sub_180), kwargs = {})
#   %add_340 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_775, 1), kwargs = {})
#   %mul_776 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_86, %add_340), kwargs = {})
#   %mul_777 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_339, %mul_776), kwargs = {})
#   %convert_element_type_589 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_777, torch.float32), kwargs = {})
#   %squeeze_102 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_231, [0, 2, 3]), kwargs = {})
#   %unsqueeze_508 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_102, 0), kwargs = {})
#   %unsqueeze_509 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_508, 2), kwargs = {})
#   %unsqueeze_510 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_509, 3), kwargs = {})
#   %convert_element_type_221 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_85, torch.float32), kwargs = {})
#   %sub_181 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_221, %unsqueeze_510), kwargs = {})
#   %mul_778 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_589, %sub_181), kwargs = {})
#   %sum_73 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_778, [0, 2, 3]), kwargs = {})
#   %squeeze_103 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_34, [0, 2, 3]), kwargs = {})
#   %mul_786 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_73, %squeeze_103), kwargs = {})
#   return %sum_73,%mul_786
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 501696, 'r0_': 0}, 'kernel_num_gb': 0.000496704, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 624, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000496704
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cb/ccb4cr2znbkpymehipfahv3xtldp5uz34gl4vb3uxcd733jm2h4p.py
# Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
#   x_108 => convert_element_type_221, squeeze_102, squeeze_103
# Graph fragment:
#   %getitem_634 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=getitem_634]
#   %convolution_87 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_87]
#   %getitem_640 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_640]
#   %convert_element_type_222 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_222]
#   %convolution_85 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_85]
#   %getitem_231 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_231]
#   %sum_73 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_73]
#   %rsqrt_34 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_34]
#   %sum_72 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_72]
#   %sub_184 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=sub_184]
#   %primals_277 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_277]
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %mul_769 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_634, %sigmoid_35), kwargs = {})
#   %expand_8 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_640, [128, 624, 14, 14]), kwargs = {})
#   %div_8 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_8, 196), kwargs = {})
#   %add_339 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_769, %div_8), kwargs = {})
#   %sigmoid_86 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_222,), kwargs = {})
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_180 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_86), kwargs = {})
#   %mul_775 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_222, %sub_180), kwargs = {})
#   %add_340 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_775, 1), kwargs = {})
#   %mul_776 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_86, %add_340), kwargs = {})
#   %mul_777 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_339, %mul_776), kwargs = {})
#   %convert_element_type_589 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_777, torch.float32), kwargs = {})
#   %squeeze_102 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_231, [0, 2, 3]), kwargs = {})
#   %unsqueeze_508 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_102, 0), kwargs = {})
#   %unsqueeze_509 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_508, 2), kwargs = {})
#   %unsqueeze_510 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_509, 3), kwargs = {})
#   %convert_element_type_221 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_85, torch.float32), kwargs = {})
#   %sub_181 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_221, %unsqueeze_510), kwargs = {})
#   %mul_779 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_72, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_511 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_779, 0), kwargs = {})
#   %unsqueeze_512 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_511, 2), kwargs = {})
#   %unsqueeze_513 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_512, 3), kwargs = {})
#   %mul_780 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_73, 3.985969387755102e-05), kwargs = {})
#   %squeeze_103 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_34, [0, 2, 3]), kwargs = {})
#   %mul_781 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_103, %squeeze_103), kwargs = {})
#   %mul_782 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_780, %mul_781), kwargs = {})
#   %unsqueeze_514 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_782, 0), kwargs = {})
#   %unsqueeze_515 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_514, 2), kwargs = {})
#   %unsqueeze_516 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_515, 3), kwargs = {})
#   %mul_783 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_103, %primals_277), kwargs = {})
#   %unsqueeze_517 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_783, 0), kwargs = {})
#   %unsqueeze_518 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_517, 2), kwargs = {})
#   %unsqueeze_519 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_518, 3), kwargs = {})
#   %mul_784 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_181, %unsqueeze_516), kwargs = {})
#   %sub_183 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_589, %mul_784), kwargs = {})
#   %sub_184 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_183, %unsqueeze_513), kwargs = {})
#   %mul_785 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_184, %unsqueeze_519), kwargs = {})
#   %convert_element_type_591 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_785, torch.float16), kwargs = {})
#   %convolution_backward_69 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_591, %convert_element_type_219, %convert_element_type_220, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 624, [True, True, False]), kwargs = {})
#   return %sub_184,%buf538
triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'in_ptr9': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 156881088}, 'kernel_num_gb': 0.125571264, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x4 = xindex
    x0 = (xindex % 624)
    x2 = xindex // 122304
    tmp0 = tl.load(in_ptr0 + (x4), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0 + 624*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr2 + (x0 + 624*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tl.load(in_ptr3 + (x4), None).to(tl.float32)
    tmp17 = tl.load(in_ptr4 + (x4), None).to(tl.float32)
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp32 = tl.load(in_ptr9 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp0 * tmp2
    tmp5 = 0.00510204081632653
    tmp6 = tmp4 * tmp5
    tmp7 = tmp3 + tmp6
    tmp9 = tl.sigmoid(tmp8)
    tmp10 = 1.0
    tmp11 = tmp10 - tmp9
    tmp12 = tmp8 * tmp11
    tmp13 = tmp12 + tmp10
    tmp14 = tmp9 * tmp13
    tmp15 = tmp7 * tmp14
    tmp16 = tmp15.to(tl.float32)
    tmp18 = tmp17.to(tl.float32)
    tmp20 = tmp18 - tmp19
    tmp22 = 3.985969387755102e-05
    tmp23 = tmp21 * tmp22
    tmp25 = tmp24 * tmp24
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp16 - tmp27
    tmp30 = tmp29 * tmp22
    tmp31 = tmp28 - tmp30
    tmp33 = tmp24 * tmp32
    tmp34 = tmp31 * tmp33
    tmp35 = tmp34.to(tl.float32)
    tl.store(out_ptr1 + (x4), tmp35, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_10 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.125571264
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uf/cufdfsvqyanbie4u6o5oqk2ezfaxgxr36qiqq6lpkja6wfrl2j75.py
# Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_105 => convert_element_type_216, squeeze_99
# Graph fragment:
#   %getitem_643 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=getitem_643]
#   %convert_element_type_217 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_217]
#   %convolution_84 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_84]
#   %getitem_229 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_229]
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sigmoid_87 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_217,), kwargs = {})
#   %sub_185 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_87), kwargs = {})
#   %mul_787 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_217, %sub_185), kwargs = {})
#   %add_341 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_787, 1), kwargs = {})
#   %mul_788 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_87, %add_341), kwargs = {})
#   %mul_789 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_643, %mul_788), kwargs = {})
#   %convert_element_type_593 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_789, torch.float32), kwargs = {})
#   %squeeze_99 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_229, [0, 2, 3]), kwargs = {})
#   %unsqueeze_520 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_99, 0), kwargs = {})
#   %unsqueeze_521 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_520, 2), kwargs = {})
#   %unsqueeze_522 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_521, 3), kwargs = {})
#   %sum_74 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_593, [0, 2, 3]), kwargs = {})
#   %convert_element_type_216 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_84, torch.float32), kwargs = {})
#   %sub_186 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_216, %unsqueeze_522), kwargs = {})
#   %mul_790 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_593, %sub_186), kwargs = {})
#   %sum_75 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_790, [0, 2, 3]), kwargs = {})
#   return %buf544,%buf546
triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 95888832, 'r0_': 0}, 'kernel_num_gb': 0.0949104, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122304
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp15 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp19 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp13 = tl.load(in_ptr2 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = 1.0
        tmp4 = tmp3 - tmp2
        tmp5 = tmp1 * tmp4
        tmp6 = tmp5 + tmp3
        tmp7 = tmp2 * tmp6
        tmp8 = tmp0 * tmp7
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
        tmp14 = tmp13.to(tl.float32)
        tmp16 = tmp14 - tmp15
        tmp17 = tmp9 * tmp16
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, R0_BLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(r0_mask & xmask, tmp20, _tmp19)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tl.store(out_ptr1 + (x3), tmp19, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((624, 196), (1, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 122304, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0949104
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nh/cnhnxmvqec35ljw2rodqvnzagivzgetxs7uocske3a4ifshysy7f.py
# Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_105 => convert_element_type_216, squeeze_100, squeeze_99
# Graph fragment:
#   %getitem_643 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=getitem_643]
#   %convert_element_type_217 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_217]
#   %convolution_84 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_84]
#   %getitem_229 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_229]
#   %sum_75 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_75]
#   %rsqrt_33 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_33]
#   %sum_74 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_74]
#   %primals_271 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_271]
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sigmoid_87 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_217,), kwargs = {})
#   %sub_185 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_87), kwargs = {})
#   %mul_787 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_217, %sub_185), kwargs = {})
#   %add_341 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_787, 1), kwargs = {})
#   %mul_788 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_87, %add_341), kwargs = {})
#   %mul_789 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_643, %mul_788), kwargs = {})
#   %convert_element_type_593 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_789, torch.float32), kwargs = {})
#   %squeeze_99 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_229, [0, 2, 3]), kwargs = {})
#   %unsqueeze_520 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_99, 0), kwargs = {})
#   %unsqueeze_521 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_520, 2), kwargs = {})
#   %unsqueeze_522 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_521, 3), kwargs = {})
#   %convert_element_type_216 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_84, torch.float32), kwargs = {})
#   %sub_186 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_216, %unsqueeze_522), kwargs = {})
#   %mul_791 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_74, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_523 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_791, 0), kwargs = {})
#   %unsqueeze_524 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_523, 2), kwargs = {})
#   %unsqueeze_525 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_524, 3), kwargs = {})
#   %mul_792 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_75, 3.985969387755102e-05), kwargs = {})
#   %squeeze_100 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_33, [0, 2, 3]), kwargs = {})
#   %mul_793 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_100, %squeeze_100), kwargs = {})
#   %mul_794 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_792, %mul_793), kwargs = {})
#   %unsqueeze_526 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_794, 0), kwargs = {})
#   %unsqueeze_527 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_526, 2), kwargs = {})
#   %unsqueeze_528 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_527, 3), kwargs = {})
#   %mul_795 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_100, %primals_271), kwargs = {})
#   %unsqueeze_529 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_795, 0), kwargs = {})
#   %unsqueeze_530 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_529, 2), kwargs = {})
#   %unsqueeze_531 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_530, 3), kwargs = {})
#   %mul_796 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_186, %unsqueeze_528), kwargs = {})
#   %sub_188 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_593, %mul_796), kwargs = {})
#   %sub_189 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_188, %unsqueeze_525), kwargs = {})
#   %mul_797 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_189, %unsqueeze_531), kwargs = {})
#   %convert_element_type_595 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_797, torch.float16), kwargs = {})
#   %convolution_backward_70 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_595, %add_172, %convert_element_type_215, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf549
triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 156561600}, 'kernel_num_gb': 0.125251776, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp10 = tl.load(in_ptr1 + (x2), None).to(tl.float32)
    tmp12 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp22 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tmp9 = tmp8.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp13 = tmp11 - tmp12
    tmp15 = 3.985969387755102e-05
    tmp16 = tmp14 * tmp15
    tmp18 = tmp17 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp13 * tmp19
    tmp21 = tmp9 - tmp20
    tmp23 = tmp22 * tmp15
    tmp24 = tmp21 - tmp23
    tmp26 = tmp17 * tmp25
    tmp27 = tmp24 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.125251776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g4/cg4putp67dedlgi36gafszu6fh4u7ayx6opibh67jnkgmgn35mm7.py
# Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_102 => convert_element_type_213
# Graph fragment:
#   %getitem_646 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=getitem_646]
#   %cat_24 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=cat_24]
#   %unsqueeze_534 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_534]
#   %convert_element_type_597 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_646, torch.float32), kwargs = {})
#   %sum_76 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_597, [0, 2, 3]), kwargs = {})
#   %convert_element_type_213 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_24, torch.float32), kwargs = {})
#   %sub_190 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_213, %unsqueeze_534), kwargs = {})
#   %mul_799 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_597, %sub_190), kwargs = {})
#   %sum_77 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_799, [0, 2, 3]), kwargs = {})
#   return %buf554,%buf556
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10763168, 'r0_': 0}, 'kernel_num_gb': 0.010600096, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 20384
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 104)
    x1 = xindex // 104
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_2 + 13312*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 104*r0_2 + 13312*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 20384, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.010600096
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sf/csflrdybpn2x7nq6fm3juvzfmi4x73bgz32ociia7opewmyc6sqx.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf554 : Tensor "f32[104, 196][1, 104]cuda:0" = PlaceHolder[target=buf554]
#   %convert_element_type_597 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_646, torch.float32), kwargs = {})
#   %sum_76 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_597, [0, 2, 3]), kwargs = {})
#   return %sum_76
triton_red_fused_native_batch_norm_backward_88 = async_compile.triton('triton_red_fused_native_batch_norm_backward_88', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 128, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_88', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 82368, 'r0_': 0}, 'kernel_num_gb': 8.1952e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_88(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 104
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 104, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_88.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_88.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.1952e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7u/c7u5lmn64dki75t3wcu5rparp77whv5l75kk2lbcxhczoyweigbk.py
# Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_102 => convert_element_type_213
# Graph fragment:
#   %buf556 : Tensor "f32[104, 196][1, 104]cuda:0" = PlaceHolder[target=buf556]
#   %sum_77 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_77]
#   %squeeze_97 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_97]
#   %convert_element_type_597 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_646, torch.float32), kwargs = {})
#   %convert_element_type_213 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_24, torch.float32), kwargs = {})
#   %sub_190 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_213, %unsqueeze_534), kwargs = {})
#   %mul_799 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_597, %sub_190), kwargs = {})
#   %sum_77 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_799, [0, 2, 3]), kwargs = {})
#   %mul_807 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_77, %squeeze_97), kwargs = {})
#   return %sum_77,%mul_807
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 128, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 83616, 'r0_': 0}, 'kernel_num_gb': 8.2784e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 104
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 104, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.2784e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rp/crpbsesqhl2d7iqyfv5pbrrafs5bgohkkzqxc7kbgpkt5q7wylox.py
# Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_102 => convert_element_type_213
# Graph fragment:
#   %getitem_646 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=getitem_646]
#   %cat_24 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=cat_24]
#   %unsqueeze_534 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_534]
#   %sum_77 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_77]
#   %squeeze_97 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_97]
#   %sum_76 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_76]
#   %primals_265 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_265]
#   %convert_element_type_597 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_646, torch.float32), kwargs = {})
#   %convert_element_type_213 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_24, torch.float32), kwargs = {})
#   %sub_190 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_213, %unsqueeze_534), kwargs = {})
#   %mul_800 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_76, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_535 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_800, 0), kwargs = {})
#   %unsqueeze_536 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_535, 2), kwargs = {})
#   %unsqueeze_537 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_536, 3), kwargs = {})
#   %mul_801 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_77, 3.985969387755102e-05), kwargs = {})
#   %mul_802 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_97, %squeeze_97), kwargs = {})
#   %mul_803 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_801, %mul_802), kwargs = {})
#   %unsqueeze_538 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_803, 0), kwargs = {})
#   %unsqueeze_539 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_538, 2), kwargs = {})
#   %unsqueeze_540 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_539, 3), kwargs = {})
#   %mul_804 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_97, %primals_265), kwargs = {})
#   %unsqueeze_541 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_804, 0), kwargs = {})
#   %unsqueeze_542 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_541, 2), kwargs = {})
#   %unsqueeze_543 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_542, 3), kwargs = {})
#   %mul_805 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_190, %unsqueeze_540), kwargs = {})
#   %sub_192 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_597, %mul_805), kwargs = {})
#   %sub_193 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_192, %unsqueeze_537), kwargs = {})
#   %mul_806 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_193, %unsqueeze_543), kwargs = {})
#   %convert_element_type_599 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_806, torch.float16), kwargs = {})
#   return %convert_element_type_599
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20875296}, 'kernel_num_gb': 0.015656992, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 3.985969387755102e-05
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015656992
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/q2/cq23zi6wfpcqaoeehs53izhoaydwjwejmcgc6vfbro3dibhp2lla.py
# Topologically Sorted Source Nodes: [x_99, sigmoid_7], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_7 => sigmoid_31
#   x_99 => convert_element_type_203, convert_element_type_204, mul_253, sigmoid_29
# Graph fragment:
#   %getitem_652 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_652]
#   %getitem_649 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_649]
#   %convert_element_type_202 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_202]
#   %sum_78 : Tensor "f16[128, 624, 1, 1][624, 1, 79872, 79872]cuda:0" = PlaceHolder[target=sum_78]
#   %convolution_81 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_81]
#   %cat_57 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_652, %getitem_649], 1), kwargs = {})
#   %convert_element_type_203 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_202, torch.float32), kwargs = {})
#   %sigmoid_29 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_203,), kwargs = {})
#   %mul_253 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_203, %sigmoid_29), kwargs = {})
#   %convert_element_type_204 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_253, torch.float16), kwargs = {})
#   %mul_808 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_57, %convert_element_type_204), kwargs = {})
#   %sigmoid_31 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_81,), kwargs = {})
#   %sum_78 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_808, [2, 3], True), kwargs = {})
#   %convert_element_type_602 : Tensor "f32[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_78, torch.float32), kwargs = {})
#   %convert_element_type_603 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_31, torch.float32), kwargs = {})
#   %sub_194 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_603), kwargs = {})
#   %mul_810 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_603, %sub_194), kwargs = {})
#   %mul_811 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_602, %mul_810), kwargs = {})
#   %convert_element_type_604 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_811, torch.float16), kwargs = {})
#   return %sum_78,%convert_element_type_604
triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91 = async_compile.triton('triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 94408704, 'r0_': 0}, 'kernel_num_gb': 0.06309888, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 79872
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 624*r0_2 + 122304*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 312, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (312*r0_2 + 61152*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 624, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (312*r0_2 + 61152*x1 + ((-312) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tl.sigmoid(tmp12)
        tmp14 = tmp12 * tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp10 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp21 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tmp18.to(tl.float32)
    tmp22 = tl.sigmoid(tmp21)
    tmp23 = tmp22.to(tl.float32)
    tmp24 = 1.0
    tmp25 = tmp24 - tmp23
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp28, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06309888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rz/crzrqmiyob32nrhtovzx7sqxixa2fs7lxo5nycxbgocjsndb6jkv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_655 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0" = PlaceHolder[target=getitem_655]
#   %convolution_80 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0" = PlaceHolder[target=convolution_80]
#   %sigmoid_88 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_80,), kwargs = {})
#   %full_default_25 : Tensor "f16[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.full.default](args = ([128, 26, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_195 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_25, %sigmoid_88), kwargs = {})
#   %mul_812 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_80, %sub_195), kwargs = {})
#   %add_342 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_812, 1), kwargs = {})
#   %mul_813 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_88, %add_342), kwargs = {})
#   %mul_814 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_655, %mul_813), kwargs = {})
#   return %mul_814
triton_poi_fused_add_fill_mul_sigmoid_sub_92 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_92', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_92', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 26624}, 'kernel_num_gb': 1.9968e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_92(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3328
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3328,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_92.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_92.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.9968e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/oz/coz5wi73cotayuysav3saacntglm5fl5zv2dhyci37ifj7zn6jqn.py
# Topologically Sorted Source Nodes: [sigmoid_7], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_7 => sigmoid_31
# Graph fragment:
#   %getitem_652 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_652]
#   %getitem_649 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_649]
#   %convolution_81 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_81]
#   %getitem_658 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_658]
#   %convert_element_type_202 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_202]
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_57 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_652, %getitem_649], 1), kwargs = {})
#   %sigmoid_31 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_81,), kwargs = {})
#   %mul_809 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_57, %sigmoid_31), kwargs = {})
#   %expand_9 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_658, [128, 624, 14, 14]), kwargs = {})
#   %div_9 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_9, 196), kwargs = {})
#   %add_343 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_809, %div_9), kwargs = {})
#   %sigmoid_89 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_202,), kwargs = {})
#   %sub_196 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_89), kwargs = {})
#   %mul_815 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_202, %sub_196), kwargs = {})
#   %add_344 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_815, 1), kwargs = {})
#   %mul_816 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_89, %add_344), kwargs = {})
#   %mul_817 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_343, %mul_816), kwargs = {})
#   %convert_element_type_609 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_817, torch.float32), kwargs = {})
#   return %convert_element_type_609
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 125239296, 'x': 94248960}, 'kernel_num_gb': 0.125558784, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 624
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 196
    y0 = (yindex % 196)
    tmp11 = tl.load(in_ptr2 + (x2 + 624*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 624*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 624*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 312, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (312*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 624, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (312*y3 + ((-312) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.00510204081632653
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 196*x2 + 122304*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 624,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.125558784
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wr/cwrcuxjitljw3ka3sd5gqi53or4bvjnqmtzyeztkndzfqvxbnqqp.py
# Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_98 => convert_element_type_201, squeeze_93
# Graph fragment:
#   %convert_element_type_609 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_609]
#   %cat_23 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_23]
#   %getitem_223 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_223]
#   %squeeze_93 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_223, [0, 2, 3]), kwargs = {})
#   %unsqueeze_544 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_93, 0), kwargs = {})
#   %unsqueeze_545 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_544, 2), kwargs = {})
#   %unsqueeze_546 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_545, 3), kwargs = {})
#   %convert_element_type_201 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_23, torch.float32), kwargs = {})
#   %sub_197 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_201, %unsqueeze_546), kwargs = {})
#   %mul_818 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_609, %sub_197), kwargs = {})
#   %sum_82 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_818, [0, 2, 3]), kwargs = {})
#   return %buf586
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 1024, 'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31312320, 'x': 978432, 'r0_': 62619648}, 'kernel_num_gb': 0.094421184, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 624
    xnumel = 196
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp3 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp7 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (196*y0 + 122304*((r0_2 + 128*x1) // 196) + (((r0_2 + 128*x1) % 196))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (y0 + 624*r0_2 + 79872*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp4 = tmp2 - tmp3
        tmp5 = tmp0 * tmp4
        tmp6 = tl.broadcast_to(tmp5, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask & xmask & ymask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 196*y0), tmp7, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 624, 196, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.094421184
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sd/csd7lkm6mkx6zsj2ludi4lkvkngei65g5awcjzflgak5xzisiaub.py
# Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_98 => convert_element_type_201, squeeze_93, squeeze_94
# Graph fragment:
#   %buf586 : Tensor "f32[624, 196][196, 1]cuda:0" = PlaceHolder[target=buf586]
#   %sum_82 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_82]
#   %rsqrt_31 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_31]
#   %squeeze_93 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_223, [0, 2, 3]), kwargs = {})
#   %unsqueeze_544 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_93, 0), kwargs = {})
#   %unsqueeze_545 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_544, 2), kwargs = {})
#   %unsqueeze_546 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_545, 3), kwargs = {})
#   %convert_element_type_201 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_23, torch.float32), kwargs = {})
#   %sub_197 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_201, %unsqueeze_546), kwargs = {})
#   %mul_818 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_609, %sub_197), kwargs = {})
#   %sum_82 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_818, [0, 2, 3]), kwargs = {})
#   %squeeze_94 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_31, [0, 2, 3]), kwargs = {})
#   %mul_826 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_82, %squeeze_94), kwargs = {})
#   return %sum_82,%mul_826
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 256},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12480, 'r0_': 489216}, 'kernel_num_gb': 0.000496704, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 196
    R0_BLOCK: tl.constexpr = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((624, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 624, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000496704
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/os/cosakbg7wypkdem7oqdxng7fgoayp6fda7hlp7xzcfzswcfbxwe4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_609 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_609]
#   %sum_81 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_609, [0, 2, 3]), kwargs = {})
#   return %sum_81
triton_red_fused_native_batch_norm_backward_96 = async_compile.triton('triton_red_fused_native_batch_norm_backward_96', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 32768},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_96', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4992, 'r0_': 62619648}, 'kernel_num_gb': 0.062622144, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_96(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 25088
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 196)
        r0_2 = r0_index // 196
        tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0 + 122304*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 624, 25088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_96.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_96.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062622144
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6s/c6s5nf6xobgdltu52uivsaoksqvxzpjxcza7s26p7nhscb2j3rrm.py
# Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_98 => convert_element_type_201, squeeze_93, squeeze_94
# Graph fragment:
#   %convert_element_type_609 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_609]
#   %cat_23 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_23]
#   %getitem_223 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_223]
#   %sum_82 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_82]
#   %rsqrt_31 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_31]
#   %sum_81 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_81]
#   %primals_254 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_254]
#   %squeeze_93 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_223, [0, 2, 3]), kwargs = {})
#   %unsqueeze_544 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_93, 0), kwargs = {})
#   %unsqueeze_545 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_544, 2), kwargs = {})
#   %unsqueeze_546 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_545, 3), kwargs = {})
#   %convert_element_type_201 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_23, torch.float32), kwargs = {})
#   %sub_197 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_201, %unsqueeze_546), kwargs = {})
#   %mul_819 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_81, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_547 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_819, 0), kwargs = {})
#   %unsqueeze_548 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_547, 2), kwargs = {})
#   %unsqueeze_549 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_548, 3), kwargs = {})
#   %mul_820 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_82, 3.985969387755102e-05), kwargs = {})
#   %squeeze_94 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_31, [0, 2, 3]), kwargs = {})
#   %mul_821 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_94, %squeeze_94), kwargs = {})
#   %mul_822 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_820, %mul_821), kwargs = {})
#   %unsqueeze_550 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_822, 0), kwargs = {})
#   %unsqueeze_551 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_550, 2), kwargs = {})
#   %unsqueeze_552 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_551, 3), kwargs = {})
#   %mul_823 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_94, %primals_254), kwargs = {})
#   %unsqueeze_553 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_823, 0), kwargs = {})
#   %unsqueeze_554 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_553, 2), kwargs = {})
#   %unsqueeze_555 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_554, 3), kwargs = {})
#   %mul_824 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_197, %unsqueeze_552), kwargs = {})
#   %sub_199 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_609, %mul_824), kwargs = {})
#   %sub_200 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_199, %unsqueeze_549), kwargs = {})
#   %mul_825 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_200, %unsqueeze_555), kwargs = {})
#   %convert_element_type_611 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_825, torch.float16), kwargs = {})
#   return %convert_element_type_611
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31322304, 'x': 125239296}, 'kernel_num_gb': 0.125251776, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 79872
    xnumel = 196
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 624)
    y1 = yindex // 624
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y3), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + 624*x2 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (y0), ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), ymask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (y0), ymask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 3.985969387755102e-05
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y3), tmp19, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.125251776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sc/cscgixjlfq3yewat3vrcbh6xmrjpchmcqatdlx33q2ju24w6sji5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_611 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_611]
#   %slice_52 : Tensor "f16[128, 156, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_611, 1, 468, 624), kwargs = {})
#   %convolution_backward_75 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_52, %getitem_221, %convert_element_type_200, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 156, [True, True, False]), kwargs = {})
#   return %buf590
triton_poi_fused_convolution_backward_slice_98 = async_compile.triton('triton_poi_fused_convolution_backward_slice_98', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_98', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_98(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (91728 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_98.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_98.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4c/c4cka6hitkoihkkohfwf3k75nycsq6c4ifaauf3cqkuc73wvd77g.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_611 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_611]
#   %slice_51 : Tensor "f16[128, 156, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_611, 1, 312, 468), kwargs = {})
#   %convolution_backward_76 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_51, %getitem_216, %convert_element_type_199, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 156, [True, True, False]), kwargs = {})
#   return %buf595
triton_poi_fused_convolution_backward_slice_99 = async_compile.triton('triton_poi_fused_convolution_backward_slice_99', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_99', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_99(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (61152 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_99.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_99.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zt/cztybmaqbygnvqdikaaiia27npci7scekszy3s4xqbesn6pqywbp.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_611 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_611]
#   %slice_50 : Tensor "f16[128, 156, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_611, 1, 156, 312), kwargs = {})
#   %convolution_backward_77 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_50, %getitem_211, %convert_element_type_198, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 156, [True, True, False]), kwargs = {})
#   return %buf600
triton_poi_fused_convolution_backward_slice_100 = async_compile.triton('triton_poi_fused_convolution_backward_slice_100', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_100', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_100(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (30576 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_100.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_100.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ob/cobjsbxxsmfzzm23vw7rfyqb6cu3ue5nazxzesbup5cnqjogot7t.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_611 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_611]
#   %slice_49 : Tensor "f16[128, 156, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_611, 1, 0, 156), kwargs = {})
#   %convolution_backward_78 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_49, %getitem_206, %convert_element_type_197, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 156, [True, True, False]), kwargs = {})
#   return %buf605
triton_poi_fused_convolution_backward_slice_101 = async_compile.triton('triton_poi_fused_convolution_backward_slice_101', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_101', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_101(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_101.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_101.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hf/chfehd6svkclg476k4fsr3itsgtjw2yvg33or7mnkszd46aurowb.py
# Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
#   x_95 => add_161, convert_element_type_194, mul_238, mul_244, sub_30, unsqueeze_120, unsqueeze_121, unsqueeze_122, unsqueeze_123
# Graph fragment:
#   %cat_22 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_22]
#   %getitem_201 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_201]
#   %rsqrt_30 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_30]
#   %primals_245 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_245]
#   %primals_246 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_246]
#   %getitem_670 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=getitem_670]
#   %getitem_667 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=getitem_667]
#   %getitem_664 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=getitem_664]
#   %getitem_661 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=getitem_661]
#   %convert_element_type_194 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_194]
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_58 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_670, %getitem_667, %getitem_664, %getitem_661], 1), kwargs = {})
#   %sub_30 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_22, %getitem_201), kwargs = {})
#   %mul_238 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_30, %rsqrt_30), kwargs = {})
#   %unsqueeze_120 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_245, -1), kwargs = {})
#   %unsqueeze_121 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_120, -1), kwargs = {})
#   %mul_244 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_238, %unsqueeze_121), kwargs = {})
#   %unsqueeze_122 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_246, -1), kwargs = {})
#   %unsqueeze_123 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_122, -1), kwargs = {})
#   %add_161 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_244, %unsqueeze_123), kwargs = {})
#   %convert_element_type_194 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_161, torch.float16), kwargs = {})
#   %sigmoid_90 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_194,), kwargs = {})
#   %sub_201 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_90), kwargs = {})
#   %mul_827 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_194, %sub_201), kwargs = {})
#   %add_345 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_827, 1), kwargs = {})
#   %mul_828 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_90, %add_345), kwargs = {})
#   %mul_829 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_58, %mul_828), kwargs = {})
#   return %convert_element_type_194,%mul_829
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 62619648, 'x': 156559104}, 'kernel_num_gb': 0.093939456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 624
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 624*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x1
    tmp12 = tl.full([1, 1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1, 1], 156, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (156*y0 + (x1)), tmp15 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1, 1], 312, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (156*y0 + ((-156) + x1)), tmp20 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1, 1], 468, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tmp22 & tmp24
    tmp26 = tl.load(in_ptr7 + (156*y0 + ((-312) + x1)), tmp25 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tmp11 >= tmp23
    tmp28 = tl.full([1, 1], 624, tl.int64)
    tmp29 = tmp11 < tmp28
    tmp30 = tl.load(in_ptr8 + (156*y0 + ((-468) + x1)), tmp27 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tl.where(tmp25, tmp26, tmp30)
    tmp32 = tl.where(tmp20, tmp21, tmp31)
    tmp33 = tl.where(tmp15, tmp16, tmp32)
    tmp34 = tl.sigmoid(tmp10)
    tmp35 = 1.0
    tmp36 = tmp35 - tmp34
    tmp37 = tmp10 * tmp36
    tmp38 = tmp37 + tmp35
    tmp39 = tmp34 * tmp38
    tmp40 = tmp33 * tmp39
    tl.store(out_ptr1 + (y2 + 196*x1 + 122304*y3), tmp40, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 25088, 624,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.093939456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pz/cpzdisxi3iafkgi63kgoijplf6soxmnygteri2h7y5buywr55bks.py
# Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_95 => convert_element_type_193, squeeze_90
# Graph fragment:
#   %mul_829 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_829]
#   %cat_22 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_22]
#   %getitem_201 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_201]
#   %convert_element_type_616 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_829, torch.float32), kwargs = {})
#   %squeeze_90 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_201, [0, 2, 3]), kwargs = {})
#   %unsqueeze_556 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_90, 0), kwargs = {})
#   %unsqueeze_557 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_556, 2), kwargs = {})
#   %unsqueeze_558 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_557, 3), kwargs = {})
#   %convert_element_type_193 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_22, torch.float32), kwargs = {})
#   %sub_202 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_193, %unsqueeze_558), kwargs = {})
#   %mul_830 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_616, %sub_202), kwargs = {})
#   %sum_84 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_830, [0, 2, 3]), kwargs = {})
#   return %buf613
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 1024, 'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31312320, 'x': 978432, 'r0_': 31309824}, 'kernel_num_gb': 0.06311136, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 624
    xnumel = 196
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp4 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp8 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (196*y0 + 122304*((r0_2 + 128*x1) // 196) + (((r0_2 + 128*x1) % 196))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (y0 + 624*r0_2 + 79872*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp5 = tmp3 - tmp4
        tmp6 = tmp1 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask & ymask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 196*y0), tmp8, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624, 196), (196, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 624, 196, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06311136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/el/cel7mmv7p4naajd2zdpci7h2klmj4mvqydjbgztm7n7w4qixgtb5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_829 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_829]
#   %convert_element_type_616 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_829, torch.float32), kwargs = {})
#   %sum_83 : Tensor "f32[624][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_616, [0, 2, 3]), kwargs = {})
#   return %sum_83
triton_red_fused_native_batch_norm_backward_104 = async_compile.triton('triton_red_fused_native_batch_norm_backward_104', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 32768},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_104', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4992, 'r0_': 31309824}, 'kernel_num_gb': 0.03131232, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_104(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 25088
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 196)
        r0_2 = r0_index // 196
        tmp0 = tl.load(in_ptr0 + (r0_1 + 196*x0 + 122304*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 624, 25088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_104.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_104.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03131232
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xe/cxevhzileatejc6r3qksmrgjvqpky5ng6yv3em6sgldvjvfackkn.py
# Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_95 => convert_element_type_193, squeeze_90, squeeze_91
# Graph fragment:
#   %mul_829 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_829]
#   %cat_22 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_22]
#   %getitem_201 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_201]
#   %sum_84 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_84]
#   %rsqrt_30 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_30]
#   %sum_83 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=sum_83]
#   %primals_245 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_245]
#   %convert_element_type_616 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_829, torch.float32), kwargs = {})
#   %squeeze_90 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_201, [0, 2, 3]), kwargs = {})
#   %unsqueeze_556 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_90, 0), kwargs = {})
#   %unsqueeze_557 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_556, 2), kwargs = {})
#   %unsqueeze_558 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_557, 3), kwargs = {})
#   %convert_element_type_193 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_22, torch.float32), kwargs = {})
#   %sub_202 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_193, %unsqueeze_558), kwargs = {})
#   %mul_831 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_83, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_559 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_831, 0), kwargs = {})
#   %unsqueeze_560 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_559, 2), kwargs = {})
#   %unsqueeze_561 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_560, 3), kwargs = {})
#   %mul_832 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_84, 3.985969387755102e-05), kwargs = {})
#   %squeeze_91 : Tensor "f32[624][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_30, [0, 2, 3]), kwargs = {})
#   %mul_833 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_91, %squeeze_91), kwargs = {})
#   %mul_834 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_832, %mul_833), kwargs = {})
#   %unsqueeze_562 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_834, 0), kwargs = {})
#   %unsqueeze_563 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_562, 2), kwargs = {})
#   %unsqueeze_564 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_563, 3), kwargs = {})
#   %mul_835 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_91, %primals_245), kwargs = {})
#   %unsqueeze_565 : Tensor "f32[1, 624][624, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_835, 0), kwargs = {})
#   %unsqueeze_566 : Tensor "f32[1, 624, 1][624, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_565, 2), kwargs = {})
#   %unsqueeze_567 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_566, 3), kwargs = {})
#   %mul_836 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_202, %unsqueeze_564), kwargs = {})
#   %sub_204 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_616, %mul_836), kwargs = {})
#   %sub_205 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_204, %unsqueeze_561), kwargs = {})
#   %mul_837 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_205, %unsqueeze_567), kwargs = {})
#   %convert_element_type_618 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_837, torch.float16), kwargs = {})
#   return %convert_element_type_618
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31322304, 'x': 93929472}, 'kernel_num_gb': 0.093941952, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 79872
    xnumel = 196
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 624)
    y1 = yindex // 624
    tmp0 = tl.load(in_out_ptr0 + (x2 + 196*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr0 + (y0 + 624*x2 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (y0), ymask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (y0), ymask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (y0), ymask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 3.985969387755102e-05
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + 196*y3), tmp20, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.093941952
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nx/cnxmm7v4woav5htjvpnwrnefjmmb765v6pxlyjxh2ohavzecajlg.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_618 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_618]
#   %slice_54 : Tensor "f16[128, 312, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_618, 1, 312, 624), kwargs = {})
#   %convolution_backward_79 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_54, %getitem_199, %convert_element_type_192, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf617
triton_poi_fused_convolution_backward_slice_106 = async_compile.triton('triton_poi_fused_convolution_backward_slice_106', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_106', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31309824, 'x': 15654912}, 'kernel_num_gb': 0.031309824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_106(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 39936
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 312)
    y1 = yindex // 312
    tmp0 = tl.load(in_ptr0 + (61152 + x2 + 196*y0 + 122304*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 312*x2 + 61152*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 39936, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_106.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_106.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.031309824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zj/czjs2fdblzodwg5whv2q34j7tuf5d5336cnltn5lfw7feaaif2d4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_618 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_618]
#   %slice_53 : Tensor "f16[128, 312, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_618, 1, 0, 312), kwargs = {})
#   %convolution_backward_80 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_53, %getitem_198, %convert_element_type_191, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf622
triton_poi_fused_convolution_backward_slice_107 = async_compile.triton('triton_poi_fused_convolution_backward_slice_107', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_107', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31309824, 'x': 15654912}, 'kernel_num_gb': 0.031309824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_107(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 39936
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 312)
    y1 = yindex // 312
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 122304*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 312*x2 + 61152*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 39936, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_107.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_107.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.031309824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mc/cmcgvzibzkzwpnuk7egzztqphhercbgynjnggndvx7w25ubping7.py
# Topologically Sorted Source Nodes: [x_92], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_92 => convert_element_type_189
# Graph fragment:
#   %getitem_646 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=getitem_646]
#   %getitem_676 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_676]
#   %getitem_673 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_673]
#   %cat_21 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=cat_21]
#   %unsqueeze_570 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_570]
#   %cat_59 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_676, %getitem_673], 1), kwargs = {})
#   %add_346 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_646, %cat_59), kwargs = {})
#   %convert_element_type_621 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_346, torch.float32), kwargs = {})
#   %sum_85 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_621, [0, 2, 3]), kwargs = {})
#   %convert_element_type_189 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_21, torch.float32), kwargs = {})
#   %sub_206 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_189, %unsqueeze_570), kwargs = {})
#   %mul_839 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_621, %sub_206), kwargs = {})
#   %sum_86 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_839, [0, 2, 3]), kwargs = {})
#   return %buf627,%buf629
triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21199776, 'r0_': 0}, 'kernel_num_gb': 0.0158184, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 20384
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 104)
    x1 = xindex // 104
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp19 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp23 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_2 + 13312*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp17 = tl.load(in_ptr3 + (x0 + 104*r0_2 + 13312*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = x0
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 52, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = tl.load(in_ptr1 + (52*r0_2 + 6656*x1 + (x0)), r0_mask & tmp5 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp1 >= tmp4
        tmp8 = tl.full([1, 1], 104, tl.int64)
        tmp9 = tmp1 < tmp8
        tmp10 = tl.load(in_ptr2 + (52*r0_2 + 6656*x1 + ((-52) + x0)), r0_mask & tmp7 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tl.where(tmp5, tmp6, tmp10)
        tmp12 = tmp0 + tmp11
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask & xmask, tmp16, _tmp15)
        tmp18 = tmp17.to(tl.float32)
        tmp20 = tmp18 - tmp19
        tmp21 = tmp13 * tmp20
        tmp22 = tl.broadcast_to(tmp21, [XBLOCK, R0_BLOCK])
        tmp24 = _tmp23 + tmp22
        _tmp23 = tl.where(r0_mask & xmask, tmp24, _tmp23)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tmp23 = tl.sum(_tmp23, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
    tl.store(out_ptr1 + (x3), tmp23, xmask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((104, 196), (1, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 20384, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0158184
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/r3/cr3eutoyudwjp35dvgo2dlfrsr4ecyfqjcgjj7u56lt5yxdy6jxs.py
# Topologically Sorted Source Nodes: [x_92], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_92 => convert_element_type_189
# Graph fragment:
#   %getitem_646 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=getitem_646]
#   %getitem_676 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_676]
#   %getitem_673 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_673]
#   %cat_21 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=cat_21]
#   %unsqueeze_570 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_570]
#   %sum_86 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_86]
#   %squeeze_88 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_88]
#   %sum_85 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_85]
#   %cat_59 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_676, %getitem_673], 1), kwargs = {})
#   %add_346 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_646, %cat_59), kwargs = {})
#   %convert_element_type_621 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_346, torch.float32), kwargs = {})
#   %convert_element_type_189 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_21, torch.float32), kwargs = {})
#   %sub_206 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_189, %unsqueeze_570), kwargs = {})
#   %mul_840 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_85, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_571 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_840, 0), kwargs = {})
#   %unsqueeze_572 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_571, 2), kwargs = {})
#   %unsqueeze_573 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_572, 3), kwargs = {})
#   %mul_841 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_86, 3.985969387755102e-05), kwargs = {})
#   %mul_842 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_88, %squeeze_88), kwargs = {})
#   %mul_843 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_841, %mul_842), kwargs = {})
#   %unsqueeze_574 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_843, 0), kwargs = {})
#   %unsqueeze_575 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_574, 2), kwargs = {})
#   %unsqueeze_576 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_575, 3), kwargs = {})
#   %mul_845 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_206, %unsqueeze_576), kwargs = {})
#   %sub_208 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_621, %mul_845), kwargs = {})
#   %sub_209 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_208, %unsqueeze_573), kwargs = {})
#   return %sub_209
triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 41748096}, 'kernel_num_gb': 0.026093184, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    x1 = xindex // 104
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 52, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (52*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 104, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (52*x1 + ((-52) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 3.985969387755102e-05
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tl.store(out_ptr0 + (x2), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.026093184
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/na/cnatjom4n3bvtkzfx5lk4uci6wokjsrgxedrebng77n75fhkgiof.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_209 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=sub_209]
#   %squeeze_88 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_88]
#   %primals_238 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_238]
#   %mul_844 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_88, %primals_238), kwargs = {})
#   %unsqueeze_577 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_844, 0), kwargs = {})
#   %unsqueeze_578 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_577, 2), kwargs = {})
#   %unsqueeze_579 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_578, 3), kwargs = {})
#   %mul_846 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_209, %unsqueeze_579), kwargs = {})
#   %convert_element_type_623 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_846, torch.float16), kwargs = {})
#   %slice_56 : Tensor "f16[128, 52, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_623, 1, 52, 104), kwargs = {})
#   %convolution_backward_81 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_56, %getitem_195, %convert_element_type_188, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf633
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10437024}, 'kernel_num_gb': 0.007828288, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1304576
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 52)
    x1 = xindex // 52
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (52 + x0 + 104*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (52 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (52 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1304576,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.007828288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sk/cskrxdmo2jkgoew6v2swt75szji6ukuf3wnzmc2ovtrua33qgh7r.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_209 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=sub_209]
#   %squeeze_88 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_88]
#   %primals_238 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_238]
#   %mul_844 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_88, %primals_238), kwargs = {})
#   %unsqueeze_577 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_844, 0), kwargs = {})
#   %unsqueeze_578 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_577, 2), kwargs = {})
#   %unsqueeze_579 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_578, 3), kwargs = {})
#   %mul_846 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_209, %unsqueeze_579), kwargs = {})
#   %convert_element_type_623 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_846, torch.float16), kwargs = {})
#   %slice_55 : Tensor "f16[128, 52, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_623, 1, 0, 52), kwargs = {})
#   %convolution_backward_82 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_55, %getitem_194, %convert_element_type_187, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf638
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10437024}, 'kernel_num_gb': 0.007828288, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1304576
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 52)
    x1 = xindex // 52
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 104*x1), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 1304576,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.007828288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7d/c7dlkpbznyup4imi7eojshvbqk456qmps57gvd75ynpsn3esgxwx.py
# Topologically Sorted Source Nodes: [x_89, sigmoid_6], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_6 => sigmoid_27
#   x_89 => convert_element_type_179, convert_element_type_180, mul_228, sigmoid_25
# Graph fragment:
#   %getitem_682 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_682]
#   %getitem_679 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_679]
#   %convert_element_type_178 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_178]
#   %sum_87 : Tensor "f16[128, 624, 1, 1][624, 1, 79872, 79872]cuda:0" = PlaceHolder[target=sum_87]
#   %convolution_71 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_71]
#   %cat_60 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_682, %getitem_679], 1), kwargs = {})
#   %convert_element_type_179 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_178, torch.float32), kwargs = {})
#   %sigmoid_25 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_179,), kwargs = {})
#   %mul_228 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_179, %sigmoid_25), kwargs = {})
#   %convert_element_type_180 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_228, torch.float16), kwargs = {})
#   %mul_848 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_60, %convert_element_type_180), kwargs = {})
#   %sigmoid_27 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_71,), kwargs = {})
#   %sum_87 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_848, [2, 3], True), kwargs = {})
#   %convert_element_type_626 : Tensor "f32[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_87, torch.float32), kwargs = {})
#   %convert_element_type_627 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_27, torch.float32), kwargs = {})
#   %sub_210 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_627), kwargs = {})
#   %mul_850 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_627, %sub_210), kwargs = {})
#   %mul_851 : Tensor "f32[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_626, %mul_850), kwargs = {})
#   %convert_element_type_628 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_851, torch.float16), kwargs = {})
#   return %sum_87,%convert_element_type_628
triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112 = async_compile.triton('triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 94408704, 'r0_': 0}, 'kernel_num_gb': 0.06309888, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 79872
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 624*r0_2 + 122304*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 312, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (312*r0_2 + 61152*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 624, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (312*r0_2 + 61152*x1 + ((-312) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tl.sigmoid(tmp12)
        tmp14 = tmp12 * tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp10 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp21 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tmp18.to(tl.float32)
    tmp22 = tl.sigmoid(tmp21)
    tmp23 = tmp22.to(tl.float32)
    tmp24 = 1.0
    tmp25 = tmp24 - tmp23
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp28, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06309888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vp/cvp24vceiaif6kqo63pjqdxee3c7sty5dizwyzc5ifzrrkuml56p.py
# Topologically Sorted Source Nodes: [sigmoid_6], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_6 => sigmoid_27
# Graph fragment:
#   %getitem_682 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_682]
#   %getitem_679 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=getitem_679]
#   %convolution_71 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_71]
#   %getitem_688 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_688]
#   %convert_element_type_178 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_178]
#   %full_default_23 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=8] = call_function[target=torch.ops.aten.full.default](args = ([128, 624, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_60 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_682, %getitem_679], 1), kwargs = {})
#   %sigmoid_27 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_71,), kwargs = {})
#   %mul_849 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_60, %sigmoid_27), kwargs = {})
#   %expand_10 : Tensor "f16[128, 624, 14, 14][624, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_688, [128, 624, 14, 14]), kwargs = {})
#   %div_10 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_10, 196), kwargs = {})
#   %add_348 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_849, %div_10), kwargs = {})
#   %sigmoid_92 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_178,), kwargs = {})
#   %sub_212 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_23, %sigmoid_92), kwargs = {})
#   %mul_855 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_178, %sub_212), kwargs = {})
#   %add_349 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_855, 1), kwargs = {})
#   %mul_856 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_92, %add_349), kwargs = {})
#   %mul_857 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_348, %mul_856), kwargs = {})
#   %convert_element_type_633 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_857, torch.float32), kwargs = {})
#   return %convert_element_type_633
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 125239296, 'x': 94248960}, 'kernel_num_gb': 0.125558784, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 624
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 196
    y0 = (yindex % 196)
    tmp11 = tl.load(in_ptr2 + (x2 + 624*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 624*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 624*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 312, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (312*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 624, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (312*y3 + ((-312) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.00510204081632653
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 196*x2 + 122304*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 624,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.125558784
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/au/cauddukull4cecpe7iucctbrdbbr6wtbxtq5n2pehlvbyozqsy4g.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_646 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=getitem_646]
#   %getitem_676 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_676]
#   %getitem_673 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_673]
#   %getitem_706 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_706]
#   %getitem_703 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_703]
#   %cat_59 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_676, %getitem_673], 1), kwargs = {})
#   %add_346 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_646, %cat_59), kwargs = {})
#   %cat_62 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_706, %getitem_703], 1), kwargs = {})
#   %add_351 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_346, %cat_62), kwargs = {})
#   return %add_351
triton_poi_fused_add_cat_114 = async_compile.triton('triton_poi_fused_add_cat_114', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_114', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 36528128}, 'kernel_num_gb': 0.020873216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_114(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    x1 = xindex // 104
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 52, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr0 + (52*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 104, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr1 + (52*x1 + ((-52) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tl.load(in_ptr2 + (52*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (52*x1 + ((-52) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp5, tmp13, tmp14)
    tmp16 = tmp12 + tmp15
    tl.store(in_out_ptr0 + (x2), tmp16, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_114.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_114.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.020873216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yy/cyyilu4tgmmohmctvnbi23jp6lmnf62b2uguotvxpxnwql3kbll7.py
# Topologically Sorted Source Nodes: [x_73], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_73 => convert_element_type_141
# Graph fragment:
#   %add_351 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=add_351]
#   %getitem_736 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_736]
#   %getitem_733 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=getitem_733]
#   %convolution_53 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convolution_53]
#   %unsqueeze_642 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_642]
#   %sum_104 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_104]
#   %squeeze_70 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=squeeze_70]
#   %sum_103 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=sum_103]
#   %sub_241 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=sub_241]
#   %primals_184 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_184]
#   %cat_65 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_736, %getitem_733], 1), kwargs = {})
#   %add_356 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_351, %cat_65), kwargs = {})
#   %convert_element_type_669 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_356, torch.float32), kwargs = {})
#   %convert_element_type_141 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_53, torch.float32), kwargs = {})
#   %sub_238 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_141, %unsqueeze_642), kwargs = {})
#   %mul_920 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_103, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_643 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_920, 0), kwargs = {})
#   %unsqueeze_644 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_643, 2), kwargs = {})
#   %unsqueeze_645 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_644, 3), kwargs = {})
#   %mul_921 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_104, 3.985969387755102e-05), kwargs = {})
#   %mul_922 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_70, %squeeze_70), kwargs = {})
#   %mul_923 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_921, %mul_922), kwargs = {})
#   %unsqueeze_646 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_923, 0), kwargs = {})
#   %unsqueeze_647 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_646, 2), kwargs = {})
#   %unsqueeze_648 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_647, 3), kwargs = {})
#   %mul_924 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_70, %primals_184), kwargs = {})
#   %unsqueeze_649 : Tensor "f32[1, 104][104, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_924, 0), kwargs = {})
#   %unsqueeze_650 : Tensor "f32[1, 104, 1][104, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_649, 2), kwargs = {})
#   %unsqueeze_651 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_650, 3), kwargs = {})
#   %mul_925 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_238, %unsqueeze_648), kwargs = {})
#   %sub_240 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_669, %mul_925), kwargs = {})
#   %sub_241 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_240, %unsqueeze_645), kwargs = {})
#   %mul_926 : Tensor "f32[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_241, %unsqueeze_651), kwargs = {})
#   %convert_element_type_671 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_926, torch.float16), kwargs = {})
#   %convolution_backward_101 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_671, %mul_180, %convert_element_type_140, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %sub_241,%buf782
triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 31311904}, 'kernel_num_gb': 0.020875296, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    x1 = xindex // 104
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 52, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (52*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 104, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (52*x1 + ((-52) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 3.985969387755102e-05
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tmp30 = tmp21 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp31.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp32, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.020875296
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bu/cbuyizo7a7om3f5azscv57lhixt2uqxh346ccp7lyyjf7knig77j.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_69 => add_119, convert_element_type_131, mul_171, mul_177, sub_22, unsqueeze_88, unsqueeze_89, unsqueeze_90, unsqueeze_91
# Graph fragment:
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=cat_15]
#   %getitem_135 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_135]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %primals_174 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_174]
#   %primals_175 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_175]
#   %sub_22 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_15, %getitem_135), kwargs = {})
#   %mul_171 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_22, %rsqrt_22), kwargs = {})
#   %unsqueeze_88 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_174, -1), kwargs = {})
#   %unsqueeze_89 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_88, -1), kwargs = {})
#   %mul_177 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_171, %unsqueeze_89), kwargs = {})
#   %unsqueeze_90 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_175, -1), kwargs = {})
#   %unsqueeze_91 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_90, -1), kwargs = {})
#   %add_119 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_177, %unsqueeze_91), kwargs = {})
#   %convert_element_type_131 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_119, torch.float16), kwargs = {})
#   return %convert_element_type_131
triton_poi_fused__native_batch_norm_legit_functional_116 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_116', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_116', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 50582784}, 'kernel_num_gb': 0.033723648, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_116(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8429568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 8429568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_116.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_116.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033723648
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g5/cg5ejy2luhlp62oeb5zc2us6voburjb3vmkzgeqln3zsb6p2eixw.py
# Topologically Sorted Source Nodes: [x_70, sigmoid_4], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
#   x_70 => convert_element_type_132, convert_element_type_133, mul_178, sigmoid_17
# Graph fragment:
#   %getitem_739 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=getitem_739]
#   %convert_element_type_131 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=convert_element_type_131]
#   %sum_105 : Tensor "f16[128, 336, 1, 1][336, 1, 43008, 43008]cuda:0" = PlaceHolder[target=sum_105]
#   %convolution_52 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_52]
#   %convert_element_type_132 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_131, torch.float32), kwargs = {})
#   %sigmoid_17 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_132,), kwargs = {})
#   %mul_178 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_132, %sigmoid_17), kwargs = {})
#   %convert_element_type_133 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_178, torch.float16), kwargs = {})
#   %mul_928 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_739, %convert_element_type_133), kwargs = {})
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %sum_105 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_928, [2, 3], True), kwargs = {})
#   %convert_element_type_673 : Tensor "f32[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_105, torch.float32), kwargs = {})
#   %convert_element_type_674 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_19, torch.float32), kwargs = {})
#   %sub_242 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_674), kwargs = {})
#   %mul_930 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_674, %sub_242), kwargs = {})
#   %mul_931 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_673, %mul_930), kwargs = {})
#   %convert_element_type_675 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_931, torch.float16), kwargs = {})
#   return %sum_105,%convert_element_type_675
triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117 = async_compile.triton('triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33976320, 'r0_': 0}, 'kernel_num_gb': 0.03397632, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43008
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 65856*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 336*r0_2 + 65856*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tmp0 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tmp11 = tl.load(in_ptr2 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp10 = tmp8.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp12.to(tl.float32)
    tmp14 = 1.0
    tmp15 = tmp14 - tmp13
    tmp16 = tmp13 * tmp15
    tmp17 = tmp10 * tmp16
    tmp18 = tmp17.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp18, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 43008, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03397632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/25/c25glcpcatgdpyrp6l7hr2doxadk2etbdqnjfoldl2kphmepcq6a.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_742 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0" = PlaceHolder[target=getitem_742]
#   %convolution_51 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0" = PlaceHolder[target=convolution_51]
#   %sigmoid_97 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_51,), kwargs = {})
#   %full_default_34 : Tensor "f16[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 14, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_243 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_34, %sigmoid_97), kwargs = {})
#   %mul_932 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_51, %sub_243), kwargs = {})
#   %add_357 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_932, 1), kwargs = {})
#   %mul_933 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_97, %add_357), kwargs = {})
#   %mul_934 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_742, %mul_933), kwargs = {})
#   return %mul_934
triton_poi_fused_add_fill_mul_sigmoid_sub_118 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_118', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_118', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 14336}, 'kernel_num_gb': 1.0752e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_118(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1792
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1792,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_118.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_118.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.0752e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dc/cdcs4t77pfechmnpgxdqtyxv7x76yc7d2jfyde7gaw5ad4oyaaq2.py
# Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
#   x_69 => convert_element_type_130, squeeze_66
# Graph fragment:
#   %getitem_739 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=getitem_739]
#   %convolution_52 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_52]
#   %getitem_745 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_745]
#   %convert_element_type_131 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=convert_element_type_131]
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=cat_15]
#   %getitem_135 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_135]
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %mul_929 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_739, %sigmoid_19), kwargs = {})
#   %expand_12 : Tensor "f16[128, 336, 14, 14][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_745, [128, 336, 14, 14]), kwargs = {})
#   %div_12 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_12, 196), kwargs = {})
#   %add_358 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_929, %div_12), kwargs = {})
#   %sigmoid_98 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_131,), kwargs = {})
#   %full_default_35 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_244 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_35, %sigmoid_98), kwargs = {})
#   %mul_935 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_131, %sub_244), kwargs = {})
#   %add_359 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_935, 1), kwargs = {})
#   %mul_936 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_98, %add_359), kwargs = {})
#   %mul_937 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_358, %mul_936), kwargs = {})
#   %convert_element_type_680 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_937, torch.float32), kwargs = {})
#   %squeeze_66 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_135, [0, 2, 3]), kwargs = {})
#   %unsqueeze_652 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_66, 0), kwargs = {})
#   %unsqueeze_653 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_652, 2), kwargs = {})
#   %unsqueeze_654 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_653, 3), kwargs = {})
#   %sum_108 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_680, [0, 2, 3]), kwargs = {})
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %sub_245 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_130, %unsqueeze_654), kwargs = {})
#   %mul_938 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_680, %sub_245), kwargs = {})
#   %sum_109 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_938, [0, 2, 3]), kwargs = {})
#   return %buf803,%buf805
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 85350720, 'r0_': 0}, 'kernel_num_gb': 0.051277632, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 65856
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp22 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp26 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 43008*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 336*((r0_2 + 128*x1) // 196)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp4 = tl.load(in_ptr2 + (x0 + 336*((r0_2 + 128*x1) // 196)), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp8 = tl.load(in_ptr3 + (x0 + 336*r0_2 + 43008*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp20 = tl.load(in_ptr4 + (x0 + 336*r0_2 + 43008*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = tmp0 * tmp2
        tmp5 = 0.00510204081632653
        tmp6 = tmp4 * tmp5
        tmp7 = tmp3 + tmp6
        tmp9 = tl.sigmoid(tmp8)
        tmp10 = 1.0
        tmp11 = tmp10 - tmp9
        tmp12 = tmp8 * tmp11
        tmp13 = tmp12 + tmp10
        tmp14 = tmp9 * tmp13
        tmp15 = tmp7 * tmp14
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
        tmp21 = tmp20.to(tl.float32)
        tmp23 = tmp21 - tmp22
        tmp24 = tmp16 * tmp23
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, R0_BLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(r0_mask & xmask, tmp27, _tmp26)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp18, xmask)
    tl.store(out_ptr1 + (x3), tmp26, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((336, 196), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((336, 196), (1, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 65856, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.051277632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qm/cqmuc6d6dcqzchg5sfga6rpboht7s22mveohxqnyect44c2dv4nn.py
# Topologically Sorted Source Nodes: [sigmoid_4], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
# Graph fragment:
#   %buf803 : Tensor "f32[336, 196][1, 336]cuda:0" = PlaceHolder[target=buf803]
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %mul_929 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_739, %sigmoid_19), kwargs = {})
#   %expand_12 : Tensor "f16[128, 336, 14, 14][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_745, [128, 336, 14, 14]), kwargs = {})
#   %div_12 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_12, 196), kwargs = {})
#   %add_358 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_929, %div_12), kwargs = {})
#   %sigmoid_98 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_131,), kwargs = {})
#   %full_default_35 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_244 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_35, %sigmoid_98), kwargs = {})
#   %mul_935 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_131, %sub_244), kwargs = {})
#   %add_359 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_935, 1), kwargs = {})
#   %mul_936 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_98, %add_359), kwargs = {})
#   %mul_937 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_358, %mul_936), kwargs = {})
#   %convert_element_type_680 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_937, torch.float32), kwargs = {})
#   %sum_108 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_680, [0, 2, 3]), kwargs = {})
#   return %sum_108
triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120 = async_compile.triton('triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 266112, 'r0_': 0}, 'kernel_num_gb': 0.000264768, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((336, 196), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 336, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000264768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kr/ckr2gdx5mlhy3xloly2dmnn4dkfh45mmgosie5xos7oc5rbd7mr2.py
# Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
#   x_69 => convert_element_type_130, squeeze_66, squeeze_67
# Graph fragment:
#   %buf805 : Tensor "f32[336, 196][1, 336]cuda:0" = PlaceHolder[target=buf805]
#   %sum_109 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_109]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %mul_929 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_739, %sigmoid_19), kwargs = {})
#   %expand_12 : Tensor "f16[128, 336, 14, 14][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_745, [128, 336, 14, 14]), kwargs = {})
#   %div_12 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_12, 196), kwargs = {})
#   %add_358 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_929, %div_12), kwargs = {})
#   %sigmoid_98 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_131,), kwargs = {})
#   %full_default_35 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_244 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_35, %sigmoid_98), kwargs = {})
#   %mul_935 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_131, %sub_244), kwargs = {})
#   %add_359 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_935, 1), kwargs = {})
#   %mul_936 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_98, %add_359), kwargs = {})
#   %mul_937 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_358, %mul_936), kwargs = {})
#   %convert_element_type_680 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_937, torch.float32), kwargs = {})
#   %squeeze_66 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_135, [0, 2, 3]), kwargs = {})
#   %unsqueeze_652 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_66, 0), kwargs = {})
#   %unsqueeze_653 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_652, 2), kwargs = {})
#   %unsqueeze_654 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_653, 3), kwargs = {})
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %sub_245 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_130, %unsqueeze_654), kwargs = {})
#   %mul_938 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_680, %sub_245), kwargs = {})
#   %sum_109 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_938, [0, 2, 3]), kwargs = {})
#   %squeeze_67 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_22, [0, 2, 3]), kwargs = {})
#   %mul_946 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_109, %squeeze_67), kwargs = {})
#   return %sum_109,%mul_946
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 270144, 'r0_': 0}, 'kernel_num_gb': 0.000267456, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((336, 196), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 336, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000267456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fe/cfesn4hdnowampckbvktmsq5hcfjs6zwryia6mx6os53jsap7nop.py
# Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
#   x_69 => convert_element_type_130, squeeze_66, squeeze_67
# Graph fragment:
#   %getitem_739 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=getitem_739]
#   %convolution_52 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_52]
#   %getitem_745 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_745]
#   %convert_element_type_131 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=convert_element_type_131]
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=cat_15]
#   %getitem_135 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_135]
#   %sum_109 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_109]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %sum_108 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_108]
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %mul_929 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_739, %sigmoid_19), kwargs = {})
#   %expand_12 : Tensor "f16[128, 336, 14, 14][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_745, [128, 336, 14, 14]), kwargs = {})
#   %div_12 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_12, 196), kwargs = {})
#   %add_358 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_929, %div_12), kwargs = {})
#   %sigmoid_98 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_131,), kwargs = {})
#   %full_default_35 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 14, 14], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_244 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_35, %sigmoid_98), kwargs = {})
#   %mul_935 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_131, %sub_244), kwargs = {})
#   %add_359 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_935, 1), kwargs = {})
#   %mul_936 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_98, %add_359), kwargs = {})
#   %mul_937 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_358, %mul_936), kwargs = {})
#   %convert_element_type_680 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_937, torch.float32), kwargs = {})
#   %squeeze_66 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_135, [0, 2, 3]), kwargs = {})
#   %unsqueeze_652 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_66, 0), kwargs = {})
#   %unsqueeze_653 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_652, 2), kwargs = {})
#   %unsqueeze_654 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_653, 3), kwargs = {})
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %sub_245 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_130, %unsqueeze_654), kwargs = {})
#   %mul_939 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_108, 3.985969387755102e-05), kwargs = {})
#   %unsqueeze_655 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_939, 0), kwargs = {})
#   %unsqueeze_656 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_655, 2), kwargs = {})
#   %unsqueeze_657 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_656, 3), kwargs = {})
#   %mul_940 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_109, 3.985969387755102e-05), kwargs = {})
#   %squeeze_67 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_22, [0, 2, 3]), kwargs = {})
#   %mul_941 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_67, %squeeze_67), kwargs = {})
#   %mul_942 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_940, %mul_941), kwargs = {})
#   %unsqueeze_658 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_942, 0), kwargs = {})
#   %unsqueeze_659 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_658, 2), kwargs = {})
#   %unsqueeze_660 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_659, 3), kwargs = {})
#   %mul_944 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_245, %unsqueeze_660), kwargs = {})
#   %sub_247 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_680, %mul_944), kwargs = {})
#   %sub_248 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_247, %unsqueeze_657), kwargs = {})
#   return %sub_248
triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 118191360}, 'kernel_num_gb': 0.084473088, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8429568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 336)
    x2 = xindex // 65856
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0 + 336*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr2 + (x0 + 336*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tl.load(in_ptr3 + (x3), None).to(tl.float32)
    tmp17 = tl.load(in_ptr4 + (x3), None).to(tl.float32)
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp0 * tmp2
    tmp5 = 0.00510204081632653
    tmp6 = tmp4 * tmp5
    tmp7 = tmp3 + tmp6
    tmp9 = tl.sigmoid(tmp8)
    tmp10 = 1.0
    tmp11 = tmp10 - tmp9
    tmp12 = tmp8 * tmp11
    tmp13 = tmp12 + tmp10
    tmp14 = tmp9 * tmp13
    tmp15 = tmp7 * tmp14
    tmp16 = tmp15.to(tl.float32)
    tmp18 = tmp17.to(tl.float32)
    tmp20 = tmp18 - tmp19
    tmp22 = 3.985969387755102e-05
    tmp23 = tmp21 * tmp22
    tmp25 = tmp24 * tmp24
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp16 - tmp27
    tmp30 = tmp29 * tmp22
    tmp31 = tmp28 - tmp30
    tl.store(out_ptr0 + (x3), tmp31, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 8429568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.084473088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ol/colot72rujt6z5prvz2vz465zzk2qcelp3frt2efjkwlabacoekm.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_69 => squeeze_67
# Graph fragment:
#   %sub_248 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=sub_248]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %primals_174 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_174]
#   %squeeze_67 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_22, [0, 2, 3]), kwargs = {})
#   %mul_943 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_67, %primals_174), kwargs = {})
#   %unsqueeze_661 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_943, 0), kwargs = {})
#   %unsqueeze_662 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_661, 2), kwargs = {})
#   %unsqueeze_663 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_662, 3), kwargs = {})
#   %mul_945 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_248, %unsqueeze_663), kwargs = {})
#   %convert_element_type_682 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_945, torch.float16), kwargs = {})
#   %slice_73 : Tensor "f16[128, 112, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_682, 1, 224, 336), kwargs = {})
#   %convolution_backward_104 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_73, %getitem_133, %convert_element_type_129, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 112, [True, True, False]), kwargs = {})
#   return %buf809
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22479744}, 'kernel_num_gb': 0.016861824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2809856
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 112)
    x1 = xindex // 112
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (224 + x0 + 336*x1), None)
    tmp1 = tl.load(in_ptr1 + (224 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (224 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2809856,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016861824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/j6/cj63vsbunepoq72ozduosj7t2n7l6w6liyt33ahgt3ioxfhtkopk.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_69 => squeeze_67
# Graph fragment:
#   %sub_248 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=sub_248]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %primals_174 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_174]
#   %squeeze_67 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_22, [0, 2, 3]), kwargs = {})
#   %mul_943 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_67, %primals_174), kwargs = {})
#   %unsqueeze_661 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_943, 0), kwargs = {})
#   %unsqueeze_662 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_661, 2), kwargs = {})
#   %unsqueeze_663 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_662, 3), kwargs = {})
#   %mul_945 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_248, %unsqueeze_663), kwargs = {})
#   %convert_element_type_682 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_945, torch.float16), kwargs = {})
#   %slice_72 : Tensor "f16[128, 112, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_682, 1, 112, 224), kwargs = {})
#   %convolution_backward_105 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_72, %getitem_129, %convert_element_type_128, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 112, [True, True, False]), kwargs = {})
#   return %buf814
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22479744}, 'kernel_num_gb': 0.016861824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2809856
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 112)
    x1 = xindex // 112
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (112 + x0 + 336*x1), None)
    tmp1 = tl.load(in_ptr1 + (112 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (112 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2809856,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016861824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4j/c4jiouomn4v6pgub7hwofepwx3zw7q76nxuakpieyha7g5vxfjlk.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_69 => squeeze_67
# Graph fragment:
#   %sub_248 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=sub_248]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %primals_174 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_174]
#   %squeeze_67 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_22, [0, 2, 3]), kwargs = {})
#   %mul_943 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_67, %primals_174), kwargs = {})
#   %unsqueeze_661 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_943, 0), kwargs = {})
#   %unsqueeze_662 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_661, 2), kwargs = {})
#   %unsqueeze_663 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_662, 3), kwargs = {})
#   %mul_945 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_248, %unsqueeze_663), kwargs = {})
#   %convert_element_type_682 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_945, torch.float16), kwargs = {})
#   %slice_71 : Tensor "f16[128, 112, 14, 14][65856, 1, 4704, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_682, 1, 0, 112), kwargs = {})
#   %convolution_backward_106 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_71, %getitem_125, %convert_element_type_127, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 112, [True, True, False]), kwargs = {})
#   return %buf819
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22479744}, 'kernel_num_gb': 0.016861824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2809856
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 112)
    x1 = xindex // 112
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 336*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2809856,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016861824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/aw/cawry5cbadonlinzmdz747xcvbg4im6qoomkkpac65j5yjozgjjp.py
# Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_66 => add_114, convert_element_type_123, convert_element_type_124, mul_163, mul_169, squeeze_63, sub_21, unsqueeze_84, unsqueeze_85, unsqueeze_86, unsqueeze_87
# Graph fragment:
#   %convolution_47 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convolution_47]
#   %getitem_121 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_121]
#   %rsqrt_21 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_21]
#   %primals_166 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_166]
#   %primals_167 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_167]
#   %getitem_754 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_754]
#   %getitem_751 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_751]
#   %getitem_748 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_748]
#   %convert_element_type_124 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_124]
#   %cat_66 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_754, %getitem_751, %getitem_748], 1), kwargs = {})
#   %sub_21 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_47, %getitem_121), kwargs = {})
#   %mul_163 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_21, %rsqrt_21), kwargs = {})
#   %unsqueeze_84 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_166, -1), kwargs = {})
#   %unsqueeze_85 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_84, -1), kwargs = {})
#   %mul_169 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_163, %unsqueeze_85), kwargs = {})
#   %unsqueeze_86 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_167, -1), kwargs = {})
#   %unsqueeze_87 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_86, -1), kwargs = {})
#   %add_114 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_169, %unsqueeze_87), kwargs = {})
#   %convert_element_type_124 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_114, torch.float16), kwargs = {})
#   %sigmoid_99 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_124,), kwargs = {})
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_249 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_99), kwargs = {})
#   %mul_947 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_124, %sub_249), kwargs = {})
#   %add_360 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_947, 1), kwargs = {})
#   %mul_948 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_99, %add_360), kwargs = {})
#   %mul_949 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_66, %mul_948), kwargs = {})
#   %convert_element_type_686 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_949, torch.float32), kwargs = {})
#   %squeeze_63 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_121, [0, 2, 3]), kwargs = {})
#   %unsqueeze_664 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_63, 0), kwargs = {})
#   %unsqueeze_665 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_664, 2), kwargs = {})
#   %unsqueeze_666 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_665, 3), kwargs = {})
#   %convert_element_type_123 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_47, torch.float32), kwargs = {})
#   %sub_250 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_123, %unsqueeze_666), kwargs = {})
#   %mul_950 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_686, %sub_250), kwargs = {})
#   return %convert_element_type_124,%mul_950
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 674370816}, 'kernel_num_gb': 0.337188096, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 336)
    x1 = xindex // 336
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x0
    tmp12 = tl.full([1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1], 112, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (112*x1 + (x0)), tmp15, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1], 224, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (112*x1 + ((-112) + x0)), tmp20, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1], 336, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tl.load(in_ptr7 + (112*x1 + ((-224) + x0)), tmp22, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp26 = tl.where(tmp20, tmp21, tmp25)
    tmp27 = tl.where(tmp15, tmp16, tmp26)
    tmp28 = tl.sigmoid(tmp10)
    tmp29 = 1.0
    tmp30 = tmp29 - tmp28
    tmp31 = tmp10 * tmp30
    tmp32 = tmp31 + tmp29
    tmp33 = tmp28 * tmp32
    tmp34 = tmp27 * tmp33
    tmp35 = tmp34.to(tl.float32)
    tmp36 = tmp35 * tmp3
    tl.store(out_ptr0 + (x2), tmp10, None)
    tl.store(out_ptr1 + (x2), tmp36, None)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.337188096
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xq/cxq3txuzbrtadbmdjhixdqrevrkpdfx65lfesvnbbbgwoyzcrwzt.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_754 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_754]
#   %getitem_751 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_751]
#   %getitem_748 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_748]
#   %convert_element_type_124 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_124]
#   %cat_66 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_754, %getitem_751, %getitem_748], 1), kwargs = {})
#   %sigmoid_99 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_124,), kwargs = {})
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_249 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_99), kwargs = {})
#   %mul_947 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_124, %sub_249), kwargs = {})
#   %add_360 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_947, 1), kwargs = {})
#   %mul_948 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_99, %add_360), kwargs = {})
#   %mul_949 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_66, %mul_948), kwargs = {})
#   %convert_element_type_686 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_949, torch.float32), kwargs = {})
#   %sum_110 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_686, [0, 2, 3]), kwargs = {})
#   return %buf825
triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127 = async_compile.triton('triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 270799872, 'r0_': 0}, 'kernel_num_gb': 0.135399936, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 131712
    r0_numel = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp27 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp17 = tl.load(in_ptr3 + (x0 + 336*r0_2 + 86016*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 112, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (112*r0_2 + 28672*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 224, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tmp6 & tmp8
        tmp10 = tl.load(in_ptr1 + (112*r0_2 + 28672*x1 + ((-112) + x0)), r0_mask & tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tmp0 >= tmp7
        tmp12 = tl.full([1, 1], 336, tl.int64)
        tmp13 = tmp0 < tmp12
        tmp14 = tl.load(in_ptr2 + (112*r0_2 + 28672*x1 + ((-224) + x0)), r0_mask & tmp11 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp15 = tl.where(tmp9, tmp10, tmp14)
        tmp16 = tl.where(tmp4, tmp5, tmp15)
        tmp18 = tl.sigmoid(tmp17)
        tmp19 = 1.0
        tmp20 = tmp19 - tmp18
        tmp21 = tmp17 * tmp20
        tmp22 = tmp21 + tmp19
        tmp23 = tmp18 * tmp22
        tmp24 = tmp16 * tmp23
        tmp25 = tmp24.to(tl.float32)
        tmp26 = tl.broadcast_to(tmp25, [XBLOCK, R0_BLOCK])
        tmp28 = _tmp27 + tmp26
        _tmp27 = tl.where(r0_mask & xmask, tmp28, _tmp27)
    tmp27 = tl.sum(_tmp27, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp27, xmask)


def get_args():
    arg_0 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 131712, 256,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.135399936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wl/cwley4w3c33yravai6b7kogqwfqv7aifsqsveaypa3jpanigj45j.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf825 : Tensor "f32[336, 392][1, 336]cuda:0" = PlaceHolder[target=buf825]
#   %cat_66 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_754, %getitem_751, %getitem_748], 1), kwargs = {})
#   %sigmoid_99 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_124,), kwargs = {})
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_249 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_99), kwargs = {})
#   %mul_947 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_124, %sub_249), kwargs = {})
#   %add_360 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_947, 1), kwargs = {})
#   %mul_948 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_99, %add_360), kwargs = {})
#   %mul_949 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_66, %mul_948), kwargs = {})
#   %convert_element_type_686 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_949, torch.float32), kwargs = {})
#   %sum_110 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_686, [0, 2, 3]), kwargs = {})
#   return %sum_110
triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128 = async_compile.triton('triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 529536, 'r0_': 0}, 'kernel_num_gb': 0.000528192, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 336, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000528192
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b6/cb6axzjfkhxbf74zkffqavczifkh5ms4dhmvthz3gu2noxygnqdu.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_950 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=mul_950]
#   %sum_111 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_950, [0, 2, 3]), kwargs = {})
#   return %buf828
triton_red_fused_native_batch_norm_backward_129 = async_compile.triton('triton_red_fused_native_batch_norm_backward_129', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_129', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 135926784, 'r0_': 0}, 'kernel_num_gb': 0.135399936, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_129(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 131712
    r0_numel = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 86016*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 131712, 256,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_129.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_129.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.135399936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yi/cyiwg7w3vmgkdyyv65jvn4ywrhwop6qrbuh3y26x6b7ptoxvlblt.py
# Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_66 => squeeze_64
# Graph fragment:
#   %buf828 : Tensor "f32[336, 392][1, 336]cuda:0" = PlaceHolder[target=buf828]
#   %sum_111 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_111]
#   %rsqrt_21 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_21]
#   %sum_111 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_950, [0, 2, 3]), kwargs = {})
#   %squeeze_64 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_21, [0, 2, 3]), kwargs = {})
#   %mul_958 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_111, %squeeze_64), kwargs = {})
#   return %sum_111,%mul_958
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 533568, 'r0_': 0}, 'kernel_num_gb': 0.00053088, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 336, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00053088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/p3/cp3rcpckica24g2u5vfnp7n5l5c4orsmvx7wziaa7ow4mn42vdva.py
# Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_66 => convert_element_type_123, squeeze_63, squeeze_64
# Graph fragment:
#   %getitem_754 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_754]
#   %getitem_751 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_751]
#   %getitem_748 : Tensor "f16[128, 112, 28, 28][87808, 1, 3136, 112]cuda:0" = PlaceHolder[target=getitem_748]
#   %convert_element_type_124 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_124]
#   %convolution_47 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convolution_47]
#   %getitem_121 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_121]
#   %sum_111 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_111]
#   %rsqrt_21 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_21]
#   %sub_252 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=sub_252]
#   %sum_110 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_110]
#   %primals_166 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_166]
#   %cat_66 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_754, %getitem_751, %getitem_748], 1), kwargs = {})
#   %sigmoid_99 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_124,), kwargs = {})
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_249 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_99), kwargs = {})
#   %mul_947 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_124, %sub_249), kwargs = {})
#   %add_360 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_947, 1), kwargs = {})
#   %mul_948 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_99, %add_360), kwargs = {})
#   %mul_949 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_66, %mul_948), kwargs = {})
#   %convert_element_type_686 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_949, torch.float32), kwargs = {})
#   %squeeze_63 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_121, [0, 2, 3]), kwargs = {})
#   %unsqueeze_664 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_63, 0), kwargs = {})
#   %unsqueeze_665 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_664, 2), kwargs = {})
#   %unsqueeze_666 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_665, 3), kwargs = {})
#   %convert_element_type_123 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_47, torch.float32), kwargs = {})
#   %sub_250 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_123, %unsqueeze_666), kwargs = {})
#   %mul_951 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_110, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_667 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_951, 0), kwargs = {})
#   %unsqueeze_668 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_667, 2), kwargs = {})
#   %unsqueeze_669 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_668, 3), kwargs = {})
#   %mul_952 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_111, 9.964923469387754e-06), kwargs = {})
#   %squeeze_64 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_21, [0, 2, 3]), kwargs = {})
#   %mul_953 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_64, %squeeze_64), kwargs = {})
#   %mul_954 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_952, %mul_953), kwargs = {})
#   %unsqueeze_670 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_954, 0), kwargs = {})
#   %unsqueeze_671 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_670, 2), kwargs = {})
#   %unsqueeze_672 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_671, 3), kwargs = {})
#   %mul_955 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_64, %primals_166), kwargs = {})
#   %unsqueeze_673 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_955, 0), kwargs = {})
#   %unsqueeze_674 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_673, 2), kwargs = {})
#   %unsqueeze_675 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_674, 3), kwargs = {})
#   %mul_956 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_250, %unsqueeze_672), kwargs = {})
#   %sub_252 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_686, %mul_956), kwargs = {})
#   %sub_253 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_252, %unsqueeze_669), kwargs = {})
#   %mul_957 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_253, %unsqueeze_675), kwargs = {})
#   %convert_element_type_688 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_957, torch.float16), kwargs = {})
#   %convolution_backward_107 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_688, %add_109, %convert_element_type_122, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %sub_252,%buf832
triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'in_ptr9': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 472062528}, 'kernel_num_gb': 0.269752896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 336)
    x1 = xindex // 336
    x2 = xindex
    tmp17 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp26 = tl.load(in_ptr4 + (x2), None).to(tl.float32)
    tmp28 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp33 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp38 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp41 = tl.load(in_ptr9 + (x0), None, eviction_policy='evict_last')
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 112, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (112*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 224, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (112*x1 + ((-112) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 336, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tl.load(in_ptr2 + (112*x1 + ((-224) + x0)), tmp11, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp9, tmp10, tmp14)
    tmp16 = tl.where(tmp4, tmp5, tmp15)
    tmp18 = tl.sigmoid(tmp17)
    tmp19 = 1.0
    tmp20 = tmp19 - tmp18
    tmp21 = tmp17 * tmp20
    tmp22 = tmp21 + tmp19
    tmp23 = tmp18 * tmp22
    tmp24 = tmp16 * tmp23
    tmp25 = tmp24.to(tl.float32)
    tmp27 = tmp26.to(tl.float32)
    tmp29 = tmp27 - tmp28
    tmp31 = 9.964923469387754e-06
    tmp32 = tmp30 * tmp31
    tmp34 = tmp33 * tmp33
    tmp35 = tmp32 * tmp34
    tmp36 = tmp29 * tmp35
    tmp37 = tmp25 - tmp36
    tmp39 = tmp38 * tmp31
    tmp40 = tmp37 - tmp39
    tmp42 = tmp33 * tmp41
    tmp43 = tmp40 * tmp42
    tmp44 = tmp43.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp44, None)


def get_args():
    arg_0 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_10 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.269752896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ug/cugr7trscbnybcdaftzud5eqt22okga7wr7y24cg5al7knyttjm3.py
# Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_63 => convert_element_type_120
# Graph fragment:
#   %getitem_757 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=getitem_757]
#   %cat_14 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=cat_14]
#   %unsqueeze_678 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_678]
#   %convert_element_type_690 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_757, torch.float32), kwargs = {})
#   %sum_112 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_690, [0, 2, 3]), kwargs = {})
#   %convert_element_type_120 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_14, torch.float32), kwargs = {})
#   %sub_254 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_120, %unsqueeze_678), kwargs = {})
#   %mul_959 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_690, %sub_254), kwargs = {})
#   %sum_113 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_959, [0, 2, 3]), kwargs = {})
#   return %buf837,%buf839
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 23181536, 'r0_': 0}, 'kernel_num_gb': 0.022830304, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43904
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 56)
    x1 = xindex // 56
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_2 + 7168*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 56*r0_2 + 7168*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 43904, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.022830304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2j/c2jd66sulfw4bhm3q5fix32lfbtktgltecpsobysqfpea2kog5e5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf837 : Tensor "f32[56, 784][1, 56]cuda:0" = PlaceHolder[target=buf837]
#   %convert_element_type_690 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_757, torch.float32), kwargs = {})
#   %sum_112 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_690, [0, 2, 3]), kwargs = {})
#   return %sum_112
triton_red_fused_native_batch_norm_backward_133 = async_compile.triton('triton_red_fused_native_batch_norm_backward_133', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 64, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_133', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 176064, 'r0_': 0}, 'kernel_num_gb': 0.00017584, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_133(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 56
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 56, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_133.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_133.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00017584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pe/cpespnynovqv5bvi5i2eccq6fmthq3kss5enuvimgn74oee5dff4.py
# Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_63 => convert_element_type_120
# Graph fragment:
#   %buf839 : Tensor "f32[56, 784][1, 56]cuda:0" = PlaceHolder[target=buf839]
#   %sum_113 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_113]
#   %squeeze_61 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_61]
#   %convert_element_type_690 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_757, torch.float32), kwargs = {})
#   %convert_element_type_120 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_14, torch.float32), kwargs = {})
#   %sub_254 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_120, %unsqueeze_678), kwargs = {})
#   %mul_959 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_690, %sub_254), kwargs = {})
#   %sum_113 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_959, [0, 2, 3]), kwargs = {})
#   %mul_967 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_113, %squeeze_61), kwargs = {})
#   return %sum_113,%mul_967
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 64, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 176736, 'r0_': 0}, 'kernel_num_gb': 0.000176288, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 56
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 56, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000176288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/st/cstuxyhhtjavk7c2gwtmjkjmd6duacycsnqjrojqphewo7ox2kjr.py
# Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_63 => convert_element_type_120
# Graph fragment:
#   %getitem_757 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=getitem_757]
#   %cat_14 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=cat_14]
#   %unsqueeze_678 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_678]
#   %sum_113 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_113]
#   %squeeze_61 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_61]
#   %sum_112 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_112]
#   %primals_160 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_160]
#   %convert_element_type_690 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_757, torch.float32), kwargs = {})
#   %convert_element_type_120 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_14, torch.float32), kwargs = {})
#   %sub_254 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_120, %unsqueeze_678), kwargs = {})
#   %mul_960 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_112, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_679 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_960, 0), kwargs = {})
#   %unsqueeze_680 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_679, 2), kwargs = {})
#   %unsqueeze_681 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_680, 3), kwargs = {})
#   %mul_961 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_113, 9.964923469387754e-06), kwargs = {})
#   %mul_962 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_61, %squeeze_61), kwargs = {})
#   %mul_963 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_961, %mul_962), kwargs = {})
#   %unsqueeze_682 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_963, 0), kwargs = {})
#   %unsqueeze_683 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_682, 2), kwargs = {})
#   %unsqueeze_684 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_683, 3), kwargs = {})
#   %mul_964 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_61, %primals_160), kwargs = {})
#   %unsqueeze_685 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_964, 0), kwargs = {})
#   %unsqueeze_686 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_685, 2), kwargs = {})
#   %unsqueeze_687 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_686, 3), kwargs = {})
#   %mul_965 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_254, %unsqueeze_684), kwargs = {})
#   %sub_256 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_690, %mul_965), kwargs = {})
#   %sub_257 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_256, %unsqueeze_681), kwargs = {})
#   %mul_966 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_257, %unsqueeze_687), kwargs = {})
#   %convert_element_type_692 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_966, torch.float16), kwargs = {})
#   return %convert_element_type_692
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 44958816}, 'kernel_num_gb': 0.033719392, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 9.964923469387754e-06
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033719392
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cu/ccu37mwpmg3siwdbishc25b5ffmdekrj4namu72e4iumq3jceizo.py
# Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_59 => add_103, convert_element_type_109, mul_146, mul_152, sub_19, unsqueeze_76, unsqueeze_77, unsqueeze_78, unsqueeze_79
# Graph fragment:
#   %cat_13 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_13]
#   %getitem_115 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_115]
#   %rsqrt_19 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_19]
#   %primals_149 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_149]
#   %primals_150 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_150]
#   %sub_19 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_13, %getitem_115), kwargs = {})
#   %mul_146 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_19, %rsqrt_19), kwargs = {})
#   %unsqueeze_76 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_149, -1), kwargs = {})
#   %unsqueeze_77 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_76, -1), kwargs = {})
#   %mul_152 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_146, %unsqueeze_77), kwargs = {})
#   %unsqueeze_78 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_150, -1), kwargs = {})
#   %unsqueeze_79 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_78, -1), kwargs = {})
#   %add_103 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_152, %unsqueeze_79), kwargs = {})
#   %convert_element_type_109 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_103, torch.float16), kwargs = {})
#   return %convert_element_type_109
triton_poi_fused__native_batch_norm_legit_functional_136 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_136', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_136', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 202315008}, 'kernel_num_gb': 0.134878464, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_136(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_136.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_136.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.134878464
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5z/c5z5juie56wgxrf4ddcamfqedpzy324bb5mpkwygesfipwihar6y.py
# Topologically Sorted Source Nodes: [x_60, sigmoid_3], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_3 => sigmoid_15
#   x_60 => convert_element_type_110, convert_element_type_111, mul_153, sigmoid_13
# Graph fragment:
#   %getitem_763 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_763]
#   %getitem_760 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_760]
#   %convert_element_type_109 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_109]
#   %sum_114 : Tensor "f16[128, 336, 1, 1][336, 1, 43008, 43008]cuda:0" = PlaceHolder[target=sum_114]
#   %convolution_44 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_44]
#   %cat_67 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_763, %getitem_760], 1), kwargs = {})
#   %convert_element_type_110 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_109, torch.float32), kwargs = {})
#   %sigmoid_13 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_110,), kwargs = {})
#   %mul_153 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_110, %sigmoid_13), kwargs = {})
#   %convert_element_type_111 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_153, torch.float16), kwargs = {})
#   %mul_968 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_67, %convert_element_type_111), kwargs = {})
#   %sigmoid_15 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_44,), kwargs = {})
#   %sum_114 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_968, [2, 3], True), kwargs = {})
#   %convert_element_type_695 : Tensor "f32[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_114, torch.float32), kwargs = {})
#   %convert_element_type_696 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_15, torch.float32), kwargs = {})
#   %sub_258 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_696), kwargs = {})
#   %mul_970 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_696, %sub_258), kwargs = {})
#   %mul_971 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_695, %mul_970), kwargs = {})
#   %convert_element_type_697 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_971, torch.float16), kwargs = {})
#   return %sum_114,%convert_element_type_697
triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137 = async_compile.triton('triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 1024},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 202567680, 'r0_': 0}, 'kernel_num_gb': 0.135131136, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43008
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 336*r0_2 + 263424*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 168, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (168*r0_2 + 131712*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 336, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (168*r0_2 + 131712*x1 + ((-168) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tl.sigmoid(tmp12)
        tmp14 = tmp12 * tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp10 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp21 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tmp18.to(tl.float32)
    tmp22 = tl.sigmoid(tmp21)
    tmp23 = tmp22.to(tl.float32)
    tmp24 = 1.0
    tmp25 = tmp24 - tmp23
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp28, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 43008, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.135131136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cm/ccmmdtgihmd3qyu44ywdgrpfqxqg4fce6rdk4daicf4tlzlgh4cn.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_766 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=getitem_766]
#   %convolution_43 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=convolution_43]
#   %sigmoid_100 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_43,), kwargs = {})
#   %full_default_37 : Tensor "f16[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.full.default](args = ([128, 28, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_259 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_37, %sigmoid_100), kwargs = {})
#   %mul_972 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_43, %sub_259), kwargs = {})
#   %add_361 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_972, 1), kwargs = {})
#   %mul_973 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_100, %add_361), kwargs = {})
#   %mul_974 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_766, %mul_973), kwargs = {})
#   return %mul_974
triton_poi_fused_add_fill_mul_sigmoid_sub_138 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_138', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_138', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 28672}, 'kernel_num_gb': 2.1504e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_138(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3584
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_138.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_138.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.1504e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fk/cfkiydn3avpfgpfxn6b5j6bjie6dchnhljvkwlmr5ksjcmqmt3bh.py
# Topologically Sorted Source Nodes: [sigmoid_3], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_3 => sigmoid_15
# Graph fragment:
#   %getitem_763 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_763]
#   %getitem_760 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_760]
#   %convolution_44 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_44]
#   %getitem_769 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_769]
#   %convert_element_type_109 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_109]
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_67 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_763, %getitem_760], 1), kwargs = {})
#   %sigmoid_15 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_44,), kwargs = {})
#   %mul_969 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_67, %sigmoid_15), kwargs = {})
#   %expand_13 : Tensor "f16[128, 336, 28, 28][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_769, [128, 336, 28, 28]), kwargs = {})
#   %div_13 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_13, 784), kwargs = {})
#   %add_362 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_969, %div_13), kwargs = {})
#   %sigmoid_101 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_109,), kwargs = {})
#   %sub_260 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_101), kwargs = {})
#   %mul_975 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_109, %sub_260), kwargs = {})
#   %add_363 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_975, 1), kwargs = {})
#   %mul_976 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_101, %add_363), kwargs = {})
#   %mul_977 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_362, %mul_976), kwargs = {})
#   %convert_element_type_702 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_977, torch.float32), kwargs = {})
#   return %convert_element_type_702
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 269746176, 'x': 202481664}, 'kernel_num_gb': 0.269918208, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 100352
    xnumel = 336
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 784
    y0 = (yindex % 784)
    tmp11 = tl.load(in_ptr2 + (x2 + 336*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 336*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 336*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 168, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (168*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 336, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (168*y3 + ((-168) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.0012755102040816326
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 784*x2 + 263424*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 100352, 336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.269918208
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rq/crqxnkja4hggf5pw4ix56545yrxqxn56otklqmw2gc4ct2kza7s3.py
# Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_59 => convert_element_type_108, squeeze_57
# Graph fragment:
#   %convert_element_type_702 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_702]
#   %cat_13 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_13]
#   %getitem_115 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_115]
#   %squeeze_57 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_115, [0, 2, 3]), kwargs = {})
#   %unsqueeze_688 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_57, 0), kwargs = {})
#   %unsqueeze_689 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_688, 2), kwargs = {})
#   %unsqueeze_690 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_689, 3), kwargs = {})
#   %convert_element_type_108 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_13, torch.float32), kwargs = {})
#   %sub_261 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_108, %unsqueeze_690), kwargs = {})
#   %mul_978 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_702, %sub_261), kwargs = {})
#   %sum_118 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_978, [0, 2, 3]), kwargs = {})
#   return %buf869
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 512, 'x': 512, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67437888, 'x': 1053696, 'r0_': 134873088}, 'kernel_num_gb': 0.202837824, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 336
    xnumel = 392
    r0_numel = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp3 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp7 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (784*y0 + 263424*((r0_2 + 256*x1) // 784) + (((r0_2 + 256*x1) % 784))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (y0 + 336*r0_2 + 86016*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp4 = tmp2 - tmp3
        tmp5 = tmp0 * tmp4
        tmp6 = tl.broadcast_to(tmp5, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask & xmask & ymask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 392*y0), tmp7, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336, 392), (392, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 336, 392, 256,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.202837824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ev/cevqtyigxyvx25qssvhgjkwo3hyujr73msuy6wrpjtrtyqa3ld5n.py
# Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_59 => convert_element_type_108, squeeze_57, squeeze_58
# Graph fragment:
#   %buf869 : Tensor "f32[336, 392][392, 1]cuda:0" = PlaceHolder[target=buf869]
#   %sum_118 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_118]
#   %rsqrt_19 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_19]
#   %squeeze_57 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_115, [0, 2, 3]), kwargs = {})
#   %unsqueeze_688 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_57, 0), kwargs = {})
#   %unsqueeze_689 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_688, 2), kwargs = {})
#   %unsqueeze_690 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_689, 3), kwargs = {})
#   %convert_element_type_108 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_13, torch.float32), kwargs = {})
#   %sub_261 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_108, %unsqueeze_690), kwargs = {})
#   %mul_978 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_702, %sub_261), kwargs = {})
#   %sum_118 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_978, [0, 2, 3]), kwargs = {})
#   %squeeze_58 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_19, [0, 2, 3]), kwargs = {})
#   %mul_986 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_118, %squeeze_58), kwargs = {})
#   return %sum_118,%mul_986
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 512},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6720, 'r0_': 526848}, 'kernel_num_gb': 0.00053088, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 392
    R0_BLOCK: tl.constexpr = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r0_1 + 392*x0), r0_mask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(r0_mask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((336, 392), (392, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 336, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00053088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d6/cd6tzm6hmndirl5xejudgd7uyaibrvmhskizagnedn2kbmmgfdc5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_702 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_702]
#   %sum_117 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_702, [0, 2, 3]), kwargs = {})
#   return %sum_117
triton_red_fused_native_batch_norm_backward_142 = async_compile.triton('triton_red_fused_native_batch_norm_backward_142', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 131072},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_142', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2688, 'r0_': 134873088}, 'kernel_num_gb': 0.134874432, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_142(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 100352
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = (r0_index % 784)
        r0_2 = r0_index // 784
        tmp0 = tl.load(in_ptr0 + (r0_1 + 784*x0 + 263424*r0_2), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 336, 100352,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_142.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_142.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.134874432
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dm/cdmnchjb4pgi74wou64i7gaxy23pwge6rdo2bymqrpueut3z7lli.py
# Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_59 => convert_element_type_108, squeeze_57, squeeze_58
# Graph fragment:
#   %convert_element_type_702 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_702]
#   %cat_13 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_13]
#   %getitem_115 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_115]
#   %sum_118 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_118]
#   %rsqrt_19 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_19]
#   %sum_117 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_117]
#   %primals_149 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_149]
#   %squeeze_57 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_115, [0, 2, 3]), kwargs = {})
#   %unsqueeze_688 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_57, 0), kwargs = {})
#   %unsqueeze_689 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_688, 2), kwargs = {})
#   %unsqueeze_690 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_689, 3), kwargs = {})
#   %convert_element_type_108 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_13, torch.float32), kwargs = {})
#   %sub_261 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_108, %unsqueeze_690), kwargs = {})
#   %mul_979 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_117, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_691 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_979, 0), kwargs = {})
#   %unsqueeze_692 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_691, 2), kwargs = {})
#   %unsqueeze_693 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_692, 3), kwargs = {})
#   %mul_980 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_118, 9.964923469387754e-06), kwargs = {})
#   %squeeze_58 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_19, [0, 2, 3]), kwargs = {})
#   %mul_981 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_58, %squeeze_58), kwargs = {})
#   %mul_982 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_980, %mul_981), kwargs = {})
#   %unsqueeze_694 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_982, 0), kwargs = {})
#   %unsqueeze_695 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_694, 2), kwargs = {})
#   %unsqueeze_696 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_695, 3), kwargs = {})
#   %mul_983 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_58, %primals_149), kwargs = {})
#   %unsqueeze_697 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_983, 0), kwargs = {})
#   %unsqueeze_698 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_697, 2), kwargs = {})
#   %unsqueeze_699 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_698, 3), kwargs = {})
#   %mul_984 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_261, %unsqueeze_696), kwargs = {})
#   %sub_263 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_702, %mul_984), kwargs = {})
#   %sub_264 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_263, %unsqueeze_693), kwargs = {})
#   %mul_985 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_264, %unsqueeze_699), kwargs = {})
#   %convert_element_type_704 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_985, torch.float16), kwargs = {})
#   return %convert_element_type_704
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67443264, 'x': 269746176}, 'kernel_num_gb': 0.269752896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 43008
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 336)
    y1 = yindex // 336
    tmp0 = tl.load(in_ptr0 + (x2 + 784*y3), xmask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + 336*x2 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (y0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (y0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 9.964923469387754e-06
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(out_ptr0 + (x2 + 784*y3), tmp19, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 43008, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.269752896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ck/cckzt6ehlzypcggotsfwf5pihhezm5zpkw3ehxxf47xy3jeil22h.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_704 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_704]
#   %slice_77 : Tensor "f16[128, 168, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_704, 1, 168, 336), kwargs = {})
#   %convolution_backward_112 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_77, %getitem_113, %convert_element_type_107, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 168, [True, True, False]), kwargs = {})
#   return %buf873
triton_poi_fused_convolution_backward_slice_144 = async_compile.triton('triton_poi_fused_convolution_backward_slice_144', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_144', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67436544, 'x': 33718272}, 'kernel_num_gb': 0.067436544, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_144(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 21504
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 168)
    y1 = yindex // 168
    tmp0 = tl.load(in_ptr0 + (131712 + x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 168*x2 + 131712*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 21504, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_144.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_144.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.067436544
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g3/cg336pmscxgtqkppkszfkimrbcisnn5eu45n42jerybfsrvb4vxv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_704 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_704]
#   %slice_76 : Tensor "f16[128, 168, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_704, 1, 0, 168), kwargs = {})
#   %convolution_backward_113 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_76, %getitem_110, %convert_element_type_106, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 168, [True, True, False]), kwargs = {})
#   return %buf878
triton_poi_fused_convolution_backward_slice_145 = async_compile.triton('triton_poi_fused_convolution_backward_slice_145', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_slice_145', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67436544, 'x': 33718272}, 'kernel_num_gb': 0.067436544, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_slice_145(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 21504
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 168)
    y1 = yindex // 168
    tmp0 = tl.load(in_ptr0 + (x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 168*x2 + 131712*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 21504, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_slice_145.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_slice_145.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.067436544
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6b/c6bv2c7q26vooy4zhac2ivopfvvgjgvwkf7togjrv77wdmplcfl2.py
# Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_56 => convert_element_type_102, squeeze_54
# Graph fragment:
#   %getitem_775 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_775]
#   %getitem_772 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_772]
#   %convert_element_type_103 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_103]
#   %cat_12 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_12]
#   %getitem_107 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_107]
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_68 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_775, %getitem_772], 1), kwargs = {})
#   %sigmoid_102 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_103,), kwargs = {})
#   %sub_265 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_102), kwargs = {})
#   %mul_987 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_103, %sub_265), kwargs = {})
#   %add_364 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_987, 1), kwargs = {})
#   %mul_988 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_102, %add_364), kwargs = {})
#   %mul_989 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_68, %mul_988), kwargs = {})
#   %convert_element_type_707 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_989, torch.float32), kwargs = {})
#   %squeeze_54 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_107, [0, 2, 3]), kwargs = {})
#   %unsqueeze_700 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_54, 0), kwargs = {})
#   %unsqueeze_701 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_700, 2), kwargs = {})
#   %unsqueeze_702 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_701, 3), kwargs = {})
#   %sum_119 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_707, [0, 2, 3]), kwargs = {})
#   %convert_element_type_102 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_12, torch.float32), kwargs = {})
#   %sub_266 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_102, %unsqueeze_702), kwargs = {})
#   %mul_990 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_707, %sub_266), kwargs = {})
#   %sum_120 : Tensor "f32[336][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_990, [0, 2, 3]), kwargs = {})
#   return %buf884,%buf886
triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 271854912, 'r0_': 0}, 'kernel_num_gb': 0.203364672, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 131712
    r0_numel = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp21 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp25 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp29 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 336*r0_2 + 86016*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp23 = tl.load(in_ptr3 + (x0 + 336*r0_2 + 86016*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 168, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (168*r0_2 + 43008*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 336, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (168*r0_2 + 43008*x1 + ((-168) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tl.sigmoid(tmp11)
        tmp13 = 1.0
        tmp14 = tmp13 - tmp12
        tmp15 = tmp11 * tmp14
        tmp16 = tmp15 + tmp13
        tmp17 = tmp12 * tmp16
        tmp18 = tmp10 * tmp17
        tmp19 = tmp18.to(tl.float32)
        tmp20 = tl.broadcast_to(tmp19, [XBLOCK, R0_BLOCK])
        tmp22 = _tmp21 + tmp20
        _tmp21 = tl.where(r0_mask & xmask, tmp22, _tmp21)
        tmp24 = tmp23.to(tl.float32)
        tmp26 = tmp24 - tmp25
        tmp27 = tmp19 * tmp26
        tmp28 = tl.broadcast_to(tmp27, [XBLOCK, R0_BLOCK])
        tmp30 = _tmp29 + tmp28
        _tmp29 = tl.where(r0_mask & xmask, tmp30, _tmp29)
    tmp21 = tl.sum(_tmp21, 1)[:, None]
    tmp29 = tl.sum(_tmp29, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp21, xmask)
    tl.store(out_ptr1 + (x3), tmp29, xmask)


def get_args():
    arg_0 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((336, 392), (1, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 131712, 256,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.203364672
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/72/c72bfrrbfgpdrcrkc2niq3y77wfj5kg6h36lteafgx4mwyii3iup.py
# Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_56 => convert_element_type_102, squeeze_54, squeeze_55
# Graph fragment:
#   %getitem_775 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_775]
#   %getitem_772 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_772]
#   %convert_element_type_103 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_103]
#   %cat_12 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_12]
#   %getitem_107 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_107]
#   %sum_120 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_120]
#   %rsqrt_18 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_18]
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_68 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_775, %getitem_772], 1), kwargs = {})
#   %sigmoid_102 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_103,), kwargs = {})
#   %sub_265 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_102), kwargs = {})
#   %mul_987 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_103, %sub_265), kwargs = {})
#   %add_364 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_987, 1), kwargs = {})
#   %mul_988 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_102, %add_364), kwargs = {})
#   %mul_989 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_68, %mul_988), kwargs = {})
#   %convert_element_type_707 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_989, torch.float32), kwargs = {})
#   %squeeze_54 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_107, [0, 2, 3]), kwargs = {})
#   %unsqueeze_700 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_54, 0), kwargs = {})
#   %unsqueeze_701 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_700, 2), kwargs = {})
#   %unsqueeze_702 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_701, 3), kwargs = {})
#   %convert_element_type_102 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_12, torch.float32), kwargs = {})
#   %sub_266 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_102, %unsqueeze_702), kwargs = {})
#   %mul_992 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_120, 9.964923469387754e-06), kwargs = {})
#   %squeeze_55 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_18, [0, 2, 3]), kwargs = {})
#   %mul_993 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_55, %squeeze_55), kwargs = {})
#   %mul_994 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_992, %mul_993), kwargs = {})
#   %unsqueeze_706 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_994, 0), kwargs = {})
#   %unsqueeze_707 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_706, 2), kwargs = {})
#   %unsqueeze_708 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_707, 3), kwargs = {})
#   %mul_996 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_266, %unsqueeze_708), kwargs = {})
#   %sub_268 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_707, %mul_996), kwargs = {})
#   return %sub_268
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 539496384}, 'kernel_num_gb': 0.337186752, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 336)
    x1 = xindex // 336
    x2 = xindex
    tmp11 = tl.load(in_ptr2 + (x2), None).to(tl.float32)
    tmp20 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp22 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 168, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (168*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 336, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (168*x1 + ((-168) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = 1.0
    tmp14 = tmp13 - tmp12
    tmp15 = tmp11 * tmp14
    tmp16 = tmp15 + tmp13
    tmp17 = tmp12 * tmp16
    tmp18 = tmp10 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tmp21 = tmp20.to(tl.float32)
    tmp23 = tmp21 - tmp22
    tmp25 = 9.964923469387754e-06
    tmp26 = tmp24 * tmp25
    tmp28 = tmp27 * tmp27
    tmp29 = tmp26 * tmp28
    tmp30 = tmp23 * tmp29
    tmp31 = tmp19 - tmp30
    tl.store(out_ptr0 + (x2), tmp31, None)


def get_args():
    arg_0 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.337186752
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/p6/cp6ptfrsgukzcly7j6ydhgcf5by4i3pxvpo4mzbswblqosvyvdul.py
# Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_56 => squeeze_55
# Graph fragment:
#   %sub_268 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=sub_268]
#   %sum_119 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_119]
#   %rsqrt_18 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_18]
#   %primals_142 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_142]
#   %mul_991 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_119, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_703 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_991, 0), kwargs = {})
#   %unsqueeze_704 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_703, 2), kwargs = {})
#   %unsqueeze_705 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_704, 3), kwargs = {})
#   %squeeze_55 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_18, [0, 2, 3]), kwargs = {})
#   %mul_995 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_55, %primals_142), kwargs = {})
#   %unsqueeze_709 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_995, 0), kwargs = {})
#   %unsqueeze_710 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_709, 2), kwargs = {})
#   %unsqueeze_711 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_710, 3), kwargs = {})
#   %sub_269 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_268, %unsqueeze_705), kwargs = {})
#   %mul_997 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_269, %unsqueeze_711), kwargs = {})
#   %convert_element_type_709 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_997, torch.float16), kwargs = {})
#   %slice_79 : Tensor "f16[128, 168, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_709, 1, 168, 336), kwargs = {})
#   %convolution_backward_114 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_79, %getitem_105, %convert_element_type_101, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf890
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 134875104}, 'kernel_num_gb': 0.101158848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16859136
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 168)
    x1 = xindex // 168
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (168 + x0 + 336*x1), None)
    tmp1 = tl.load(in_ptr1 + (168 + x0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (168 + x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (168 + x0), None, eviction_policy='evict_last')
    tmp2 = 9.964923469387754e-06
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tmp9 = tmp8.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp9, None)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 16859136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.101158848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/is/cisfhhhcmnrh374qyqibbgleiq4idsggo2abkvta3wtqmfmndnvn.py
# Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_56 => squeeze_55
# Graph fragment:
#   %sub_268 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=sub_268]
#   %sum_119 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=sum_119]
#   %rsqrt_18 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_18]
#   %primals_142 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_142]
#   %mul_991 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_119, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_703 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_991, 0), kwargs = {})
#   %unsqueeze_704 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_703, 2), kwargs = {})
#   %unsqueeze_705 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_704, 3), kwargs = {})
#   %squeeze_55 : Tensor "f32[336][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_18, [0, 2, 3]), kwargs = {})
#   %mul_995 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_55, %primals_142), kwargs = {})
#   %unsqueeze_709 : Tensor "f32[1, 336][336, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_995, 0), kwargs = {})
#   %unsqueeze_710 : Tensor "f32[1, 336, 1][336, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_709, 2), kwargs = {})
#   %unsqueeze_711 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_710, 3), kwargs = {})
#   %sub_269 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_268, %unsqueeze_705), kwargs = {})
#   %mul_997 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_269, %unsqueeze_711), kwargs = {})
#   %convert_element_type_709 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_997, torch.float16), kwargs = {})
#   %slice_78 : Tensor "f16[128, 168, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_709, 1, 0, 168), kwargs = {})
#   %convolution_backward_115 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_78, %getitem_104, %convert_element_type_100, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf895
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 134875104}, 'kernel_num_gb': 0.101158848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16859136
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 168)
    x1 = xindex // 168
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 336*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp2 = 9.964923469387754e-06
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tmp9 = tmp8.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp9, None)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 16859136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.101158848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ej/cejstpst5g4u32tuq7aoqj2nafsv5aqt3wppodo4lmqgiyzowp6a.py
# Topologically Sorted Source Nodes: [x_53], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_53 => convert_element_type_98
# Graph fragment:
#   %getitem_757 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=getitem_757]
#   %getitem_781 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_781]
#   %getitem_778 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_778]
#   %cat_11 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=cat_11]
#   %unsqueeze_714 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_714]
#   %cat_69 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_781, %getitem_778], 1), kwargs = {})
#   %add_365 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_757, %cat_69), kwargs = {})
#   %convert_element_type_712 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_365, torch.float32), kwargs = {})
#   %sum_121 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_712, [0, 2, 3]), kwargs = {})
#   %convert_element_type_98 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_11, torch.float32), kwargs = {})
#   %sub_270 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_98, %unsqueeze_714), kwargs = {})
#   %mul_999 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_712, %sub_270), kwargs = {})
#   %sum_122 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_999, [0, 2, 3]), kwargs = {})
#   return %buf900,%buf902
triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 45660384, 'r0_': 0}, 'kernel_num_gb': 0.034069728, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43904
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 56)
    x1 = xindex // 56
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp19 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp23 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_2 + 7168*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp17 = tl.load(in_ptr3 + (x0 + 56*r0_2 + 7168*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = x0
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 28, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = tl.load(in_ptr1 + (28*r0_2 + 3584*x1 + (x0)), r0_mask & tmp5 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp1 >= tmp4
        tmp8 = tl.full([1, 1], 56, tl.int64)
        tmp9 = tmp1 < tmp8
        tmp10 = tl.load(in_ptr2 + (28*r0_2 + 3584*x1 + ((-28) + x0)), r0_mask & tmp7 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tl.where(tmp5, tmp6, tmp10)
        tmp12 = tmp0 + tmp11
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask & xmask, tmp16, _tmp15)
        tmp18 = tmp17.to(tl.float32)
        tmp20 = tmp18 - tmp19
        tmp21 = tmp13 * tmp20
        tmp22 = tl.broadcast_to(tmp21, [XBLOCK, R0_BLOCK])
        tmp24 = _tmp23 + tmp22
        _tmp23 = tl.where(r0_mask & xmask, tmp24, _tmp23)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tmp23 = tl.sum(_tmp23, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
    tl.store(out_ptr1 + (x3), tmp23, xmask)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((56, 784), (1, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 43904, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.034069728
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zz/czz2aaudbe3sfig4zofitn6a4cdqffgn6kfvwg463fhxyot6rvqu.py
# Topologically Sorted Source Nodes: [x_53], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_53 => convert_element_type_98
# Graph fragment:
#   %getitem_757 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=getitem_757]
#   %getitem_781 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_781]
#   %getitem_778 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_778]
#   %cat_11 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=cat_11]
#   %unsqueeze_714 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_714]
#   %sum_122 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_122]
#   %squeeze_52 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_52]
#   %sum_121 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_121]
#   %cat_69 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_781, %getitem_778], 1), kwargs = {})
#   %add_365 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_757, %cat_69), kwargs = {})
#   %convert_element_type_712 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_365, torch.float32), kwargs = {})
#   %convert_element_type_98 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_11, torch.float32), kwargs = {})
#   %sub_270 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_98, %unsqueeze_714), kwargs = {})
#   %mul_1000 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_121, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_715 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1000, 0), kwargs = {})
#   %unsqueeze_716 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_715, 2), kwargs = {})
#   %unsqueeze_717 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_716, 3), kwargs = {})
#   %mul_1001 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_122, 9.964923469387754e-06), kwargs = {})
#   %mul_1002 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_52, %squeeze_52), kwargs = {})
#   %mul_1003 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1001, %mul_1002), kwargs = {})
#   %unsqueeze_718 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1003, 0), kwargs = {})
#   %unsqueeze_719 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_718, 2), kwargs = {})
#   %unsqueeze_720 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_719, 3), kwargs = {})
#   %mul_1005 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_270, %unsqueeze_720), kwargs = {})
#   %sub_272 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_712, %mul_1005), kwargs = {})
#   %sub_273 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_272, %unsqueeze_717), kwargs = {})
#   return %sub_273
triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 89916288}, 'kernel_num_gb': 0.056198016, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    x1 = xindex // 56
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 28, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (28*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 56, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (28*x1 + ((-28) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 9.964923469387754e-06
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tl.store(out_ptr0 + (x2), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.056198016
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/eo/ceocbrnrczvpgylro73lj3fnex3cqnkfaxmvadnz7o7vzr5asqgq.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_273 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=sub_273]
#   %squeeze_52 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_52]
#   %primals_135 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_135]
#   %mul_1004 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_52, %primals_135), kwargs = {})
#   %unsqueeze_721 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1004, 0), kwargs = {})
#   %unsqueeze_722 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_721, 2), kwargs = {})
#   %unsqueeze_723 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_722, 3), kwargs = {})
#   %mul_1006 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_273, %unsqueeze_723), kwargs = {})
#   %convert_element_type_714 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1006, torch.float16), kwargs = {})
#   %slice_81 : Tensor "f16[128, 28, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_714, 1, 28, 56), kwargs = {})
#   %convolution_backward_116 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_81, %getitem_101, %convert_element_type_97, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf906
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22479072}, 'kernel_num_gb': 0.016859584, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2809856
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 28)
    x1 = xindex // 28
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (28 + x0 + 56*x1), None)
    tmp1 = tl.load(in_ptr1 + (28 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (28 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2809856,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016859584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/up/cupofd6qa2ei7s54e7g2trk55rzkmqbasenvzg7kd4mvcrpuifxi.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_273 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=sub_273]
#   %squeeze_52 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_52]
#   %primals_135 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_135]
#   %mul_1004 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_52, %primals_135), kwargs = {})
#   %unsqueeze_721 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1004, 0), kwargs = {})
#   %unsqueeze_722 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_721, 2), kwargs = {})
#   %unsqueeze_723 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_722, 3), kwargs = {})
#   %mul_1006 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_273, %unsqueeze_723), kwargs = {})
#   %convert_element_type_714 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1006, torch.float16), kwargs = {})
#   %slice_80 : Tensor "f16[128, 28, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_714, 1, 0, 28), kwargs = {})
#   %convolution_backward_117 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_80, %getitem_100, %convert_element_type_96, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf911
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22479072}, 'kernel_num_gb': 0.016859584, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2809856
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 28)
    x1 = xindex // 28
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 56*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 2809856,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016859584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lb/clbpvbvr2looeuyfe7xsb5cwzujedcriq4njf4xurw6qhus7tqks.py
# Topologically Sorted Source Nodes: [x_50, sigmoid_2], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid_2 => sigmoid_11
#   x_50 => convert_element_type_88, convert_element_type_89, mul_128, sigmoid_9
# Graph fragment:
#   %getitem_787 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_787]
#   %getitem_784 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_784]
#   %convert_element_type_87 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_87]
#   %sum_123 : Tensor "f16[128, 336, 1, 1][336, 1, 43008, 43008]cuda:0" = PlaceHolder[target=sum_123]
#   %convolution_36 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_36]
#   %cat_70 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_787, %getitem_784], 1), kwargs = {})
#   %convert_element_type_88 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_87, torch.float32), kwargs = {})
#   %sigmoid_9 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_88,), kwargs = {})
#   %mul_128 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_88, %sigmoid_9), kwargs = {})
#   %convert_element_type_89 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_128, torch.float16), kwargs = {})
#   %mul_1008 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_70, %convert_element_type_89), kwargs = {})
#   %sigmoid_11 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_36,), kwargs = {})
#   %sum_123 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1008, [2, 3], True), kwargs = {})
#   %convert_element_type_717 : Tensor "f32[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_123, torch.float32), kwargs = {})
#   %convert_element_type_718 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_11, torch.float32), kwargs = {})
#   %sub_274 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_718), kwargs = {})
#   %mul_1010 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_718, %sub_274), kwargs = {})
#   %mul_1011 : Tensor "f32[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_717, %mul_1010), kwargs = {})
#   %convert_element_type_719 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1011, torch.float16), kwargs = {})
#   return %sum_123,%convert_element_type_719
triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154 = async_compile.triton('triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 1024},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 202567680, 'r0_': 0}, 'kernel_num_gb': 0.135131136, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43008
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp11 = tl.load(in_ptr2 + (x0 + 336*r0_2 + 263424*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 168, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (168*r0_2 + 131712*x1 + (x0)), r0_mask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 336, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (168*r0_2 + 131712*x1 + ((-168) + x0)), r0_mask & tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tl.sigmoid(tmp12)
        tmp14 = tmp12 * tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp10 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask & xmask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp21 = tl.load(in_ptr3 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tmp18.to(tl.float32)
    tmp22 = tl.sigmoid(tmp21)
    tmp23 = tmp22.to(tl.float32)
    tmp24 = 1.0
    tmp25 = tmp24 - tmp23
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp27.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp28, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 43008, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.135131136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fz/cfz4d6b4tisfowyjxvay4efgg63apll572zz6b3vmo3wava6ofmu.py
# Topologically Sorted Source Nodes: [sigmoid_2], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid_2 => sigmoid_11
# Graph fragment:
#   %getitem_787 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_787]
#   %getitem_784 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=getitem_784]
#   %convolution_36 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_36]
#   %getitem_793 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_793]
#   %convert_element_type_87 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_87]
#   %full_default_36 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([128, 336, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_70 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_787, %getitem_784], 1), kwargs = {})
#   %sigmoid_11 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_36,), kwargs = {})
#   %mul_1009 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_70, %sigmoid_11), kwargs = {})
#   %expand_14 : Tensor "f16[128, 336, 28, 28][336, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_793, [128, 336, 28, 28]), kwargs = {})
#   %div_14 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_14, 784), kwargs = {})
#   %add_367 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1009, %div_14), kwargs = {})
#   %sigmoid_104 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_87,), kwargs = {})
#   %sub_276 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_36, %sigmoid_104), kwargs = {})
#   %mul_1015 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_87, %sub_276), kwargs = {})
#   %add_368 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1015, 1), kwargs = {})
#   %mul_1016 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_104, %add_368), kwargs = {})
#   %mul_1017 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_367, %mul_1016), kwargs = {})
#   %convert_element_type_724 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1017, torch.float32), kwargs = {})
#   return %convert_element_type_724
triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155 = async_compile.triton('triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 269746176, 'x': 202481664}, 'kernel_num_gb': 0.269918208, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 100352
    xnumel = 336
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 784
    y0 = (yindex % 784)
    tmp11 = tl.load(in_ptr2 + (x2 + 336*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2 + 336*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2 + 336*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp0 = x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 168, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (168*y3 + (x2)), tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 336, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (168*y3 + ((-168) + x2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp10 * tmp12
    tmp15 = 0.0012755102040816326
    tmp16 = tmp14 * tmp15
    tmp17 = tmp13 + tmp16
    tmp19 = tl.sigmoid(tmp18)
    tmp20 = 1.0
    tmp21 = tmp20 - tmp19
    tmp22 = tmp18 * tmp21
    tmp23 = tmp22 + tmp20
    tmp24 = tmp19 * tmp23
    tmp25 = tmp17 * tmp24
    tmp26 = tmp25.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 784*x2 + 263424*y1), tmp26, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 100352, 336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.269918208
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/km/ckmnoopvpxkaz2hmqjcgtluloyb6ks6ydjhz6jzqtl6gwxjggtlz.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_757 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=getitem_757]
#   %getitem_781 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_781]
#   %getitem_778 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_778]
#   %getitem_805 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_805]
#   %getitem_802 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_802]
#   %cat_69 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_781, %getitem_778], 1), kwargs = {})
#   %add_365 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_757, %cat_69), kwargs = {})
#   %cat_72 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_805, %getitem_802], 1), kwargs = {})
#   %add_370 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_365, %cat_72), kwargs = {})
#   return %add_370
triton_poi_fused_add_cat_156 = async_compile.triton('triton_poi_fused_add_cat_156', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_cat_156', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 78675968}, 'kernel_num_gb': 0.044957696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_cat_156(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    x1 = xindex // 56
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 28, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr0 + (28*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 56, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr1 + (28*x1 + ((-28) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tl.load(in_ptr2 + (28*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (28*x1 + ((-28) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp5, tmp13, tmp14)
    tmp16 = tmp12 + tmp15
    tl.store(in_out_ptr0 + (x2), tmp16, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_cat_156.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_cat_156.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.044957696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3w/c3wqtwqscllcvlou7pk6r24ljb3h33sngbvvjlbklknlvrjfua4m.py
# Topologically Sorted Source Nodes: [x_34], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_34 => convert_element_type_54
# Graph fragment:
#   %add_370 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=add_370]
#   %getitem_829 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_829]
#   %getitem_826 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=getitem_826]
#   %convolution_22 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convolution_22]
#   %unsqueeze_786 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_786]
#   %sum_140 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_140]
#   %squeeze_34 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=squeeze_34]
#   %sum_139 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=sum_139]
#   %sub_305 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=sub_305]
#   %primals_85 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_85]
#   %cat_75 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_829, %getitem_826], 1), kwargs = {})
#   %add_375 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_370, %cat_75), kwargs = {})
#   %convert_element_type_756 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_375, torch.float32), kwargs = {})
#   %convert_element_type_54 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_22, torch.float32), kwargs = {})
#   %sub_302 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_54, %unsqueeze_786), kwargs = {})
#   %mul_1080 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_139, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_787 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1080, 0), kwargs = {})
#   %unsqueeze_788 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_787, 2), kwargs = {})
#   %unsqueeze_789 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_788, 3), kwargs = {})
#   %mul_1081 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_140, 9.964923469387754e-06), kwargs = {})
#   %mul_1082 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_34, %squeeze_34), kwargs = {})
#   %mul_1083 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1081, %mul_1082), kwargs = {})
#   %unsqueeze_790 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1083, 0), kwargs = {})
#   %unsqueeze_791 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_790, 2), kwargs = {})
#   %unsqueeze_792 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_791, 3), kwargs = {})
#   %mul_1084 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_34, %primals_85), kwargs = {})
#   %unsqueeze_793 : Tensor "f32[1, 56][56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1084, 0), kwargs = {})
#   %unsqueeze_794 : Tensor "f32[1, 56, 1][56, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_793, 2), kwargs = {})
#   %unsqueeze_795 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_794, 3), kwargs = {})
#   %mul_1085 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_302, %unsqueeze_792), kwargs = {})
#   %sub_304 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_756, %mul_1085), kwargs = {})
#   %sub_305 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_304, %unsqueeze_789), kwargs = {})
#   %mul_1086 : Tensor "f32[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_305, %unsqueeze_795), kwargs = {})
#   %convert_element_type_758 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1086, torch.float16), kwargs = {})
#   %convolution_backward_132 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_758, %mul_80, %convert_element_type_53, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %sub_305,%buf1035
triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 67437664}, 'kernel_num_gb': 0.044958816, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    x1 = xindex // 56
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 28, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (28*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 56, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (28*x1 + ((-28) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 9.964923469387754e-06
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tmp30 = tmp21 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp31.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp32, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.044958816
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rf/crf26yya7mwc6wxa3mdcyy47nb3ufvn6nzps7ddfizhjwsryq3cq.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_30 => add_56, convert_element_type_44, mul_71, mul_77, sub_10, unsqueeze_40, unsqueeze_41, unsqueeze_42, unsqueeze_43
# Graph fragment:
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=cat_5]
#   %getitem_69 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_69]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %primals_76 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_76]
#   %sub_10 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_5, %getitem_69), kwargs = {})
#   %mul_71 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_10, %rsqrt_10), kwargs = {})
#   %unsqueeze_40 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_75, -1), kwargs = {})
#   %unsqueeze_41 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_40, -1), kwargs = {})
#   %mul_77 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_71, %unsqueeze_41), kwargs = {})
#   %unsqueeze_42 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_76, -1), kwargs = {})
#   %unsqueeze_43 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_42, -1), kwargs = {})
#   %add_56 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_77, %unsqueeze_43), kwargs = {})
#   %convert_element_type_44 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_56, torch.float16), kwargs = {})
#   return %convert_element_type_44
triton_poi_fused__native_batch_norm_legit_functional_158 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_158', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_158', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 144510720}, 'kernel_num_gb': 0.09634176, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_158(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 24084480
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 240)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 24084480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_158.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_158.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09634176
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wa/cwayrunje5a4khzpujuwxqltflqgzzkweohgj5q3hgehkuumyl6z.py
# Topologically Sorted Source Nodes: [x_31, sigmoid], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
#   x_31 => convert_element_type_45, convert_element_type_46, mul_78, sigmoid_1
# Graph fragment:
#   %getitem_832 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=getitem_832]
#   %convert_element_type_44 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=convert_element_type_44]
#   %sum_141 : Tensor "f16[128, 240, 1, 1][240, 1, 30720, 30720]cuda:0" = PlaceHolder[target=sum_141]
#   %convolution_21 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=convolution_21]
#   %convert_element_type_45 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_44, torch.float32), kwargs = {})
#   %sigmoid_1 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_45,), kwargs = {})
#   %mul_78 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_45, %sigmoid_1), kwargs = {})
#   %convert_element_type_46 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_78, torch.float16), kwargs = {})
#   %mul_1088 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_832, %convert_element_type_46), kwargs = {})
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %sum_141 : Tensor "f16[128, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1088, [2, 3], True), kwargs = {})
#   %convert_element_type_760 : Tensor "f32[128, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_141, torch.float32), kwargs = {})
#   %convert_element_type_761 : Tensor "f32[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_3, torch.float32), kwargs = {})
#   %sub_306 : Tensor "f32[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %convert_element_type_761), kwargs = {})
#   %mul_1090 : Tensor "f32[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_761, %sub_306), kwargs = {})
#   %mul_1091 : Tensor "f32[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_760, %mul_1090), kwargs = {})
#   %convert_element_type_762 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1091, torch.float16), kwargs = {})
#   return %sum_141,%convert_element_type_762
triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159 = async_compile.triton('triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 1024},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96522240, 'r0_': 0}, 'kernel_num_gb': 0.09652224, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 30720
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    _tmp8 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 188160*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 240*r0_2 + 188160*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tmp1.to(tl.float32)
        tmp3 = tl.sigmoid(tmp2)
        tmp4 = tmp2 * tmp3
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tmp0 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tmp11 = tl.load(in_ptr2 + (x3), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp10 = tmp8.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp12.to(tl.float32)
    tmp14 = 1.0
    tmp15 = tmp14 - tmp13
    tmp16 = tmp13 * tmp15
    tmp17 = tmp10 * tmp16
    tmp18 = tmp17.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3), tmp18, xmask)


def get_args():
    arg_0 = rand_strided((128, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 30720, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09652224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vx/cvx33al7lebh2oxxwiguygdnye74yjajnnyqjooggksirb2hpg2r.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_835 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=getitem_835]
#   %convolution_20 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=convolution_20]
#   %sigmoid_109 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_20,), kwargs = {})
#   %full_default_46 : Tensor "f16[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 20, 1, 1], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_307 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_46, %sigmoid_109), kwargs = {})
#   %mul_1092 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution_20, %sub_307), kwargs = {})
#   %add_376 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1092, 1), kwargs = {})
#   %mul_1093 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_109, %add_376), kwargs = {})
#   %mul_1094 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_835, %mul_1093), kwargs = {})
#   return %mul_1094
triton_poi_fused_add_fill_mul_sigmoid_sub_160 = async_compile.triton('triton_poi_fused_add_fill_mul_sigmoid_sub_160', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_fill_mul_sigmoid_sub_160', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20480}, 'kernel_num_gb': 1.536e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_fill_mul_sigmoid_sub_160(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2560
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = 1.0
    tmp4 = tmp3 - tmp2
    tmp5 = tmp1 * tmp4
    tmp6 = tmp5 + tmp3
    tmp7 = tmp2 * tmp6
    tmp8 = tmp0 * tmp7
    tl.store(in_out_ptr0 + (x0), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2560,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_fill_mul_sigmoid_sub_160.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_fill_mul_sigmoid_sub_160.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.536e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/m7/cm7lscnpwqlrdjcc62nj4rymqycx3ldttjn5aqj6fhndkaazqul2.py
# Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
#   x_30 => convert_element_type_43, squeeze_30
# Graph fragment:
#   %getitem_832 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=getitem_832]
#   %convolution_21 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=convolution_21]
#   %getitem_838 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_838]
#   %convert_element_type_44 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=convert_element_type_44]
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=cat_5]
#   %getitem_69 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_69]
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %mul_1089 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_832, %sigmoid_3), kwargs = {})
#   %expand_16 : Tensor "f16[128, 240, 28, 28][240, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_838, [128, 240, 28, 28]), kwargs = {})
#   %div_16 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_16, 784), kwargs = {})
#   %add_377 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1089, %div_16), kwargs = {})
#   %sigmoid_110 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_44,), kwargs = {})
#   %full_default_47 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 240, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_308 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_47, %sigmoid_110), kwargs = {})
#   %mul_1095 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_44, %sub_308), kwargs = {})
#   %add_378 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1095, 1), kwargs = {})
#   %mul_1096 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_110, %add_378), kwargs = {})
#   %mul_1097 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_377, %mul_1096), kwargs = {})
#   %convert_element_type_767 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1097, torch.float32), kwargs = {})
#   %squeeze_30 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_69, [0, 2, 3]), kwargs = {})
#   %unsqueeze_796 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_30, 0), kwargs = {})
#   %unsqueeze_797 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_796, 2), kwargs = {})
#   %unsqueeze_798 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_797, 3), kwargs = {})
#   %sum_144 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_767, [0, 2, 3]), kwargs = {})
#   %convert_element_type_43 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_5, torch.float32), kwargs = {})
#   %sub_309 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_43, %unsqueeze_798), kwargs = {})
#   %mul_1098 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_767, %sub_309), kwargs = {})
#   %sum_145 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1098, [0, 2, 3]), kwargs = {})
#   return %buf1056,%buf1058
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 242811840, 'r0_': 0}, 'kernel_num_gb': 0.14561376, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp22 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    _tmp26 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (x0 + 240*((r0_2 + 196*x1) // 784)), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp4 = tl.load(in_ptr2 + (x0 + 240*((r0_2 + 196*x1) // 784)), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp8 = tl.load(in_ptr3 + (x0 + 240*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp20 = tl.load(in_ptr4 + (x0 + 240*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = tmp0 * tmp2
        tmp5 = 0.0012755102040816326
        tmp6 = tmp4 * tmp5
        tmp7 = tmp3 + tmp6
        tmp9 = tl.sigmoid(tmp8)
        tmp10 = 1.0
        tmp11 = tmp10 - tmp9
        tmp12 = tmp8 * tmp11
        tmp13 = tmp12 + tmp10
        tmp14 = tmp9 * tmp13
        tmp15 = tmp7 * tmp14
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask, tmp19, _tmp18)
        tmp21 = tmp20.to(tl.float32)
        tmp23 = tmp21 - tmp22
        tmp24 = tmp16 * tmp23
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, R0_BLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(r0_mask, tmp27, _tmp26)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp18, None)
    tl.store(out_ptr1 + (x3), tmp26, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((240, 512), (1, 240), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((240, 512), (1, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 122880, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.14561376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fr/cfr3crelrcxmuvd5xf4hndqlk7rknjmyctrlcirze5ctjvgo24j4.py
# Topologically Sorted Source Nodes: [sigmoid], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
# Graph fragment:
#   %buf1056 : Tensor "f32[240, 512][1, 240]cuda:0" = PlaceHolder[target=buf1056]
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %mul_1089 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_832, %sigmoid_3), kwargs = {})
#   %expand_16 : Tensor "f16[128, 240, 28, 28][240, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_838, [128, 240, 28, 28]), kwargs = {})
#   %div_16 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_16, 784), kwargs = {})
#   %add_377 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1089, %div_16), kwargs = {})
#   %sigmoid_110 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_44,), kwargs = {})
#   %full_default_47 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 240, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_308 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_47, %sigmoid_110), kwargs = {})
#   %mul_1095 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_44, %sub_308), kwargs = {})
#   %add_378 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1095, 1), kwargs = {})
#   %mul_1096 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_110, %add_378), kwargs = {})
#   %mul_1097 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_377, %mul_1096), kwargs = {})
#   %convert_element_type_767 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1097, torch.float32), kwargs = {})
#   %sum_144 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_767, [0, 2, 3]), kwargs = {})
#   return %sum_144
triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162 = async_compile.triton('triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 493440, 'r0_': 0}, 'kernel_num_gb': 0.00049248, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((240, 512), (1, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 240, 512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00049248
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/me/cmehdl3ugpdkoxw32inldtx4ok6hh4r2xpbll2slmterpur3iyja.py
# Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
#   x_30 => convert_element_type_43, squeeze_30, squeeze_31
# Graph fragment:
#   %buf1058 : Tensor "f32[240, 512][1, 240]cuda:0" = PlaceHolder[target=buf1058]
#   %sum_145 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_145]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %mul_1089 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_832, %sigmoid_3), kwargs = {})
#   %expand_16 : Tensor "f16[128, 240, 28, 28][240, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_838, [128, 240, 28, 28]), kwargs = {})
#   %div_16 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_16, 784), kwargs = {})
#   %add_377 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1089, %div_16), kwargs = {})
#   %sigmoid_110 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_44,), kwargs = {})
#   %full_default_47 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 240, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_308 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_47, %sigmoid_110), kwargs = {})
#   %mul_1095 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_44, %sub_308), kwargs = {})
#   %add_378 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1095, 1), kwargs = {})
#   %mul_1096 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_110, %add_378), kwargs = {})
#   %mul_1097 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_377, %mul_1096), kwargs = {})
#   %convert_element_type_767 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1097, torch.float32), kwargs = {})
#   %squeeze_30 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_69, [0, 2, 3]), kwargs = {})
#   %unsqueeze_796 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_30, 0), kwargs = {})
#   %unsqueeze_797 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_796, 2), kwargs = {})
#   %unsqueeze_798 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_797, 3), kwargs = {})
#   %convert_element_type_43 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_5, torch.float32), kwargs = {})
#   %sub_309 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_43, %unsqueeze_798), kwargs = {})
#   %mul_1098 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_767, %sub_309), kwargs = {})
#   %sum_145 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1098, [0, 2, 3]), kwargs = {})
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1106 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_145, %squeeze_31), kwargs = {})
#   return %sum_145,%mul_1106
triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 496320, 'r0_': 0}, 'kernel_num_gb': 0.0004944, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((240, 512), (1, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 240, 512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004944
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d4/cd4xcjlkpmleomq5uwr43mcavw2le3sippdxpi2etfn4iqjujt2m.py
# Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
#   x_30 => convert_element_type_43, squeeze_30, squeeze_31
# Graph fragment:
#   %getitem_832 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=getitem_832]
#   %convolution_21 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=convolution_21]
#   %getitem_838 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_838]
#   %convert_element_type_44 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=convert_element_type_44]
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=cat_5]
#   %getitem_69 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_69]
#   %sum_145 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_145]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %sum_144 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_144]
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %mul_1089 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_832, %sigmoid_3), kwargs = {})
#   %expand_16 : Tensor "f16[128, 240, 28, 28][240, 1, 0, 0]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%getitem_838, [128, 240, 28, 28]), kwargs = {})
#   %div_16 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.div.Scalar](args = (%expand_16, 784), kwargs = {})
#   %add_377 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1089, %div_16), kwargs = {})
#   %sigmoid_110 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_44,), kwargs = {})
#   %full_default_47 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 240, 28, 28], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_308 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_47, %sigmoid_110), kwargs = {})
#   %mul_1095 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_44, %sub_308), kwargs = {})
#   %add_378 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1095, 1), kwargs = {})
#   %mul_1096 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_110, %add_378), kwargs = {})
#   %mul_1097 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_377, %mul_1096), kwargs = {})
#   %convert_element_type_767 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1097, torch.float32), kwargs = {})
#   %squeeze_30 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_69, [0, 2, 3]), kwargs = {})
#   %unsqueeze_796 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_30, 0), kwargs = {})
#   %unsqueeze_797 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_796, 2), kwargs = {})
#   %unsqueeze_798 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_797, 3), kwargs = {})
#   %convert_element_type_43 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_5, torch.float32), kwargs = {})
#   %sub_309 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_43, %unsqueeze_798), kwargs = {})
#   %mul_1099 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_144, 9.964923469387754e-06), kwargs = {})
#   %unsqueeze_799 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1099, 0), kwargs = {})
#   %unsqueeze_800 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_799, 2), kwargs = {})
#   %unsqueeze_801 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_800, 3), kwargs = {})
#   %mul_1100 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_145, 9.964923469387754e-06), kwargs = {})
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1101 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_31, %squeeze_31), kwargs = {})
#   %mul_1102 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1100, %mul_1101), kwargs = {})
#   %unsqueeze_802 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1102, 0), kwargs = {})
#   %unsqueeze_803 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_802, 2), kwargs = {})
#   %unsqueeze_804 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_803, 3), kwargs = {})
#   %mul_1104 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_309, %unsqueeze_804), kwargs = {})
#   %sub_311 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_767, %mul_1104), kwargs = {})
#   %sub_312 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_311, %unsqueeze_801), kwargs = {})
#   return %sub_312
triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 337309440}, 'kernel_num_gb': 0.24097152, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 24084480
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 240)
    x2 = xindex // 188160
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0 + 240*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr2 + (x0 + 240*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tl.load(in_ptr3 + (x3), None).to(tl.float32)
    tmp17 = tl.load(in_ptr4 + (x3), None).to(tl.float32)
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.sigmoid(tmp1)
    tmp3 = tmp0 * tmp2
    tmp5 = 0.0012755102040816326
    tmp6 = tmp4 * tmp5
    tmp7 = tmp3 + tmp6
    tmp9 = tl.sigmoid(tmp8)
    tmp10 = 1.0
    tmp11 = tmp10 - tmp9
    tmp12 = tmp8 * tmp11
    tmp13 = tmp12 + tmp10
    tmp14 = tmp9 * tmp13
    tmp15 = tmp7 * tmp14
    tmp16 = tmp15.to(tl.float32)
    tmp18 = tmp17.to(tl.float32)
    tmp20 = tmp18 - tmp19
    tmp22 = 9.964923469387754e-06
    tmp23 = tmp21 * tmp22
    tmp25 = tmp24 * tmp24
    tmp26 = tmp23 * tmp25
    tmp27 = tmp20 * tmp26
    tmp28 = tmp16 - tmp27
    tmp30 = tmp29 * tmp22
    tmp31 = tmp28 - tmp30
    tl.store(out_ptr0 + (x3), tmp31, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 24084480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.24097152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/aj/cajqupk57evyjkjuvp63tcf5gulgluvt6ssj2pmxslsc7ydao7gc.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_30 => squeeze_31
# Graph fragment:
#   %sub_312 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=sub_312]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1103 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_31, %primals_75), kwargs = {})
#   %unsqueeze_805 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1103, 0), kwargs = {})
#   %unsqueeze_806 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_805, 2), kwargs = {})
#   %unsqueeze_807 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_806, 3), kwargs = {})
#   %mul_1105 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_312, %unsqueeze_807), kwargs = {})
#   %convert_element_type_769 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1105, torch.float16), kwargs = {})
#   %slice_95 : Tensor "f16[128, 60, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_769, 1, 180, 240), kwargs = {})
#   %convolution_backward_135 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_95, %getitem_67, %convert_element_type_42, [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 60, [True, True, False]), kwargs = {})
#   return %buf1062
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48169440}, 'kernel_num_gb': 0.03612864, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 60)
    x1 = xindex // 60
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (180 + x0 + 240*x1), None)
    tmp1 = tl.load(in_ptr1 + (180 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (180 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03612864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mw/cmwtiix52f66tvmgy5lvpbhekmh33hionbelcov6blo4dbbm7nla.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_30 => squeeze_31
# Graph fragment:
#   %sub_312 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=sub_312]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1103 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_31, %primals_75), kwargs = {})
#   %unsqueeze_805 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1103, 0), kwargs = {})
#   %unsqueeze_806 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_805, 2), kwargs = {})
#   %unsqueeze_807 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_806, 3), kwargs = {})
#   %mul_1105 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_312, %unsqueeze_807), kwargs = {})
#   %convert_element_type_769 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1105, torch.float16), kwargs = {})
#   %slice_94 : Tensor "f16[128, 60, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_769, 1, 120, 180), kwargs = {})
#   %convolution_backward_136 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_94, %getitem_62, %convert_element_type_41, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 60, [True, True, False]), kwargs = {})
#   return %buf1067
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48169440}, 'kernel_num_gb': 0.03612864, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 60)
    x1 = xindex // 60
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (120 + x0 + 240*x1), None)
    tmp1 = tl.load(in_ptr1 + (120 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (120 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03612864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kn/cknlrgc3k6tji6rmmyfpaauq6xk5akua2ndceuldepmz7jjrekmv.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_30 => squeeze_31
# Graph fragment:
#   %sub_312 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=sub_312]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1103 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_31, %primals_75), kwargs = {})
#   %unsqueeze_805 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1103, 0), kwargs = {})
#   %unsqueeze_806 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_805, 2), kwargs = {})
#   %unsqueeze_807 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_806, 3), kwargs = {})
#   %mul_1105 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_312, %unsqueeze_807), kwargs = {})
#   %convert_element_type_769 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1105, torch.float16), kwargs = {})
#   %slice_93 : Tensor "f16[128, 60, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_769, 1, 60, 120), kwargs = {})
#   %convolution_backward_137 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_93, %getitem_57, %convert_element_type_40, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 60, [True, True, False]), kwargs = {})
#   return %buf1072
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48169440}, 'kernel_num_gb': 0.03612864, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 60)
    x1 = xindex // 60
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (60 + x0 + 240*x1), None)
    tmp1 = tl.load(in_ptr1 + (60 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (60 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03612864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rk/crkdm4t6r2jg2by4cblraw6dd2jbmono4ht3rfwp7dnmbfodbjeb.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_30 => squeeze_31
# Graph fragment:
#   %sub_312 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=sub_312]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %squeeze_31 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_10, [0, 2, 3]), kwargs = {})
#   %mul_1103 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_31, %primals_75), kwargs = {})
#   %unsqueeze_805 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1103, 0), kwargs = {})
#   %unsqueeze_806 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_805, 2), kwargs = {})
#   %unsqueeze_807 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_806, 3), kwargs = {})
#   %mul_1105 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_312, %unsqueeze_807), kwargs = {})
#   %convert_element_type_769 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1105, torch.float16), kwargs = {})
#   %slice_92 : Tensor "f16[128, 60, 28, 28][188160, 1, 6720, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_769, 1, 0, 60), kwargs = {})
#   %convolution_backward_138 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_92, %getitem_52, %convert_element_type_39, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 60, [True, True, False]), kwargs = {})
#   return %buf1077
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48169440}, 'kernel_num_gb': 0.03612864, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 60)
    x1 = xindex // 60
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 240*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03612864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vd/cvd74hdamjb3ddqby6dxrnp3aumgxa2ufm7rl36bbcqk35dypd2l.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
# Source node to ATen node mapping:
#   x_27 => add_51, convert_element_type_36, mul_63, mul_69, sub_9, unsqueeze_36, unsqueeze_37, unsqueeze_38, unsqueeze_39
# Graph fragment:
#   %convolution_15 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convolution_15]
#   %getitem_47 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_47]
#   %rsqrt_9 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_9]
#   %primals_66 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_66]
#   %primals_67 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_67]
#   %getitem_850 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_850]
#   %getitem_847 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_847]
#   %getitem_844 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_844]
#   %getitem_841 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_841]
#   %convert_element_type_36 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convert_element_type_36]
#   %cat_76 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_850, %getitem_847, %getitem_844, %getitem_841], 1), kwargs = {})
#   %sub_9 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_15, %getitem_47), kwargs = {})
#   %mul_63 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_9, %rsqrt_9), kwargs = {})
#   %unsqueeze_36 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_66, -1), kwargs = {})
#   %unsqueeze_37 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_36, -1), kwargs = {})
#   %mul_69 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_63, %unsqueeze_37), kwargs = {})
#   %unsqueeze_38 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_67, -1), kwargs = {})
#   %unsqueeze_39 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_38, -1), kwargs = {})
#   %add_51 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_69, %unsqueeze_39), kwargs = {})
#   %convert_element_type_36 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_51, torch.float16), kwargs = {})
#   %sigmoid_111 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_36,), kwargs = {})
#   %full_default_48 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([128, 240, 56, 56], 1), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %sub_313 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%full_default_48, %sigmoid_111), kwargs = {})
#   %mul_1107 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_36, %sub_313), kwargs = {})
#   %add_379 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Scalar](args = (%mul_1107, 1), kwargs = {})
#   %mul_1108 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_111, %add_379), kwargs = {})
#   %mul_1109 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_76, %mul_1108), kwargs = {})
#   return %convert_element_type_36,%mul_1109
triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 524288, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 385351680, 'x': 963383040}, 'kernel_num_gb': 0.57803136, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 401408
    xnumel = 240
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 3136)
    y3 = yindex // 3136
    tmp0 = tl.load(in_ptr0 + (x1 + 240*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = x1
    tmp12 = tl.full([1, 1], 0, tl.int64)
    tmp13 = tmp11 >= tmp12
    tmp14 = tl.full([1, 1], 60, tl.int64)
    tmp15 = tmp11 < tmp14
    tmp16 = tl.load(in_ptr5 + (60*y0 + (x1)), tmp15 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp11 >= tmp14
    tmp18 = tl.full([1, 1], 120, tl.int64)
    tmp19 = tmp11 < tmp18
    tmp20 = tmp17 & tmp19
    tmp21 = tl.load(in_ptr6 + (60*y0 + ((-60) + x1)), tmp20 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp22 = tmp11 >= tmp18
    tmp23 = tl.full([1, 1], 180, tl.int64)
    tmp24 = tmp11 < tmp23
    tmp25 = tmp22 & tmp24
    tmp26 = tl.load(in_ptr7 + (60*y0 + ((-120) + x1)), tmp25 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tmp11 >= tmp23
    tmp28 = tl.full([1, 1], 240, tl.int64)
    tmp29 = tmp11 < tmp28
    tmp30 = tl.load(in_ptr8 + (60*y0 + ((-180) + x1)), tmp27 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tl.where(tmp25, tmp26, tmp30)
    tmp32 = tl.where(tmp20, tmp21, tmp31)
    tmp33 = tl.where(tmp15, tmp16, tmp32)
    tmp34 = tl.sigmoid(tmp10)
    tmp35 = 1.0
    tmp36 = tmp35 - tmp34
    tmp37 = tmp10 * tmp36
    tmp38 = tmp37 + tmp35
    tmp39 = tmp34 * tmp38
    tmp40 = tmp33 * tmp39
    tl.store(out_ptr1 + (y2 + 3136*x1 + 752640*y3), tmp40, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_9 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 401408, 240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.57803136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4m/c4mnxrcwicgnul74ggv6axikd6si2e3f4cpimi4kh5s2hm5hupum.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_1109 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=mul_1109]
#   %convert_element_type_774 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1109, torch.float32), kwargs = {})
#   %sum_146 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_774, [0, 2, 3]), kwargs = {})
#   return %buf1084
triton_red_fused_native_batch_norm_backward_170 = async_compile.triton('triton_red_fused_native_batch_norm_backward_170', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 131072},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_170', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 7680, 'r0_': 192675840}, 'kernel_num_gb': 0.19267968, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_170(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 100352
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (3136*x0 + 752640*(r0_2 // 3136) + 24084480*x1 + ((r0_2 % 3136))), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 4), (1, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 960, 100352,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_170.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_170.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.19267968
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ch/cchbzoxyjrhyzen45iiwd3cj7erccq3otko6daja3shucxmlt4k5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1084 : Tensor "f32[240, 4][1, 240]cuda:0" = PlaceHolder[target=buf1084]
#   %convert_element_type_774 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1109, torch.float32), kwargs = {})
#   %sum_146 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_774, [0, 2, 3]), kwargs = {})
#   return %sum_146
triton_per_fused_native_batch_norm_backward_171 = async_compile.triton('triton_per_fused_native_batch_norm_backward_171', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 4},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_171', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5760, 'r0_': 0}, 'kernel_num_gb': 4.8e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_native_batch_norm_backward_171(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 4
    R0_BLOCK: tl.constexpr = 4
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((240, 4), (1, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 240, 4,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_native_batch_norm_backward_171.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_native_batch_norm_backward_171.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 4.8e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ze/czehujfmqkp2meywn5oiaalzls4eqkt7fy4dcbagrse5hczdml6f.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_27 => convert_element_type_35, squeeze_27
# Graph fragment:
#   %mul_1109 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=mul_1109]
#   %convolution_15 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convolution_15]
#   %getitem_47 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_47]
#   %convert_element_type_774 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1109, torch.float32), kwargs = {})
#   %squeeze_27 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_47, [0, 2, 3]), kwargs = {})
#   %unsqueeze_808 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_27, 0), kwargs = {})
#   %unsqueeze_809 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_808, 2), kwargs = {})
#   %unsqueeze_810 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_809, 3), kwargs = {})
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %sub_314 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_35, %unsqueeze_810), kwargs = {})
#   %mul_1110 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_774, %sub_314), kwargs = {})
#   %sum_147 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1110, [0, 2, 3]), kwargs = {})
#   return %buf1086
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 256, 'x': 512, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 192676800, 'x': 983040, 'r0_': 192675840}, 'kernel_num_gb': 0.38584416, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172(in_ptr0, in_ptr1, in_ptr2, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 240
    xnumel = 512
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    x1 = xindex
    y0 = yindex
    tmp4 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    _tmp8 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (56*((((r0_2 + 784*x1) // 56) % 56)) + 3136*y0 + 752640*((r0_2 + 784*x1) // 3136) + ((r0_2 % 56))), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tl.load(in_ptr1 + (y0 + 240*r0_2 + 188160*x1), r0_mask & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp3 = tmp2.to(tl.float32)
        tmp5 = tmp3 - tmp4
        tmp6 = tmp1 * tmp5
        tmp7 = tl.broadcast_to(tmp6, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(r0_mask & xmask & ymask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 512*y0), tmp8, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240, 512), (512, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 240, 512, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.38584416
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5k/c5k7ymjsghc2mqa2ond2qbohxcdxjk2ll65jjvpodni6bctbpreh.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_27 => convert_element_type_35, squeeze_27, squeeze_28
# Graph fragment:
#   %buf1086 : Tensor "f32[240, 512][512, 1]cuda:0" = PlaceHolder[target=buf1086]
#   %sum_147 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_147]
#   %rsqrt_9 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_9]
#   %convert_element_type_774 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1109, torch.float32), kwargs = {})
#   %squeeze_27 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_47, [0, 2, 3]), kwargs = {})
#   %unsqueeze_808 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_27, 0), kwargs = {})
#   %unsqueeze_809 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_808, 2), kwargs = {})
#   %unsqueeze_810 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_809, 3), kwargs = {})
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %sub_314 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_35, %unsqueeze_810), kwargs = {})
#   %mul_1110 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_774, %sub_314), kwargs = {})
#   %sum_147 : Tensor "f32[240][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1110, [0, 2, 3]), kwargs = {})
#   %squeeze_28 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_9, [0, 2, 3]), kwargs = {})
#   %mul_1118 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_147, %squeeze_28), kwargs = {})
#   return %sum_147,%mul_1118
triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 512},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4800, 'r0_': 491520}, 'kernel_num_gb': 0.0004944, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 512
    R0_BLOCK: tl.constexpr = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r0_1 + 512*x0), xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((240, 512), (512, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 240, 512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004944
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mg/cmgozqnick76dephpdr3kluhxpjkowxn2byusbo4235udnygxunl.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_27 => convert_element_type_35, squeeze_27, squeeze_28
# Graph fragment:
#   %mul_1109 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=mul_1109]
#   %convolution_15 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convolution_15]
#   %getitem_47 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_47]
#   %sum_147 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_147]
#   %rsqrt_9 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_9]
#   %sum_146 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=sum_146]
#   %primals_66 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_66]
#   %convert_element_type_774 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1109, torch.float32), kwargs = {})
#   %squeeze_27 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_47, [0, 2, 3]), kwargs = {})
#   %unsqueeze_808 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_27, 0), kwargs = {})
#   %unsqueeze_809 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_808, 2), kwargs = {})
#   %unsqueeze_810 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_809, 3), kwargs = {})
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %sub_314 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_35, %unsqueeze_810), kwargs = {})
#   %mul_1111 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_146, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_811 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1111, 0), kwargs = {})
#   %unsqueeze_812 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_811, 2), kwargs = {})
#   %unsqueeze_813 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_812, 3), kwargs = {})
#   %mul_1112 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_147, 2.4912308673469386e-06), kwargs = {})
#   %squeeze_28 : Tensor "f32[240][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_9, [0, 2, 3]), kwargs = {})
#   %mul_1113 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_28, %squeeze_28), kwargs = {})
#   %mul_1114 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1112, %mul_1113), kwargs = {})
#   %unsqueeze_814 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1114, 0), kwargs = {})
#   %unsqueeze_815 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_814, 2), kwargs = {})
#   %unsqueeze_816 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_815, 3), kwargs = {})
#   %mul_1115 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_28, %primals_66), kwargs = {})
#   %unsqueeze_817 : Tensor "f32[1, 240][240, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1115, 0), kwargs = {})
#   %unsqueeze_818 : Tensor "f32[1, 240, 1][240, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_817, 2), kwargs = {})
#   %unsqueeze_819 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_818, 3), kwargs = {})
#   %mul_1116 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_314, %unsqueeze_816), kwargs = {})
#   %sub_316 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_774, %mul_1116), kwargs = {})
#   %sub_317 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_316, %unsqueeze_813), kwargs = {})
#   %mul_1117 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_317, %unsqueeze_819), kwargs = {})
#   %convert_element_type_776 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1117, torch.float16), kwargs = {})
#   %convolution_backward_139 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_776, %add_46, %convert_element_type_34, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf1089
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 524288, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 192675840, 'x': 578032320}, 'kernel_num_gb': 0.57803232, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 401408
    xnumel = 240
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 3136)
    y1 = yindex // 3136
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 3136*x2 + 752640*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2 + 240*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x2), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 2.4912308673469386e-06
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + 240*y3), tmp20, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 401408, 240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.57803232
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lt/cltnsw34wbhqpwogonc6jcm6xofhscmdtddthbyw4fxjat6s7lpp.py
# Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_24 => convert_element_type_32
# Graph fragment:
#   %getitem_853 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=getitem_853]
#   %cat_4 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_4]
#   %unsqueeze_822 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_822]
#   %convert_element_type_778 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_853, torch.float32), kwargs = {})
#   %sum_148 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_778, [0, 2, 3]), kwargs = {})
#   %convert_element_type_32 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_4, torch.float32), kwargs = {})
#   %sub_318 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_32, %unsqueeze_822), kwargs = {})
#   %mul_1119 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_778, %sub_318), kwargs = {})
#   %sum_149 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1119, [0, 2, 3]), kwargs = {})
#   return %buf1094,%buf1096
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 64880800, 'r0_': 0}, 'kernel_num_gb': 0.06455312, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 40960
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 40)
    x1 = xindex // 40
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_2 + 15680*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 40*r0_2 + 15680*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 40960, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06455312
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gk/cgk3nxswfdezmolfpmdmetj4hdgamcbq4no32xmgzlxsmu4gmg6r.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1094 : Tensor "f32[40, 1024][1, 40]cuda:0" = PlaceHolder[target=buf1094]
#   %convert_element_type_778 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_853, torch.float32), kwargs = {})
#   %sum_148 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_778, [0, 2, 3]), kwargs = {})
#   return %sum_148
triton_red_fused_native_batch_norm_backward_176 = async_compile.triton('triton_red_fused_native_batch_norm_backward_176', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 64, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_176', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 164160, 'r0_': 0}, 'kernel_num_gb': 0.000164, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_176(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 40
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 40, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_176.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_176.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000164
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kb/ckbn3v4ks6fykbofpdbii6wbqfk7x2keyv3pfubtswflaolatvs6.py
# Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_24 => convert_element_type_32
# Graph fragment:
#   %buf1096 : Tensor "f32[40, 1024][1, 40]cuda:0" = PlaceHolder[target=buf1096]
#   %sum_149 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=sum_149]
#   %squeeze_25 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=squeeze_25]
#   %convert_element_type_778 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_853, torch.float32), kwargs = {})
#   %convert_element_type_32 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_4, torch.float32), kwargs = {})
#   %sub_318 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_32, %unsqueeze_822), kwargs = {})
#   %mul_1119 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_778, %sub_318), kwargs = {})
#   %sum_149 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1119, [0, 2, 3]), kwargs = {})
#   %mul_1127 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_149, %squeeze_25), kwargs = {})
#   return %sum_149,%mul_1127
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 64, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 164640, 'r0_': 0}, 'kernel_num_gb': 0.00016432, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 40
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 40, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00016432
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d6/cd6ytvswhcibgzmfu5i2rszud54q5n7ax63k6z6ooiijndw4glgs.py
# Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_24 => convert_element_type_32
# Graph fragment:
#   %getitem_853 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=getitem_853]
#   %cat_4 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_4]
#   %unsqueeze_822 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_822]
#   %sum_149 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=sum_149]
#   %squeeze_25 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=squeeze_25]
#   %sum_148 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=sum_148]
#   %primals_60 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_60]
#   %convert_element_type_778 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_853, torch.float32), kwargs = {})
#   %convert_element_type_32 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_4, torch.float32), kwargs = {})
#   %sub_318 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_32, %unsqueeze_822), kwargs = {})
#   %mul_1120 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_148, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_823 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1120, 0), kwargs = {})
#   %unsqueeze_824 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_823, 2), kwargs = {})
#   %unsqueeze_825 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_824, 3), kwargs = {})
#   %mul_1121 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_149, 2.4912308673469386e-06), kwargs = {})
#   %mul_1122 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_25, %squeeze_25), kwargs = {})
#   %mul_1123 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1121, %mul_1122), kwargs = {})
#   %unsqueeze_826 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1123, 0), kwargs = {})
#   %unsqueeze_827 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_826, 2), kwargs = {})
#   %unsqueeze_828 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_827, 3), kwargs = {})
#   %mul_1124 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_25, %primals_60), kwargs = {})
#   %unsqueeze_829 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1124, 0), kwargs = {})
#   %unsqueeze_830 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_829, 2), kwargs = {})
#   %unsqueeze_831 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_830, 3), kwargs = {})
#   %mul_1125 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_318, %unsqueeze_828), kwargs = {})
#   %sub_320 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_778, %mul_1125), kwargs = {})
#   %sub_321 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_320, %unsqueeze_825), kwargs = {})
#   %mul_1126 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_321, %unsqueeze_831), kwargs = {})
#   %convert_element_type_780 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1126, torch.float16), kwargs = {})
#   return %convert_element_type_780
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 128451360}, 'kernel_num_gb': 0.09633872, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16056320
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 40)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 2.4912308673469386e-06
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 16056320,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633872
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bi/cbi5wyki4iuljakqxof7q5gtwfwean2lzqqt3fo7jzdaixyy54hh.py
# Topologically Sorted Source Nodes: [x_21, x_22], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_21 => add_40, convert_element_type_29, mul_49, mul_55, sub_7, unsqueeze_28, unsqueeze_29, unsqueeze_30, unsqueeze_31
#   x_22 => relu_5
# Graph fragment:
#   %convolution_12 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convolution_12]
#   %getitem_37 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_37]
#   %rsqrt_7 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=rsqrt_7]
#   %primals_53 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_53]
#   %primals_54 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_54]
#   %getitem_859 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_859]
#   %getitem_856 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=getitem_856]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_77 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_859, %getitem_856], 1), kwargs = {})
#   %sub_7 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_12, %getitem_37), kwargs = {})
#   %mul_49 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_7, %rsqrt_7), kwargs = {})
#   %unsqueeze_28 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_53, -1), kwargs = {})
#   %unsqueeze_29 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_28, -1), kwargs = {})
#   %mul_55 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_49, %unsqueeze_29), kwargs = {})
#   %unsqueeze_30 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_54, -1), kwargs = {})
#   %unsqueeze_31 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_30, -1), kwargs = {})
#   %add_40 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_55, %unsqueeze_31), kwargs = {})
#   %convert_element_type_29 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_40, torch.float16), kwargs = {})
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_29,), kwargs = {})
#   %le_1 : Tensor "b8[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_5, 0), kwargs = {})
#   %where_1 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_1, %full_default, %cat_77), kwargs = {})
#   %convert_element_type_783 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_1, torch.float32), kwargs = {})
#   return %convert_element_type_783
triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 674367360}, 'kernel_num_gb': 0.3853536, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 48168960
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 120)
    x1 = xindex // 120
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tmp13 = 0.0
    tmp14 = tmp12 <= tmp13
    tmp15 = x0
    tmp16 = tl.full([1], 0, tl.int64)
    tmp17 = tmp15 >= tmp16
    tmp18 = tl.full([1], 60, tl.int64)
    tmp19 = tmp15 < tmp18
    tmp20 = tl.load(in_ptr5 + (60*x1 + (x0)), tmp19, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp21 = tmp15 >= tmp18
    tmp22 = tl.full([1], 120, tl.int64)
    tmp23 = tmp15 < tmp22
    tmp24 = tl.load(in_ptr6 + (60*x1 + ((-60) + x0)), tmp21, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp25 = tl.where(tmp19, tmp20, tmp24)
    tmp26 = tl.where(tmp14, tmp13, tmp25)
    tmp27 = tmp26.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp27, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 48168960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.3853536
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/k3/ck3v6ifu6lpjlkvjldinxvteghywi3bz7oatbkrnajkgsahye6vc.py
# Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_21 => convert_element_type_28, squeeze_21
# Graph fragment:
#   %convert_element_type_783 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convert_element_type_783]
#   %convolution_12 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convolution_12]
#   %getitem_37 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_37]
#   %squeeze_21 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_37, [0, 2, 3]), kwargs = {})
#   %unsqueeze_832 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_21, 0), kwargs = {})
#   %unsqueeze_833 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_832, 2), kwargs = {})
#   %unsqueeze_834 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_833, 3), kwargs = {})
#   %sum_150 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_783, [0, 2, 3]), kwargs = {})
#   %convert_element_type_28 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_12, torch.float32), kwargs = {})
#   %sub_322 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_28, %unsqueeze_834), kwargs = {})
#   %mul_1128 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_783, %sub_322), kwargs = {})
#   %sum_151 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1128, [0, 2, 3]), kwargs = {})
#   return %buf1109,%buf1111
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 290980320, 'r0_': 0}, 'kernel_num_gb': 0.28999728, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 120)
    x1 = xindex // 120
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    _tmp10 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask, tmp3, _tmp2)
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp5 - tmp6
        tmp8 = tmp0 * tmp7
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(r0_mask, tmp11, _tmp10)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, None)
    tl.store(out_ptr1 + (x3), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 122880, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.28999728
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5u/c5uukwm4chzbgr3cczme64qtg5d2ysdfo5etiv3vd5sa2fwjutm5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1109 : Tensor "f32[120, 1024][1, 120]cuda:0" = PlaceHolder[target=buf1109]
#   %sum_150 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_783, [0, 2, 3]), kwargs = {})
#   return %sum_150
triton_red_fused_native_batch_norm_backward_181 = async_compile.triton('triton_red_fused_native_batch_norm_backward_181', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 128, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_181', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 492480, 'r0_': 0}, 'kernel_num_gb': 0.000492, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_181(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 120
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 120, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_181.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_181.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000492
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hf/chfdhagfvns6wbi75wurxplmk6mitkamsyxialdyodiwmgtzvo7v.py
# Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_21 => convert_element_type_28, squeeze_21, squeeze_22
# Graph fragment:
#   %buf1111 : Tensor "f32[120, 1024][1, 120]cuda:0" = PlaceHolder[target=buf1111]
#   %sum_151 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_151]
#   %rsqrt_7 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=rsqrt_7]
#   %squeeze_21 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_37, [0, 2, 3]), kwargs = {})
#   %unsqueeze_832 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_21, 0), kwargs = {})
#   %unsqueeze_833 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_832, 2), kwargs = {})
#   %unsqueeze_834 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_833, 3), kwargs = {})
#   %convert_element_type_28 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_12, torch.float32), kwargs = {})
#   %sub_322 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_28, %unsqueeze_834), kwargs = {})
#   %mul_1128 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_783, %sub_322), kwargs = {})
#   %sum_151 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1128, [0, 2, 3]), kwargs = {})
#   %squeeze_22 : Tensor "f32[120][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_7, [0, 2, 3]), kwargs = {})
#   %mul_1136 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_151, %squeeze_22), kwargs = {})
#   return %sum_151,%mul_1136
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 128, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 493920, 'r0_': 0}, 'kernel_num_gb': 0.00049296, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 120
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 120, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00049296
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ny/cnylgx5rnetbrejurbcqavv6ffecq3sjeymptcitnhsabr7hbr2t.py
# Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_21 => convert_element_type_28, squeeze_21, squeeze_22
# Graph fragment:
#   %convert_element_type_783 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convert_element_type_783]
#   %convolution_12 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convolution_12]
#   %getitem_37 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_37]
#   %sum_151 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_151]
#   %rsqrt_7 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=rsqrt_7]
#   %sum_150 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_150]
#   %primals_53 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_53]
#   %squeeze_21 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_37, [0, 2, 3]), kwargs = {})
#   %unsqueeze_832 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_21, 0), kwargs = {})
#   %unsqueeze_833 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_832, 2), kwargs = {})
#   %unsqueeze_834 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_833, 3), kwargs = {})
#   %convert_element_type_28 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_12, torch.float32), kwargs = {})
#   %sub_322 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_28, %unsqueeze_834), kwargs = {})
#   %mul_1129 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_150, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_835 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1129, 0), kwargs = {})
#   %unsqueeze_836 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_835, 2), kwargs = {})
#   %unsqueeze_837 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_836, 3), kwargs = {})
#   %mul_1130 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_151, 2.4912308673469386e-06), kwargs = {})
#   %squeeze_22 : Tensor "f32[120][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_7, [0, 2, 3]), kwargs = {})
#   %mul_1131 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_22, %squeeze_22), kwargs = {})
#   %mul_1132 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1130, %mul_1131), kwargs = {})
#   %unsqueeze_838 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1132, 0), kwargs = {})
#   %unsqueeze_839 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_838, 2), kwargs = {})
#   %unsqueeze_840 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_839, 3), kwargs = {})
#   %mul_1133 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_22, %primals_53), kwargs = {})
#   %unsqueeze_841 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1133, 0), kwargs = {})
#   %unsqueeze_842 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_841, 2), kwargs = {})
#   %unsqueeze_843 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_842, 3), kwargs = {})
#   %mul_1134 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_322, %unsqueeze_840), kwargs = {})
#   %sub_324 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_783, %mul_1134), kwargs = {})
#   %sub_325 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_324, %unsqueeze_837), kwargs = {})
#   %mul_1135 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_325, %unsqueeze_843), kwargs = {})
#   %convert_element_type_785 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1135, torch.float16), kwargs = {})
#   %convolution_backward_142 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_785, %relu_4, %convert_element_type_27, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False]), kwargs = {})
#   return %buf1114
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 481692000}, 'kernel_num_gb': 0.38535408, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 48168960
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 120)
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp1 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp3 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 2.4912308673469386e-06
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp19, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 48168960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.38535408
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pv/cpvsqnsacdkapysnnitbmncehcik4idj25bhfueenvrccf3bgjpv.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_18 => convert_element_type_25
# Graph fragment:
#   %relu_4 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=relu_4]
#   %getitem_862 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=getitem_862]
#   %cat_3 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=cat_3]
#   %unsqueeze_846 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_846]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_2 : Tensor "b8[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_4, 0), kwargs = {})
#   %where_2 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_2, %full_default, %getitem_862), kwargs = {})
#   %convert_element_type_787 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_2, torch.float32), kwargs = {})
#   %sum_152 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_787, [0, 2, 3]), kwargs = {})
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %sub_326 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_25, %unsqueeze_846), kwargs = {})
#   %mul_1137 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_787, %sub_326), kwargs = {})
#   %sum_153 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1137, [0, 2, 3]), kwargs = {})
#   return %buf1119,%buf1121
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 290980320, 'r0_': 0}, 'kernel_num_gb': 0.28999728, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 120)
    x1 = xindex // 120
    _tmp7 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp3 = tl.load(in_ptr1 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr2 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask, tmp8, _tmp7)
        tmp10 = tmp9.to(tl.float32)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp5 * tmp12
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask, tmp16, _tmp15)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, None)
    tl.store(out_ptr1 + (x3), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 120, 1, 1), (120, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 122880, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.28999728
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4w/c4wcnrcqjbe6uz3vqovx65kcobpyahq4ixwswgcpjq7wjfik5xp7.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_18 => convert_element_type_25
# Graph fragment:
#   %buf1121 : Tensor "f32[120, 1024][1, 120]cuda:0" = PlaceHolder[target=buf1121]
#   %sum_153 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_153]
#   %squeeze_19 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=squeeze_19]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_2 : Tensor "b8[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_4, 0), kwargs = {})
#   %where_2 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_2, %full_default, %getitem_862), kwargs = {})
#   %convert_element_type_787 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_2, torch.float32), kwargs = {})
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %sub_326 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_25, %unsqueeze_846), kwargs = {})
#   %mul_1137 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_787, %sub_326), kwargs = {})
#   %sum_153 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1137, [0, 2, 3]), kwargs = {})
#   %mul_1145 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_153, %squeeze_19), kwargs = {})
#   return %sum_153,%mul_1145
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 128, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 493920, 'r0_': 0}, 'kernel_num_gb': 0.00049296, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 120
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((120, 1024), (1, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 120, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00049296
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sm/csmlgl5h7uwtd2a5vx36kj4vh3kbvw4sufx3tjj4cy2tplg7fef4.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_18 => convert_element_type_25
# Graph fragment:
#   %relu_4 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=relu_4]
#   %getitem_862 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=getitem_862]
#   %cat_3 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=cat_3]
#   %unsqueeze_846 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_846]
#   %sum_153 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_153]
#   %squeeze_19 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=squeeze_19]
#   %sum_152 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=sum_152]
#   %primals_47 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_47]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_2 : Tensor "b8[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_4, 0), kwargs = {})
#   %where_2 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_2, %full_default, %getitem_862), kwargs = {})
#   %convert_element_type_787 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_2, torch.float32), kwargs = {})
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %sub_326 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_25, %unsqueeze_846), kwargs = {})
#   %mul_1138 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_152, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_847 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1138, 0), kwargs = {})
#   %unsqueeze_848 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_847, 2), kwargs = {})
#   %unsqueeze_849 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_848, 3), kwargs = {})
#   %mul_1139 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_153, 2.4912308673469386e-06), kwargs = {})
#   %mul_1140 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_19, %squeeze_19), kwargs = {})
#   %mul_1141 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1139, %mul_1140), kwargs = {})
#   %unsqueeze_850 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1141, 0), kwargs = {})
#   %unsqueeze_851 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_850, 2), kwargs = {})
#   %unsqueeze_852 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_851, 3), kwargs = {})
#   %mul_1142 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_19, %primals_47), kwargs = {})
#   %unsqueeze_853 : Tensor "f32[1, 120][120, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1142, 0), kwargs = {})
#   %unsqueeze_854 : Tensor "f32[1, 120, 1][120, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_853, 2), kwargs = {})
#   %unsqueeze_855 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_854, 3), kwargs = {})
#   %mul_1143 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_326, %unsqueeze_852), kwargs = {})
#   %sub_328 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_787, %mul_1143), kwargs = {})
#   %sub_329 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_328, %unsqueeze_849), kwargs = {})
#   %mul_1144 : Tensor "f32[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_329, %unsqueeze_855), kwargs = {})
#   %convert_element_type_789 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1144, torch.float16), kwargs = {})
#   return %convert_element_type_789
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 481692000}, 'kernel_num_gb': 0.38535408, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, XBLOCK : tl.constexpr):
    xnumel = 48168960
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 120)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp3 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp6 = tl.load(in_ptr1 + (x2), None).to(tl.float32)
    tmp8 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp5 = tmp4.to(tl.float32)
    tmp7 = tmp6.to(tl.float32)
    tmp9 = tmp7 - tmp8
    tmp11 = 2.4912308673469386e-06
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp5 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp23.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp24, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 120, 1, 1), (120, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 48168960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.38535408
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rc/crcjj57q7y7tci67id5v6s2b6vriywx5poua674a6cr5woc72ifh.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_16 => convert_element_type_21
# Graph fragment:
#   %getitem_853 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=getitem_853]
#   %getitem_868 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=getitem_868]
#   %getitem_865 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=getitem_865]
#   %cat_2 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_2]
#   %unsqueeze_858 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_858]
#   %cat_78 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_868, %getitem_865], 1), kwargs = {})
#   %add_380 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_853, %cat_78), kwargs = {})
#   %convert_element_type_792 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_380, torch.float32), kwargs = {})
#   %sum_154 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_792, [0, 2, 3]), kwargs = {})
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %sub_330 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_21, %unsqueeze_858), kwargs = {})
#   %mul_1146 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_792, %sub_330), kwargs = {})
#   %sum_155 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1146, [0, 2, 3]), kwargs = {})
#   return %buf1133,%buf1135
triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 129106080, 'r0_': 0}, 'kernel_num_gb': 0.09666576, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 40960
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 40)
    x1 = xindex // 40
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp19 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    _tmp23 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_2 + 15680*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp17 = tl.load(in_ptr3 + (x0 + 40*r0_2 + 15680*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = x0
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 20, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = tl.load(in_ptr1 + (20*r0_2 + 7840*x1 + (x0)), r0_mask & tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp1 >= tmp4
        tmp8 = tl.full([1, 1], 40, tl.int64)
        tmp9 = tmp1 < tmp8
        tmp10 = tl.load(in_ptr2 + (20*r0_2 + 7840*x1 + ((-20) + x0)), r0_mask & tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tl.where(tmp5, tmp6, tmp10)
        tmp12 = tmp0 + tmp11
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask, tmp16, _tmp15)
        tmp18 = tmp17.to(tl.float32)
        tmp20 = tmp18 - tmp19
        tmp21 = tmp13 * tmp20
        tmp22 = tl.broadcast_to(tmp21, [XBLOCK, R0_BLOCK])
        tmp24 = _tmp23 + tmp22
        _tmp23 = tl.where(r0_mask, tmp24, _tmp23)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tmp23 = tl.sum(_tmp23, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, None)
    tl.store(out_ptr1 + (x3), tmp23, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((40, 1024), (1, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 40960, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09666576
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lb/clbzhfjfqzsmrmuotxddijb4bxsdvdghmefxvub4gunj6b3efnuw.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_16 => convert_element_type_21
# Graph fragment:
#   %getitem_853 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=getitem_853]
#   %getitem_868 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=getitem_868]
#   %getitem_865 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=getitem_865]
#   %cat_2 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_2]
#   %unsqueeze_858 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_858]
#   %sum_155 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=sum_155]
#   %squeeze_16 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=squeeze_16]
#   %sum_154 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=sum_154]
#   %cat_78 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_868, %getitem_865], 1), kwargs = {})
#   %add_380 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_853, %cat_78), kwargs = {})
#   %convert_element_type_792 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_380, torch.float32), kwargs = {})
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %sub_330 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_21, %unsqueeze_858), kwargs = {})
#   %mul_1147 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_154, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_859 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1147, 0), kwargs = {})
#   %unsqueeze_860 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_859, 2), kwargs = {})
#   %unsqueeze_861 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_860, 3), kwargs = {})
#   %mul_1148 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_155, 2.4912308673469386e-06), kwargs = {})
#   %mul_1149 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_16, %squeeze_16), kwargs = {})
#   %mul_1150 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1148, %mul_1149), kwargs = {})
#   %unsqueeze_862 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1150, 0), kwargs = {})
#   %unsqueeze_863 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_862, 2), kwargs = {})
#   %unsqueeze_864 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_863, 3), kwargs = {})
#   %mul_1152 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_330, %unsqueeze_864), kwargs = {})
#   %sub_332 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_792, %mul_1152), kwargs = {})
#   %sub_333 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_332, %unsqueeze_861), kwargs = {})
#   return %sub_333
triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 256901760}, 'kernel_num_gb': 0.16056384, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16056320
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 40)
    x1 = xindex // 40
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp1 = x0
    tmp2 = tl.full([1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1], 20, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = tl.load(in_ptr1 + (20*x1 + (x0)), tmp5, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp7 = tmp1 >= tmp4
    tmp8 = tl.full([1], 40, tl.int64)
    tmp9 = tmp1 < tmp8
    tmp10 = tl.load(in_ptr2 + (20*x1 + ((-20) + x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tl.where(tmp5, tmp6, tmp10)
    tmp12 = tmp0 + tmp11
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 - tmp16
    tmp19 = 2.4912308673469386e-06
    tmp20 = tmp18 * tmp19
    tmp22 = tmp21 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp17 * tmp23
    tmp25 = tmp13 - tmp24
    tmp27 = tmp26 * tmp19
    tmp28 = tmp25 - tmp27
    tl.store(out_ptr0 + (x2), tmp28, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 16056320,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.16056384
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xc/cxch3lihokn6orfyo7xl7uux3dlqz4c744qqdg3tz4vue22ybgvr.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_333 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=sub_333]
#   %squeeze_16 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=squeeze_16]
#   %primals_40 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_40]
#   %mul_1151 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_16, %primals_40), kwargs = {})
#   %unsqueeze_865 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1151, 0), kwargs = {})
#   %unsqueeze_866 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_865, 2), kwargs = {})
#   %unsqueeze_867 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_866, 3), kwargs = {})
#   %mul_1153 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_333, %unsqueeze_867), kwargs = {})
#   %convert_element_type_794 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1153, torch.float16), kwargs = {})
#   %slice_101 : Tensor "f16[128, 20, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_794, 1, 20, 40), kwargs = {})
#   %convolution_backward_145 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_101, %getitem_29, %convert_element_type_20, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf1139
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 64225440}, 'kernel_num_gb': 0.04816928, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8028160
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 20)
    x1 = xindex // 20
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (20 + x0 + 40*x1), None)
    tmp1 = tl.load(in_ptr1 + (20 + x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (20 + x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 8028160,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816928
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3z/c3zcug27vjrasck3koufejs64f7heokf6xqhw2vcglon3xnr2bxr.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %sub_333 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=sub_333]
#   %squeeze_16 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=squeeze_16]
#   %primals_40 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_40]
#   %mul_1151 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_16, %primals_40), kwargs = {})
#   %unsqueeze_865 : Tensor "f32[1, 40][40, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1151, 0), kwargs = {})
#   %unsqueeze_866 : Tensor "f32[1, 40, 1][40, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_865, 2), kwargs = {})
#   %unsqueeze_867 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_866, 3), kwargs = {})
#   %mul_1153 : Tensor "f32[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_333, %unsqueeze_867), kwargs = {})
#   %convert_element_type_794 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1153, torch.float16), kwargs = {})
#   %slice_100 : Tensor "f16[128, 20, 56, 56][125440, 1, 2240, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%convert_element_type_794, 1, 0, 20), kwargs = {})
#   %convolution_backward_146 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%slice_100, %getitem_26, %convert_element_type_19, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %buf1144
triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190 = async_compile.triton('triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 64225440}, 'kernel_num_gb': 0.04816928, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8028160
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 20)
    x1 = xindex // 20
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 40*x1), None)
    tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp5, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 8028160,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816928
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ys/cysfo2azofy43enxmf7id4h3tpmxgsfufszuqp75rbzvh7rcfdkw.py
# Topologically Sorted Source Nodes: [x_13, x_14], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_13 => add_25, convert_element_type_18, mul_28, mul_34, sub_4, unsqueeze_16, unsqueeze_17, unsqueeze_18, unsqueeze_19
#   x_14 => relu_3
# Graph fragment:
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=cat_1]
#   %getitem_23 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_23]
#   %rsqrt_4 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_4]
#   %primals_33 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_33]
#   %primals_34 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_34]
#   %getitem_874 : Tensor "f16[128, 96, 56, 56][301056, 1, 5376, 96]cuda:0" = PlaceHolder[target=getitem_874]
#   %getitem_871 : Tensor "f16[128, 96, 56, 56][301056, 1, 5376, 96]cuda:0" = PlaceHolder[target=getitem_871]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_79 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_874, %getitem_871], 1), kwargs = {})
#   %sub_4 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_1, %getitem_23), kwargs = {})
#   %mul_28 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_4, %rsqrt_4), kwargs = {})
#   %unsqueeze_16 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_33, -1), kwargs = {})
#   %unsqueeze_17 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_16, -1), kwargs = {})
#   %mul_34 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_28, %unsqueeze_17), kwargs = {})
#   %unsqueeze_18 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_34, -1), kwargs = {})
#   %unsqueeze_19 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_18, -1), kwargs = {})
#   %add_25 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_34, %unsqueeze_19), kwargs = {})
#   %convert_element_type_18 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_25, torch.float16), kwargs = {})
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_18,), kwargs = {})
#   %le_3 : Tensor "b8[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_3, 0), kwargs = {})
#   %where_3 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_3, %full_default, %cat_79), kwargs = {})
#   %convert_element_type_797 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_3, torch.float32), kwargs = {})
#   return %convert_element_type_797
triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 134217728}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1078987776}, 'kernel_num_gb': 0.61656576, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 77070336
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tmp13 = 0.0
    tmp14 = tmp12 <= tmp13
    tmp15 = x0
    tmp16 = tl.full([1], 0, tl.int64)
    tmp17 = tmp15 >= tmp16
    tmp18 = tl.full([1], 96, tl.int64)
    tmp19 = tmp15 < tmp18
    tmp20 = tl.load(in_ptr5 + (96*x1 + (x0)), tmp19, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp21 = tmp15 >= tmp18
    tmp22 = tl.full([1], 192, tl.int64)
    tmp23 = tmp15 < tmp22
    tmp24 = tl.load(in_ptr6 + (96*x1 + ((-96) + x0)), tmp21, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp25 = tl.where(tmp19, tmp20, tmp24)
    tmp26 = tl.where(tmp14, tmp13, tmp25)
    tmp27 = tmp26.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp27, None)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 96, 56, 56), (301056, 1, 5376, 96), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 96, 56, 56), (301056, 1, 5376, 96), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 77070336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.61656576
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gn/cgnz52r2em7yq5zeorjjhxbwzyi563ubfgdujnohxttrezrcqxem.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_13 => convert_element_type_17, squeeze_12
# Graph fragment:
#   %convert_element_type_797 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=convert_element_type_797]
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=cat_1]
#   %getitem_23 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_23]
#   %squeeze_12 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_23, [0, 2, 3]), kwargs = {})
#   %unsqueeze_868 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_12, 0), kwargs = {})
#   %unsqueeze_869 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_868, 2), kwargs = {})
#   %unsqueeze_870 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_869, 3), kwargs = {})
#   %sum_156 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_797, [0, 2, 3]), kwargs = {})
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %sub_334 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_17, %unsqueeze_870), kwargs = {})
#   %mul_1155 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_797, %sub_334), kwargs = {})
#   %sum_157 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1155, [0, 2, 3]), kwargs = {})
#   return %buf1150,%buf1152
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 463995648, 'r0_': 0}, 'kernel_num_gb': 0.463209216, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 98304
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    _tmp10 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 150528*x1), r0_mask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + 192*r0_2 + 150528*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask, tmp3, _tmp2)
        tmp5 = tmp4.to(tl.float32)
        tmp7 = tmp5 - tmp6
        tmp8 = tmp0 * tmp7
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(r0_mask, tmp11, _tmp10)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, None)
    tl.store(out_ptr1 + (x3), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192, 512), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192, 512), (1, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 98304, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.463209216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kt/cktaxrsgvapexexwdowgiaqnaido3icchzc3f3hlpbusz6xvu5fh.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1150 : Tensor "f32[192, 512][1, 192]cuda:0" = PlaceHolder[target=buf1150]
#   %sum_156 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_797, [0, 2, 3]), kwargs = {})
#   return %sum_156
triton_red_fused_native_batch_norm_backward_193 = async_compile.triton('triton_red_fused_native_batch_norm_backward_193', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_193', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 394752, 'r0_': 0}, 'kernel_num_gb': 0.000393984, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_193(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((192, 512), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 192, 512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_193.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_193.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000393984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/65/c65qikdeyjyqpqrkzp2f7sha7gqua3l7jjrh77wqsg6wrbjyqsn5.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_13 => convert_element_type_17, squeeze_12, squeeze_13
# Graph fragment:
#   %buf1152 : Tensor "f32[192, 512][1, 192]cuda:0" = PlaceHolder[target=buf1152]
#   %sum_157 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_157]
#   %rsqrt_4 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_4]
#   %squeeze_12 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_23, [0, 2, 3]), kwargs = {})
#   %unsqueeze_868 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_12, 0), kwargs = {})
#   %unsqueeze_869 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_868, 2), kwargs = {})
#   %unsqueeze_870 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_869, 3), kwargs = {})
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %sub_334 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_17, %unsqueeze_870), kwargs = {})
#   %mul_1155 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_797, %sub_334), kwargs = {})
#   %sum_157 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1155, [0, 2, 3]), kwargs = {})
#   %squeeze_13 : Tensor "f32[192][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_4, [0, 2, 3]), kwargs = {})
#   %mul_1163 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_157, %squeeze_13), kwargs = {})
#   return %sum_157,%mul_1163
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 397056, 'r0_': 0}, 'kernel_num_gb': 0.00039552, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 512
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((192, 512), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 192, 512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00039552
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gp/cgpp5v3xedcdvohsunimp3ui3fp3z6cmtsxxmcxqaksjy2ttzhes.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
#   x_13 => convert_element_type_17, squeeze_12, squeeze_13
# Graph fragment:
#   %convert_element_type_797 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=convert_element_type_797]
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=cat_1]
#   %getitem_23 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_23]
#   %sum_157 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_157]
#   %rsqrt_4 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_4]
#   %sum_156 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_156]
#   %primals_33 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_33]
#   %squeeze_12 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_23, [0, 2, 3]), kwargs = {})
#   %unsqueeze_868 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_12, 0), kwargs = {})
#   %unsqueeze_869 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_868, 2), kwargs = {})
#   %unsqueeze_870 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_869, 3), kwargs = {})
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %sub_334 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_17, %unsqueeze_870), kwargs = {})
#   %mul_1156 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_156, 2.4912308673469386e-06), kwargs = {})
#   %unsqueeze_871 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1156, 0), kwargs = {})
#   %unsqueeze_872 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_871, 2), kwargs = {})
#   %unsqueeze_873 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_872, 3), kwargs = {})
#   %mul_1157 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_157, 2.4912308673469386e-06), kwargs = {})
#   %squeeze_13 : Tensor "f32[192][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_4, [0, 2, 3]), kwargs = {})
#   %mul_1158 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_13, %squeeze_13), kwargs = {})
#   %mul_1159 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1157, %mul_1158), kwargs = {})
#   %unsqueeze_874 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1159, 0), kwargs = {})
#   %unsqueeze_875 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_874, 2), kwargs = {})
#   %unsqueeze_876 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_875, 3), kwargs = {})
#   %mul_1160 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_13, %primals_33), kwargs = {})
#   %unsqueeze_877 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1160, 0), kwargs = {})
#   %unsqueeze_878 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_877, 2), kwargs = {})
#   %unsqueeze_879 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_878, 3), kwargs = {})
#   %mul_1161 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_334, %unsqueeze_876), kwargs = {})
#   %sub_336 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_797, %mul_1161), kwargs = {})
#   %sub_337 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_336, %unsqueeze_873), kwargs = {})
#   %mul_1162 : Tensor "f32[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_337, %unsqueeze_879), kwargs = {})
#   %convert_element_type_799 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1162, torch.float16), kwargs = {})
#   return %convert_element_type_799
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 134217728}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 770707200}, 'kernel_num_gb': 0.616566528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 77070336
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 192)
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp1 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp3 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp4 = tmp2 - tmp3
    tmp6 = 2.4912308673469386e-06
    tmp7 = tmp5 * tmp6
    tmp9 = tmp8 * tmp8
    tmp10 = tmp7 * tmp9
    tmp11 = tmp4 * tmp10
    tmp12 = tmp0 - tmp11
    tmp14 = tmp13 * tmp6
    tmp15 = tmp12 - tmp14
    tmp17 = tmp8 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp18.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp19, None)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 77070336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.616566528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/y2/cy2qlehh2cpwxsfbkzjobrixdq2ingp2r6kt32ohgc6qqdjqghdw.py
# Topologically Sorted Source Nodes: [x_10, x_11], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_10 => add_20, convert_element_type_13, mul_21, mul_27, sub_3, unsqueeze_12, unsqueeze_13, unsqueeze_14, unsqueeze_15
#   x_11 => relu_2
# Graph fragment:
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=cat]
#   %getitem_9 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_9]
#   %rsqrt_3 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_3]
#   %primals_25 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_25]
#   %primals_26 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_26]
#   %getitem_883 : Tensor "f16[128, 64, 112, 112][802816, 1, 7168, 64]cuda:0" = PlaceHolder[target=getitem_883]
#   %getitem_880 : Tensor "f16[128, 64, 112, 112][802816, 1, 7168, 64]cuda:0" = PlaceHolder[target=getitem_880]
#   %getitem_877 : Tensor "f16[128, 64, 112, 112][802816, 1, 7168, 64]cuda:0" = PlaceHolder[target=getitem_877]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_80 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_883, %getitem_880, %getitem_877], 1), kwargs = {})
#   %sub_3 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat, %getitem_9), kwargs = {})
#   %mul_21 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_3, %rsqrt_3), kwargs = {})
#   %unsqueeze_12 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_25, -1), kwargs = {})
#   %unsqueeze_13 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, -1), kwargs = {})
#   %mul_27 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_21, %unsqueeze_13), kwargs = {})
#   %unsqueeze_14 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_26, -1), kwargs = {})
#   %unsqueeze_15 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, -1), kwargs = {})
#   %add_20 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_27, %unsqueeze_15), kwargs = {})
#   %convert_element_type_13 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_20, torch.float16), kwargs = {})
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_13,), kwargs = {})
#   %le_4 : Tensor "b8[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_2, 0), kwargs = {})
#   %where_4 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_4, %full_default, %cat_80), kwargs = {})
#   return %where_4
triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 536870912}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3699379200}, 'kernel_num_gb': 1.849691136, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 308281344
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tmp13 = 0.0
    tmp14 = tmp12 <= tmp13
    tmp15 = x0
    tmp16 = tl.full([1], 0, tl.int64)
    tmp17 = tmp15 >= tmp16
    tmp18 = tl.full([1], 64, tl.int64)
    tmp19 = tmp15 < tmp18
    tmp20 = tl.load(in_ptr5 + (64*x1 + (x0)), tmp19, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp21 = tmp15 >= tmp18
    tmp22 = tl.full([1], 128, tl.int64)
    tmp23 = tmp15 < tmp22
    tmp24 = tmp21 & tmp23
    tmp25 = tl.load(in_ptr6 + (64*x1 + ((-64) + x0)), tmp24, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp26 = tmp15 >= tmp22
    tmp27 = tl.full([1], 192, tl.int64)
    tmp28 = tmp15 < tmp27
    tmp29 = tl.load(in_ptr7 + (64*x1 + ((-128) + x0)), tmp26, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.where(tmp24, tmp25, tmp29)
    tmp31 = tl.where(tmp19, tmp20, tmp30)
    tmp32 = tl.where(tmp14, tmp13, tmp31)
    tl.store(out_ptr0 + (x2), tmp32, None)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    arg_7 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    arg_8 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 308281344,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.849691136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cy/ccyozr4fgfgkh24fh6mlob2272dwo4z4tggphaqzkblhfcr4y7ue.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_10 => convert_element_type_12, squeeze_9
# Graph fragment:
#   %where_4 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=where_4]
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=cat]
#   %getitem_9 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_9]
#   %convert_element_type_803 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_4, torch.float32), kwargs = {})
#   %squeeze_9 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_9, [0, 2, 3]), kwargs = {})
#   %unsqueeze_880 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_9, 0), kwargs = {})
#   %unsqueeze_881 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_880, 2), kwargs = {})
#   %unsqueeze_882 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_881, 3), kwargs = {})
#   %sum_158 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_803, [0, 2, 3]), kwargs = {})
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %sub_338 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_12, %unsqueeze_882), kwargs = {})
#   %mul_1164 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_803, %sub_338), kwargs = {})
#   %sum_159 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1164, [0, 2, 3]), kwargs = {})
#   return %buf1169,%buf1171
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1235534592, 'r0_': 0}, 'kernel_num_gb': 1.234330368, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 150528
    r0_numel = 2048
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    _tmp3 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp7 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 393216*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp5 = tl.load(in_ptr1 + (x0 + 192*r0_2 + 393216*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp4 = _tmp3 + tmp2
        _tmp3 = tl.where(r0_mask & xmask, tmp4, _tmp3)
        tmp6 = tmp5.to(tl.float32)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp1 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, R0_BLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(r0_mask & xmask, tmp12, _tmp11)
    tmp3 = tl.sum(_tmp3, 1)[:, None]
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192, 784), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192, 784), (1, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 150528, 2048,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.234330368
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uy/cuyw5v5vua4zszt2hmwswe5rchcyb76yzn53cq7auyyzcpbhpnid.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1169 : Tensor "f32[192, 784][1, 192]cuda:0" = PlaceHolder[target=buf1169]
#   %convert_element_type_803 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_4, torch.float32), kwargs = {})
#   %sum_158 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_803, [0, 2, 3]), kwargs = {})
#   return %sum_158
triton_red_fused_native_batch_norm_backward_198 = async_compile.triton('triton_red_fused_native_batch_norm_backward_198', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_198', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 603648, 'r0_': 0}, 'kernel_num_gb': 0.00060288, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_198(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((192, 784), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 192, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_198.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_198.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00060288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/f4/cf4alk4ez4hrlgrgmpmxphidaky6sg36fpjoqdui5xngilhs4z4j.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_10 => convert_element_type_12, squeeze_10, squeeze_9
# Graph fragment:
#   %buf1171 : Tensor "f32[192, 784][1, 192]cuda:0" = PlaceHolder[target=buf1171]
#   %sum_159 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_159]
#   %rsqrt_3 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_3]
#   %convert_element_type_803 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_4, torch.float32), kwargs = {})
#   %squeeze_9 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_9, [0, 2, 3]), kwargs = {})
#   %unsqueeze_880 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_9, 0), kwargs = {})
#   %unsqueeze_881 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_880, 2), kwargs = {})
#   %unsqueeze_882 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_881, 3), kwargs = {})
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %sub_338 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_12, %unsqueeze_882), kwargs = {})
#   %mul_1164 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_803, %sub_338), kwargs = {})
#   %sum_159 : Tensor "f32[192][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1164, [0, 2, 3]), kwargs = {})
#   %squeeze_10 : Tensor "f32[192][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_3, [0, 2, 3]), kwargs = {})
#   %mul_1172 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_159, %squeeze_10), kwargs = {})
#   return %sum_159,%mul_1172
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 605952, 'r0_': 0}, 'kernel_num_gb': 0.000604416, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((192, 784), (1, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 192, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000604416
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/s6/cs6xx3etiypelx2advztw6gzp6bhhhf6uvmhcxnckvkgypd2asxp.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_10 => convert_element_type_12, squeeze_10, squeeze_9
# Graph fragment:
#   %where_4 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=where_4]
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=cat]
#   %getitem_9 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_9]
#   %sum_159 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_159]
#   %rsqrt_3 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_3]
#   %sum_158 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=sum_158]
#   %primals_25 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_25]
#   %convert_element_type_803 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_4, torch.float32), kwargs = {})
#   %squeeze_9 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_9, [0, 2, 3]), kwargs = {})
#   %unsqueeze_880 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%squeeze_9, 0), kwargs = {})
#   %unsqueeze_881 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_880, 2), kwargs = {})
#   %unsqueeze_882 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_881, 3), kwargs = {})
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %sub_338 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_12, %unsqueeze_882), kwargs = {})
#   %mul_1165 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_158, 6.228077168367346e-07), kwargs = {})
#   %unsqueeze_883 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1165, 0), kwargs = {})
#   %unsqueeze_884 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_883, 2), kwargs = {})
#   %unsqueeze_885 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_884, 3), kwargs = {})
#   %mul_1166 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_159, 6.228077168367346e-07), kwargs = {})
#   %squeeze_10 : Tensor "f32[192][1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.squeeze.dims](args = (%rsqrt_3, [0, 2, 3]), kwargs = {})
#   %mul_1167 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_10, %squeeze_10), kwargs = {})
#   %mul_1168 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1166, %mul_1167), kwargs = {})
#   %unsqueeze_886 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1168, 0), kwargs = {})
#   %unsqueeze_887 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_886, 2), kwargs = {})
#   %unsqueeze_888 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_887, 3), kwargs = {})
#   %mul_1169 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_10, %primals_25), kwargs = {})
#   %unsqueeze_889 : Tensor "f32[1, 192][192, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1169, 0), kwargs = {})
#   %unsqueeze_890 : Tensor "f32[1, 192, 1][192, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_889, 2), kwargs = {})
#   %unsqueeze_891 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_890, 3), kwargs = {})
#   %mul_1170 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_338, %unsqueeze_888), kwargs = {})
#   %sub_340 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_803, %mul_1170), kwargs = {})
#   %sub_341 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_340, %unsqueeze_885), kwargs = {})
#   %mul_1171 : Tensor "f32[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_341, %unsqueeze_891), kwargs = {})
#   %convert_element_type_805 : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1171, torch.float16), kwargs = {})
#   return %convert_element_type_805
triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 536870912}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2466254592}, 'kernel_num_gb': 1.849691904, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 308281344
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 192)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp4 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 6.228077168367346e-07
    tmp8 = tmp6 * tmp7
    tmp10 = tmp9 * tmp9
    tmp11 = tmp8 * tmp10
    tmp12 = tmp5 * tmp11
    tmp13 = tmp1 - tmp12
    tmp15 = tmp14 * tmp7
    tmp16 = tmp13 - tmp15
    tmp18 = tmp9 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp19.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 308281344,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.849691904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ls/clsdqvyi6mtwwk3jyzpgwimbnb4u3zdxx4swyyqgxk2uix7qcfu5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_889 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_889]
#   %getitem_886 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_886]
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %convert_element_type_808 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_81, torch.float32), kwargs = {})
#   %sum_160 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_808, [0, 2, 3]), kwargs = {})
#   return %buf1183
triton_red_fused_cat_native_batch_norm_backward_201 = async_compile.triton('triton_red_fused_cat_native_batch_norm_backward_201', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'y': 32, 'x': 1024, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp32', 'ynumel': 'i32', 'xnumel': 'i32', 'r0_numel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_native_batch_norm_backward_201', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 205520896, 'x': 262144, 'r0_': 0}, 'kernel_num_gb': 0.10289152, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_cat_native_batch_norm_backward_201(in_ptr0, in_ptr1, out_ptr0, ynumel, xnumel, r0_numel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    ynumel = 32
    xnumel = 1024
    r0_numel = 1568
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, None, :]
    rbase = r0_base
    y0 = yindex
    x1 = xindex
    _tmp13 = tl.full([YBLOCK, XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = y0
        tmp1 = tl.full([1, 1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1, 1], 16, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (16*r0_2 + 25088*x1 + (y0)), r0_mask & tmp4 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1, 1], 32, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (16*r0_2 + 25088*x1 + ((-16) + y0)), r0_mask & tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tl.broadcast_to(tmp11, [YBLOCK, XBLOCK, R0_BLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(r0_mask & xmask & ymask, tmp14, _tmp13)
    tmp13 = tl.sum(_tmp13, 2)[:, :, None]
    tl.store(out_ptr0 + (x1 + 1024*y0), tmp13, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((32, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, 32, 1024, 1568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_cat_native_batch_norm_backward_201.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_cat_native_batch_norm_backward_201.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.10289152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/se/csendjreaoiwexsxiako5insttaaclzg3k47cwrbwdc5brxzhmqx.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1183 : Tensor "f32[32, 1024][1024, 1]cuda:0" = PlaceHolder[target=buf1183]
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %convert_element_type_808 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_81, torch.float32), kwargs = {})
#   %sum_160 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_808, [0, 2, 3]), kwargs = {})
#   return %sum_160
triton_per_fused_cat_native_batch_norm_backward_202 = async_compile.triton('triton_per_fused_cat_native_batch_norm_backward_202', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 32, 'r0_': 1024},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_cat_native_batch_norm_backward_202', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 256, 'r0_': 131072}, 'kernel_num_gb': 0.0001312, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_cat_native_batch_norm_backward_202(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 32
    r0_numel = 1024
    R0_BLOCK: tl.constexpr = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r0_1 + 1024*x0), xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((32, 1024), (1024, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 32, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_cat_native_batch_norm_backward_202.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_cat_native_batch_norm_backward_202.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001312
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/br/cbrt4sm6ap22lzbxi7njgif75chywy6i7vxqzcbb5ytxgwiythdl.py
# Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_7 => convert_element_type_8
# Graph fragment:
#   %getitem_889 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_889]
#   %getitem_886 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_886]
#   %convolution_2 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution_2]
#   %unsqueeze_894 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_894]
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %convert_element_type_808 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_81, torch.float32), kwargs = {})
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %sub_342 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_8, %unsqueeze_894), kwargs = {})
#   %mul_1173 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_808, %sub_342), kwargs = {})
#   %sum_161 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1173, [0, 2, 3]), kwargs = {})
#   return %buf1185
triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 308543616, 'r0_': 0}, 'kernel_num_gb': 0.205652096, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32768
    r0_numel = 1568
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp14 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    _tmp18 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp12 = tl.load(in_ptr2 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp0 = x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 16, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (16*r0_2 + 25088*x1 + (x0)), r0_mask & tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 32, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tl.load(in_ptr1 + (16*r0_2 + 25088*x1 + ((-16) + x0)), r0_mask & tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tl.where(tmp4, tmp5, tmp9)
        tmp11 = tmp10.to(tl.float32)
        tmp13 = tmp12.to(tl.float32)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp11 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, R0_BLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(r0_mask, tmp19, _tmp18)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp18, None)


def get_args():
    arg_0 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 32768, 1568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.205652096
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wm/cwmbw6qo46mk6663bqqo7wwgmcqmxxi5fr27wcgeitucbpskx3n7.py
# Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_7 => convert_element_type_8
# Graph fragment:
#   %buf1185 : Tensor "f32[32, 1024][1, 32]cuda:0" = PlaceHolder[target=buf1185]
#   %sum_161 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_161]
#   %squeeze_7 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=squeeze_7]
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %convert_element_type_808 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_81, torch.float32), kwargs = {})
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %sub_342 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_8, %unsqueeze_894), kwargs = {})
#   %mul_1173 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_808, %sub_342), kwargs = {})
#   %sum_161 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1173, [0, 2, 3]), kwargs = {})
#   %mul_1181 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_161, %squeeze_7), kwargs = {})
#   return %sum_161,%mul_1181
triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 131712, 'r0_': 0}, 'kernel_num_gb': 0.000131456, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 32, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000131456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vp/cvpvokfr4ykbck5x53jtvtjejnarc3mu2v5lzfkbcuupo74b5r67.py
# Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_7 => convert_element_type_8
# Graph fragment:
#   %getitem_889 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_889]
#   %getitem_886 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_886]
#   %convolution_2 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution_2]
#   %unsqueeze_894 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_894]
#   %sum_161 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_161]
#   %squeeze_7 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=squeeze_7]
#   %sum_160 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_160]
#   %primals_18 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_18]
#   %mul_1180 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=mul_1180]
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %convert_element_type_808 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_81, torch.float32), kwargs = {})
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %sub_342 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_8, %unsqueeze_894), kwargs = {})
#   %mul_1174 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_160, 6.228077168367346e-07), kwargs = {})
#   %unsqueeze_895 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1174, 0), kwargs = {})
#   %unsqueeze_896 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_895, 2), kwargs = {})
#   %unsqueeze_897 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_896, 3), kwargs = {})
#   %mul_1175 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_161, 6.228077168367346e-07), kwargs = {})
#   %mul_1176 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_7, %squeeze_7), kwargs = {})
#   %mul_1177 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1175, %mul_1176), kwargs = {})
#   %unsqueeze_898 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1177, 0), kwargs = {})
#   %unsqueeze_899 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_898, 2), kwargs = {})
#   %unsqueeze_900 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_899, 3), kwargs = {})
#   %mul_1178 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_7, %primals_18), kwargs = {})
#   %unsqueeze_901 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1178, 0), kwargs = {})
#   %unsqueeze_902 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_901, 2), kwargs = {})
#   %unsqueeze_903 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_902, 3), kwargs = {})
#   %mul_1179 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_342, %unsqueeze_900), kwargs = {})
#   %sub_344 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_808, %mul_1179), kwargs = {})
#   %sub_345 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_344, %unsqueeze_897), kwargs = {})
#   %mul_1180 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_345, %unsqueeze_903), kwargs = {})
#   %convert_element_type_810 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1180, torch.float16), kwargs = {})
#   %convolution_backward_152 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_810, %relu_1, %convert_element_type_7, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), kwargs = {})
#   return %mul_1180,%buf1189
triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 513802880}, 'kernel_num_gb': 0.308281984, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 51380224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 32)
    x1 = xindex // 32
    x2 = xindex
    tmp12 = tl.load(in_ptr2 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 16, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (16*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 32, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (16*x1 + ((-16) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tmp11 = tmp10.to(tl.float32)
    tmp13 = tmp12.to(tl.float32)
    tmp15 = tmp13 - tmp14
    tmp17 = 6.228077168367346e-07
    tmp18 = tmp16 * tmp17
    tmp20 = tmp19 * tmp19
    tmp21 = tmp18 * tmp20
    tmp22 = tmp15 * tmp21
    tmp23 = tmp11 - tmp22
    tmp25 = tmp24 * tmp17
    tmp26 = tmp23 - tmp25
    tmp28 = tmp19 * tmp27
    tmp29 = tmp26 * tmp28
    tmp30 = tmp29.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp30, None)


def get_args():
    arg_0 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 51380224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.308281984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/i7/ci7svgtyolo2rx6fikwbfsdtmt7pqzoor3fw7gkuz53hg7n54zyy.py
# Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_4 => convert_element_type_5
# Graph fragment:
#   %relu_1 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=relu_1]
#   %getitem_892 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=getitem_892]
#   %convolution_1 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution_1]
#   %unsqueeze_906 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_906]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_5 : Tensor "b8[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})
#   %where_5 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_5, %full_default, %getitem_892), kwargs = {})
#   %convert_element_type_812 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_5, torch.float32), kwargs = {})
#   %sum_162 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_812, [0, 2, 3]), kwargs = {})
#   %convert_element_type_5 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_1, torch.float32), kwargs = {})
#   %sub_346 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_5, %unsqueeze_906), kwargs = {})
#   %mul_1182 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_812, %sub_346), kwargs = {})
#   %sum_163 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1182, [0, 2, 3]), kwargs = {})
#   return %buf1194,%buf1196
triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 308805760, 'r0_': 0}, 'kernel_num_gb': 0.308543616, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32768
    r0_numel = 1568
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 32)
    x1 = xindex // 32
    _tmp7 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    _tmp15 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp3 = tl.load(in_ptr1 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr2 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp5 = tmp4.to(tl.float32)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, R0_BLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(r0_mask, tmp8, _tmp7)
        tmp10 = tmp9.to(tl.float32)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp5 * tmp12
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(r0_mask, tmp16, _tmp15)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, None)
    tl.store(out_ptr1 + (x3), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 32768, 1568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.308543616
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sg/csgthnzjfikvf2xgidtaikk5ktuckzwyyktegzavuamqtaesn7xr.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward]
# Source node to ATen node mapping:
# Graph fragment:
#   %buf1194 : Tensor "f32[32, 1024][1, 32]cuda:0" = PlaceHolder[target=buf1194]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_5 : Tensor "b8[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})
#   %where_5 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_5, %full_default, %getitem_892), kwargs = {})
#   %convert_element_type_812 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_5, torch.float32), kwargs = {})
#   %sum_162 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_812, [0, 2, 3]), kwargs = {})
#   return %sum_162
triton_red_fused_native_batch_norm_backward_threshold_backward_207 = async_compile.triton('triton_red_fused_native_batch_norm_backward_threshold_backward_207', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_207', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 131328, 'r0_': 0}, 'kernel_num_gb': 0.0001312, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_native_batch_norm_backward_threshold_backward_207(in_ptr0, out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32
    r0_numel = 1024
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)


def get_args():
    arg_0 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 32, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_native_batch_norm_backward_threshold_backward_207.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_native_batch_norm_backward_threshold_backward_207.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001312
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w2/cw23kfb5zywyxwyjb2j55ivhhdubtzgziheoybgecsksdixn2oxz.py
# Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_4 => convert_element_type_5
# Graph fragment:
#   %relu_1 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=relu_1]
#   %getitem_892 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=getitem_892]
#   %convolution_1 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution_1]
#   %unsqueeze_906 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_906]
#   %sum_163 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_163]
#   %squeeze_4 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=squeeze_4]
#   %sum_162 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_162]
#   %primals_12 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_12]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %le_5 : Tensor "b8[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu_1, 0), kwargs = {})
#   %where_5 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_5, %full_default, %getitem_892), kwargs = {})
#   %convert_element_type_812 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_5, torch.float32), kwargs = {})
#   %convert_element_type_5 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_1, torch.float32), kwargs = {})
#   %sub_346 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_5, %unsqueeze_906), kwargs = {})
#   %mul_1183 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_162, 6.228077168367346e-07), kwargs = {})
#   %unsqueeze_907 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1183, 0), kwargs = {})
#   %unsqueeze_908 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_907, 2), kwargs = {})
#   %unsqueeze_909 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_908, 3), kwargs = {})
#   %mul_1184 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_163, 6.228077168367346e-07), kwargs = {})
#   %mul_1185 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_4, %squeeze_4), kwargs = {})
#   %mul_1186 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1184, %mul_1185), kwargs = {})
#   %unsqueeze_910 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1186, 0), kwargs = {})
#   %unsqueeze_911 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_910, 2), kwargs = {})
#   %unsqueeze_912 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_911, 3), kwargs = {})
#   %mul_1187 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_4, %primals_12), kwargs = {})
#   %unsqueeze_913 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1187, 0), kwargs = {})
#   %unsqueeze_914 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_913, 2), kwargs = {})
#   %unsqueeze_915 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_914, 3), kwargs = {})
#   %mul_1188 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_346, %unsqueeze_912), kwargs = {})
#   %sub_348 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_812, %mul_1188), kwargs = {})
#   %sub_349 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_348, %unsqueeze_909), kwargs = {})
#   %mul_1189 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_349, %unsqueeze_915), kwargs = {})
#   %convert_element_type_814 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1189, torch.float16), kwargs = {})
#   %convolution_backward_153 : [num_users=2] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_814, %relu, %convert_element_type_4, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 32, [True, True, False]), kwargs = {})
#   return %buf1199
triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 8, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 513802880}, 'kernel_num_gb': 0.411042432, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, XBLOCK : tl.constexpr):
    xnumel = 51380224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 32)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp3 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp6 = tl.load(in_ptr1 + (x2), None).to(tl.float32)
    tmp8 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp5 = tmp4.to(tl.float32)
    tmp7 = tmp6.to(tl.float32)
    tmp9 = tmp7 - tmp8
    tmp11 = 6.228077168367346e-07
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp5 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp24 = tmp23.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp24, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 51380224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.411042432
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/po/cpoehbldiv5amk3rvu4ez6x7yahjq7efvylom4y6i4xzv33p52i4.py
# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_1 => convert_element_type_2
# Graph fragment:
#   %relu : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=relu]
#   %getitem_889 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_889]
#   %getitem_886 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_886]
#   %getitem_895 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=getitem_895]
#   %convolution : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution]
#   %unsqueeze_918 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_918]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %add_381 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%cat_81, %getitem_895), kwargs = {})
#   %le_6 : Tensor "b8[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})
#   %where_6 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_6, %full_default, %add_381), kwargs = {})
#   %convert_element_type_816 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_6, torch.float32), kwargs = {})
#   %sum_164 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_816, [0, 2, 3]), kwargs = {})
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %sub_350 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_2, %unsqueeze_918), kwargs = {})
#   %mul_1191 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_816, %sub_350), kwargs = {})
#   %sum_165 : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1191, [0, 2, 3]), kwargs = {})
#   return %buf1204,%buf1206
triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 514326656, 'r0_': 0}, 'kernel_num_gb': 0.411304064, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32768
    r0_numel = 1568
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 32)
    x1 = xindex // 32
    _tmp19 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    tmp23 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    _tmp27 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp14 = tl.load(in_ptr3 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp21 = tl.load(in_ptr4 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp3 = x0
        tmp4 = tl.full([1, 1], 0, tl.int64)
        tmp5 = tmp3 >= tmp4
        tmp6 = tl.full([1, 1], 16, tl.int64)
        tmp7 = tmp3 < tmp6
        tmp8 = tl.load(in_ptr1 + (16*r0_2 + 25088*x1 + (x0)), r0_mask & tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp9 = tmp3 >= tmp6
        tmp10 = tl.full([1, 1], 32, tl.int64)
        tmp11 = tmp3 < tmp10
        tmp12 = tl.load(in_ptr2 + (16*r0_2 + 25088*x1 + ((-16) + x0)), r0_mask & tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp13 = tl.where(tmp7, tmp8, tmp12)
        tmp15 = tmp13 + tmp14
        tmp16 = tl.where(tmp2, tmp1, tmp15)
        tmp17 = tmp16.to(tl.float32)
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, R0_BLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(r0_mask, tmp20, _tmp19)
        tmp22 = tmp21.to(tl.float32)
        tmp24 = tmp22 - tmp23
        tmp25 = tmp17 * tmp24
        tmp26 = tl.broadcast_to(tmp25, [XBLOCK, R0_BLOCK])
        tmp28 = _tmp27 + tmp26
        _tmp27 = tl.where(r0_mask, tmp28, _tmp27)
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tmp27 = tl.sum(_tmp27, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp19, None)
    tl.store(out_ptr1 + (x3), tmp27, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((32, 1024), (1, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, 32768, 1568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.411304064
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zz/czzslmeokxib5ohnefqvwsnyhzsz2youcg7mzkxuuzgnp52kl6aw.py
# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
# Source node to ATen node mapping:
#   x_1 => convert_element_type_2
# Graph fragment:
#   %relu : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=relu]
#   %getitem_889 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_889]
#   %getitem_886 : Tensor "f16[128, 16, 112, 112][200704, 1, 1792, 16]cuda:0" = PlaceHolder[target=getitem_886]
#   %getitem_895 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=getitem_895]
#   %convolution : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution]
#   %unsqueeze_918 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=unsqueeze_918]
#   %sum_165 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_165]
#   %squeeze_1 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=squeeze_1]
#   %sub_352 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=sub_352]
#   %sum_164 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=sum_164]
#   %primals_6 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_6]
#   %full_default : Tensor "f16[][]cuda:0"[num_users=7] = call_function[target=torch.ops.aten.full.default](args = ([], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %cat_81 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_889, %getitem_886], 1), kwargs = {})
#   %add_381 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%cat_81, %getitem_895), kwargs = {})
#   %le_6 : Tensor "b8[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})
#   %where_6 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le_6, %full_default, %add_381), kwargs = {})
#   %convert_element_type_816 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_6, torch.float32), kwargs = {})
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %sub_350 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_2, %unsqueeze_918), kwargs = {})
#   %mul_1192 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_164, 6.228077168367346e-07), kwargs = {})
#   %unsqueeze_919 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1192, 0), kwargs = {})
#   %unsqueeze_920 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_919, 2), kwargs = {})
#   %unsqueeze_921 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_920, 3), kwargs = {})
#   %mul_1193 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sum_165, 6.228077168367346e-07), kwargs = {})
#   %mul_1194 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %squeeze_1), kwargs = {})
#   %mul_1195 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1193, %mul_1194), kwargs = {})
#   %unsqueeze_922 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1195, 0), kwargs = {})
#   %unsqueeze_923 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_922, 2), kwargs = {})
#   %unsqueeze_924 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_923, 3), kwargs = {})
#   %mul_1196 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %primals_6), kwargs = {})
#   %unsqueeze_925 : Tensor "f32[1, 32][32, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1196, 0), kwargs = {})
#   %unsqueeze_926 : Tensor "f32[1, 32, 1][32, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_925, 2), kwargs = {})
#   %unsqueeze_927 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_926, 3), kwargs = {})
#   %mul_1197 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_350, %unsqueeze_924), kwargs = {})
#   %sub_352 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_816, %mul_1197), kwargs = {})
#   %sub_353 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%sub_352, %unsqueeze_921), kwargs = {})
#   %mul_1198 : Tensor "f32[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_353, %unsqueeze_927), kwargs = {})
#   %convert_element_type_818 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1198, torch.float16), kwargs = {})
#   %convolution_backward_154 : [num_users=1] = call_function[target=torch.ops.aten.convolution_backward.default](args = (%convert_element_type_818, %convert_element_type_1, %convert_element_type, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), kwargs = {})
#   return %sub_352,%buf1210
triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp32', 'in_ptr6': '*fp32', 'in_ptr7': '*fp32', 'in_ptr8': '*fp32', 'in_ptr9': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]], (11,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 719323776}, 'kernel_num_gb': 0.51380288, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 51380224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp14 = tl.load(in_ptr3 + (x2), None).to(tl.float32)
    tmp18 = tl.load(in_ptr4 + (x2), None).to(tl.float32)
    tmp20 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp22 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp33 = tl.load(in_ptr9 + (x0), None, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp3 = x0
    tmp4 = tl.full([1], 0, tl.int64)
    tmp5 = tmp3 >= tmp4
    tmp6 = tl.full([1], 16, tl.int64)
    tmp7 = tmp3 < tmp6
    tmp8 = tl.load(in_ptr1 + (16*x1 + (x0)), tmp7, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp9 = tmp3 >= tmp6
    tmp10 = tl.full([1], 32, tl.int64)
    tmp11 = tmp3 < tmp10
    tmp12 = tl.load(in_ptr2 + (16*x1 + ((-16) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp13 = tl.where(tmp7, tmp8, tmp12)
    tmp15 = tmp13 + tmp14
    tmp16 = tl.where(tmp2, tmp1, tmp15)
    tmp17 = tmp16.to(tl.float32)
    tmp19 = tmp18.to(tl.float32)
    tmp21 = tmp19 - tmp20
    tmp23 = 6.228077168367346e-07
    tmp24 = tmp22 * tmp23
    tmp26 = tmp25 * tmp25
    tmp27 = tmp24 * tmp26
    tmp28 = tmp21 * tmp27
    tmp29 = tmp17 - tmp28
    tmp31 = tmp30 * tmp23
    tmp32 = tmp29 - tmp31
    tmp34 = tmp25 * tmp33
    tmp35 = tmp32 * tmp34
    tmp36 = tmp35.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp36, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_5 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_10 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, 51380224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.51380288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jj/cjj6twxeaijoahgfqz6spnalsfrrssn2jlhhyh2qytepnx5yuwx4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_896 : Tensor "f16[32, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_896]
#   %convert_element_type_815 : Tensor "f32[32, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_896, torch.float32), kwargs = {})
#   return %convert_element_type_815
triton_poi_fused__to_copy_211 = async_compile.triton('triton_poi_fused__to_copy_211', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 512}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_211', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2880}, 'kernel_num_gb': 1.728e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_211(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 288
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((32, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((32, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 288,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_211.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_211.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.728e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5j/c5jbgw7aolsee5vwwgqcqr5w6evozszsvfj6sx2exxz5pebichzv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_851 : Tensor "f16[60, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_851]
#   %convert_element_type_773 : Tensor "f32[60, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_851, torch.float32), kwargs = {})
#   return %convert_element_type_773
triton_poi_fused__to_copy_212 = async_compile.triton('triton_poi_fused__to_copy_212', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_212', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5400}, 'kernel_num_gb': 3.24e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_212(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 540
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 540,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_212.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_212.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.24e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rj/crjckfrfyipsvh7jbg3urhjkg2jo75up2cc4tcwt5wbicihngeiw.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_884 : Tensor "f16[64, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_884]
#   %convert_element_type_802 : Tensor "f32[64, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_884, torch.float32), kwargs = {})
#   return %convert_element_type_802
triton_poi_fused__to_copy_213 = async_compile.triton('triton_poi_fused__to_copy_213', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_213', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5760}, 'kernel_num_gb': 3.456e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_213(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 576
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((64, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 576,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_213.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_213.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.456e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ms/cmsfdm3llvijd2uslfbtt5dacryg3f7ynvrluw7sxixrk7r6qydf.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_934 : Tensor "f16[128, 14, 1, 1][14, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_934]
#   %sum_107 : Tensor "f16[14][1]cuda:0" = PlaceHolder[target=sum_107]
#   %sum_107 : Tensor "f16[14][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_934, [0, 2, 3]), kwargs = {})
#   %convert_element_type_679 : Tensor "f32[14][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_107, torch.float32), kwargs = {})
#   return %sum_107,%convert_element_type_679
triton_per_fused__to_copy_convolution_backward_214 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_214', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 16, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_214', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3696, 'r0_': 0}, 'kernel_num_gb': 3.64e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_214(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 14
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 14*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 14, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_214.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_214.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.64e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zp/czpczgel65rsxhqj6mkghzxoac5dx4jjwup7ghbs5xjrjci4haq3.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_899 : Tensor "f16[32, 3, 3, 3][27, 1, 9, 3]cuda:0" = PlaceHolder[target=getitem_899]
#   %convert_element_type_819 : Tensor "f32[32, 3, 3, 3][27, 1, 9, 3]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_899, torch.float32), kwargs = {})
#   return %convert_element_type_819
triton_poi_fused__to_copy_215 = async_compile.triton('triton_poi_fused__to_copy_215', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_215', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8640}, 'kernel_num_gb': 5.184e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_215(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((32, 3, 3, 3), (27, 1, 9, 3), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((32, 3, 3, 3), (27, 1, 9, 3), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 864,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_215.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_215.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.184e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mv/cmvsuihawtnmfy7zzo2ujjla7lcn3eb5wn5sghvwl6e6uaf7dwy3.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_1094 : Tensor "f16[128, 20, 1, 1][20, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_1094]
#   %sum_143 : Tensor "f16[20][1]cuda:0" = PlaceHolder[target=sum_143]
#   %sum_143 : Tensor "f16[20][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_1094, [0, 2, 3]), kwargs = {})
#   %convert_element_type_766 : Tensor "f32[20][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_143, torch.float32), kwargs = {})
#   return %sum_143,%convert_element_type_766
triton_per_fused__to_copy_convolution_backward_216 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_216', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 32, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_216', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5280, 'r0_': 0}, 'kernel_num_gb': 5.2e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_216(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 20
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 20*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 20, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_216.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_216.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kd/ckdw7vizslht6s4tncmwsytfi7gpqdzf2dwaqkolmshplj44lrei.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_755 : Tensor "f16[112, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_755]
#   %convert_element_type_685 : Tensor "f32[112, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_755, torch.float32), kwargs = {})
#   return %convert_element_type_685
triton_poi_fused__to_copy_217 = async_compile.triton('triton_poi_fused__to_copy_217', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_217', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10080}, 'kernel_num_gb': 6.048e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_217(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1008
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((112, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1008,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_217.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_217.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.048e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/c6/cc6bf3a4eynj3aguveaidwsn2spilj3txchtnw7cg3kr7iuitwdo.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_893 : Tensor "f16[32, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=getitem_893]
#   %convert_element_type_811 : Tensor "f32[32, 32, 1, 1][32, 1, 32, 32]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_893, torch.float32), kwargs = {})
#   return %convert_element_type_811
triton_poi_fused__to_copy_218 = async_compile.triton('triton_poi_fused__to_copy_218', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_218', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10240}, 'kernel_num_gb': 6.144e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_218(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((32, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((32, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_218.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_218.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.144e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ia/cia3a5mc4uwb3kpanwtqnmpkwd6mpur6n5sdtwbotmjslru2rhxc.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_566 : Tensor "f16[120, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_566]
#   %convert_element_type_524 : Tensor "f32[120, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_566, torch.float32), kwargs = {})
#   return %convert_element_type_524
triton_poi_fused__to_copy_219 = async_compile.triton('triton_poi_fused__to_copy_219', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_219', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 10800}, 'kernel_num_gb': 6.48e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_219(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_219.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_219.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.48e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fi/cfibs7zxq7kssd3s6xzhucmo5cx3q6tsxegx27astphdwvyhzfvx.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_814 : Tensor "f16[128, 26, 1, 1][26, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_814]
#   %sum_80 : Tensor "f16[26][1]cuda:0" = PlaceHolder[target=sum_80]
#   %sum_80 : Tensor "f16[26][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_814, [0, 2, 3]), kwargs = {})
#   %convert_element_type_608 : Tensor "f32[26][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_80, torch.float32), kwargs = {})
#   return %sum_80,%convert_element_type_608
triton_per_fused__to_copy_convolution_backward_220 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_220', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 32, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_220', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 6864, 'r0_': 0}, 'kernel_num_gb': 6.76e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_220(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 26
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 26*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((26,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 26, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_220.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_220.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.76e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ea/ceabmtczn6mh5pj2w2lbv7dimk2softwelbxbjm5bstuzgb4z2r4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_857 : Tensor "f16[20, 60, 1, 1][60, 1, 60, 60]cuda:0" = PlaceHolder[target=getitem_857]
#   %convert_element_type_781 : Tensor "f32[20, 60, 1, 1][60, 1, 60, 60]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_857, torch.float32), kwargs = {})
#   return %convert_element_type_781
triton_poi_fused__to_copy_221 = async_compile.triton('triton_poi_fused__to_copy_221', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_221', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12000}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_221(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_221.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_221.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yc/cycbwn3sldlhjce7ee4dyuop5wgr5n53e6vbtfdrhsepwk57rgm2.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_860 : Tensor "f16[20, 60, 1, 1][60, 1, 60, 60]cuda:0" = PlaceHolder[target=getitem_860]
#   %convert_element_type_782 : Tensor "f32[20, 60, 1, 1][60, 1, 60, 60]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_860, torch.float32), kwargs = {})
#   return %convert_element_type_782
triton_poi_fused__to_copy_222 = async_compile.triton('triton_poi_fused__to_copy_222', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_222', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12000}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_222(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_222.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_222.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5k/c5kmk5tdun72weeow2zmnrqykn2ohcoi2xjgi44acf7wktyce6vh.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_866 : Tensor "f16[60, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=getitem_866]
#   %convert_element_type_790 : Tensor "f32[60, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_866, torch.float32), kwargs = {})
#   return %convert_element_type_790
triton_poi_fused__to_copy_223 = async_compile.triton('triton_poi_fused__to_copy_223', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_223', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12000}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_223(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_223.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_223.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pz/cpzq6b47nuxjvmjbbtewvzkxiflknkyyvixahltawcyv22eutcot.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_869 : Tensor "f16[60, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=getitem_869]
#   %convert_element_type_791 : Tensor "f32[60, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_869, torch.float32), kwargs = {})
#   return %convert_element_type_791
triton_poi_fused__to_copy_224 = async_compile.triton('triton_poi_fused__to_copy_224', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_224', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12000}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_224(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_224.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_224.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ji/cjiibv7lbxwjadjcdkqjclw2hmzfkrud7oxod2qklbbqcuffovcs.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_974 : Tensor "f16[128, 28, 1, 1][28, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_974]
#   %sum_116 : Tensor "f16[28][1]cuda:0" = PlaceHolder[target=sum_116]
#   %sum_116 : Tensor "f16[28][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_974, [0, 2, 3]), kwargs = {})
#   %convert_element_type_701 : Tensor "f32[28][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_116, torch.float32), kwargs = {})
#   return %sum_116,%convert_element_type_701
triton_per_fused__to_copy_convolution_backward_225 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_225', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 32, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_225', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 7392, 'r0_': 0}, 'kernel_num_gb': 7.28e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_225(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 28
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 28*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((28,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 28, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_225.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_225.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.28e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yy/cyyoukeq4sixduu3nftz3g5anlea64mkmbcdz6dmrpihjhpbi6e5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_671 : Tensor "f16[156, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_671]
#   %convert_element_type_615 : Tensor "f32[156, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_671, torch.float32), kwargs = {})
#   return %convert_element_type_615
triton_poi_fused__to_copy_226 = async_compile.triton('triton_poi_fused__to_copy_226', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_226', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 14040}, 'kernel_num_gb': 8.424e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_226(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1404
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1404,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_226.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_226.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.424e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3b/c3bzsel2hustwomccmlv5cfzz642l6tdesgwqgged4qfjsglbph7.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_848 : Tensor "f16[60, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_848]
#   %convert_element_type_772 : Tensor "f32[60, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_848, torch.float32), kwargs = {})
#   return %convert_element_type_772
triton_poi_fused__to_copy_227 = async_compile.triton('triton_poi_fused__to_copy_227', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_227', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15000}, 'kernel_num_gb': 9e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_227(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1500
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1500,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_227.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_227.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sf/csfzymkxuer3vz6nfvbpnfm5z6svv7arizj5os7oam6drr2t5d3e.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_776 : Tensor "f16[168, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_776]
#   %convert_element_type_706 : Tensor "f32[168, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_776, torch.float32), kwargs = {})
#   return %convert_element_type_706
triton_poi_fused__to_copy_228 = async_compile.triton('triton_poi_fused__to_copy_228', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_228', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15120}, 'kernel_num_gb': 9.072e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_228(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_228.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_228.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.072e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bj/cbjlbzksq4sizxmasgfzz36ctt65jcy4y4qqho6vbfnplw74b5j6.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_887 : Tensor "f16[96, 16, 1, 1][16, 1, 16, 16]cuda:0" = PlaceHolder[target=getitem_887]
#   %convert_element_type_806 : Tensor "f32[96, 16, 1, 1][16, 1, 16, 16]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_887, torch.float32), kwargs = {})
#   return %convert_element_type_806
triton_poi_fused__to_copy_229 = async_compile.triton('triton_poi_fused__to_copy_229', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_229', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15360}, 'kernel_num_gb': 9.216e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_229(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1536
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((96, 16, 1, 1), (16, 1, 16, 16), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((96, 16, 1, 1), (16, 1, 16, 16), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1536,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_229.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_229.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.216e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/i5/ci5ptnpco4sbwcjjymq463xgj3r6l7aandz7zt2j46ftxmbgiu7b.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_881 : Tensor "f16[64, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_881]
#   %convert_element_type_801 : Tensor "f32[64, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_881, torch.float32), kwargs = {})
#   return %convert_element_type_801
triton_poi_fused__to_copy_230 = async_compile.triton('triton_poi_fused__to_copy_230', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_230', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16000}, 'kernel_num_gb': 9.6e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_230(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((64, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_230.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_230.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.6e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rn/crnxfq7aamf5lpvyrxu476c3l3lgw2df56by3fpwqushp47ujlm7.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_872 : Tensor "f16[20, 96, 1, 1][96, 1, 96, 96]cuda:0" = PlaceHolder[target=getitem_872]
#   %convert_element_type_795 : Tensor "f32[20, 96, 1, 1][96, 1, 96, 96]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_872, torch.float32), kwargs = {})
#   return %convert_element_type_795
triton_poi_fused__to_copy_231 = async_compile.triton('triton_poi_fused__to_copy_231', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_231', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 19200}, 'kernel_num_gb': 1.152e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_231(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1920
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 96, 1, 1), (96, 1, 96, 96), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20, 96, 1, 1), (96, 1, 96, 96), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1920,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_231.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_231.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.152e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3m/c3mlc7jkulghxq5xrc3eg3ihj3ivqwogwxpaxc556ltdv7d5mzk6.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_539 : Tensor "f16[240, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_539]
#   %convert_element_type_501 : Tensor "f32[240, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_539, torch.float32), kwargs = {})
#   return %convert_element_type_501
triton_poi_fused__to_copy_232 = async_compile.triton('triton_poi_fused__to_copy_232', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_232', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21600}, 'kernel_num_gb': 1.296e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_232(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2160
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 2160,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_232.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_232.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.296e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kj/ckjulct4rzyowqsgub6ucctgqa35j462v4sjmos5wfe5evyz3i45.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_774 : Tensor "f16[128, 52, 1, 1][52, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_774]
#   %sum_71 : Tensor "f16[52][1]cuda:0" = PlaceHolder[target=sum_71]
#   %sum_71 : Tensor "f16[52][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_774, [0, 2, 3]), kwargs = {})
#   %convert_element_type_588 : Tensor "f32[52][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_71, torch.float32), kwargs = {})
#   return %sum_71,%convert_element_type_588
triton_per_fused__to_copy_convolution_backward_233 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_233', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 64, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_233', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13728, 'r0_': 0}, 'kernel_num_gb': 1.352e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_233(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 52
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 52*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((52,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 52, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_233.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_233.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.352e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/x4/cx4h3dobdetsmpiazcztppht7cfyaqwg537fyx52n2yucczlmhhp.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_752 : Tensor "f16[112, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_752]
#   %convert_element_type_684 : Tensor "f32[112, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_752, torch.float32), kwargs = {})
#   return %convert_element_type_684
triton_poi_fused__to_copy_234 = async_compile.triton('triton_poi_fused__to_copy_234', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_234', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 28000}, 'kernel_num_gb': 1.68e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_234(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((112, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 2800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_234.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_234.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.68e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hy/chydodwyztf454xjht2rgqr6f22ygfpqabxrem5huovyk6w3d4ub.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_845 : Tensor "f16[60, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_845]
#   %convert_element_type_771 : Tensor "f32[60, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_845, torch.float32), kwargs = {})
#   return %convert_element_type_771
triton_poi_fused__to_copy_235 = async_compile.triton('triton_poi_fused__to_copy_235', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_235', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 29400}, 'kernel_num_gb': 1.764e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_235(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2940
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 2940,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_235.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_235.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.764e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pi/cpimcgchvw62bt3rzxtzkucdgqaj7x7pzlh3njve65yttdhbtwpj.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_563 : Tensor "f16[120, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_563]
#   %convert_element_type_523 : Tensor "f32[120, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_563, torch.float32), kwargs = {})
#   return %convert_element_type_523
triton_poi_fused__to_copy_236 = async_compile.triton('triton_poi_fused__to_copy_236', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_236', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 30000}, 'kernel_num_gb': 1.8e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_236(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 3000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_236.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_236.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.8e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hm/chm2msdijwcksauf3ccgsqfp7qqesgb4336vg66fjbged4et5ate.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_878 : Tensor "f16[64, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_878]
#   %convert_element_type_800 : Tensor "f32[64, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_878, torch.float32), kwargs = {})
#   return %convert_element_type_800
triton_poi_fused__to_copy_237 = async_compile.triton('triton_poi_fused__to_copy_237', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_237', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 31360}, 'kernel_num_gb': 1.8816e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_237(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3136
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((64, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_237.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_237.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.8816e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/p7/cp7dmsa6xdqaxocv6icq74su3mrzct4yskanlzjls4nv6yrezu2p.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_614 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_614]
#   %sum_35 : Tensor "f16[80][1]cuda:0" = PlaceHolder[target=sum_35]
#   %sum_35 : Tensor "f16[80][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_614, [0, 2, 3]), kwargs = {})
#   %convert_element_type_494 : Tensor "f32[80][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_35, torch.float32), kwargs = {})
#   return %sum_35,%convert_element_type_494
triton_per_fused__to_copy_convolution_backward_238 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_238', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 128, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_238', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21120, 'r0_': 0}, 'kernel_num_gb': 2.08e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_238(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 80
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 80*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 80, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_238.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_238.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.08e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gw/cgwzcp3pyojxcn3qti5k3hwxkfxqc4hrfwvjqbyqcibctiuelwb4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_461 : Tensor "f16[396, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_461]
#   %convert_element_type_433 : Tensor "f32[396, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_461, torch.float32), kwargs = {})
#   return %convert_element_type_433
triton_poi_fused__to_copy_239 = async_compile.triton('triton_poi_fused__to_copy_239', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_239', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 35640}, 'kernel_num_gb': 2.1384e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_239(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3564
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 3564,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_239.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_239.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.1384e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/c5/cc5hvysxxupeg2rdh5ctz3ghuu7fwln33lg6r4n2t4m26qnunzjz.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_668 : Tensor "f16[156, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_668]
#   %convert_element_type_614 : Tensor "f32[156, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_668, torch.float32), kwargs = {})
#   return %convert_element_type_614
triton_poi_fused__to_copy_240 = async_compile.triton('triton_poi_fused__to_copy_240', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_240', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 39000}, 'kernel_num_gb': 2.34e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_240(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3900
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 3900,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_240.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_240.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.34e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3j/c3jkhkfi6n3g3jut6mxnpeone6m3mso7vxovenaohpqfc7qegcfw.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_773 : Tensor "f16[168, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_773]
#   %convert_element_type_705 : Tensor "f32[168, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_773, torch.float32), kwargs = {})
#   return %convert_element_type_705
triton_poi_fused__to_copy_241 = async_compile.triton('triton_poi_fused__to_copy_241', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_241', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 42000}, 'kernel_num_gb': 2.52e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_241(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_241.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_241.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.52e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vn/cvn5cma6xwggqksizb3qlidvuwle6rytksx5zl6grulc7a2rlgam.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_743 : Tensor "f16[336, 14, 1, 1][14, 1, 14, 14]cuda:0" = PlaceHolder[target=getitem_743]
#   %convert_element_type_676 : Tensor "f32[336, 14, 1, 1][14, 1, 14, 14]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_743, torch.float32), kwargs = {})
#   return %convert_element_type_676
triton_poi_fused__to_copy_242 = async_compile.triton('triton_poi_fused__to_copy_242', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_242', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_242(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_242.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_242.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ox/coxkvvvliefb2nt6qxlevxqtar7t45z44c2jfzzajpwwzp3pykeh.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_746 : Tensor "f16[14, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_746]
#   %convert_element_type_678 : Tensor "f32[14, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_746, torch.float32), kwargs = {})
#   return %convert_element_type_678
triton_poi_fused__to_copy_243 = async_compile.triton('triton_poi_fused__to_copy_243', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_243', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_243(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((14, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_243.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_243.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/tz/ctzhht5a7xvauz3iaukiydfmzekttxptgiblpukjr6hjviwl2nea.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_761 : Tensor "f16[28, 168, 1, 1][168, 1, 168, 168]cuda:0" = PlaceHolder[target=getitem_761]
#   %convert_element_type_693 : Tensor "f32[28, 168, 1, 1][168, 1, 168, 168]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_761, torch.float32), kwargs = {})
#   return %convert_element_type_693
triton_poi_fused__to_copy_244 = async_compile.triton('triton_poi_fused__to_copy_244', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_244', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_244(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_244.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_244.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/72/c72eovje4nsqhs3vaew4f5myzf5fcdd5nhcrkssjudbg6vmy2gqe.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_764 : Tensor "f16[28, 168, 1, 1][168, 1, 168, 168]cuda:0" = PlaceHolder[target=getitem_764]
#   %convert_element_type_694 : Tensor "f32[28, 168, 1, 1][168, 1, 168, 168]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_764, torch.float32), kwargs = {})
#   return %convert_element_type_694
triton_poi_fused__to_copy_245 = async_compile.triton('triton_poi_fused__to_copy_245', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_245', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_245(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_245.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_245.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mf/cmfaianorbn2n2incmb2e6xc7dkeaphk2njv6ss3ro7wpgds2afj.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_779 : Tensor "f16[168, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=getitem_779]
#   %convert_element_type_710 : Tensor "f32[168, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_779, torch.float32), kwargs = {})
#   return %convert_element_type_710
triton_poi_fused__to_copy_246 = async_compile.triton('triton_poi_fused__to_copy_246', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_246', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_246(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_246.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_246.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nb/cnbmeoo6uvgeo7yiywdohuraqm4jgcetwqyxmrlcr4rfk7rvuyxf.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_836 : Tensor "f16[240, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=getitem_836]
#   %convert_element_type_763 : Tensor "f32[240, 20, 1, 1][20, 1, 20, 20]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_836, torch.float32), kwargs = {})
#   return %convert_element_type_763
triton_poi_fused__to_copy_247 = async_compile.triton('triton_poi_fused__to_copy_247', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_247', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48000}, 'kernel_num_gb': 2.88e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_247(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_247.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_247.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.88e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2f/c2fxq6bsedtzyqn64c4zlta42jcymkp6zu626bl6exhja2cd6mwv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_839 : Tensor "f16[20, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_839]
#   %convert_element_type_765 : Tensor "f32[20, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_839, torch.float32), kwargs = {})
#   return %convert_element_type_765
triton_poi_fused__to_copy_248 = async_compile.triton('triton_poi_fused__to_copy_248', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_248', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48000}, 'kernel_num_gb': 2.88e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_248(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_248.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_248.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.88e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7u/c7ugzrlqgfo6tk75bt34w2vc57igjcc2wuvr4tcmmxs6ycfit5nx.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_842 : Tensor "f16[60, 1, 9, 9][81, 1, 9, 1]cuda:0" = PlaceHolder[target=getitem_842]
#   %convert_element_type_770 : Tensor "f32[60, 1, 9, 9][81, 1, 9, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_842, torch.float32), kwargs = {})
#   return %convert_element_type_770
triton_poi_fused__to_copy_249 = async_compile.triton('triton_poi_fused__to_copy_249', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_249', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48600}, 'kernel_num_gb': 2.916e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_249(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4860
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((60, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 4860,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_249.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_249.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.916e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/eh/cehosrfdt62ke4lacsikgsszj2ypflrnmbjeolls2dbpnztwfiqm.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_749 : Tensor "f16[112, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_749]
#   %convert_element_type_683 : Tensor "f32[112, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_749, torch.float32), kwargs = {})
#   return %convert_element_type_683
triton_poi_fused__to_copy_250 = async_compile.triton('triton_poi_fused__to_copy_250', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_250', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 54880}, 'kernel_num_gb': 3.2928e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_250(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5488
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((112, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 5488,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_250.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_250.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.2928e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rp/crpucdfkjqulptdkq6qab7gygmszzmktkkb3dz7tuvfznkr7zqmo.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_644 : Tensor "f16[624, 1, 3, 3][9, 1, 3, 1]cuda:0" = PlaceHolder[target=getitem_644]
#   %convert_element_type_592 : Tensor "f32[624, 1, 3, 3][9, 1, 3, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_644, torch.float32), kwargs = {})
#   return %convert_element_type_592
triton_poi_fused__to_copy_251 = async_compile.triton('triton_poi_fused__to_copy_251', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_251', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 56160}, 'kernel_num_gb': 3.3696e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_251(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5616
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 5616,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_251.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_251.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.3696e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wj/cwjjbbdkrbuudtzax2um2ufi5bssqxfbsw3w2ycwuvpdlpm6pgwv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mul_494 : Tensor "f16[128, 132, 1, 1][132, 1, 1, 1]cuda:0" = PlaceHolder[target=mul_494]
#   %sum_8 : Tensor "f16[132][1]cuda:0" = PlaceHolder[target=sum_8]
#   %sum_8 : Tensor "f16[132][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%mul_494, [0, 2, 3]), kwargs = {})
#   %convert_element_type_426 : Tensor "f32[132][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_8, torch.float32), kwargs = {})
#   return %sum_8,%convert_element_type_426
triton_per_fused__to_copy_convolution_backward_252 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_252', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_252', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 34848, 'r0_': 0}, 'kernel_num_gb': 3.432e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_252(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 132
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 132*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((132,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 132, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_252.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_252.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.432e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ap/cap6rsc6oj2b4i3svsudtx5chy4mg4wqch6suoio6fvsequvqyme.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_560 : Tensor "f16[120, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_560]
#   %convert_element_type_522 : Tensor "f32[120, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_560, torch.float32), kwargs = {})
#   return %convert_element_type_522
triton_poi_fused__to_copy_253 = async_compile.triton('triton_poi_fused__to_copy_253', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_253', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 58800}, 'kernel_num_gb': 3.528e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_253(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5880
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 5880,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_253.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_253.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.528e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pv/cpvsasam2d36jpujg6e3iek352yzxow4d3gpyowatszoymed4hsi.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_536 : Tensor "f16[240, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_536]
#   %convert_element_type_500 : Tensor "f32[240, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_536, torch.float32), kwargs = {})
#   return %convert_element_type_500
triton_poi_fused__to_copy_254 = async_compile.triton('triton_poi_fused__to_copy_254', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_254', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 60000}, 'kernel_num_gb': 3.6e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_254(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 6000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_254.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_254.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.6e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ok/cokejkg66rrgxqn7dfnge2hwg7etameiuir42nwhbgotkjzaz45x.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_665 : Tensor "f16[156, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_665]
#   %convert_element_type_613 : Tensor "f32[156, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_665, torch.float32), kwargs = {})
#   return %convert_element_type_613
triton_poi_fused__to_copy_255 = async_compile.triton('triton_poi_fused__to_copy_255', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_255', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 76440}, 'kernel_num_gb': 4.5864e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_255(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 7644
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 7644,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_255.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_255.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 4.5864e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w7/cw7ptrffsy3uxtgq46irmzkgexwde33mdul3irroutjfdc2vmkn5.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_767 : Tensor "f16[336, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=getitem_767]
#   %convert_element_type_698 : Tensor "f32[336, 28, 1, 1][28, 1, 28, 28]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_767, torch.float32), kwargs = {})
#   return %convert_element_type_698
triton_poi_fused__to_copy_256 = async_compile.triton('triton_poi_fused__to_copy_256', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_256', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 94080}, 'kernel_num_gb': 5.6448e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_256(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9408
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9408,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_256.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_256.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.6448e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5q/c5qbpfv4ioqgbvvfkzvdyrtsvkcxdaztka7mbsvvjasfi6k46ywl.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_770 : Tensor "f16[28, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_770]
#   %convert_element_type_700 : Tensor "f32[28, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_770, torch.float32), kwargs = {})
#   return %convert_element_type_700
triton_poi_fused__to_copy_257 = async_compile.triton('triton_poi_fused__to_copy_257', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_257', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 94080}, 'kernel_num_gb': 5.6448e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_257(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9408
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9408,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_257.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_257.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.6448e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/tc/ctcbsmk5tciuug6jj3ldekxgnfj2gc4554vs32yvlnfreidz6too.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_854 : Tensor "f16[240, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=getitem_854]
#   %convert_element_type_777 : Tensor "f32[240, 40, 1, 1][40, 1, 40, 40]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_854, torch.float32), kwargs = {})
#   return %convert_element_type_777
triton_poi_fused__to_copy_258 = async_compile.triton('triton_poi_fused__to_copy_258', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_258', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96000}, 'kernel_num_gb': 5.76e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_258(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_258.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_258.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.76e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jf/cjfwbrj33ah6gban5jrsfkfa5cfzl5a2a33lr5llanzzd6bi7te7.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_557 : Tensor "f16[120, 1, 9, 9][81, 1, 9, 1]cuda:0" = PlaceHolder[target=getitem_557]
#   %convert_element_type_521 : Tensor "f32[120, 1, 9, 9][81, 1, 9, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_557, torch.float32), kwargs = {})
#   return %convert_element_type_521
triton_poi_fused__to_copy_259 = async_compile.triton('triton_poi_fused__to_copy_259', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_259', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 97200}, 'kernel_num_gb': 5.832e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_259(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9720
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9720,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_259.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_259.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.832e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gs/cgsnvmasadzdgj4t5jkgxmrda2jg5tdgxpqnxkbwvewy53puk4gm.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_458 : Tensor "f16[396, 1, 5, 5][25, 1, 5, 1]cuda:0" = PlaceHolder[target=getitem_458]
#   %convert_element_type_432 : Tensor "f32[396, 1, 5, 5][25, 1, 5, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_458, torch.float32), kwargs = {})
#   return %convert_element_type_432
triton_poi_fused__to_copy_260 = async_compile.triton('triton_poi_fused__to_copy_260', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_260', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 99000}, 'kernel_num_gb': 5.94e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_260(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9900
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 9900,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_260.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_260.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.94e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/p6/cp63azr4am6rxh7kyga3wo6flywurlpp63ljd73ejgfxwi5hwcjt.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_762 : Tensor "f16[128, 240, 1, 1][240, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_762]
#   %sum_142 : Tensor "f16[240][1]cuda:0" = PlaceHolder[target=sum_142]
#   %sum_142 : Tensor "f16[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_762, [0, 2, 3]), kwargs = {})
#   %convert_element_type_764 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_142, torch.float32), kwargs = {})
#   return %sum_142,%convert_element_type_764
triton_per_fused__to_copy_convolution_backward_261 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_261', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_261', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 63360, 'r0_': 0}, 'kernel_num_gb': 6.24e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_261(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 240, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_261.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_261.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.24e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gd/cgdgmjoi24duf5jj7o5ov27wgwwa6nfnux2higefll6lqt2ara6b.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_533 : Tensor "f16[240, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_533]
#   %convert_element_type_499 : Tensor "f32[240, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_533, torch.float32), kwargs = {})
#   return %convert_element_type_499
triton_poi_fused__to_copy_262 = async_compile.triton('triton_poi_fused__to_copy_262', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_262', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 117600}, 'kernel_num_gb': 7.056e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_262(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 11760
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 11760,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_262.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_262.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.056e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/op/copxa2x2gwutdqvj3hb2y3jbruwvucmonnezzdf7iglvvbnsp67m.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_662 : Tensor "f16[156, 1, 9, 9][81, 1, 9, 1]cuda:0" = PlaceHolder[target=getitem_662]
#   %convert_element_type_612 : Tensor "f32[156, 1, 9, 9][81, 1, 9, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_662, torch.float32), kwargs = {})
#   return %convert_element_type_612
triton_poi_fused__to_copy_263 = async_compile.triton('triton_poi_fused__to_copy_263', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_263', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 126360}, 'kernel_num_gb': 7.5816e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_263(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12636
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 12636,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_263.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_263.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.5816e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b4/cb47frpreo63qsnalbvg65id75sck5gttx3tuktrfreugfwdooik.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_833 : Tensor "f16[56, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_833]
#   %convert_element_type_759 : Tensor "f32[56, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_833, torch.float32), kwargs = {})
#   return %convert_element_type_759
triton_poi_fused__to_copy_264 = async_compile.triton('triton_poi_fused__to_copy_264', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_264', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 134400}, 'kernel_num_gb': 8.064e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_264(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 13440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((56, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((56, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 13440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_264.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_264.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.064e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hc/chcu6gfozblzimroicufz3tn3qqt736ixvumjpe3dxh6vjcjb6z4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_675 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_675]
#   %sum_106 : Tensor "f16[336][1]cuda:0" = PlaceHolder[target=sum_106]
#   %sum_106 : Tensor "f16[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_675, [0, 2, 3]), kwargs = {})
#   %convert_element_type_677 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_106, torch.float32), kwargs = {})
#   return %sum_106,%convert_element_type_677
triton_per_fused__to_copy_convolution_backward_265 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_265', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_265', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 88704, 'r0_': 0}, 'kernel_num_gb': 8.736e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_265(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 336, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_265.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_265.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.736e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2p/c2p47begy4oehxskrc6xvqwnuoh3ls2xktwqu6tmst7jo4r7rycw.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_650 : Tensor "f16[52, 312, 1, 1][312, 1, 312, 312]cuda:0" = PlaceHolder[target=getitem_650]
#   %convert_element_type_600 : Tensor "f32[52, 312, 1, 1][312, 1, 312, 312]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_650, torch.float32), kwargs = {})
#   return %convert_element_type_600
triton_poi_fused__to_copy_266 = async_compile.triton('triton_poi_fused__to_copy_266', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_266', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 162240}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_266(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_266.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_266.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5q/c5qfss2ibjivafc37wszvayhh4efbya7366bdg4nyp2jiuyho45t.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_653 : Tensor "f16[52, 312, 1, 1][312, 1, 312, 312]cuda:0" = PlaceHolder[target=getitem_653]
#   %convert_element_type_601 : Tensor "f32[52, 312, 1, 1][312, 1, 312, 312]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_653, torch.float32), kwargs = {})
#   return %convert_element_type_601
triton_poi_fused__to_copy_267 = async_compile.triton('triton_poi_fused__to_copy_267', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_267', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 162240}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_267(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_267.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_267.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vi/cvib4fju77jdmokhert6ld33k3bvzktyvtwop5rprqyg576pxeix.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_656 : Tensor "f16[624, 26, 1, 1][26, 1, 26, 26]cuda:0" = PlaceHolder[target=getitem_656]
#   %convert_element_type_605 : Tensor "f32[624, 26, 1, 1][26, 1, 26, 26]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_656, torch.float32), kwargs = {})
#   return %convert_element_type_605
triton_poi_fused__to_copy_268 = async_compile.triton('triton_poi_fused__to_copy_268', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_268', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 162240}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_268(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_268.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_268.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7e/c7e7gna55fppnjgyomsd24cdi45cqjnydmeykujrw6m35j47fsak.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_659 : Tensor "f16[26, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_659]
#   %convert_element_type_607 : Tensor "f32[26, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_659, torch.float32), kwargs = {})
#   return %convert_element_type_607
triton_poi_fused__to_copy_269 = async_compile.triton('triton_poi_fused__to_copy_269', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_269', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 162240}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_269(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_269.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_269.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gb/cgbq4acrd7otyxgwswajrklkdajzjhlampgpgkoesahlkklrufc3.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_674 : Tensor "f16[312, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=getitem_674]
#   %convert_element_type_619 : Tensor "f32[312, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_674, torch.float32), kwargs = {})
#   return %convert_element_type_619
triton_poi_fused__to_copy_270 = async_compile.triton('triton_poi_fused__to_copy_270', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_270', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 162240}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_270(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_270.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_270.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jf/cjf6ga3ppfqgnezbjxjutpeimknzhhkve5j3jmakth4wxctr35z7.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_758 : Tensor "f16[336, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=getitem_758]
#   %convert_element_type_689 : Tensor "f32[336, 56, 1, 1][56, 1, 56, 56]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_758, torch.float32), kwargs = {})
#   return %convert_element_type_689
triton_poi_fused__to_copy_271 = async_compile.triton('triton_poi_fused__to_copy_271', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_271', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 188160}, 'kernel_num_gb': 0.000112896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_271(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 18816
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 18816,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_271.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_271.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000112896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wi/cwiv6ab3atkinh2so32ihfkwd6ztqizxldxmlunwg46cw33tfte4.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_545 : Tensor "f16[80, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_545]
#   %convert_element_type_509 : Tensor "f32[80, 240, 1, 1][240, 1, 240, 240]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_545, torch.float32), kwargs = {})
#   return %convert_element_type_509
triton_poi_fused__to_copy_272 = async_compile.triton('triton_poi_fused__to_copy_272', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_272', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 192000}, 'kernel_num_gb': 0.0001152, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_272(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 19200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_272.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_272.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sp/cspmbxqc4rwiqjg7qt45i5zttfvrb75bb6neaolygm2auw5tywpg.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_569 : Tensor "f16[240, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=getitem_569]
#   %convert_element_type_528 : Tensor "f32[240, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_569, torch.float32), kwargs = {})
#   return %convert_element_type_528
triton_poi_fused__to_copy_273 = async_compile.triton('triton_poi_fused__to_copy_273', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_273', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 192000}, 'kernel_num_gb': 0.0001152, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_273(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 19200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_273.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_273.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ml/cmllqj6h7kfoqylskigxubco4jhve5anddoncgdxdn7qmlcdqiro.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_455 : Tensor "f16[396, 1, 7, 7][49, 1, 7, 1]cuda:0" = PlaceHolder[target=getitem_455]
#   %convert_element_type_431 : Tensor "f32[396, 1, 7, 7][49, 1, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_455, torch.float32), kwargs = {})
#   return %convert_element_type_431
triton_poi_fused__to_copy_274 = async_compile.triton('triton_poi_fused__to_copy_274', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_274', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 194040}, 'kernel_num_gb': 0.000116424, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_274(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19404
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 19404,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_274.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_274.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000116424
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/th/ctho5fu6skoz5zn3aqsq66rdx2jsu6cxxh7rzt4txdvburfos7ob.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_530 : Tensor "f16[240, 1, 9, 9][81, 1, 9, 1]cuda:0" = PlaceHolder[target=getitem_530]
#   %convert_element_type_498 : Tensor "f32[240, 1, 9, 9][81, 1, 9, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_530, torch.float32), kwargs = {})
#   return %convert_element_type_498
triton_poi_fused__to_copy_275 = async_compile.triton('triton_poi_fused__to_copy_275', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_275', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 194400}, 'kernel_num_gb': 0.00011664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_275(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 19440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_275.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_275.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00011664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wm/cwm2cwzabglkwzgbvp53iqp26elppenceokmq65xx5gsla3pdnbz.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_513 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_513]
#   %sum_43 : Tensor "f16[480][1]cuda:0" = PlaceHolder[target=sum_43]
#   %sum_43 : Tensor "f16[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_513, [0, 2, 3]), kwargs = {})
#   %convert_element_type_515 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_43, torch.float32), kwargs = {})
#   return %sum_43,%convert_element_type_515
triton_per_fused__to_copy_convolution_backward_276 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_276', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_276', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 126720, 'r0_': 0}, 'kernel_num_gb': 0.0001248, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_276(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 480
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 480*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 480, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_276.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_276.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001248
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/u2/cu2owi55rc4gam2v3ov7wtjco4qxh3bca3u3e6bwlem7f6osxoym.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_584 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_584]
#   %sum_70 : Tensor "f16[624][1]cuda:0" = PlaceHolder[target=sum_70]
#   %sum_70 : Tensor "f16[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_584, [0, 2, 3]), kwargs = {})
#   %convert_element_type_586 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_70, torch.float32), kwargs = {})
#   return %sum_70,%convert_element_type_586
triton_per_fused__to_copy_convolution_backward_277 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_277', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_277', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 164736, 'r0_': 0}, 'kernel_num_gb': 0.00016224, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_277(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 624, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_277.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_277.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00016224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/oy/coyyo2x6crxkmsay633w5urr7tioh4ohr3576qv3v22kyhp2wy62.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_452 : Tensor "f16[396, 1, 9, 9][81, 1, 9, 1]cuda:0" = PlaceHolder[target=getitem_452]
#   %convert_element_type_430 : Tensor "f32[396, 1, 9, 9][81, 1, 9, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_452, torch.float32), kwargs = {})
#   return %convert_element_type_430
triton_poi_fused__to_copy_278 = async_compile.triton('triton_poi_fused__to_copy_278', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_278', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 320760}, 'kernel_num_gb': 0.000192456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_278(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32076
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 32076,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_278.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_278.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000192456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d2/cd2bkfb267kwd52pzfuygxjbuz3qqncd7ccw6nhgb5mfdthdb3ol.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_638 : Tensor "f16[624, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=getitem_638]
#   %convert_element_type_585 : Tensor "f32[624, 52, 1, 1][52, 1, 52, 52]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_638, torch.float32), kwargs = {})
#   return %convert_element_type_585
triton_poi_fused__to_copy_279 = async_compile.triton('triton_poi_fused__to_copy_279', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_279', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 324480}, 'kernel_num_gb': 0.000194688, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_279(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32448
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 32448,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_279.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_279.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000194688
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/33/c33orszxz5lm4veremhjptemrb3tge2rnvppkh72hp62j5mbrd6y.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_641 : Tensor "f16[52, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_641]
#   %convert_element_type_587 : Tensor "f32[52, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_641, torch.float32), kwargs = {})
#   return %convert_element_type_587
triton_poi_fused__to_copy_280 = async_compile.triton('triton_poi_fused__to_copy_280', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_280', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 324480}, 'kernel_num_gb': 0.000194688, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_280(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32448
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((52, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((52, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 32448,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_280.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_280.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000194688
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xk/cxklr7kvowu7f7acjbstywrdqu2awnhkf5r6ey6nrv3wmkh5hlww.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_740 : Tensor "f16[104, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_740]
#   %convert_element_type_672 : Tensor "f32[104, 336, 1, 1][336, 1, 336, 336]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_740, torch.float32), kwargs = {})
#   return %convert_element_type_672
triton_poi_fused__to_copy_281 = async_compile.triton('triton_poi_fused__to_copy_281', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_281', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 349440}, 'kernel_num_gb': 0.000209664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_281(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 34944
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((104, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((104, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 34944,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_281.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_281.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000209664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qe/cqeouzx5s2oarvlx5lbvqcugdqypuutdacsck4t4rwqrmfzkdmti.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_551 : Tensor "f16[480, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=getitem_551]
#   %convert_element_type_514 : Tensor "f32[480, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_551, torch.float32), kwargs = {})
#   return %convert_element_type_514
triton_poi_fused__to_copy_282 = async_compile.triton('triton_poi_fused__to_copy_282', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_282', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 384000}, 'kernel_num_gb': 0.0002304, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_282(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 38400
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 38400,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_282.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_282.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0002304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/my/cmydifmnhk2u3wc535pi5wpbgq6imy65vq2y6gzg3fy34e3ke7fk.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_554 : Tensor "f16[80, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_554]
#   %convert_element_type_516 : Tensor "f32[80, 480, 1, 1][480, 1, 480, 480]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_554, torch.float32), kwargs = {})
#   return %convert_element_type_516
triton_poi_fused__to_copy_283 = async_compile.triton('triton_poi_fused__to_copy_283', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_283', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 384000}, 'kernel_num_gb': 0.0002304, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_283(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 38400
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 38400,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_283.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_283.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0002304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/de/cde6hmlfs6455622awjifiwtj2uq3f6xvcglij6nbaeud5gw4mnq.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_490 : Tensor "f16[128, 960, 1, 1][960, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_490]
#   %sum_34 : Tensor "f16[960][1]cuda:0" = PlaceHolder[target=sum_34]
#   %sum_34 : Tensor "f16[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_490, [0, 2, 3]), kwargs = {})
#   %convert_element_type_492 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_34, torch.float32), kwargs = {})
#   return %sum_34,%convert_element_type_492
triton_per_fused__to_copy_convolution_backward_284 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_284', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_284', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 253440, 'r0_': 0}, 'kernel_num_gb': 0.0002496, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_284(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 960, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_284.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_284.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0002496
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ih/cihzx5pr3u2w736udclt36awsdzw2ibhu7nkjaacbyt4qqrinpmm.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.sum, aten.view, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %tangents_1 : Tensor "f16[128, 1000][1000, 1]cuda:0" = PlaceHolder[target=tangents_1]
#   %sum_1 : Tensor "f16[1, 1000][1000, 1]cuda:0" = PlaceHolder[target=sum_1]
#   %sum_1 : Tensor "f16[1, 1000][1000, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%tangents_1, [0], True), kwargs = {})
#   %view_1 : Tensor "f16[1000][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%sum_1, [1000]), kwargs = {})
#   %convert_element_type_410 : Tensor "f32[1000][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_1, torch.float32), kwargs = {})
#   return %sum_1,%convert_element_type_410
triton_red_fused__to_copy_sum_view_285 = async_compile.triton('triton_red_fused__to_copy_sum_view_285', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__to_copy_sum_view_285', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 264000, 'r0_': 0}, 'kernel_num_gb': 0.00026, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__to_copy_sum_view_285(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1000
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 1000*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(r0_mask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tmp4 = tmp2.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp4, xmask)


def get_args():
    arg_0 = rand_strided((128, 1000), (1000, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1000,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1000, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__to_copy_sum_view_285.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__to_copy_sum_view_285.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00026
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4z/c4zgycbcayzxj52w2rwyrjcrrvuwzic3gkuo6743ntaba6rj4ycy.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_647 : Tensor "f16[624, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=getitem_647]
#   %convert_element_type_596 : Tensor "f32[624, 104, 1, 1][104, 1, 104, 104]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_647, torch.float32), kwargs = {})
#   return %convert_element_type_596
triton_poi_fused__to_copy_286 = async_compile.triton('triton_poi_fused__to_copy_286', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_286', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 648960}, 'kernel_num_gb': 0.000389376, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_286(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 64896,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_286.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_286.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000389376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w2/cw2fim4qtkyzjdvd2psasuw5gvdqiltch3734ta2mwqzxhzvuhx7.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %convert_element_type_422 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0" = PlaceHolder[target=convert_element_type_422]
#   %sum_7 : Tensor "f16[1584][1]cuda:0" = PlaceHolder[target=sum_7]
#   %sum_7 : Tensor "f16[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%convert_element_type_422, [0, 2, 3]), kwargs = {})
#   %convert_element_type_424 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sum_7, torch.float32), kwargs = {})
#   return %sum_7,%convert_element_type_424
triton_per_fused__to_copy_convolution_backward_287 = async_compile.triton('triton_per_fused__to_copy_convolution_backward_287', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 128},
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__to_copy_convolution_backward_287', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 418176, 'r0_': 0}, 'kernel_num_gb': 0.00041184, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__to_copy_convolution_backward_287(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1584
    r0_numel = 128
    R0_BLOCK: tl.constexpr = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1584*r0_1), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp3 = tl.where(xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None].to(tl.float32)
    tmp5 = tmp4.to(tl.float32)
    tl.store(out_ptr1 + (x0), tmp5, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1584, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__to_copy_convolution_backward_287.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__to_copy_convolution_backward_287.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00041184
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pu/cpuxmsexzpwolj6hryhq6ejmsgbluf64ivd43b6uprfhkbidc5by.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_524 : Tensor "f16[960, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=getitem_524]
#   %convert_element_type_491 : Tensor "f32[960, 80, 1, 1][80, 1, 80, 80]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_524, torch.float32), kwargs = {})
#   return %convert_element_type_491
triton_poi_fused__to_copy_288 = async_compile.triton('triton_poi_fused__to_copy_288', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_288', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 768000}, 'kernel_num_gb': 0.0004608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_288(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 76800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((960, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((960, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 76800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_288.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_288.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pg/cpgd7ktnqxe2imlayf5n365kfwvf25p42cycm63kzqbvzjnok46b.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_527 : Tensor "f16[80, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_527]
#   %convert_element_type_493 : Tensor "f32[80, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_527, torch.float32), kwargs = {})
#   return %convert_element_type_493
triton_poi_fused__to_copy_289 = async_compile.triton('triton_poi_fused__to_copy_289', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_289', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 768000}, 'kernel_num_gb': 0.0004608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_289(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 76800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 76800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_289.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_289.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/n7/cn7t7gztp44qvcjbekibj3r23fnrtf2hda3saztgrfotqmdcus6w.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_635 : Tensor "f16[160, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_635]
#   %convert_element_type_581 : Tensor "f32[160, 624, 1, 1][624, 1, 624, 624]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_635, torch.float32), kwargs = {})
#   return %convert_element_type_581
triton_poi_fused__to_copy_290 = async_compile.triton('triton_poi_fused__to_copy_290', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_290', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 998400}, 'kernel_num_gb': 0.00059904, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_290(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 99840
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((160, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((160, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 99840,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_290.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_290.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00059904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/63/c63qu6m2ux4kwsy3b4qpleehr3y4clu5n7oktcv26ml4u6375hmv.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_440 : Tensor "f16[132, 792, 1, 1][792, 1, 792, 792]cuda:0" = PlaceHolder[target=getitem_440]
#   %convert_element_type_418 : Tensor "f32[132, 792, 1, 1][792, 1, 792, 792]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_440, torch.float32), kwargs = {})
#   return %convert_element_type_418
triton_poi_fused__to_copy_291 = async_compile.triton('triton_poi_fused__to_copy_291', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_291', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1045440}, 'kernel_num_gb': 0.000627264, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_291(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 104544
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 104544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_291.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_291.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000627264
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rr/crra3wrfw6sh2uwkrcsjprfllib2fe5bo2krpzolqh55olmhbkkd.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_443 : Tensor "f16[132, 792, 1, 1][792, 1, 792, 792]cuda:0" = PlaceHolder[target=getitem_443]
#   %convert_element_type_419 : Tensor "f32[132, 792, 1, 1][792, 1, 792, 792]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_443, torch.float32), kwargs = {})
#   return %convert_element_type_419
triton_poi_fused__to_copy_292 = async_compile.triton('triton_poi_fused__to_copy_292', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_292', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1045440}, 'kernel_num_gb': 0.000627264, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_292(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 104544
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 104544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_292.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_292.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000627264
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/et/cet3u4nvdkbf6tua5ypqvxpnbp5bytmikle6g5ird7iip2cvubic.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_542 : Tensor "f16[960, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=getitem_542]
#   %convert_element_type_505 : Tensor "f32[960, 160, 1, 1][160, 1, 160, 160]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_542, torch.float32), kwargs = {})
#   return %convert_element_type_505
triton_poi_fused__to_copy_293 = async_compile.triton('triton_poi_fused__to_copy_293', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_293', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1536000}, 'kernel_num_gb': 0.0009216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_293(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 153600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((960, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((960, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 153600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_293.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_293.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0009216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qx/cqxi7o6cti7bvcdghav643mjo6ivfxwwgsu5nyxhukpcej5am2mw.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_446 : Tensor "f16[1584, 132, 1, 1][132, 1, 132, 132]cuda:0" = PlaceHolder[target=getitem_446]
#   %convert_element_type_423 : Tensor "f32[1584, 132, 1, 1][132, 1, 132, 132]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_446, torch.float32), kwargs = {})
#   return %convert_element_type_423
triton_poi_fused__to_copy_294 = async_compile.triton('triton_poi_fused__to_copy_294', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_294', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2090880}, 'kernel_num_gb': 0.001254528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_294(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 209088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 209088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_294.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_294.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001254528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yz/cyzgxmrjdqn4kijuaogjwiddth3jqyqioqyuiekumuzhvllubftb.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_449 : Tensor "f16[132, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_449]
#   %convert_element_type_425 : Tensor "f32[132, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_449, torch.float32), kwargs = {})
#   return %convert_element_type_425
triton_poi_fused__to_copy_295 = async_compile.triton('triton_poi_fused__to_copy_295', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_295', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2090880}, 'kernel_num_gb': 0.001254528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_295(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 209088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 209088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_295.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_295.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001254528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lj/cljojbrjda27dek5ku7edcxwqkw62isquty4de7ruchvakolob6x.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_521 : Tensor "f16[264, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_521]
#   %convert_element_type_487 : Tensor "f32[264, 960, 1, 1][960, 1, 960, 960]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_521, torch.float32), kwargs = {})
#   return %convert_element_type_487
triton_poi_fused__to_copy_296 = async_compile.triton('triton_poi_fused__to_copy_296', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_296', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2534400}, 'kernel_num_gb': 0.00152064, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_296(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 253440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((264, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((264, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 253440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_296.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_296.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00152064
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cx/ccxhiozkovxqcklw7jnx3xro75vs6kq4dywzxeziepwue77rhvib.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_437 : Tensor "f16[1536, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=getitem_437]
#   %convert_element_type_414 : Tensor "f32[1536, 264, 1, 1][264, 1, 264, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_437, torch.float32), kwargs = {})
#   return %convert_element_type_414
triton_poi_fused__to_copy_297 = async_compile.triton('triton_poi_fused__to_copy_297', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 524288}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_297', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4055040}, 'kernel_num_gb': 0.002433024, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_297(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 405504
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, None)


def get_args():
    arg_0 = rand_strided((1536, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1536, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 405504,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_297.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_297.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.002433024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bt/cbtms2b4u3efx7hwbwr5msmgc5ffguehalnbzbsnwz5tgffiikgd.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %getitem_464 : Tensor "f16[1584, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=getitem_464]
#   %convert_element_type_437 : Tensor "f32[1584, 264, 1, 1][264, 1, 264, 264]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%getitem_464, torch.float32), kwargs = {})
#   return %convert_element_type_437
triton_poi_fused__to_copy_298 = async_compile.triton('triton_poi_fused__to_copy_298', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 524288}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_298', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4181760}, 'kernel_num_gb': 0.002509056, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_298(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 418176
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 418176,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_298.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_298.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.002509056
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dl/cdll7ly7yrcopcbysqxihfk3a3uv4bgnwzgckajknnpdq3mfsv5d.py
# Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
# Graph fragment:
#   %mm_1 : Tensor "f16[1000, 1536][1536, 1]cuda:0" = PlaceHolder[target=mm_1]
#   %convert_element_type_409 : Tensor "f32[1000, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mm_1, torch.float32), kwargs = {})
#   return %convert_element_type_409
triton_poi_fused__to_copy_299 = async_compile.triton('triton_poi_fused__to_copy_299', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_299', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15360000}, 'kernel_num_gb': 0.009216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_299(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1536000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, None)


def get_args():
    arg_0 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 1536000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_299.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_299.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        primals_6, primals_12, primals_18, primals_25, primals_26, primals_33, primals_34, primals_40, primals_47, primals_53, primals_54, primals_60, primals_66, primals_67, primals_75, primals_76, primals_85, primals_92, primals_93, primals_99, primals_100, primals_110, primals_117, primals_118, primals_124, primals_125, primals_135, primals_142, primals_143, primals_149, primals_150, primals_160, primals_166, primals_167, primals_174, primals_175, primals_184, primals_191, primals_192, primals_200, primals_201, primals_211, primals_218, primals_219, primals_227, primals_228, primals_238, primals_245, primals_246, primals_254, primals_255, primals_265, primals_271, primals_272, primals_277, primals_278, primals_287, primals_294, primals_295, primals_303, primals_304, primals_314, primals_321, primals_322, primals_330, primals_331, primals_341, primals_348, primals_349, primals_357, primals_358, primals_368, primals_374, primals_375, primals_383, primals_384, primals_393, primals_399, primals_400, primals_408, primals_409, primals_419, primals_425, primals_426, primals_434, primals_435, primals_445, primals_451, primals_452, primals_460, primals_461, primals_471, primals_477, primals_478, convert_element_type, convert_element_type_1, convolution, squeeze_1, relu, convert_element_type_4, convolution_1, squeeze_4, relu_1, convert_element_type_7, convolution_2, squeeze_7, getitem_6, getitem_7, convert_element_type_10, convert_element_type_11, cat, getitem_9, rsqrt_3, convert_element_type_14, getitem_13, convert_element_type_15, getitem_17, convert_element_type_16, getitem_21, cat_1, getitem_23, rsqrt_4, convert_element_type_19, getitem_26, convert_element_type_20, getitem_29, cat_2, squeeze_16, getitem_32, getitem_33, convert_element_type_23, convert_element_type_24, cat_3, squeeze_19, relu_4, convert_element_type_27, convolution_12, getitem_37, rsqrt_7, convert_element_type_30, getitem_40, convert_element_type_31, getitem_43, cat_4, squeeze_25, add_46, convert_element_type_34, convolution_15, getitem_47, rsqrt_9, convert_element_type_39, getitem_52, convert_element_type_40, getitem_57, convert_element_type_41, getitem_62, convert_element_type_42, getitem_67, cat_5, getitem_69, rsqrt_10, mean, convert_element_type_48, convolution_20, convert_element_type_50, convert_element_type_52, convolution_21, mul_80, convert_element_type_53, convolution_22, squeeze_34, getitem_72, getitem_73, convert_element_type_56, convert_element_type_57, cat_6, getitem_75, rsqrt_12, convert_element_type_62, getitem_78, convert_element_type_63, getitem_81, cat_7, getitem_83, rsqrt_13, mean_1, convert_element_type_69, convolution_27, convert_element_type_71, convert_element_type_73, convolution_28, getitem_84, getitem_85, convert_element_type_74, convert_element_type_75, cat_8, squeeze_43, getitem_88, getitem_89, convert_element_type_78, convert_element_type_79, cat_9, getitem_91, rsqrt_15, convert_element_type_84, getitem_94, convert_element_type_85, getitem_97, cat_10, getitem_99, rsqrt_16, mean_2, convert_element_type_91, convolution_35, convert_element_type_93, convert_element_type_95, convolution_36, getitem_100, getitem_101, convert_element_type_96, convert_element_type_97, cat_11, squeeze_52, getitem_104, getitem_105, convert_element_type_100, convert_element_type_101, cat_12, getitem_107, rsqrt_18, convert_element_type_106, getitem_110, convert_element_type_107, getitem_113, cat_13, getitem_115, rsqrt_19, mean_3, convert_element_type_113, convolution_43, convert_element_type_115, convert_element_type_117, convolution_44, getitem_116, getitem_117, convert_element_type_118, convert_element_type_119, cat_14, squeeze_61, add_109, convert_element_type_122, convolution_47, getitem_121, rsqrt_21, convert_element_type_127, getitem_125, convert_element_type_128, getitem_129, convert_element_type_129, getitem_133, cat_15, getitem_135, rsqrt_22, mean_4, convert_element_type_135, convolution_51, convert_element_type_137, convert_element_type_139, convolution_52, mul_180, convert_element_type_140, convolution_53, squeeze_70, getitem_138, getitem_139, convert_element_type_143, convert_element_type_144, cat_16, getitem_141, rsqrt_24, convert_element_type_149, getitem_146, convert_element_type_150, getitem_151, convert_element_type_151, getitem_156, convert_element_type_152, getitem_161, cat_17, getitem_163, rsqrt_25, mean_5, convert_element_type_158, convolution_60, convert_element_type_160, convert_element_type_162, convolution_61, getitem_164, getitem_165, convert_element_type_163, convert_element_type_164, cat_18, squeeze_79, getitem_168, getitem_169, convert_element_type_167, convert_element_type_168, cat_19, getitem_171, rsqrt_27, convert_element_type_173, getitem_176, convert_element_type_174, getitem_181, convert_element_type_175, getitem_186, convert_element_type_176, getitem_191, cat_20, getitem_193, rsqrt_28, mean_6, convert_element_type_182, convolution_70, convert_element_type_184, convert_element_type_186, convolution_71, getitem_194, getitem_195, convert_element_type_187, convert_element_type_188, cat_21, squeeze_88, getitem_198, getitem_199, convert_element_type_191, convert_element_type_192, cat_22, getitem_201, rsqrt_30, convert_element_type_197, getitem_206, convert_element_type_198, getitem_211, convert_element_type_199, getitem_216, convert_element_type_200, getitem_221, cat_23, getitem_223, rsqrt_31, mean_7, convert_element_type_206, convolution_80, convert_element_type_208, convert_element_type_210, convolution_81, getitem_224, getitem_225, convert_element_type_211, convert_element_type_212, cat_24, squeeze_97, add_172, convert_element_type_215, convolution_84, getitem_229, rsqrt_33, convert_element_type_219, convert_element_type_220, convolution_85, getitem_231, rsqrt_34, mean_8, convert_element_type_226, convolution_86, convert_element_type_228, convert_element_type_230, convolution_87, mul_280, convert_element_type_231, convolution_88, squeeze_106, getitem_234, getitem_235, convert_element_type_234, convert_element_type_235, cat_25, getitem_237, rsqrt_36, convert_element_type_240, getitem_242, convert_element_type_241, getitem_247, convert_element_type_242, getitem_252, convert_element_type_243, getitem_257, cat_26, getitem_259, rsqrt_37, mean_9, convert_element_type_249, convolution_95, convert_element_type_251, convert_element_type_253, convolution_96, getitem_260, getitem_261, convert_element_type_254, convert_element_type_255, cat_27, squeeze_115, getitem_264, getitem_265, convert_element_type_258, convert_element_type_259, cat_28, getitem_267, rsqrt_39, convert_element_type_264, getitem_272, convert_element_type_265, getitem_277, convert_element_type_266, getitem_282, convert_element_type_267, getitem_287, cat_29, getitem_289, rsqrt_40, mean_10, convert_element_type_273, convolution_105, convert_element_type_275, convert_element_type_277, convolution_106, getitem_290, getitem_291, convert_element_type_278, convert_element_type_279, cat_30, squeeze_124, getitem_294, getitem_295, convert_element_type_282, convert_element_type_283, cat_31, getitem_297, rsqrt_42, convert_element_type_288, getitem_302, convert_element_type_289, getitem_307, convert_element_type_290, getitem_312, convert_element_type_291, getitem_317, cat_32, getitem_319, rsqrt_43, mean_11, convert_element_type_297, convolution_115, convert_element_type_299, convert_element_type_301, convolution_116, getitem_320, getitem_321, convert_element_type_302, convert_element_type_303, cat_33, squeeze_133, add_235, convert_element_type_306, convolution_119, getitem_325, rsqrt_45, convert_element_type_311, getitem_330, convert_element_type_312, getitem_335, convert_element_type_313, getitem_340, convert_element_type_314, getitem_345, cat_34, getitem_347, rsqrt_46, mean_12, convert_element_type_320, convolution_124, convert_element_type_322, convert_element_type_324, convolution_125, mul_380, convert_element_type_325, convolution_126, squeeze_142, convert_element_type_327, convert_element_type_328, convolution_127, getitem_351, rsqrt_48, convert_element_type_333, getitem_356, convert_element_type_334, getitem_361, convert_element_type_335, getitem_366, convert_element_type_336, getitem_371, cat_35, getitem_373, rsqrt_49, mean_13, convert_element_type_342, convolution_132, convert_element_type_344, convert_element_type_346, convolution_133, getitem_374, getitem_375, convert_element_type_347, convert_element_type_348, cat_36, squeeze_151, add_266, convert_element_type_351, convolution_136, getitem_379, rsqrt_51, convert_element_type_356, getitem_384, convert_element_type_357, getitem_389, convert_element_type_358, getitem_394, convert_element_type_359, getitem_399, cat_37, getitem_401, rsqrt_52, mean_14, convert_element_type_365, convolution_141, convert_element_type_367, convert_element_type_369, convolution_142, getitem_402, getitem_403, convert_element_type_370, convert_element_type_371, cat_38, squeeze_160, add_282, convert_element_type_374, convolution_145, getitem_407, rsqrt_54, convert_element_type_379, getitem_412, convert_element_type_380, getitem_417, convert_element_type_381, getitem_422, convert_element_type_382, getitem_427, cat_39, getitem_429, rsqrt_55, mean_15, convert_element_type_388, convolution_150, convert_element_type_390, convert_element_type_392, convolution_151, getitem_430, getitem_431, convert_element_type_393, convert_element_type_394, cat_40, squeeze_169, add_298, convert_element_type_397, convolution_154, getitem_435, rsqrt_57, view, permute_1, unsqueeze_246, unsqueeze_282, unsqueeze_318, unsqueeze_354, unsqueeze_390, unsqueeze_426, unsqueeze_462, unsqueeze_498, unsqueeze_534, unsqueeze_570, unsqueeze_606, unsqueeze_642, unsqueeze_678, unsqueeze_714, unsqueeze_750, unsqueeze_786, unsqueeze_822, unsqueeze_846, unsqueeze_858, unsqueeze_894, unsqueeze_906, unsqueeze_918, tangents_1 = args
        args.clear()
        assert_size_stride(primals_6, (32, ), (1, ))
        assert_size_stride(primals_12, (32, ), (1, ))
        assert_size_stride(primals_18, (32, ), (1, ))
        assert_size_stride(primals_25, (192, ), (1, ))
        assert_size_stride(primals_26, (192, ), (1, ))
        assert_size_stride(primals_33, (192, ), (1, ))
        assert_size_stride(primals_34, (192, ), (1, ))
        assert_size_stride(primals_40, (40, ), (1, ))
        assert_size_stride(primals_47, (120, ), (1, ))
        assert_size_stride(primals_53, (120, ), (1, ))
        assert_size_stride(primals_54, (120, ), (1, ))
        assert_size_stride(primals_60, (40, ), (1, ))
        assert_size_stride(primals_66, (240, ), (1, ))
        assert_size_stride(primals_67, (240, ), (1, ))
        assert_size_stride(primals_75, (240, ), (1, ))
        assert_size_stride(primals_76, (240, ), (1, ))
        assert_size_stride(primals_85, (56, ), (1, ))
        assert_size_stride(primals_92, (336, ), (1, ))
        assert_size_stride(primals_93, (336, ), (1, ))
        assert_size_stride(primals_99, (336, ), (1, ))
        assert_size_stride(primals_100, (336, ), (1, ))
        assert_size_stride(primals_110, (56, ), (1, ))
        assert_size_stride(primals_117, (336, ), (1, ))
        assert_size_stride(primals_118, (336, ), (1, ))
        assert_size_stride(primals_124, (336, ), (1, ))
        assert_size_stride(primals_125, (336, ), (1, ))
        assert_size_stride(primals_135, (56, ), (1, ))
        assert_size_stride(primals_142, (336, ), (1, ))
        assert_size_stride(primals_143, (336, ), (1, ))
        assert_size_stride(primals_149, (336, ), (1, ))
        assert_size_stride(primals_150, (336, ), (1, ))
        assert_size_stride(primals_160, (56, ), (1, ))
        assert_size_stride(primals_166, (336, ), (1, ))
        assert_size_stride(primals_167, (336, ), (1, ))
        assert_size_stride(primals_174, (336, ), (1, ))
        assert_size_stride(primals_175, (336, ), (1, ))
        assert_size_stride(primals_184, (104, ), (1, ))
        assert_size_stride(primals_191, (624, ), (1, ))
        assert_size_stride(primals_192, (624, ), (1, ))
        assert_size_stride(primals_200, (624, ), (1, ))
        assert_size_stride(primals_201, (624, ), (1, ))
        assert_size_stride(primals_211, (104, ), (1, ))
        assert_size_stride(primals_218, (624, ), (1, ))
        assert_size_stride(primals_219, (624, ), (1, ))
        assert_size_stride(primals_227, (624, ), (1, ))
        assert_size_stride(primals_228, (624, ), (1, ))
        assert_size_stride(primals_238, (104, ), (1, ))
        assert_size_stride(primals_245, (624, ), (1, ))
        assert_size_stride(primals_246, (624, ), (1, ))
        assert_size_stride(primals_254, (624, ), (1, ))
        assert_size_stride(primals_255, (624, ), (1, ))
        assert_size_stride(primals_265, (104, ), (1, ))
        assert_size_stride(primals_271, (624, ), (1, ))
        assert_size_stride(primals_272, (624, ), (1, ))
        assert_size_stride(primals_277, (624, ), (1, ))
        assert_size_stride(primals_278, (624, ), (1, ))
        assert_size_stride(primals_287, (160, ), (1, ))
        assert_size_stride(primals_294, (480, ), (1, ))
        assert_size_stride(primals_295, (480, ), (1, ))
        assert_size_stride(primals_303, (480, ), (1, ))
        assert_size_stride(primals_304, (480, ), (1, ))
        assert_size_stride(primals_314, (160, ), (1, ))
        assert_size_stride(primals_321, (480, ), (1, ))
        assert_size_stride(primals_322, (480, ), (1, ))
        assert_size_stride(primals_330, (480, ), (1, ))
        assert_size_stride(primals_331, (480, ), (1, ))
        assert_size_stride(primals_341, (160, ), (1, ))
        assert_size_stride(primals_348, (480, ), (1, ))
        assert_size_stride(primals_349, (480, ), (1, ))
        assert_size_stride(primals_357, (480, ), (1, ))
        assert_size_stride(primals_358, (480, ), (1, ))
        assert_size_stride(primals_368, (160, ), (1, ))
        assert_size_stride(primals_374, (960, ), (1, ))
        assert_size_stride(primals_375, (960, ), (1, ))
        assert_size_stride(primals_383, (960, ), (1, ))
        assert_size_stride(primals_384, (960, ), (1, ))
        assert_size_stride(primals_393, (264, ), (1, ))
        assert_size_stride(primals_399, (1584, ), (1, ))
        assert_size_stride(primals_400, (1584, ), (1, ))
        assert_size_stride(primals_408, (1584, ), (1, ))
        assert_size_stride(primals_409, (1584, ), (1, ))
        assert_size_stride(primals_419, (264, ), (1, ))
        assert_size_stride(primals_425, (1584, ), (1, ))
        assert_size_stride(primals_426, (1584, ), (1, ))
        assert_size_stride(primals_434, (1584, ), (1, ))
        assert_size_stride(primals_435, (1584, ), (1, ))
        assert_size_stride(primals_445, (264, ), (1, ))
        assert_size_stride(primals_451, (1584, ), (1, ))
        assert_size_stride(primals_452, (1584, ), (1, ))
        assert_size_stride(primals_460, (1584, ), (1, ))
        assert_size_stride(primals_461, (1584, ), (1, ))
        assert_size_stride(primals_471, (264, ), (1, ))
        assert_size_stride(primals_477, (1536, ), (1, ))
        assert_size_stride(primals_478, (1536, ), (1, ))
        assert_size_stride(convert_element_type, (32, 3, 3, 3), (27, 1, 9, 3))
        assert_size_stride(convert_element_type_1, (128, 3, 224, 224), (150528, 1, 672, 3))
        assert_size_stride(convolution, (128, 32, 112, 112), (401408, 1, 3584, 32))
        assert_size_stride(squeeze_1, (32, ), (1, ))
        assert_size_stride(relu, (128, 32, 112, 112), (401408, 1, 3584, 32))
        assert_size_stride(convert_element_type_4, (32, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(convolution_1, (128, 32, 112, 112), (401408, 1, 3584, 32))
        assert_size_stride(squeeze_4, (32, ), (1, ))
        assert_size_stride(relu_1, (128, 32, 112, 112), (401408, 1, 3584, 32))
        assert_size_stride(convert_element_type_7, (32, 32, 1, 1), (32, 1, 32, 32))
        assert_size_stride(convolution_2, (128, 32, 112, 112), (401408, 1, 3584, 32))
        assert_size_stride(squeeze_7, (32, ), (1, ))
        assert_size_stride(getitem_6, (128, 16, 112, 112), (401408, 12544, 112, 1))
        assert_size_stride(getitem_7, (128, 16, 112, 112), (401408, 12544, 112, 1))
        assert_size_stride(convert_element_type_10, (96, 16, 1, 1), (16, 1, 16, 16))
        assert_size_stride(convert_element_type_11, (96, 16, 1, 1), (16, 1, 16, 16))
        assert_size_stride(cat, (128, 192, 112, 112), (2408448, 1, 21504, 192))
        assert_size_stride(getitem_9, (1, 192, 1, 1), (192, 1, 192, 192))
        assert_size_stride(rsqrt_3, (1, 192, 1, 1), (192, 1, 192, 192))
        assert_size_stride(convert_element_type_14, (64, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_13, (128, 64, 112, 112), (2408448, 12544, 112, 1))
        assert_size_stride(convert_element_type_15, (64, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_17, (128, 64, 112, 112), (2408448, 12544, 112, 1))
        assert_size_stride(convert_element_type_16, (64, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_21, (128, 64, 112, 112), (2408448, 12544, 112, 1))
        assert_size_stride(cat_1, (128, 192, 56, 56), (602112, 1, 10752, 192))
        assert_size_stride(getitem_23, (1, 192, 1, 1), (192, 1, 192, 192))
        assert_size_stride(rsqrt_4, (1, 192, 1, 1), (192, 1, 192, 192))
        assert_size_stride(convert_element_type_19, (20, 96, 1, 1), (96, 1, 96, 96))
        assert_size_stride(getitem_26, (128, 96, 56, 56), (602112, 3136, 56, 1))
        assert_size_stride(convert_element_type_20, (20, 96, 1, 1), (96, 1, 96, 96))
        assert_size_stride(getitem_29, (128, 96, 56, 56), (602112, 3136, 56, 1))
        assert_size_stride(cat_2, (128, 40, 56, 56), (125440, 1, 2240, 40))
        assert_size_stride(squeeze_16, (40, ), (1, ))
        assert_size_stride(getitem_32, (128, 20, 56, 56), (62720, 3136, 56, 1))
        assert_size_stride(getitem_33, (128, 20, 56, 56), (62720, 3136, 56, 1))
        assert_size_stride(convert_element_type_23, (60, 20, 1, 1), (20, 1, 20, 20))
        assert_size_stride(convert_element_type_24, (60, 20, 1, 1), (20, 1, 20, 20))
        assert_size_stride(cat_3, (128, 120, 56, 56), (376320, 1, 6720, 120))
        assert_size_stride(squeeze_19, (120, ), (1, ))
        assert_size_stride(relu_4, (128, 120, 56, 56), (376320, 1, 6720, 120))
        assert_size_stride(convert_element_type_27, (120, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(convolution_12, (128, 120, 56, 56), (376320, 1, 6720, 120))
        assert_size_stride(getitem_37, (1, 120, 1, 1), (120, 1, 120, 120))
        assert_size_stride(rsqrt_7, (1, 120, 1, 1), (120, 1, 120, 120))
        assert_size_stride(convert_element_type_30, (20, 60, 1, 1), (60, 1, 60, 60))
        assert_size_stride(getitem_40, (128, 60, 56, 56), (376320, 3136, 56, 1))
        assert_size_stride(convert_element_type_31, (20, 60, 1, 1), (60, 1, 60, 60))
        assert_size_stride(getitem_43, (128, 60, 56, 56), (376320, 3136, 56, 1))
        assert_size_stride(cat_4, (128, 40, 56, 56), (125440, 1, 2240, 40))
        assert_size_stride(squeeze_25, (40, ), (1, ))
        assert_size_stride(add_46, (128, 40, 56, 56), (125440, 1, 2240, 40))
        assert_size_stride(convert_element_type_34, (240, 40, 1, 1), (40, 1, 40, 40))
        assert_size_stride(convolution_15, (128, 240, 56, 56), (752640, 1, 13440, 240))
        assert_size_stride(getitem_47, (1, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(rsqrt_9, (1, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convert_element_type_39, (60, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_52, (128, 60, 56, 56), (752640, 3136, 56, 1))
        assert_size_stride(convert_element_type_40, (60, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_57, (128, 60, 56, 56), (752640, 3136, 56, 1))
        assert_size_stride(convert_element_type_41, (60, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_62, (128, 60, 56, 56), (752640, 3136, 56, 1))
        assert_size_stride(convert_element_type_42, (60, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_67, (128, 60, 56, 56), (752640, 3136, 56, 1))
        assert_size_stride(cat_5, (128, 240, 28, 28), (188160, 1, 6720, 240))
        assert_size_stride(getitem_69, (1, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(rsqrt_10, (1, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(mean, (128, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convert_element_type_48, (20, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convolution_20, (128, 20, 1, 1), (20, 1, 20, 20))
        assert_size_stride(convert_element_type_50, (128, 20, 1, 1), (20, 1, 20, 20))
        assert_size_stride(convert_element_type_52, (240, 20, 1, 1), (20, 1, 20, 20))
        assert_size_stride(convolution_21, (128, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(mul_80, (128, 240, 28, 28), (188160, 1, 6720, 240))
        assert_size_stride(convert_element_type_53, (56, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convolution_22, (128, 56, 28, 28), (43904, 1, 1568, 56))
        assert_size_stride(squeeze_34, (56, ), (1, ))
        assert_size_stride(getitem_72, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(getitem_73, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(convert_element_type_56, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_57, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(cat_6, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_75, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_12, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_62, (168, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_78, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_63, (168, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_81, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(cat_7, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_83, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_13, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(mean_1, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_69, (28, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convolution_27, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_71, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_73, (336, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convolution_28, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(getitem_84, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(getitem_85, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_74, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(convert_element_type_75, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(cat_8, (128, 56, 28, 28), (43904, 1, 1568, 56))
        assert_size_stride(squeeze_43, (56, ), (1, ))
        assert_size_stride(getitem_88, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(getitem_89, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(convert_element_type_78, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_79, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(cat_9, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_91, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_15, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_84, (168, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_94, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_85, (168, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_97, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(cat_10, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_99, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_16, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(mean_2, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_91, (28, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convolution_35, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_93, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_95, (336, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convolution_36, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(getitem_100, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(getitem_101, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_96, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(convert_element_type_97, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(cat_11, (128, 56, 28, 28), (43904, 1, 1568, 56))
        assert_size_stride(squeeze_52, (56, ), (1, ))
        assert_size_stride(getitem_104, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(getitem_105, (128, 28, 28, 28), (21952, 784, 28, 1))
        assert_size_stride(convert_element_type_100, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_101, (168, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(cat_12, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_107, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_18, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_106, (168, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_110, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_107, (168, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_113, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(cat_13, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_115, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_19, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(mean_3, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_113, (28, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convolution_43, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_115, (128, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convert_element_type_117, (336, 28, 1, 1), (28, 1, 28, 28))
        assert_size_stride(convolution_44, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(getitem_116, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(getitem_117, (128, 168, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_118, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(convert_element_type_119, (28, 168, 1, 1), (168, 1, 168, 168))
        assert_size_stride(cat_14, (128, 56, 28, 28), (43904, 1, 1568, 56))
        assert_size_stride(squeeze_61, (56, ), (1, ))
        assert_size_stride(add_109, (128, 56, 28, 28), (43904, 1, 1568, 56))
        assert_size_stride(convert_element_type_122, (336, 56, 1, 1), (56, 1, 56, 56))
        assert_size_stride(convolution_47, (128, 336, 28, 28), (263424, 1, 9408, 336))
        assert_size_stride(getitem_121, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_21, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_127, (112, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_125, (128, 112, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_128, (112, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_129, (128, 112, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(convert_element_type_129, (112, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_133, (128, 112, 28, 28), (263424, 784, 28, 1))
        assert_size_stride(cat_15, (128, 336, 14, 14), (65856, 1, 4704, 336))
        assert_size_stride(getitem_135, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(rsqrt_22, (1, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(mean_4, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convert_element_type_135, (14, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convolution_51, (128, 14, 1, 1), (14, 1, 14, 14))
        assert_size_stride(convert_element_type_137, (128, 14, 1, 1), (14, 1, 14, 14))
        assert_size_stride(convert_element_type_139, (336, 14, 1, 1), (14, 1, 14, 14))
        assert_size_stride(convolution_52, (128, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(mul_180, (128, 336, 14, 14), (65856, 1, 4704, 336))
        assert_size_stride(convert_element_type_140, (104, 336, 1, 1), (336, 1, 336, 336))
        assert_size_stride(convolution_53, (128, 104, 14, 14), (20384, 1, 1456, 104))
        assert_size_stride(squeeze_70, (104, ), (1, ))
        assert_size_stride(getitem_138, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(getitem_139, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(convert_element_type_143, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convert_element_type_144, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(cat_16, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_141, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_24, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_149, (156, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_146, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_150, (156, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_151, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_151, (156, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_156, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_152, (156, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_161, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(cat_17, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_163, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_25, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(mean_5, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_158, (26, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convolution_60, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_160, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_162, (624, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convolution_61, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(getitem_164, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(getitem_165, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_163, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(convert_element_type_164, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(cat_18, (128, 104, 14, 14), (20384, 1, 1456, 104))
        assert_size_stride(squeeze_79, (104, ), (1, ))
        assert_size_stride(getitem_168, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(getitem_169, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(convert_element_type_167, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convert_element_type_168, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(cat_19, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_171, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_27, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_173, (156, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_176, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_174, (156, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_181, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_175, (156, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_186, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_176, (156, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_191, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(cat_20, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_193, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_28, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(mean_6, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_182, (26, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convolution_70, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_184, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_186, (624, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convolution_71, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(getitem_194, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(getitem_195, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_187, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(convert_element_type_188, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(cat_21, (128, 104, 14, 14), (20384, 1, 1456, 104))
        assert_size_stride(squeeze_88, (104, ), (1, ))
        assert_size_stride(getitem_198, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(getitem_199, (128, 52, 14, 14), (10240, 196, 14, 1))
        assert_size_stride(convert_element_type_191, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convert_element_type_192, (312, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(cat_22, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_201, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_30, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_197, (156, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_206, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_198, (156, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_211, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_199, (156, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_216, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_200, (156, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_221, (128, 156, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(cat_23, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_223, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_31, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(mean_7, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_206, (26, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convolution_80, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_208, (128, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convert_element_type_210, (624, 26, 1, 1), (26, 1, 26, 26))
        assert_size_stride(convolution_81, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(getitem_224, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(getitem_225, (128, 312, 14, 14), (122304, 196, 14, 1))
        assert_size_stride(convert_element_type_211, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(convert_element_type_212, (52, 312, 1, 1), (312, 1, 312, 312))
        assert_size_stride(cat_24, (128, 104, 14, 14), (20384, 1, 1456, 104))
        assert_size_stride(squeeze_97, (104, ), (1, ))
        assert_size_stride(add_172, (128, 104, 14, 14), (20384, 1, 1456, 104))
        assert_size_stride(convert_element_type_215, (624, 104, 1, 1), (104, 1, 104, 104))
        assert_size_stride(convolution_84, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_229, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_33, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_219, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(convert_element_type_220, (624, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(convolution_85, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(getitem_231, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(rsqrt_34, (1, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(mean_8, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convert_element_type_226, (52, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convolution_86, (128, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convert_element_type_228, (128, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convert_element_type_230, (624, 52, 1, 1), (52, 1, 52, 52))
        assert_size_stride(convolution_87, (128, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(mul_280, (128, 624, 14, 14), (122304, 1, 8736, 624))
        assert_size_stride(convert_element_type_231, (160, 624, 1, 1), (624, 1, 624, 624))
        assert_size_stride(convolution_88, (128, 160, 14, 14), (31360, 1, 2240, 160))
        assert_size_stride(squeeze_106, (160, ), (1, ))
        assert_size_stride(getitem_234, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(getitem_235, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(convert_element_type_234, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_235, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(cat_25, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_237, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_36, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_240, (120, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_242, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_241, (120, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_247, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_242, (120, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_252, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_243, (120, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_257, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(cat_26, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_259, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_37, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(mean_9, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_249, (80, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convolution_95, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_251, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_253, (480, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convolution_96, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(getitem_260, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(getitem_261, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_254, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convert_element_type_255, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(cat_27, (128, 160, 14, 14), (31360, 1, 2240, 160))
        assert_size_stride(squeeze_115, (160, ), (1, ))
        assert_size_stride(getitem_264, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(getitem_265, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(convert_element_type_258, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_259, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(cat_28, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_267, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_39, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_264, (120, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_272, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_265, (120, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_277, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_266, (120, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_282, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_267, (120, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_287, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(cat_29, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_289, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_40, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(mean_10, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_273, (80, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convolution_105, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_275, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_277, (480, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convolution_106, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(getitem_290, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(getitem_291, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_278, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convert_element_type_279, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(cat_30, (128, 160, 14, 14), (31360, 1, 2240, 160))
        assert_size_stride(squeeze_124, (160, ), (1, ))
        assert_size_stride(getitem_294, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(getitem_295, (128, 80, 14, 14), (15680, 196, 14, 1))
        assert_size_stride(convert_element_type_282, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_283, (240, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(cat_31, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_297, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_42, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_288, (120, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_302, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_289, (120, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_307, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_290, (120, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_312, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_291, (120, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_317, (128, 120, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(cat_32, (128, 480, 14, 14), (94080, 1, 6720, 480))
        assert_size_stride(getitem_319, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(rsqrt_43, (1, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(mean_11, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convert_element_type_297, (80, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(convolution_115, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_299, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_301, (480, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convolution_116, (128, 480, 1, 1), (480, 1, 480, 480))
        assert_size_stride(getitem_320, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(getitem_321, (128, 240, 14, 14), (94080, 196, 14, 1))
        assert_size_stride(convert_element_type_302, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(convert_element_type_303, (80, 240, 1, 1), (240, 1, 240, 240))
        assert_size_stride(cat_33, (128, 160, 14, 14), (31360, 1, 2240, 160))
        assert_size_stride(squeeze_133, (160, ), (1, ))
        assert_size_stride(add_235, (128, 160, 14, 14), (31360, 1, 2240, 160))
        assert_size_stride(convert_element_type_306, (960, 160, 1, 1), (160, 1, 160, 160))
        assert_size_stride(convolution_119, (128, 960, 14, 14), (188160, 1, 13440, 960))
        assert_size_stride(getitem_325, (1, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(rsqrt_45, (1, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(convert_element_type_311, (240, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_330, (128, 240, 14, 14), (188160, 196, 14, 1))
        assert_size_stride(convert_element_type_312, (240, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_335, (128, 240, 14, 14), (188160, 196, 14, 1))
        assert_size_stride(convert_element_type_313, (240, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_340, (128, 240, 14, 14), (188160, 196, 14, 1))
        assert_size_stride(convert_element_type_314, (240, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_345, (128, 240, 14, 14), (188160, 196, 14, 1))
        assert_size_stride(cat_34, (128, 960, 7, 7), (47040, 1, 6720, 960))
        assert_size_stride(getitem_347, (1, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(rsqrt_46, (1, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(mean_12, (128, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(convert_element_type_320, (80, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(convolution_124, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_322, (128, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convert_element_type_324, (960, 80, 1, 1), (80, 1, 80, 80))
        assert_size_stride(convolution_125, (128, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(mul_380, (128, 960, 7, 7), (47040, 1, 6720, 960))
        assert_size_stride(convert_element_type_325, (264, 960, 1, 1), (960, 1, 960, 960))
        assert_size_stride(convolution_126, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(squeeze_142, (264, ), (1, ))
        assert_size_stride(convert_element_type_327, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(convert_element_type_328, (1584, 264, 1, 1), (264, 1, 264, 264))
        assert_size_stride(convolution_127, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_351, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_48, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_333, (396, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_356, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_334, (396, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_361, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_335, (396, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_366, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_336, (396, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_371, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(cat_35, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_373, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_49, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(mean_13, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_342, (132, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convolution_132, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_344, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_346, (1584, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convolution_133, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(getitem_374, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(getitem_375, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_347, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(convert_element_type_348, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(cat_36, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(squeeze_151, (264, ), (1, ))
        assert_size_stride(add_266, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(convert_element_type_351, (1584, 264, 1, 1), (264, 1, 264, 264))
        assert_size_stride(convolution_136, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_379, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_51, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_356, (396, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_384, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_357, (396, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_389, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_358, (396, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_394, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_359, (396, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_399, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(cat_37, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_401, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_52, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(mean_14, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_365, (132, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convolution_141, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_367, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_369, (1584, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convolution_142, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(getitem_402, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(getitem_403, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_370, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(convert_element_type_371, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(cat_38, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(squeeze_160, (264, ), (1, ))
        assert_size_stride(add_282, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(convert_element_type_374, (1584, 264, 1, 1), (264, 1, 264, 264))
        assert_size_stride(convolution_145, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_407, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_54, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_379, (396, 1, 3, 3), (9, 1, 3, 1))
        assert_size_stride(getitem_412, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_380, (396, 1, 5, 5), (25, 1, 5, 1))
        assert_size_stride(getitem_417, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_381, (396, 1, 7, 7), (49, 1, 7, 1))
        assert_size_stride(getitem_422, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_382, (396, 1, 9, 9), (81, 1, 9, 1))
        assert_size_stride(getitem_427, (128, 396, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(cat_39, (128, 1584, 7, 7), (77616, 1, 11088, 1584))
        assert_size_stride(getitem_429, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(rsqrt_55, (1, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(mean_15, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convert_element_type_388, (132, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(convolution_150, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_390, (128, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convert_element_type_392, (1584, 132, 1, 1), (132, 1, 132, 132))
        assert_size_stride(convolution_151, (128, 1584, 1, 1), (1584, 1, 1584, 1584))
        assert_size_stride(getitem_430, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(getitem_431, (128, 792, 7, 7), (77632, 49, 7, 1))
        assert_size_stride(convert_element_type_393, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(convert_element_type_394, (132, 792, 1, 1), (792, 1, 792, 792))
        assert_size_stride(cat_40, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(squeeze_169, (264, ), (1, ))
        assert_size_stride(add_298, (128, 264, 7, 7), (12936, 1, 1848, 264))
        assert_size_stride(convert_element_type_397, (1536, 264, 1, 1), (264, 1, 264, 264))
        assert_size_stride(convolution_154, (128, 1536, 7, 7), (75264, 1, 10752, 1536))
        assert_size_stride(getitem_435, (1, 1536, 1, 1), (1536, 1, 1536, 1536))
        assert_size_stride(rsqrt_57, (1, 1536, 1, 1), (1536, 1, 1536, 1536))
        assert_size_stride(view, (128, 1536), (1536, 1))
        assert_size_stride(permute_1, (1000, 1536), (1536, 1))
        assert_size_stride(unsqueeze_246, (1, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(unsqueeze_282, (1, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(unsqueeze_318, (1, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(unsqueeze_354, (1, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(unsqueeze_390, (1, 160, 1, 1), (160, 1, 1, 1))
        assert_size_stride(unsqueeze_426, (1, 160, 1, 1), (160, 1, 1, 1))
        assert_size_stride(unsqueeze_462, (1, 160, 1, 1), (160, 1, 1, 1))
        assert_size_stride(unsqueeze_498, (1, 160, 1, 1), (160, 1, 1, 1))
        assert_size_stride(unsqueeze_534, (1, 104, 1, 1), (104, 1, 1, 1))
        assert_size_stride(unsqueeze_570, (1, 104, 1, 1), (104, 1, 1, 1))
        assert_size_stride(unsqueeze_606, (1, 104, 1, 1), (104, 1, 1, 1))
        assert_size_stride(unsqueeze_642, (1, 104, 1, 1), (104, 1, 1, 1))
        assert_size_stride(unsqueeze_678, (1, 56, 1, 1), (56, 1, 1, 1))
        assert_size_stride(unsqueeze_714, (1, 56, 1, 1), (56, 1, 1, 1))
        assert_size_stride(unsqueeze_750, (1, 56, 1, 1), (56, 1, 1, 1))
        assert_size_stride(unsqueeze_786, (1, 56, 1, 1), (56, 1, 1, 1))
        assert_size_stride(unsqueeze_822, (1, 40, 1, 1), (40, 1, 1, 1))
        assert_size_stride(unsqueeze_846, (1, 120, 1, 1), (120, 1, 1, 1))
        assert_size_stride(unsqueeze_858, (1, 40, 1, 1), (40, 1, 1, 1))
        assert_size_stride(unsqueeze_894, (1, 32, 1, 1), (32, 1, 1, 1))
        assert_size_stride(unsqueeze_906, (1, 32, 1, 1), (32, 1, 1, 1))
        assert_size_stride(unsqueeze_918, (1, 32, 1, 1), (32, 1, 1, 1))
        assert_size_stride(tangents_1, (128, 1000), (1000, 1))
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((128, 1536), (1536, 1), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
            extern_kernels.mm(tangents_1, permute_1, out=buf0)
            del permute_1
            buf5 = empty_strided_cuda((128, 1536, 7, 7), (75264, 1, 10752, 1536), torch.float32)
            # Topologically Sorted Source Nodes: [x_183, x_184], Original ATen: [aten.view, aten.expand, aten.div, aten._native_batch_norm_legit_functional, aten.relu, aten.threshold_backward, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_div_expand_native_batch_norm_backward_relu_threshold_backward_view_0.run(convolution_154, getitem_435, rsqrt_57, primals_477, primals_478, buf0, buf5, 9633792, stream=stream0)
            del buf0
            del primals_478
            buf6 = empty_strided_cuda((1536, 49), (1, 1536), torch.float32)
            buf8 = empty_strided_cuda((1536, 49), (1, 1536), torch.float32)
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_1.run(buf5, convolution_154, getitem_435, buf6, buf8, 75264, 128, stream=stream0)
            buf7 = empty_strided_cuda((1536, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_2.run(buf6, buf7, 1536, 49, stream=stream0)
            del buf6
            buf9 = empty_strided_cuda((1536, ), (1, ), torch.float32)
            buf10 = empty_strided_cuda((1536, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_3.run(buf8, rsqrt_57, buf9, buf10, 1536, 49, stream=stream0)
            del buf8
            buf11 = convolution_154; del convolution_154  # reuse
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_4.run(buf11, buf5, getitem_435, buf9, rsqrt_57, buf7, primals_477, 9633792, stream=stream0)
            del buf5
            del getitem_435
            del primals_477
            del rsqrt_57
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
            buf12 = torch.ops.aten.convolution_backward.default(buf11, add_298, convert_element_type_397, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_298
            del buf11
            del convert_element_type_397
            buf13 = buf12[0]
            assert_size_stride(buf13, (128, 264, 7, 7), (12936, 1, 1848, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf13, 16, 'torch.ops.aten.convolution_backward.default')
            buf14 = buf12[1]
            assert_size_stride(buf14, (1536, 264, 1, 1), (264, 1, 264, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf14, 16, 'torch.ops.aten.convolution_backward.default')
            del buf12
            buf16 = empty_strided_cuda((264, 49), (1, 264), torch.float32)
            buf18 = empty_strided_cuda((264, 49), (1, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_5.run(buf13, cat_40, unsqueeze_246, buf16, buf18, 12936, 128, stream=stream0)
            buf17 = empty_strided_cuda((264, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_6.run(buf16, buf17, 264, 49, stream=stream0)
            buf19 = empty_strided_cuda((264, ), (1, ), torch.float32)
            buf20 = empty_strided_cuda((264, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.run(buf18, squeeze_169, buf19, buf20, 264, 49, stream=stream0)
            buf21 = cat_40; del cat_40  # reuse
            # Topologically Sorted Source Nodes: [x_180], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_8.run(buf21, buf13, unsqueeze_246, buf19, squeeze_169, buf17, primals_471, 1655808, stream=stream0)
            del primals_471
            del squeeze_169
            del unsqueeze_246
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf22 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf21, (128, 132, 7, 7), (12936, 1, 1848, 264), 132), getitem_431, convert_element_type_394, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_394
            del getitem_431
            buf23 = buf22[0]
            assert_size_stride(buf23, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            # buffer buf23 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf24 = buf22[1]
            assert_size_stride(buf24, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            # buffer buf24 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf22
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf26 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf21, (128, 132, 7, 7), (12936, 1, 1848, 264), 0), getitem_430, convert_element_type_393, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf21
            del convert_element_type_393
            del getitem_430
            buf27 = buf26[0]
            assert_size_stride(buf27, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf27, 16, 'torch.ops.aten.convolution_backward.default')
            buf28 = buf26[1]
            assert_size_stride(buf28, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf28, 16, 'torch.ops.aten.convolution_backward.default')
            del buf26
            buf30 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_9.run(cat_39, getitem_429, rsqrt_55, primals_460, primals_461, buf30, 9934848, stream=stream0)
            del primals_461
            buf31 = empty_strided_cuda((128, 1584, 1, 1), (1584, 1, 202752, 202752), torch.float16)
            buf32 = reinterpret_tensor(buf31, (128, 1584, 1, 1), (1584, 1, 1, 1), 0); del buf31  # reuse
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10.run(buf32, buf27, buf23, buf30, convolution_151, 202752, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf34 = torch.ops.aten.convolution_backward.default(buf32, convert_element_type_390, convert_element_type_392, [1584], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_390
            del convert_element_type_392
            buf35 = buf34[0]
            assert_size_stride(buf35, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf35, 16, 'torch.ops.aten.convolution_backward.default')
            buf36 = buf34[1]
            assert_size_stride(buf36, (1584, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf36, 16, 'torch.ops.aten.convolution_backward.default')
            del buf34
            buf39 = reinterpret_tensor(buf35, (128, 132, 1, 1), (132, 1, 1, 1), 0); del buf35  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_11.run(buf39, convolution_150, 16896, stream=stream0)
            del convolution_150
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf41 = torch.ops.aten.convolution_backward.default(buf39, mean_15, convert_element_type_388, [132], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_388
            del mean_15
            buf42 = buf41[0]
            assert_size_stride(buf42, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf42, 16, 'torch.ops.aten.convolution_backward.default')
            buf43 = buf41[1]
            assert_size_stride(buf43, (132, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf43, 16, 'torch.ops.aten.convolution_backward.default')
            del buf41
            buf46 = empty_strided_cuda((128, 1584, 7, 7), (77616, 49, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_15], Original ATen: [aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12.run(buf27, buf23, convolution_151, buf42, buf30, buf46, 6272, 1584, stream=stream0)
            del buf23
            del buf27
            del convolution_151
            buf48 = empty_strided_cuda((1584, 49), (1, 1600), torch.float32)
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13.run(buf46, cat_39, getitem_429, buf48, 77616, 128, stream=stream0)
            buf49 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            buf50 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf48, rsqrt_55, buf49, buf50, 1584, 49, stream=stream0)
            buf47 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_15.run(buf46, buf47, 1584, 6272, stream=stream0)
            buf51 = reinterpret_tensor(buf30, (128, 1584, 7, 7), (77616, 49, 7, 1), 0); del buf30  # reuse
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16.run(buf46, cat_39, getitem_429, buf49, rsqrt_55, buf47, primals_460, buf51, 202752, 49, stream=stream0)
            del cat_39
            del getitem_429
            del primals_460
            del rsqrt_55
            buf52 = empty_strided_cuda((128, 396, 7, 7), (19404, 1, 2772, 396), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_17.run(buf51, buf52, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf53 = torch.ops.aten.convolution_backward.default(buf52, getitem_427, convert_element_type_382, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_382
            del getitem_427
            buf54 = buf53[0]
            assert_size_stride(buf54, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf54 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf55 = buf53[1]
            assert_size_stride(buf55, (396, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf55 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf53
            buf57 = buf52; del buf52  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_18.run(buf51, buf57, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf58 = torch.ops.aten.convolution_backward.default(buf57, getitem_422, convert_element_type_381, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_381
            del getitem_422
            buf59 = buf58[0]
            assert_size_stride(buf59, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf59, 16, 'torch.ops.aten.convolution_backward.default')
            buf60 = buf58[1]
            assert_size_stride(buf60, (396, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf60, 16, 'torch.ops.aten.convolution_backward.default')
            del buf58
            buf62 = buf57; del buf57  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_19.run(buf51, buf62, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf63 = torch.ops.aten.convolution_backward.default(buf62, getitem_417, convert_element_type_380, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_380
            del getitem_417
            buf64 = buf63[0]
            assert_size_stride(buf64, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf64 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf65 = buf63[1]
            assert_size_stride(buf65, (396, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf65 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf63
            buf67 = buf62; del buf62  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_20.run(buf51, buf67, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf68 = torch.ops.aten.convolution_backward.default(buf67, getitem_412, convert_element_type_379, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False])
            del buf67
            del convert_element_type_379
            del getitem_412
            buf69 = buf68[0]
            assert_size_stride(buf69, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf69, 16, 'torch.ops.aten.convolution_backward.default')
            buf70 = buf68[1]
            assert_size_stride(buf70, (396, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf70, 16, 'torch.ops.aten.convolution_backward.default')
            del buf68
            buf73 = buf51; del buf51  # reuse
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21.run(convolution_145, getitem_407, rsqrt_54, primals_451, primals_452, buf69, buf64, buf59, buf54, buf73, 6272, 1584, stream=stream0)
            del buf54
            del buf59
            del buf64
            del primals_452
            buf75 = buf48; del buf48  # reuse
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22.run(buf73, convolution_145, getitem_407, buf75, 77616, 128, stream=stream0)
            buf76 = buf49; del buf49  # reuse
            buf77 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf75, rsqrt_54, buf76, buf77, 1584, 49, stream=stream0)
            buf74 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_23.run(buf73, buf74, 1584, 6272, stream=stream0)
            buf78 = convolution_145; del convolution_145  # reuse
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24.run(buf78, buf73, getitem_407, buf76, rsqrt_54, buf74, primals_451, 6272, 1584, stream=stream0)
            del buf73
            del getitem_407
            del primals_451
            del rsqrt_54
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf79 = torch.ops.aten.convolution_backward.default(buf78, add_282, convert_element_type_374, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_282
            del convert_element_type_374
            buf80 = buf79[0]
            assert_size_stride(buf80, (128, 264, 7, 7), (12936, 1, 1848, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf80, 16, 'torch.ops.aten.convolution_backward.default')
            buf81 = buf79[1]
            assert_size_stride(buf81, (1584, 264, 1, 1), (264, 1, 264, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf81, 16, 'torch.ops.aten.convolution_backward.default')
            del buf79
            buf83 = buf18; del buf18  # reuse
            buf85 = buf16; del buf16  # reuse
            # Topologically Sorted Source Nodes: [x_170], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_25.run(buf13, buf80, cat_38, unsqueeze_282, buf83, buf85, 12936, 128, stream=stream0)
            buf84 = buf19; del buf19  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_6.run(buf83, buf84, 264, 49, stream=stream0)
            buf86 = empty_strided_cuda((264, ), (1, ), torch.float32)
            buf87 = empty_strided_cuda((264, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_170], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.run(buf85, squeeze_160, buf86, buf87, 264, 49, stream=stream0)
            buf88 = cat_38; del cat_38  # reuse
            # Topologically Sorted Source Nodes: [x_170], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_26.run(buf88, buf13, buf80, unsqueeze_282, buf86, squeeze_160, buf84, primals_445, 1655808, stream=stream0)
            del primals_445
            del squeeze_160
            del unsqueeze_282
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf89 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf88, (128, 132, 7, 7), (12936, 1, 1848, 264), 132), getitem_403, convert_element_type_371, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_371
            del getitem_403
            buf90 = buf89[0]
            assert_size_stride(buf90, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            # buffer buf90 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf91 = buf89[1]
            assert_size_stride(buf91, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            # buffer buf91 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf89
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf93 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf88, (128, 132, 7, 7), (12936, 1, 1848, 264), 0), getitem_402, convert_element_type_370, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_370
            del getitem_402
            buf94 = buf93[0]
            assert_size_stride(buf94, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf94, 16, 'torch.ops.aten.convolution_backward.default')
            buf95 = buf93[1]
            assert_size_stride(buf95, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf95, 16, 'torch.ops.aten.convolution_backward.default')
            del buf93
            buf97 = buf78; del buf78  # reuse
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_9.run(cat_37, getitem_401, rsqrt_52, primals_434, primals_435, buf97, 9934848, stream=stream0)
            del primals_435
            buf98 = reinterpret_tensor(buf42, (128, 1584, 1, 1), (1584, 1, 202752, 202752), 0); del buf42  # reuse
            buf99 = reinterpret_tensor(buf98, (128, 1584, 1, 1), (1584, 1, 1, 1), 0); del buf98  # reuse
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_10.run(buf99, buf94, buf90, buf97, convolution_142, 202752, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf101 = torch.ops.aten.convolution_backward.default(buf99, convert_element_type_367, convert_element_type_369, [1584], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_367
            del convert_element_type_369
            buf102 = buf101[0]
            assert_size_stride(buf102, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf102, 16, 'torch.ops.aten.convolution_backward.default')
            buf103 = buf101[1]
            assert_size_stride(buf103, (1584, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf103, 16, 'torch.ops.aten.convolution_backward.default')
            del buf101
            buf106 = reinterpret_tensor(buf102, (128, 132, 1, 1), (132, 1, 1, 1), 0); del buf102  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_11.run(buf106, convolution_141, 16896, stream=stream0)
            del convolution_141
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf108 = torch.ops.aten.convolution_backward.default(buf106, mean_14, convert_element_type_365, [132], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_365
            del mean_14
            buf109 = buf108[0]
            assert_size_stride(buf109, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf109, 16, 'torch.ops.aten.convolution_backward.default')
            buf110 = buf108[1]
            assert_size_stride(buf110, (132, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf110, 16, 'torch.ops.aten.convolution_backward.default')
            del buf108
            buf113 = buf46; del buf46  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_14], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_12.run(buf94, buf90, convolution_142, buf109, buf97, buf113, 6272, 1584, stream=stream0)
            del buf90
            del buf94
            del convolution_142
            buf115 = buf75; del buf75  # reuse
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13.run(buf113, cat_37, getitem_401, buf115, 77616, 128, stream=stream0)
            buf116 = buf76; del buf76  # reuse
            buf117 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf115, rsqrt_52, buf116, buf117, 1584, 49, stream=stream0)
            buf114 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_15.run(buf113, buf114, 1584, 6272, stream=stream0)
            buf118 = reinterpret_tensor(buf97, (128, 1584, 7, 7), (77616, 49, 7, 1), 0); del buf97  # reuse
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16.run(buf113, cat_37, getitem_401, buf116, rsqrt_52, buf114, primals_434, buf118, 202752, 49, stream=stream0)
            del cat_37
            del getitem_401
            del primals_434
            del rsqrt_52
            buf119 = buf69; del buf69  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_17.run(buf118, buf119, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf120 = torch.ops.aten.convolution_backward.default(buf119, getitem_399, convert_element_type_359, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_359
            del getitem_399
            buf121 = buf120[0]
            assert_size_stride(buf121, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf121 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf122 = buf120[1]
            assert_size_stride(buf122, (396, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf122 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf120
            buf124 = buf119; del buf119  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_18.run(buf118, buf124, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf125 = torch.ops.aten.convolution_backward.default(buf124, getitem_394, convert_element_type_358, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_358
            del getitem_394
            buf126 = buf125[0]
            assert_size_stride(buf126, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf126, 16, 'torch.ops.aten.convolution_backward.default')
            buf127 = buf125[1]
            assert_size_stride(buf127, (396, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf127, 16, 'torch.ops.aten.convolution_backward.default')
            del buf125
            buf129 = buf124; del buf124  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_19.run(buf118, buf129, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf130 = torch.ops.aten.convolution_backward.default(buf129, getitem_389, convert_element_type_357, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_357
            del getitem_389
            buf131 = buf130[0]
            assert_size_stride(buf131, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf131 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf132 = buf130[1]
            assert_size_stride(buf132, (396, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf132 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf130
            buf134 = buf129; del buf129  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_20.run(buf118, buf134, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf135 = torch.ops.aten.convolution_backward.default(buf134, getitem_384, convert_element_type_356, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False])
            del buf134
            del convert_element_type_356
            del getitem_384
            buf136 = buf135[0]
            assert_size_stride(buf136, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf136, 16, 'torch.ops.aten.convolution_backward.default')
            buf137 = buf135[1]
            assert_size_stride(buf137, (396, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf137, 16, 'torch.ops.aten.convolution_backward.default')
            del buf135
            buf140 = buf118; del buf118  # reuse
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21.run(convolution_136, getitem_379, rsqrt_51, primals_425, primals_426, buf136, buf131, buf126, buf121, buf140, 6272, 1584, stream=stream0)
            del buf121
            del buf126
            del buf131
            del primals_426
            buf142 = buf115; del buf115  # reuse
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22.run(buf140, convolution_136, getitem_379, buf142, 77616, 128, stream=stream0)
            buf143 = buf116; del buf116  # reuse
            buf144 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf142, rsqrt_51, buf143, buf144, 1584, 49, stream=stream0)
            buf141 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_23.run(buf140, buf141, 1584, 6272, stream=stream0)
            buf145 = convolution_136; del convolution_136  # reuse
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24.run(buf145, buf140, getitem_379, buf143, rsqrt_51, buf141, primals_425, 6272, 1584, stream=stream0)
            del buf140
            del getitem_379
            del primals_425
            del rsqrt_51
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf146 = torch.ops.aten.convolution_backward.default(buf145, add_266, convert_element_type_351, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_266
            del convert_element_type_351
            buf147 = buf146[0]
            assert_size_stride(buf147, (128, 264, 7, 7), (12936, 1, 1848, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf147, 16, 'torch.ops.aten.convolution_backward.default')
            buf148 = buf146[1]
            assert_size_stride(buf148, (1584, 264, 1, 1), (264, 1, 264, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf148, 16, 'torch.ops.aten.convolution_backward.default')
            del buf146
            buf150 = buf85; del buf85  # reuse
            buf152 = buf83; del buf83  # reuse
            # Topologically Sorted Source Nodes: [x_160], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_27.run(buf13, buf80, buf147, cat_36, unsqueeze_318, buf150, buf152, 12936, 128, stream=stream0)
            buf151 = buf86; del buf86  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_6.run(buf150, buf151, 264, 49, stream=stream0)
            buf153 = empty_strided_cuda((264, ), (1, ), torch.float32)
            buf155 = empty_strided_cuda((264, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_160], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.run(buf152, squeeze_151, buf153, buf155, 264, 49, stream=stream0)
            buf154 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_160], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_28.run(buf13, buf80, buf147, cat_36, unsqueeze_318, buf153, squeeze_151, buf151, primals_419, buf154, 1655808, stream=stream0)
            del cat_36
            del primals_419
            del squeeze_151
            del unsqueeze_318
            buf156 = empty_strided_cuda((128, 132, 7, 7), (6468, 1, 924, 132), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_29.run(buf154, buf156, 827904, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf157 = torch.ops.aten.convolution_backward.default(buf156, getitem_375, convert_element_type_348, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_348
            del getitem_375
            buf158 = buf157[0]
            assert_size_stride(buf158, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf158, 16, 'torch.ops.aten.convolution_backward.default')
            buf159 = buf157[1]
            assert_size_stride(buf159, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf159, 16, 'torch.ops.aten.convolution_backward.default')
            del buf157
            buf161 = buf156; del buf156  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_30.run(buf154, buf161, 827904, stream=stream0)
            del buf154
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf162 = torch.ops.aten.convolution_backward.default(buf161, getitem_374, convert_element_type_347, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf161
            del convert_element_type_347
            del getitem_374
            buf163 = buf162[0]
            assert_size_stride(buf163, (128, 792, 7, 7), (38808, 1, 5544, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf163, 16, 'torch.ops.aten.convolution_backward.default')
            buf164 = buf162[1]
            assert_size_stride(buf164, (132, 792, 1, 1), (792, 1, 792, 792), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf164, 16, 'torch.ops.aten.convolution_backward.default')
            del buf162
            buf166 = buf145; del buf145  # reuse
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_9.run(cat_35, getitem_373, rsqrt_49, primals_408, primals_409, buf166, 9934848, stream=stream0)
            del primals_409
            buf167 = reinterpret_tensor(buf109, (128, 1584, 1, 1), (1584, 1, 202752, 202752), 0); del buf109  # reuse
            buf168 = reinterpret_tensor(buf167, (128, 1584, 1, 1), (1584, 1, 1, 1), 0); del buf167  # reuse
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_31.run(buf168, buf163, buf158, buf166, convolution_133, 202752, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf170 = torch.ops.aten.convolution_backward.default(buf168, convert_element_type_344, convert_element_type_346, [1584], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_344
            del convert_element_type_346
            buf171 = buf170[0]
            assert_size_stride(buf171, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf171, 16, 'torch.ops.aten.convolution_backward.default')
            buf172 = buf170[1]
            assert_size_stride(buf172, (1584, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf172, 16, 'torch.ops.aten.convolution_backward.default')
            del buf170
            buf175 = reinterpret_tensor(buf171, (128, 132, 1, 1), (132, 1, 1, 1), 0); del buf171  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_11.run(buf175, convolution_132, 16896, stream=stream0)
            del convolution_132
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf177 = torch.ops.aten.convolution_backward.default(buf175, mean_13, convert_element_type_342, [132], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_342
            del mean_13
            buf178 = buf177[0]
            assert_size_stride(buf178, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf178, 16, 'torch.ops.aten.convolution_backward.default')
            buf179 = buf177[1]
            assert_size_stride(buf179, (132, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf179, 16, 'torch.ops.aten.convolution_backward.default')
            del buf177
            buf182 = buf113; del buf113  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_13], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_32.run(buf163, buf158, convolution_133, buf178, buf166, buf182, 6272, 1584, stream=stream0)
            del buf158
            del buf163
            del buf178
            del convolution_133
            buf184 = buf142; del buf142  # reuse
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_13.run(buf182, cat_35, getitem_373, buf184, 77616, 128, stream=stream0)
            buf185 = buf143; del buf143  # reuse
            buf186 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf184, rsqrt_49, buf185, buf186, 1584, 49, stream=stream0)
            buf183 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_15.run(buf182, buf183, 1584, 6272, stream=stream0)
            buf187 = reinterpret_tensor(buf166, (128, 1584, 7, 7), (77616, 49, 7, 1), 0); del buf166  # reuse
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_16.run(buf182, cat_35, getitem_373, buf185, rsqrt_49, buf183, primals_408, buf187, 202752, 49, stream=stream0)
            del buf182
            del cat_35
            del getitem_373
            del primals_408
            del rsqrt_49
            buf188 = buf136; del buf136  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_17.run(buf187, buf188, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf189 = torch.ops.aten.convolution_backward.default(buf188, getitem_371, convert_element_type_336, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_336
            del getitem_371
            buf190 = buf189[0]
            assert_size_stride(buf190, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf190 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf191 = buf189[1]
            assert_size_stride(buf191, (396, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf191 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf189
            buf193 = buf188; del buf188  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_18.run(buf187, buf193, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf194 = torch.ops.aten.convolution_backward.default(buf193, getitem_366, convert_element_type_335, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_335
            del getitem_366
            buf195 = buf194[0]
            assert_size_stride(buf195, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf195, 16, 'torch.ops.aten.convolution_backward.default')
            buf196 = buf194[1]
            assert_size_stride(buf196, (396, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf196, 16, 'torch.ops.aten.convolution_backward.default')
            del buf194
            buf198 = buf193; del buf193  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_19.run(buf187, buf198, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf199 = torch.ops.aten.convolution_backward.default(buf198, getitem_361, convert_element_type_334, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False])
            del convert_element_type_334
            del getitem_361
            buf200 = buf199[0]
            assert_size_stride(buf200, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            # buffer buf200 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf201 = buf199[1]
            assert_size_stride(buf201, (396, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            # buffer buf201 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf199
            buf203 = buf198; del buf198  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_20.run(buf187, buf203, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf204 = torch.ops.aten.convolution_backward.default(buf203, getitem_356, convert_element_type_333, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False])
            del buf203
            del convert_element_type_333
            del getitem_356
            buf205 = buf204[0]
            assert_size_stride(buf205, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf205, 16, 'torch.ops.aten.convolution_backward.default')
            buf206 = buf204[1]
            assert_size_stride(buf206, (396, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf206, 16, 'torch.ops.aten.convolution_backward.default')
            del buf204
            buf209 = buf187; del buf187  # reuse
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_21.run(convolution_127, getitem_351, rsqrt_48, primals_399, primals_400, buf205, buf200, buf195, buf190, buf209, 6272, 1584, stream=stream0)
            del buf190
            del buf195
            del buf200
            del buf205
            del primals_400
            buf211 = buf184; del buf184  # reuse
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_22.run(buf209, convolution_127, getitem_351, buf211, 77616, 128, stream=stream0)
            buf212 = buf185; del buf185  # reuse
            buf213 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_14.run(buf211, rsqrt_48, buf212, buf213, 1584, 49, stream=stream0)
            del buf211
            buf210 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_23.run(buf209, buf210, 1584, 6272, stream=stream0)
            buf214 = convolution_127; del convolution_127  # reuse
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_24.run(buf214, buf209, getitem_351, buf212, rsqrt_48, buf210, primals_399, 6272, 1584, stream=stream0)
            del buf209
            del getitem_351
            del primals_399
            del rsqrt_48
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf215 = torch.ops.aten.convolution_backward.default(buf214, convert_element_type_327, convert_element_type_328, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf214
            del convert_element_type_327
            del convert_element_type_328
            buf216 = buf215[0]
            assert_size_stride(buf216, (128, 264, 7, 7), (12936, 1, 1848, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf216, 16, 'torch.ops.aten.convolution_backward.default')
            buf217 = buf215[1]
            assert_size_stride(buf217, (1584, 264, 1, 1), (264, 1, 264, 264), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf217, 16, 'torch.ops.aten.convolution_backward.default')
            del buf215
            buf219 = buf152; del buf152  # reuse
            buf221 = buf150; del buf150  # reuse
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_native_batch_norm_backward_33.run(buf13, buf80, buf147, buf216, convolution_126, unsqueeze_354, buf219, buf221, 12936, 128, stream=stream0)
            buf220 = buf153; del buf153  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_6.run(buf219, buf220, 264, 49, stream=stream0)
            del buf219
            buf222 = empty_strided_cuda((264, ), (1, ), torch.float32)
            buf224 = empty_strided_cuda((264, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_7.run(buf221, squeeze_142, buf222, buf224, 264, 49, stream=stream0)
            del buf221
            buf225 = buf88; del buf88  # reuse
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_native_batch_norm_backward_34.run(buf13, buf80, buf147, buf216, convolution_126, unsqueeze_354, buf222, squeeze_142, buf220, primals_393, buf225, 1655808, stream=stream0)
            del buf13
            del buf147
            del buf216
            del buf222
            del buf80
            del convolution_126
            del primals_393
            del squeeze_142
            del unsqueeze_354
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf226 = torch.ops.aten.convolution_backward.default(buf225, mul_380, convert_element_type_325, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf225
            del convert_element_type_325
            del mul_380
            buf227 = buf226[0]
            assert_size_stride(buf227, (128, 960, 7, 7), (47040, 1, 6720, 960), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf227, 16, 'torch.ops.aten.convolution_backward.default')
            buf228 = buf226[1]
            assert_size_stride(buf228, (264, 960, 1, 1), (960, 1, 960, 960), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf228, 16, 'torch.ops.aten.convolution_backward.default')
            del buf226
            buf230 = empty_strided_cuda((128, 960, 7, 7), (47040, 1, 6720, 960), torch.float16)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_35.run(cat_34, getitem_347, rsqrt_46, primals_383, primals_384, buf230, 6021120, stream=stream0)
            del primals_384
            buf231 = empty_strided_cuda((128, 960, 1, 1), (960, 1, 122880, 122880), torch.float16)
            buf232 = reinterpret_tensor(buf231, (128, 960, 1, 1), (960, 1, 1, 1), 0); del buf231  # reuse
            # Topologically Sorted Source Nodes: [x_148, sigmoid_12], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_mul_sigmoid_sigmoid_backward_silu_sum_36.run(buf232, buf227, buf230, convolution_125, 122880, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf234 = torch.ops.aten.convolution_backward.default(buf232, convert_element_type_322, convert_element_type_324, [960], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_322
            del convert_element_type_324
            buf235 = buf234[0]
            assert_size_stride(buf235, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf235, 16, 'torch.ops.aten.convolution_backward.default')
            buf236 = buf234[1]
            assert_size_stride(buf236, (960, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf236, 16, 'torch.ops.aten.convolution_backward.default')
            del buf234
            buf239 = reinterpret_tensor(buf235, (128, 80, 1, 1), (80, 1, 1, 1), 0); del buf235  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_37.run(buf239, convolution_124, 10240, stream=stream0)
            del convolution_124
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf241 = torch.ops.aten.convolution_backward.default(buf239, mean_12, convert_element_type_320, [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_320
            del mean_12
            buf242 = buf241[0]
            assert_size_stride(buf242, (128, 960, 1, 1), (960, 1, 960, 960), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf242, 16, 'torch.ops.aten.convolution_backward.default')
            buf243 = buf241[1]
            assert_size_stride(buf243, (80, 960, 1, 1), (960, 1, 960, 960), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf243, 16, 'torch.ops.aten.convolution_backward.default')
            del buf241
            buf246 = empty_strided_cuda((960, 49), (1, 960), torch.float32)
            buf248 = empty_strided_cuda((960, 49), (1, 960), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_38.run(buf227, convolution_125, buf242, buf230, cat_34, getitem_347, buf246, buf248, 47040, 128, stream=stream0)
            buf247 = empty_strided_cuda((960, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_12], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_39.run(buf246, buf247, 960, 49, stream=stream0)
            del buf246
            buf249 = empty_strided_cuda((960, ), (1, ), torch.float32)
            buf251 = empty_strided_cuda((960, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_40.run(buf248, rsqrt_46, buf249, buf251, 960, 49, stream=stream0)
            del buf248
            buf250 = empty_strided_cuda((128, 960, 7, 7), (47040, 1, 6720, 960), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_12, x_147], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_41.run(buf227, convolution_125, buf242, buf230, cat_34, getitem_347, buf249, rsqrt_46, buf247, buf250, 6021120, stream=stream0)
            del buf227
            del buf230
            del buf242
            del cat_34
            del convolution_125
            del getitem_347
            buf252 = empty_strided_cuda((128, 240, 7, 7), (11760, 1, 1680, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_42.run(buf250, rsqrt_46, primals_383, buf252, 1505280, stream=stream0)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf253 = torch.ops.aten.convolution_backward.default(buf252, getitem_345, convert_element_type_314, [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 240, [True, True, False])
            del convert_element_type_314
            del getitem_345
            buf254 = buf253[0]
            assert_size_stride(buf254, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf254, 16, 'torch.ops.aten.convolution_backward.default')
            buf255 = buf253[1]
            assert_size_stride(buf255, (240, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf255, 16, 'torch.ops.aten.convolution_backward.default')
            del buf253
            buf257 = buf252; del buf252  # reuse
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_43.run(buf250, rsqrt_46, primals_383, buf257, 1505280, stream=stream0)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf258 = torch.ops.aten.convolution_backward.default(buf257, getitem_340, convert_element_type_313, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 240, [True, True, False])
            del convert_element_type_313
            del getitem_340
            buf259 = buf258[0]
            assert_size_stride(buf259, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf259, 16, 'torch.ops.aten.convolution_backward.default')
            buf260 = buf258[1]
            assert_size_stride(buf260, (240, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf260, 16, 'torch.ops.aten.convolution_backward.default')
            del buf258
            buf262 = buf257; del buf257  # reuse
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_44.run(buf250, rsqrt_46, primals_383, buf262, 1505280, stream=stream0)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf263 = torch.ops.aten.convolution_backward.default(buf262, getitem_335, convert_element_type_312, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 240, [True, True, False])
            del convert_element_type_312
            del getitem_335
            buf264 = buf263[0]
            assert_size_stride(buf264, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf264, 16, 'torch.ops.aten.convolution_backward.default')
            buf265 = buf263[1]
            assert_size_stride(buf265, (240, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf265, 16, 'torch.ops.aten.convolution_backward.default')
            del buf263
            buf267 = buf262; del buf262  # reuse
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_45.run(buf250, rsqrt_46, primals_383, buf267, 1505280, stream=stream0)
            del buf250
            del primals_383
            del rsqrt_46
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf268 = torch.ops.aten.convolution_backward.default(buf267, getitem_330, convert_element_type_311, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 240, [True, True, False])
            del buf267
            del convert_element_type_311
            del getitem_330
            buf269 = buf268[0]
            assert_size_stride(buf269, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf269, 16, 'torch.ops.aten.convolution_backward.default')
            buf270 = buf268[1]
            assert_size_stride(buf270, (240, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf270, 16, 'torch.ops.aten.convolution_backward.default')
            del buf268
            buf273 = empty_strided_cuda((128, 960, 14, 14), (188160, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_46.run(convolution_119, getitem_325, rsqrt_45, primals_374, primals_375, buf269, buf264, buf259, buf254, buf273, 25088, 960, stream=stream0)
            del buf254
            del buf259
            del buf264
            del buf269
            del primals_375
            buf275 = empty_strided_cuda((960, 128), (1, 960), torch.float32)
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_47.run(buf273, convolution_119, getitem_325, buf275, 122880, 196, stream=stream0)
            buf276 = buf249; del buf249  # reuse
            buf277 = empty_strided_cuda((960, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_48.run(buf275, rsqrt_45, buf276, buf277, 960, 128, stream=stream0)
            buf274 = empty_strided_cuda((960, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_49.run(buf273, buf274, 960, 25088, stream=stream0)
            buf278 = convolution_119; del convolution_119  # reuse
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_50.run(buf278, buf273, getitem_325, buf276, rsqrt_45, buf274, primals_374, 25088, 960, stream=stream0)
            del buf273
            del getitem_325
            del primals_374
            del rsqrt_45
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf279 = torch.ops.aten.convolution_backward.default(buf278, add_235, convert_element_type_306, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_235
            del convert_element_type_306
            buf280 = buf279[0]
            assert_size_stride(buf280, (128, 160, 14, 14), (31360, 1, 2240, 160), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf280, 16, 'torch.ops.aten.convolution_backward.default')
            buf281 = buf279[1]
            assert_size_stride(buf281, (960, 160, 1, 1), (160, 1, 160, 160), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf281, 16, 'torch.ops.aten.convolution_backward.default')
            del buf279
            buf283 = empty_strided_cuda((160, 196), (1, 160), torch.float32)
            buf285 = empty_strided_cuda((160, 196), (1, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51.run(buf280, cat_33, unsqueeze_390, buf283, buf285, 31360, 128, stream=stream0)
            buf284 = empty_strided_cuda((160, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_52.run(buf283, buf284, 160, 196, stream=stream0)
            buf286 = empty_strided_cuda((160, ), (1, ), torch.float32)
            buf287 = empty_strided_cuda((160, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.run(buf285, squeeze_133, buf286, buf287, 160, 196, stream=stream0)
            buf288 = cat_33; del cat_33  # reuse
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54.run(buf288, buf280, unsqueeze_390, buf286, squeeze_133, buf284, primals_368, 4014080, stream=stream0)
            del primals_368
            del squeeze_133
            del unsqueeze_390
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf289 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf288, (128, 80, 14, 14), (31360, 1, 2240, 160), 80), getitem_321, convert_element_type_303, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_303
            del getitem_321
            buf290 = buf289[0]
            assert_size_stride(buf290, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf290, 16, 'torch.ops.aten.convolution_backward.default')
            buf291 = buf289[1]
            assert_size_stride(buf291, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf291, 16, 'torch.ops.aten.convolution_backward.default')
            del buf289
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf293 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf288, (128, 80, 14, 14), (31360, 1, 2240, 160), 0), getitem_320, convert_element_type_302, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf288
            del convert_element_type_302
            del getitem_320
            buf294 = buf293[0]
            assert_size_stride(buf294, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf294, 16, 'torch.ops.aten.convolution_backward.default')
            buf295 = buf293[1]
            assert_size_stride(buf295, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf295, 16, 'torch.ops.aten.convolution_backward.default')
            del buf293
            buf297 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_55.run(cat_32, getitem_319, rsqrt_43, primals_357, primals_358, buf297, 12042240, stream=stream0)
            del primals_358
            buf298 = empty_strided_cuda((128, 480, 1, 1), (480, 1, 61440, 61440), torch.float16)
            buf299 = reinterpret_tensor(buf298, (128, 480, 1, 1), (480, 1, 1, 1), 0); del buf298  # reuse
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56.run(buf299, buf294, buf290, buf297, convolution_116, 61440, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf301 = torch.ops.aten.convolution_backward.default(buf299, convert_element_type_299, convert_element_type_301, [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_299
            del convert_element_type_301
            buf302 = buf301[0]
            assert_size_stride(buf302, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf302, 16, 'torch.ops.aten.convolution_backward.default')
            buf303 = buf301[1]
            assert_size_stride(buf303, (480, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf303, 16, 'torch.ops.aten.convolution_backward.default')
            del buf301
            buf306 = reinterpret_tensor(buf302, (128, 80, 1, 1), (80, 1, 1, 1), 0); del buf302  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_37.run(buf306, convolution_115, 10240, stream=stream0)
            del convolution_115
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf308 = torch.ops.aten.convolution_backward.default(buf306, mean_11, convert_element_type_297, [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_297
            del mean_11
            buf309 = buf308[0]
            assert_size_stride(buf309, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf309, 16, 'torch.ops.aten.convolution_backward.default')
            buf310 = buf308[1]
            assert_size_stride(buf310, (80, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf310, 16, 'torch.ops.aten.convolution_backward.default')
            del buf308
            buf313 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_11], Original ATen: [aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57.run(buf294, buf290, convolution_116, buf309, buf297, buf313, 25088, 480, stream=stream0)
            del buf290
            del convolution_116
            buf315 = empty_strided_cuda((480, 196), (196, 1), torch.float32)
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58.run(buf313, cat_32, getitem_319, buf315, 480, 196, 128, stream=stream0)
            buf316 = empty_strided_cuda((480, ), (1, ), torch.float32)
            buf317 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf315, rsqrt_43, buf316, buf317, 480, 196, stream=stream0)
            buf314 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_60.run(buf313, buf314, 480, 25088, stream=stream0)
            buf318 = reinterpret_tensor(buf297, (128, 480, 14, 14), (94080, 196, 14, 1), 0); del buf297  # reuse
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61.run(buf313, cat_32, getitem_319, buf316, rsqrt_43, buf314, primals_357, buf318, 61440, 196, stream=stream0)
            del cat_32
            del getitem_319
            del primals_357
            del rsqrt_43
            buf319 = empty_strided_cuda((128, 120, 14, 14), (23520, 1, 1680, 120), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_62.run(buf318, buf319, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf320 = torch.ops.aten.convolution_backward.default(buf319, getitem_317, convert_element_type_291, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_291
            del getitem_317
            buf321 = buf320[0]
            assert_size_stride(buf321, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf321, 16, 'torch.ops.aten.convolution_backward.default')
            buf322 = buf320[1]
            assert_size_stride(buf322, (120, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf322, 16, 'torch.ops.aten.convolution_backward.default')
            del buf320
            buf324 = buf319; del buf319  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_63.run(buf318, buf324, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf325 = torch.ops.aten.convolution_backward.default(buf324, getitem_312, convert_element_type_290, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_290
            del getitem_312
            buf326 = buf325[0]
            assert_size_stride(buf326, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf326, 16, 'torch.ops.aten.convolution_backward.default')
            buf327 = buf325[1]
            assert_size_stride(buf327, (120, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf327, 16, 'torch.ops.aten.convolution_backward.default')
            del buf325
            buf329 = buf324; del buf324  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_64.run(buf318, buf329, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf330 = torch.ops.aten.convolution_backward.default(buf329, getitem_307, convert_element_type_289, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_289
            del getitem_307
            buf331 = buf330[0]
            assert_size_stride(buf331, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf331, 16, 'torch.ops.aten.convolution_backward.default')
            buf332 = buf330[1]
            assert_size_stride(buf332, (120, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf332, 16, 'torch.ops.aten.convolution_backward.default')
            del buf330
            buf334 = buf329; del buf329  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_65.run(buf318, buf334, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf335 = torch.ops.aten.convolution_backward.default(buf334, getitem_302, convert_element_type_288, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False])
            del buf334
            del convert_element_type_288
            del getitem_302
            buf336 = buf335[0]
            assert_size_stride(buf336, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf336, 16, 'torch.ops.aten.convolution_backward.default')
            buf337 = buf335[1]
            assert_size_stride(buf337, (120, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf337, 16, 'torch.ops.aten.convolution_backward.default')
            del buf335
            buf340 = buf318; del buf318  # reuse
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66.run(cat_31, getitem_297, rsqrt_42, primals_348, primals_349, buf336, buf331, buf326, buf321, buf340, 25088, 480, stream=stream0)
            del buf321
            del buf326
            del buf331
            del primals_349
            buf342 = buf315; del buf315  # reuse
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67.run(buf340, cat_31, getitem_297, buf342, 480, 196, 128, stream=stream0)
            buf343 = buf316; del buf316  # reuse
            buf344 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf342, rsqrt_42, buf343, buf344, 480, 196, stream=stream0)
            buf341 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_68.run(buf340, buf341, 480, 25088, stream=stream0)
            buf345 = buf340; del buf340  # reuse
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69.run(buf345, cat_31, getitem_297, buf343, rsqrt_42, buf341, primals_348, 61440, 196, stream=stream0)
            del cat_31
            del getitem_297
            del primals_348
            del rsqrt_42
            buf346 = buf294; del buf294  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_70.run(buf345, buf346, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf347 = torch.ops.aten.convolution_backward.default(buf346, getitem_295, convert_element_type_283, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_283
            del getitem_295
            buf348 = buf347[0]
            assert_size_stride(buf348, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf348, 16, 'torch.ops.aten.convolution_backward.default')
            buf349 = buf347[1]
            assert_size_stride(buf349, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf349, 16, 'torch.ops.aten.convolution_backward.default')
            del buf347
            buf351 = buf346; del buf346  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_71.run(buf345, buf351, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf352 = torch.ops.aten.convolution_backward.default(buf351, getitem_294, convert_element_type_282, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf351
            del convert_element_type_282
            del getitem_294
            buf353 = buf352[0]
            assert_size_stride(buf353, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf353, 16, 'torch.ops.aten.convolution_backward.default')
            buf354 = buf352[1]
            assert_size_stride(buf354, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf354, 16, 'torch.ops.aten.convolution_backward.default')
            del buf352
            buf356 = buf285; del buf285  # reuse
            buf358 = buf283; del buf283  # reuse
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72.run(buf280, buf353, buf348, cat_30, unsqueeze_426, buf356, buf358, 31360, 128, stream=stream0)
            buf357 = buf286; del buf286  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_52.run(buf356, buf357, 160, 196, stream=stream0)
            buf359 = empty_strided_cuda((160, ), (1, ), torch.float32)
            buf361 = empty_strided_cuda((160, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.run(buf358, squeeze_124, buf359, buf361, 160, 196, stream=stream0)
            buf360 = empty_strided_cuda((128, 160, 14, 14), (31360, 1, 2240, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_73.run(buf280, buf353, buf348, cat_30, unsqueeze_426, buf359, squeeze_124, buf357, buf360, 4014080, stream=stream0)
            del cat_30
            del unsqueeze_426
            buf362 = empty_strided_cuda((128, 80, 14, 14), (15680, 1, 1120, 80), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_74.run(buf360, squeeze_124, primals_341, buf362, 2007040, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf363 = torch.ops.aten.convolution_backward.default(buf362, getitem_291, convert_element_type_279, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_279
            del getitem_291
            buf364 = buf363[0]
            assert_size_stride(buf364, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf364, 16, 'torch.ops.aten.convolution_backward.default')
            buf365 = buf363[1]
            assert_size_stride(buf365, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf365, 16, 'torch.ops.aten.convolution_backward.default')
            del buf363
            buf367 = buf362; del buf362  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_75.run(buf360, squeeze_124, primals_341, buf367, 2007040, stream=stream0)
            del buf360
            del primals_341
            del squeeze_124
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf368 = torch.ops.aten.convolution_backward.default(buf367, getitem_290, convert_element_type_278, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf367
            del convert_element_type_278
            del getitem_290
            buf369 = buf368[0]
            assert_size_stride(buf369, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf369, 16, 'torch.ops.aten.convolution_backward.default')
            buf370 = buf368[1]
            assert_size_stride(buf370, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf370, 16, 'torch.ops.aten.convolution_backward.default')
            del buf368
            buf372 = reinterpret_tensor(buf345, (128, 480, 14, 14), (94080, 1, 6720, 480), 0); del buf345  # reuse
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_55.run(cat_29, getitem_289, rsqrt_40, primals_330, primals_331, buf372, 12042240, stream=stream0)
            del primals_331
            buf373 = reinterpret_tensor(buf309, (128, 480, 1, 1), (480, 1, 61440, 61440), 0); del buf309  # reuse
            buf374 = reinterpret_tensor(buf373, (128, 480, 1, 1), (480, 1, 1, 1), 0); del buf373  # reuse
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56.run(buf374, buf369, buf364, buf372, convolution_106, 61440, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf376 = torch.ops.aten.convolution_backward.default(buf374, convert_element_type_275, convert_element_type_277, [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_275
            del convert_element_type_277
            buf377 = buf376[0]
            assert_size_stride(buf377, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf377, 16, 'torch.ops.aten.convolution_backward.default')
            buf378 = buf376[1]
            assert_size_stride(buf378, (480, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf378, 16, 'torch.ops.aten.convolution_backward.default')
            del buf376
            buf381 = reinterpret_tensor(buf377, (128, 80, 1, 1), (80, 1, 1, 1), 0); del buf377  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_37.run(buf381, convolution_105, 10240, stream=stream0)
            del convolution_105
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf383 = torch.ops.aten.convolution_backward.default(buf381, mean_10, convert_element_type_273, [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_273
            del mean_10
            buf384 = buf383[0]
            assert_size_stride(buf384, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf384, 16, 'torch.ops.aten.convolution_backward.default')
            buf385 = buf383[1]
            assert_size_stride(buf385, (80, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf385, 16, 'torch.ops.aten.convolution_backward.default')
            del buf383
            buf388 = buf313; del buf313  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_10], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57.run(buf369, buf364, convolution_106, buf384, buf372, buf388, 25088, 480, stream=stream0)
            del buf364
            del convolution_106
            buf390 = buf342; del buf342  # reuse
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58.run(buf388, cat_29, getitem_289, buf390, 480, 196, 128, stream=stream0)
            buf391 = buf343; del buf343  # reuse
            buf392 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf390, rsqrt_40, buf391, buf392, 480, 196, stream=stream0)
            buf389 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_60.run(buf388, buf389, 480, 25088, stream=stream0)
            buf393 = reinterpret_tensor(buf372, (128, 480, 14, 14), (94080, 196, 14, 1), 0); del buf372  # reuse
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61.run(buf388, cat_29, getitem_289, buf391, rsqrt_40, buf389, primals_330, buf393, 61440, 196, stream=stream0)
            del cat_29
            del getitem_289
            del primals_330
            del rsqrt_40
            buf394 = buf336; del buf336  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_62.run(buf393, buf394, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf395 = torch.ops.aten.convolution_backward.default(buf394, getitem_287, convert_element_type_267, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_267
            del getitem_287
            buf396 = buf395[0]
            assert_size_stride(buf396, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf396, 16, 'torch.ops.aten.convolution_backward.default')
            buf397 = buf395[1]
            assert_size_stride(buf397, (120, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf397, 16, 'torch.ops.aten.convolution_backward.default')
            del buf395
            buf399 = buf394; del buf394  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_63.run(buf393, buf399, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf400 = torch.ops.aten.convolution_backward.default(buf399, getitem_282, convert_element_type_266, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_266
            del getitem_282
            buf401 = buf400[0]
            assert_size_stride(buf401, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf401, 16, 'torch.ops.aten.convolution_backward.default')
            buf402 = buf400[1]
            assert_size_stride(buf402, (120, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf402, 16, 'torch.ops.aten.convolution_backward.default')
            del buf400
            buf404 = buf399; del buf399  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_64.run(buf393, buf404, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf405 = torch.ops.aten.convolution_backward.default(buf404, getitem_277, convert_element_type_265, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_265
            del getitem_277
            buf406 = buf405[0]
            assert_size_stride(buf406, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf406, 16, 'torch.ops.aten.convolution_backward.default')
            buf407 = buf405[1]
            assert_size_stride(buf407, (120, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf407, 16, 'torch.ops.aten.convolution_backward.default')
            del buf405
            buf409 = buf404; del buf404  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_65.run(buf393, buf409, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf410 = torch.ops.aten.convolution_backward.default(buf409, getitem_272, convert_element_type_264, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False])
            del buf409
            del convert_element_type_264
            del getitem_272
            buf411 = buf410[0]
            assert_size_stride(buf411, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf411, 16, 'torch.ops.aten.convolution_backward.default')
            buf412 = buf410[1]
            assert_size_stride(buf412, (120, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf412, 16, 'torch.ops.aten.convolution_backward.default')
            del buf410
            buf415 = buf393; del buf393  # reuse
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66.run(cat_28, getitem_267, rsqrt_39, primals_321, primals_322, buf411, buf406, buf401, buf396, buf415, 25088, 480, stream=stream0)
            del buf396
            del buf401
            del buf406
            del primals_322
            buf417 = buf390; del buf390  # reuse
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67.run(buf415, cat_28, getitem_267, buf417, 480, 196, 128, stream=stream0)
            buf418 = buf391; del buf391  # reuse
            buf419 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf417, rsqrt_39, buf418, buf419, 480, 196, stream=stream0)
            buf416 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_68.run(buf415, buf416, 480, 25088, stream=stream0)
            buf420 = buf415; del buf415  # reuse
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69.run(buf420, cat_28, getitem_267, buf418, rsqrt_39, buf416, primals_321, 61440, 196, stream=stream0)
            del cat_28
            del getitem_267
            del primals_321
            del rsqrt_39
            buf421 = buf369; del buf369  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_70.run(buf420, buf421, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf422 = torch.ops.aten.convolution_backward.default(buf421, getitem_265, convert_element_type_259, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_259
            del getitem_265
            buf423 = buf422[0]
            assert_size_stride(buf423, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf423, 16, 'torch.ops.aten.convolution_backward.default')
            buf424 = buf422[1]
            assert_size_stride(buf424, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf424, 16, 'torch.ops.aten.convolution_backward.default')
            del buf422
            buf426 = buf421; del buf421  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_71.run(buf420, buf426, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf427 = torch.ops.aten.convolution_backward.default(buf426, getitem_264, convert_element_type_258, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf426
            del convert_element_type_258
            del getitem_264
            buf428 = buf427[0]
            assert_size_stride(buf428, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf428, 16, 'torch.ops.aten.convolution_backward.default')
            buf429 = buf427[1]
            assert_size_stride(buf429, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf429, 16, 'torch.ops.aten.convolution_backward.default')
            del buf427
            buf431 = buf280; del buf280  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_76.run(buf431, buf353, buf348, buf428, buf423, 4014080, stream=stream0)
            del buf348
            del buf353
            del buf423
            del buf428
            buf432 = buf358; del buf358  # reuse
            buf434 = buf356; del buf356  # reuse
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_51.run(buf431, cat_27, unsqueeze_462, buf432, buf434, 31360, 128, stream=stream0)
            buf433 = buf359; del buf359  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_52.run(buf432, buf433, 160, 196, stream=stream0)
            buf435 = empty_strided_cuda((160, ), (1, ), torch.float32)
            buf436 = empty_strided_cuda((160, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.run(buf434, squeeze_115, buf435, buf436, 160, 196, stream=stream0)
            buf437 = cat_27; del cat_27  # reuse
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_54.run(buf437, buf431, unsqueeze_462, buf435, squeeze_115, buf433, primals_314, 4014080, stream=stream0)
            del primals_314
            del squeeze_115
            del unsqueeze_462
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf438 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf437, (128, 80, 14, 14), (31360, 1, 2240, 160), 80), getitem_261, convert_element_type_255, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_255
            del getitem_261
            buf439 = buf438[0]
            assert_size_stride(buf439, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf439, 16, 'torch.ops.aten.convolution_backward.default')
            buf440 = buf438[1]
            assert_size_stride(buf440, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf440, 16, 'torch.ops.aten.convolution_backward.default')
            del buf438
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf442 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf437, (128, 80, 14, 14), (31360, 1, 2240, 160), 0), getitem_260, convert_element_type_254, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_254
            del getitem_260
            buf443 = buf442[0]
            assert_size_stride(buf443, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf443, 16, 'torch.ops.aten.convolution_backward.default')
            buf444 = buf442[1]
            assert_size_stride(buf444, (80, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf444, 16, 'torch.ops.aten.convolution_backward.default')
            del buf442
            buf446 = reinterpret_tensor(buf420, (128, 480, 14, 14), (94080, 1, 6720, 480), 0); del buf420  # reuse
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_55.run(cat_26, getitem_259, rsqrt_37, primals_303, primals_304, buf446, 12042240, stream=stream0)
            del primals_304
            buf447 = reinterpret_tensor(buf384, (128, 480, 1, 1), (480, 1, 61440, 61440), 0); del buf384  # reuse
            buf448 = reinterpret_tensor(buf447, (128, 480, 1, 1), (480, 1, 1, 1), 0); del buf447  # reuse
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_56.run(buf448, buf443, buf439, buf446, convolution_96, 61440, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf450 = torch.ops.aten.convolution_backward.default(buf448, convert_element_type_251, convert_element_type_253, [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_251
            del convert_element_type_253
            buf451 = buf450[0]
            assert_size_stride(buf451, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf451, 16, 'torch.ops.aten.convolution_backward.default')
            buf452 = buf450[1]
            assert_size_stride(buf452, (480, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf452, 16, 'torch.ops.aten.convolution_backward.default')
            del buf450
            buf455 = reinterpret_tensor(buf451, (128, 80, 1, 1), (80, 1, 1, 1), 0); del buf451  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_37.run(buf455, convolution_95, 10240, stream=stream0)
            del convolution_95
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf457 = torch.ops.aten.convolution_backward.default(buf455, mean_9, convert_element_type_249, [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_249
            del mean_9
            buf458 = buf457[0]
            assert_size_stride(buf458, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf458, 16, 'torch.ops.aten.convolution_backward.default')
            buf459 = buf457[1]
            assert_size_stride(buf459, (80, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf459, 16, 'torch.ops.aten.convolution_backward.default')
            del buf457
            buf462 = buf388; del buf388  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_9], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_57.run(buf443, buf439, convolution_96, buf458, buf446, buf462, 25088, 480, stream=stream0)
            del buf439
            del buf458
            del convolution_96
            buf464 = buf417; del buf417  # reuse
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_58.run(buf462, cat_26, getitem_259, buf464, 480, 196, 128, stream=stream0)
            buf465 = buf418; del buf418  # reuse
            buf466 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf464, rsqrt_37, buf465, buf466, 480, 196, stream=stream0)
            buf463 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_60.run(buf462, buf463, 480, 25088, stream=stream0)
            buf467 = reinterpret_tensor(buf446, (128, 480, 14, 14), (94080, 196, 14, 1), 0); del buf446  # reuse
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_61.run(buf462, cat_26, getitem_259, buf465, rsqrt_37, buf463, primals_303, buf467, 61440, 196, stream=stream0)
            del buf462
            del cat_26
            del getitem_259
            del primals_303
            del rsqrt_37
            buf468 = buf411; del buf411  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_62.run(buf467, buf468, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf469 = torch.ops.aten.convolution_backward.default(buf468, getitem_257, convert_element_type_243, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_243
            del getitem_257
            buf470 = buf469[0]
            assert_size_stride(buf470, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf470, 16, 'torch.ops.aten.convolution_backward.default')
            buf471 = buf469[1]
            assert_size_stride(buf471, (120, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf471, 16, 'torch.ops.aten.convolution_backward.default')
            del buf469
            buf473 = buf468; del buf468  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_63.run(buf467, buf473, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf474 = torch.ops.aten.convolution_backward.default(buf473, getitem_252, convert_element_type_242, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_242
            del getitem_252
            buf475 = buf474[0]
            assert_size_stride(buf475, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf475, 16, 'torch.ops.aten.convolution_backward.default')
            buf476 = buf474[1]
            assert_size_stride(buf476, (120, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf476, 16, 'torch.ops.aten.convolution_backward.default')
            del buf474
            buf478 = buf473; del buf473  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_64.run(buf467, buf478, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf479 = torch.ops.aten.convolution_backward.default(buf478, getitem_247, convert_element_type_241, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False])
            del convert_element_type_241
            del getitem_247
            buf480 = buf479[0]
            assert_size_stride(buf480, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf480, 16, 'torch.ops.aten.convolution_backward.default')
            buf481 = buf479[1]
            assert_size_stride(buf481, (120, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf481, 16, 'torch.ops.aten.convolution_backward.default')
            del buf479
            buf483 = buf478; del buf478  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_65.run(buf467, buf483, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf484 = torch.ops.aten.convolution_backward.default(buf483, getitem_242, convert_element_type_240, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False])
            del buf483
            del convert_element_type_240
            del getitem_242
            buf485 = buf484[0]
            assert_size_stride(buf485, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf485, 16, 'torch.ops.aten.convolution_backward.default')
            buf486 = buf484[1]
            assert_size_stride(buf486, (120, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf486, 16, 'torch.ops.aten.convolution_backward.default')
            del buf484
            buf489 = buf467; del buf467  # reuse
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_66.run(cat_25, getitem_237, rsqrt_36, primals_294, primals_295, buf485, buf480, buf475, buf470, buf489, 25088, 480, stream=stream0)
            del buf470
            del buf475
            del buf480
            del buf485
            del primals_295
            buf491 = buf464; del buf464  # reuse
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_67.run(buf489, cat_25, getitem_237, buf491, 480, 196, 128, stream=stream0)
            buf492 = buf465; del buf465  # reuse
            buf493 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_59.run(buf491, rsqrt_36, buf492, buf493, 480, 196, stream=stream0)
            del buf491
            buf490 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_68.run(buf489, buf490, 480, 25088, stream=stream0)
            buf494 = buf489; del buf489  # reuse
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_69.run(buf494, cat_25, getitem_237, buf492, rsqrt_36, buf490, primals_294, 61440, 196, stream=stream0)
            del cat_25
            del getitem_237
            del primals_294
            del rsqrt_36
            buf495 = buf443; del buf443  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_70.run(buf494, buf495, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf496 = torch.ops.aten.convolution_backward.default(buf495, getitem_235, convert_element_type_235, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_235
            del getitem_235
            buf497 = buf496[0]
            assert_size_stride(buf497, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf497, 16, 'torch.ops.aten.convolution_backward.default')
            buf498 = buf496[1]
            assert_size_stride(buf498, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf498, 16, 'torch.ops.aten.convolution_backward.default')
            del buf496
            buf500 = buf495; del buf495  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_71.run(buf494, buf500, 30720, 196, stream=stream0)
            del buf494
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf501 = torch.ops.aten.convolution_backward.default(buf500, getitem_234, convert_element_type_234, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_234
            del getitem_234
            buf502 = buf501[0]
            assert_size_stride(buf502, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf502, 16, 'torch.ops.aten.convolution_backward.default')
            buf503 = buf501[1]
            assert_size_stride(buf503, (240, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf503, 16, 'torch.ops.aten.convolution_backward.default')
            del buf501
            buf505 = buf434; del buf434  # reuse
            buf507 = buf432; del buf432  # reuse
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_72.run(buf431, buf502, buf497, convolution_88, unsqueeze_498, buf505, buf507, 31360, 128, stream=stream0)
            buf506 = buf435; del buf435  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_52.run(buf505, buf506, 160, 196, stream=stream0)
            del buf505
            buf508 = empty_strided_cuda((160, ), (1, ), torch.float32)
            buf510 = empty_strided_cuda((160, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_53.run(buf507, squeeze_106, buf508, buf510, 160, 196, stream=stream0)
            del buf507
            buf511 = buf437; del buf437  # reuse
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_77.run(buf431, buf502, buf497, convolution_88, unsqueeze_498, buf508, squeeze_106, buf506, primals_287, buf511, 4014080, stream=stream0)
            del buf431
            del buf497
            del buf502
            del buf508
            del convolution_88
            del primals_287
            del squeeze_106
            del unsqueeze_498
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf512 = torch.ops.aten.convolution_backward.default(buf511, mul_280, convert_element_type_231, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf511
            del convert_element_type_231
            del mul_280
            buf513 = buf512[0]
            assert_size_stride(buf513, (128, 624, 14, 14), (122304, 1, 8736, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf513, 16, 'torch.ops.aten.convolution_backward.default')
            buf514 = buf512[1]
            assert_size_stride(buf514, (160, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf514, 16, 'torch.ops.aten.convolution_backward.default')
            del buf512
            buf516 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(convolution_85, getitem_231, rsqrt_34, primals_277, primals_278, buf516, 15654912, stream=stream0)
            del primals_278
            buf517 = empty_strided_cuda((128, 624, 1, 1), (624, 1, 79872, 79872), torch.float16)
            buf518 = reinterpret_tensor(buf517, (128, 624, 1, 1), (624, 1, 1, 1), 0); del buf517  # reuse
            # Topologically Sorted Source Nodes: [x_109, sigmoid_8], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_79.run(buf518, buf513, buf516, convolution_87, 79872, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf520 = torch.ops.aten.convolution_backward.default(buf518, convert_element_type_228, convert_element_type_230, [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_228
            del convert_element_type_230
            buf521 = buf520[0]
            assert_size_stride(buf521, (128, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf521, 16, 'torch.ops.aten.convolution_backward.default')
            buf522 = buf520[1]
            assert_size_stride(buf522, (624, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf522, 16, 'torch.ops.aten.convolution_backward.default')
            del buf520
            buf525 = reinterpret_tensor(buf521, (128, 52, 1, 1), (52, 1, 1, 1), 0); del buf521  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_80.run(buf525, convolution_86, 6656, stream=stream0)
            del convolution_86
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf527 = torch.ops.aten.convolution_backward.default(buf525, mean_8, convert_element_type_226, [52], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_226
            del mean_8
            buf528 = buf527[0]
            assert_size_stride(buf528, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf528, 16, 'torch.ops.aten.convolution_backward.default')
            buf529 = buf527[1]
            assert_size_stride(buf529, (52, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf529, 16, 'torch.ops.aten.convolution_backward.default')
            del buf527
            buf532 = empty_strided_cuda((624, 196), (1, 624), torch.float32)
            buf534 = empty_strided_cuda((624, 196), (1, 624), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_81.run(buf513, convolution_87, buf528, buf516, convolution_85, getitem_231, buf532, buf534, 122304, 128, stream=stream0)
            buf533 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_8], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82.run(buf532, buf533, 624, 196, stream=stream0)
            buf535 = empty_strided_cuda((624, ), (1, ), torch.float32)
            buf537 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83.run(buf534, rsqrt_34, buf535, buf537, 624, 196, stream=stream0)
            buf538 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [sigmoid_8, x_108], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_84.run(buf513, convolution_87, buf528, buf516, convolution_85, getitem_231, buf535, rsqrt_34, buf533, primals_277, buf538, 15654912, stream=stream0)
            del buf513
            del buf516
            del convolution_85
            del convolution_87
            del getitem_231
            del primals_277
            del rsqrt_34
            # Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
            buf539 = torch.ops.aten.convolution_backward.default(buf538, convert_element_type_219, convert_element_type_220, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 624, [True, True, False])
            del convert_element_type_219
            del convert_element_type_220
            buf540 = buf539[0]
            assert_size_stride(buf540, (128, 624, 14, 14), (122304, 1, 8736, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf540, 16, 'torch.ops.aten.convolution_backward.default')
            buf541 = buf539[1]
            assert_size_stride(buf541, (624, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf541, 16, 'torch.ops.aten.convolution_backward.default')
            del buf539
            buf543 = buf538; del buf538  # reuse
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(convolution_84, getitem_229, rsqrt_33, primals_271, primals_272, buf543, 15654912, stream=stream0)
            del primals_272
            buf544 = buf534; del buf534  # reuse
            buf546 = buf532; del buf532  # reuse
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_fill_mul_native_batch_norm_backward_sigmoid_sub_85.run(buf540, buf543, convolution_84, getitem_229, buf544, buf546, 122304, 128, stream=stream0)
            buf545 = buf535; del buf535  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_82.run(buf544, buf545, 624, 196, stream=stream0)
            del buf544
            buf547 = empty_strided_cuda((624, ), (1, ), torch.float32)
            buf548 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_83.run(buf546, rsqrt_33, buf547, buf548, 624, 196, stream=stream0)
            buf549 = buf540; del buf540  # reuse
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_86.run(buf549, buf543, convolution_84, getitem_229, buf547, rsqrt_33, buf545, primals_271, 15654912, stream=stream0)
            del buf543
            del convolution_84
            del getitem_229
            del primals_271
            del rsqrt_33
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf550 = torch.ops.aten.convolution_backward.default(buf549, add_172, convert_element_type_215, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_172
            del convert_element_type_215
            buf551 = buf550[0]
            assert_size_stride(buf551, (128, 104, 14, 14), (20384, 1, 1456, 104), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf551, 16, 'torch.ops.aten.convolution_backward.default')
            buf552 = buf550[1]
            assert_size_stride(buf552, (624, 104, 1, 1), (104, 1, 104, 104), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf552, 16, 'torch.ops.aten.convolution_backward.default')
            del buf550
            buf554 = empty_strided_cuda((104, 196), (1, 104), torch.float32)
            buf556 = empty_strided_cuda((104, 196), (1, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87.run(buf551, cat_24, unsqueeze_534, buf554, buf556, 20384, 128, stream=stream0)
            buf555 = empty_strided_cuda((104, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_88.run(buf554, buf555, 104, 196, stream=stream0)
            buf557 = empty_strided_cuda((104, ), (1, ), torch.float32)
            buf558 = empty_strided_cuda((104, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.run(buf556, squeeze_97, buf557, buf558, 104, 196, stream=stream0)
            buf559 = cat_24; del cat_24  # reuse
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90.run(buf559, buf551, unsqueeze_534, buf557, squeeze_97, buf555, primals_265, 2609152, stream=stream0)
            del primals_265
            del squeeze_97
            del unsqueeze_534
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf560 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf559, (128, 52, 14, 14), (20384, 1, 1456, 104), 52), getitem_225, convert_element_type_212, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_212
            del getitem_225
            buf561 = buf560[0]
            assert_size_stride(buf561, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            # buffer buf561 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf562 = buf560[1]
            assert_size_stride(buf562, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            # buffer buf562 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf560
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf564 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf559, (128, 52, 14, 14), (20384, 1, 1456, 104), 0), getitem_224, convert_element_type_211, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf559
            del convert_element_type_211
            del getitem_224
            buf565 = buf564[0]
            assert_size_stride(buf565, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf565, 16, 'torch.ops.aten.convolution_backward.default')
            buf566 = buf564[1]
            assert_size_stride(buf566, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf566, 16, 'torch.ops.aten.convolution_backward.default')
            del buf564
            buf568 = buf549; del buf549  # reuse
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(cat_23, getitem_223, rsqrt_31, primals_254, primals_255, buf568, 15654912, stream=stream0)
            del primals_255
            buf569 = reinterpret_tensor(buf528, (128, 624, 1, 1), (624, 1, 79872, 79872), 0); del buf528  # reuse
            buf570 = reinterpret_tensor(buf569, (128, 624, 1, 1), (624, 1, 1, 1), 0); del buf569  # reuse
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91.run(buf570, buf565, buf561, buf568, convolution_81, 79872, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf572 = torch.ops.aten.convolution_backward.default(buf570, convert_element_type_208, convert_element_type_210, [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_208
            del convert_element_type_210
            buf573 = buf572[0]
            assert_size_stride(buf573, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf573, 16, 'torch.ops.aten.convolution_backward.default')
            buf574 = buf572[1]
            assert_size_stride(buf574, (624, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf574, 16, 'torch.ops.aten.convolution_backward.default')
            del buf572
            buf577 = reinterpret_tensor(buf573, (128, 26, 1, 1), (26, 1, 1, 1), 0); del buf573  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_92.run(buf577, convolution_80, 3328, stream=stream0)
            del convolution_80
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf579 = torch.ops.aten.convolution_backward.default(buf577, mean_7, convert_element_type_206, [26], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_206
            del mean_7
            buf580 = buf579[0]
            assert_size_stride(buf580, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf580, 16, 'torch.ops.aten.convolution_backward.default')
            buf581 = buf579[1]
            assert_size_stride(buf581, (26, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf581, 16, 'torch.ops.aten.convolution_backward.default')
            del buf579
            buf584 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_7], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93.run(buf565, buf561, convolution_81, buf580, buf568, buf584, 25088, 624, stream=stream0)
            del buf561
            del convolution_81
            buf586 = reinterpret_tensor(buf546, (624, 196), (196, 1), 0); del buf546  # reuse
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94.run(buf584, cat_23, getitem_223, buf586, 624, 196, 128, stream=stream0)
            buf587 = buf547; del buf547  # reuse
            buf588 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf586, rsqrt_31, buf587, buf588, 624, 196, stream=stream0)
            buf585 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_96.run(buf584, buf585, 624, 25088, stream=stream0)
            buf589 = reinterpret_tensor(buf568, (128, 624, 14, 14), (122304, 196, 14, 1), 0); del buf568  # reuse
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97.run(buf584, cat_23, getitem_223, buf587, rsqrt_31, buf585, primals_254, buf589, 79872, 196, stream=stream0)
            del cat_23
            del getitem_223
            del primals_254
            del rsqrt_31
            buf590 = empty_strided_cuda((128, 156, 14, 14), (30576, 1, 2184, 156), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_98.run(buf589, buf590, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf591 = torch.ops.aten.convolution_backward.default(buf590, getitem_221, convert_element_type_200, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_200
            del getitem_221
            buf592 = buf591[0]
            assert_size_stride(buf592, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf592, 16, 'torch.ops.aten.convolution_backward.default')
            buf593 = buf591[1]
            assert_size_stride(buf593, (156, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf593, 16, 'torch.ops.aten.convolution_backward.default')
            del buf591
            buf595 = buf590; del buf590  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_99.run(buf589, buf595, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf596 = torch.ops.aten.convolution_backward.default(buf595, getitem_216, convert_element_type_199, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_199
            del getitem_216
            buf597 = buf596[0]
            assert_size_stride(buf597, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf597, 16, 'torch.ops.aten.convolution_backward.default')
            buf598 = buf596[1]
            assert_size_stride(buf598, (156, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf598, 16, 'torch.ops.aten.convolution_backward.default')
            del buf596
            buf600 = buf595; del buf595  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_100.run(buf589, buf600, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf601 = torch.ops.aten.convolution_backward.default(buf600, getitem_211, convert_element_type_198, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_198
            del getitem_211
            buf602 = buf601[0]
            assert_size_stride(buf602, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf602, 16, 'torch.ops.aten.convolution_backward.default')
            buf603 = buf601[1]
            assert_size_stride(buf603, (156, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf603, 16, 'torch.ops.aten.convolution_backward.default')
            del buf601
            buf605 = buf600; del buf600  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_101.run(buf589, buf605, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf606 = torch.ops.aten.convolution_backward.default(buf605, getitem_206, convert_element_type_197, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 156, [True, True, False])
            del buf605
            del convert_element_type_197
            del getitem_206
            buf607 = buf606[0]
            assert_size_stride(buf607, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf607, 16, 'torch.ops.aten.convolution_backward.default')
            buf608 = buf606[1]
            assert_size_stride(buf608, (156, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf608, 16, 'torch.ops.aten.convolution_backward.default')
            del buf606
            buf611 = buf589; del buf589  # reuse
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102.run(cat_22, getitem_201, rsqrt_30, primals_245, primals_246, buf607, buf602, buf597, buf592, buf611, 25088, 624, stream=stream0)
            del buf592
            del buf597
            del buf602
            del primals_246
            buf613 = buf586; del buf586  # reuse
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103.run(buf611, cat_22, getitem_201, buf613, 624, 196, 128, stream=stream0)
            buf614 = buf587; del buf587  # reuse
            buf615 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf613, rsqrt_30, buf614, buf615, 624, 196, stream=stream0)
            buf612 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_104.run(buf611, buf612, 624, 25088, stream=stream0)
            buf616 = buf611; del buf611  # reuse
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105.run(buf616, cat_22, getitem_201, buf614, rsqrt_30, buf612, primals_245, 79872, 196, stream=stream0)
            del cat_22
            del getitem_201
            del primals_245
            del rsqrt_30
            buf617 = buf565; del buf565  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_106.run(buf616, buf617, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf618 = torch.ops.aten.convolution_backward.default(buf617, getitem_199, convert_element_type_192, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_192
            del getitem_199
            buf619 = buf618[0]
            assert_size_stride(buf619, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf619, 16, 'torch.ops.aten.convolution_backward.default')
            buf620 = buf618[1]
            assert_size_stride(buf620, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf620, 16, 'torch.ops.aten.convolution_backward.default')
            del buf618
            buf622 = buf617; del buf617  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_107.run(buf616, buf622, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf623 = torch.ops.aten.convolution_backward.default(buf622, getitem_198, convert_element_type_191, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf622
            del convert_element_type_191
            del getitem_198
            buf624 = buf623[0]
            assert_size_stride(buf624, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf624, 16, 'torch.ops.aten.convolution_backward.default')
            buf625 = buf623[1]
            assert_size_stride(buf625, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf625, 16, 'torch.ops.aten.convolution_backward.default')
            del buf623
            buf627 = buf556; del buf556  # reuse
            buf629 = buf554; del buf554  # reuse
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108.run(buf551, buf624, buf619, cat_21, unsqueeze_570, buf627, buf629, 20384, 128, stream=stream0)
            buf628 = buf557; del buf557  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_88.run(buf627, buf628, 104, 196, stream=stream0)
            buf630 = empty_strided_cuda((104, ), (1, ), torch.float32)
            buf632 = empty_strided_cuda((104, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.run(buf629, squeeze_88, buf630, buf632, 104, 196, stream=stream0)
            buf631 = empty_strided_cuda((128, 104, 14, 14), (20384, 1, 1456, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_109.run(buf551, buf624, buf619, cat_21, unsqueeze_570, buf630, squeeze_88, buf628, buf631, 2609152, stream=stream0)
            del cat_21
            del unsqueeze_570
            buf633 = empty_strided_cuda((128, 52, 14, 14), (10192, 1, 728, 52), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_110.run(buf631, squeeze_88, primals_238, buf633, 1304576, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf634 = torch.ops.aten.convolution_backward.default(buf633, getitem_195, convert_element_type_188, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_188
            del getitem_195
            buf635 = buf634[0]
            assert_size_stride(buf635, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf635, 16, 'torch.ops.aten.convolution_backward.default')
            buf636 = buf634[1]
            assert_size_stride(buf636, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf636, 16, 'torch.ops.aten.convolution_backward.default')
            del buf634
            buf638 = buf633; del buf633  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_111.run(buf631, squeeze_88, primals_238, buf638, 1304576, stream=stream0)
            del buf631
            del primals_238
            del squeeze_88
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf639 = torch.ops.aten.convolution_backward.default(buf638, getitem_194, convert_element_type_187, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf638
            del convert_element_type_187
            del getitem_194
            buf640 = buf639[0]
            assert_size_stride(buf640, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf640, 16, 'torch.ops.aten.convolution_backward.default')
            buf641 = buf639[1]
            assert_size_stride(buf641, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf641, 16, 'torch.ops.aten.convolution_backward.default')
            del buf639
            buf643 = reinterpret_tensor(buf616, (128, 624, 14, 14), (122304, 1, 8736, 624), 0); del buf616  # reuse
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(cat_20, getitem_193, rsqrt_28, primals_227, primals_228, buf643, 15654912, stream=stream0)
            del primals_228
            buf644 = reinterpret_tensor(buf580, (128, 624, 1, 1), (624, 1, 79872, 79872), 0); del buf580  # reuse
            buf645 = reinterpret_tensor(buf644, (128, 624, 1, 1), (624, 1, 1, 1), 0); del buf644  # reuse
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_112.run(buf645, buf640, buf635, buf643, convolution_71, 79872, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf647 = torch.ops.aten.convolution_backward.default(buf645, convert_element_type_184, convert_element_type_186, [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_184
            del convert_element_type_186
            buf648 = buf647[0]
            assert_size_stride(buf648, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf648, 16, 'torch.ops.aten.convolution_backward.default')
            buf649 = buf647[1]
            assert_size_stride(buf649, (624, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf649, 16, 'torch.ops.aten.convolution_backward.default')
            del buf647
            buf652 = reinterpret_tensor(buf648, (128, 26, 1, 1), (26, 1, 1, 1), 0); del buf648  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_92.run(buf652, convolution_70, 3328, stream=stream0)
            del convolution_70
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf654 = torch.ops.aten.convolution_backward.default(buf652, mean_6, convert_element_type_182, [26], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_182
            del mean_6
            buf655 = buf654[0]
            assert_size_stride(buf655, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf655, 16, 'torch.ops.aten.convolution_backward.default')
            buf656 = buf654[1]
            assert_size_stride(buf656, (26, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf656, 16, 'torch.ops.aten.convolution_backward.default')
            del buf654
            buf659 = buf584; del buf584  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_6], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_113.run(buf640, buf635, convolution_71, buf655, buf643, buf659, 25088, 624, stream=stream0)
            del buf635
            del convolution_71
            buf661 = buf613; del buf613  # reuse
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94.run(buf659, cat_20, getitem_193, buf661, 624, 196, 128, stream=stream0)
            buf662 = buf614; del buf614  # reuse
            buf663 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf661, rsqrt_28, buf662, buf663, 624, 196, stream=stream0)
            buf660 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_96.run(buf659, buf660, 624, 25088, stream=stream0)
            buf664 = reinterpret_tensor(buf643, (128, 624, 14, 14), (122304, 196, 14, 1), 0); del buf643  # reuse
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97.run(buf659, cat_20, getitem_193, buf662, rsqrt_28, buf660, primals_227, buf664, 79872, 196, stream=stream0)
            del cat_20
            del getitem_193
            del primals_227
            del rsqrt_28
            buf665 = buf607; del buf607  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_98.run(buf664, buf665, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf666 = torch.ops.aten.convolution_backward.default(buf665, getitem_191, convert_element_type_176, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_176
            del getitem_191
            buf667 = buf666[0]
            assert_size_stride(buf667, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf667, 16, 'torch.ops.aten.convolution_backward.default')
            buf668 = buf666[1]
            assert_size_stride(buf668, (156, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf668, 16, 'torch.ops.aten.convolution_backward.default')
            del buf666
            buf670 = buf665; del buf665  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_99.run(buf664, buf670, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf671 = torch.ops.aten.convolution_backward.default(buf670, getitem_186, convert_element_type_175, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_175
            del getitem_186
            buf672 = buf671[0]
            assert_size_stride(buf672, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf672, 16, 'torch.ops.aten.convolution_backward.default')
            buf673 = buf671[1]
            assert_size_stride(buf673, (156, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf673, 16, 'torch.ops.aten.convolution_backward.default')
            del buf671
            buf675 = buf670; del buf670  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_100.run(buf664, buf675, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf676 = torch.ops.aten.convolution_backward.default(buf675, getitem_181, convert_element_type_174, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_174
            del getitem_181
            buf677 = buf676[0]
            assert_size_stride(buf677, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf677, 16, 'torch.ops.aten.convolution_backward.default')
            buf678 = buf676[1]
            assert_size_stride(buf678, (156, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf678, 16, 'torch.ops.aten.convolution_backward.default')
            del buf676
            buf680 = buf675; del buf675  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_101.run(buf664, buf680, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf681 = torch.ops.aten.convolution_backward.default(buf680, getitem_176, convert_element_type_173, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 156, [True, True, False])
            del buf680
            del convert_element_type_173
            del getitem_176
            buf682 = buf681[0]
            assert_size_stride(buf682, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf682, 16, 'torch.ops.aten.convolution_backward.default')
            buf683 = buf681[1]
            assert_size_stride(buf683, (156, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf683, 16, 'torch.ops.aten.convolution_backward.default')
            del buf681
            buf686 = buf664; del buf664  # reuse
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102.run(cat_19, getitem_171, rsqrt_27, primals_218, primals_219, buf682, buf677, buf672, buf667, buf686, 25088, 624, stream=stream0)
            del buf667
            del buf672
            del buf677
            del primals_219
            buf688 = buf661; del buf661  # reuse
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103.run(buf686, cat_19, getitem_171, buf688, 624, 196, 128, stream=stream0)
            buf689 = buf662; del buf662  # reuse
            buf690 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf688, rsqrt_27, buf689, buf690, 624, 196, stream=stream0)
            buf687 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_104.run(buf686, buf687, 624, 25088, stream=stream0)
            buf691 = buf686; del buf686  # reuse
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105.run(buf691, cat_19, getitem_171, buf689, rsqrt_27, buf687, primals_218, 79872, 196, stream=stream0)
            del cat_19
            del getitem_171
            del primals_218
            del rsqrt_27
            buf692 = buf640; del buf640  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_106.run(buf691, buf692, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf693 = torch.ops.aten.convolution_backward.default(buf692, getitem_169, convert_element_type_168, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_168
            del getitem_169
            buf694 = buf693[0]
            assert_size_stride(buf694, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf694, 16, 'torch.ops.aten.convolution_backward.default')
            buf695 = buf693[1]
            assert_size_stride(buf695, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf695, 16, 'torch.ops.aten.convolution_backward.default')
            del buf693
            buf697 = buf692; del buf692  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_107.run(buf691, buf697, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf698 = torch.ops.aten.convolution_backward.default(buf697, getitem_168, convert_element_type_167, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf697
            del convert_element_type_167
            del getitem_168
            buf699 = buf698[0]
            assert_size_stride(buf699, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf699, 16, 'torch.ops.aten.convolution_backward.default')
            buf700 = buf698[1]
            assert_size_stride(buf700, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf700, 16, 'torch.ops.aten.convolution_backward.default')
            del buf698
            buf702 = buf551; del buf551  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_114.run(buf702, buf624, buf619, buf699, buf694, 2609152, stream=stream0)
            del buf619
            del buf624
            del buf694
            del buf699
            buf703 = buf629; del buf629  # reuse
            buf705 = buf627; del buf627  # reuse
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_87.run(buf702, cat_18, unsqueeze_606, buf703, buf705, 20384, 128, stream=stream0)
            buf704 = buf630; del buf630  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_88.run(buf703, buf704, 104, 196, stream=stream0)
            buf706 = empty_strided_cuda((104, ), (1, ), torch.float32)
            buf707 = empty_strided_cuda((104, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.run(buf705, squeeze_79, buf706, buf707, 104, 196, stream=stream0)
            buf708 = cat_18; del cat_18  # reuse
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_90.run(buf708, buf702, unsqueeze_606, buf706, squeeze_79, buf704, primals_211, 2609152, stream=stream0)
            del primals_211
            del squeeze_79
            del unsqueeze_606
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf709 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf708, (128, 52, 14, 14), (20384, 1, 1456, 104), 52), getitem_165, convert_element_type_164, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_164
            del getitem_165
            buf710 = buf709[0]
            assert_size_stride(buf710, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            # buffer buf710 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf711 = buf709[1]
            assert_size_stride(buf711, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            # buffer buf711 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf709
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf713 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf708, (128, 52, 14, 14), (20384, 1, 1456, 104), 0), getitem_164, convert_element_type_163, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_163
            del getitem_164
            buf714 = buf713[0]
            assert_size_stride(buf714, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf714, 16, 'torch.ops.aten.convolution_backward.default')
            buf715 = buf713[1]
            assert_size_stride(buf715, (52, 312, 1, 1), (312, 1, 312, 312), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf715, 16, 'torch.ops.aten.convolution_backward.default')
            del buf713
            buf717 = reinterpret_tensor(buf691, (128, 624, 14, 14), (122304, 1, 8736, 624), 0); del buf691  # reuse
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(cat_17, getitem_163, rsqrt_25, primals_200, primals_201, buf717, 15654912, stream=stream0)
            del primals_201
            buf718 = reinterpret_tensor(buf655, (128, 624, 1, 1), (624, 1, 79872, 79872), 0); del buf655  # reuse
            buf719 = reinterpret_tensor(buf718, (128, 624, 1, 1), (624, 1, 1, 1), 0); del buf718  # reuse
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_91.run(buf719, buf714, buf710, buf717, convolution_61, 79872, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf721 = torch.ops.aten.convolution_backward.default(buf719, convert_element_type_160, convert_element_type_162, [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_160
            del convert_element_type_162
            buf722 = buf721[0]
            assert_size_stride(buf722, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf722, 16, 'torch.ops.aten.convolution_backward.default')
            buf723 = buf721[1]
            assert_size_stride(buf723, (624, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf723, 16, 'torch.ops.aten.convolution_backward.default')
            del buf721
            buf726 = reinterpret_tensor(buf722, (128, 26, 1, 1), (26, 1, 1, 1), 0); del buf722  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_92.run(buf726, convolution_60, 3328, stream=stream0)
            del convolution_60
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf728 = torch.ops.aten.convolution_backward.default(buf726, mean_5, convert_element_type_158, [26], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_158
            del mean_5
            buf729 = buf728[0]
            assert_size_stride(buf729, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf729, 16, 'torch.ops.aten.convolution_backward.default')
            buf730 = buf728[1]
            assert_size_stride(buf730, (26, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf730, 16, 'torch.ops.aten.convolution_backward.default')
            del buf728
            buf733 = buf659; del buf659  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_5], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_93.run(buf714, buf710, convolution_61, buf729, buf717, buf733, 25088, 624, stream=stream0)
            del buf710
            del buf729
            del convolution_61
            buf735 = buf688; del buf688  # reuse
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_94.run(buf733, cat_17, getitem_163, buf735, 624, 196, 128, stream=stream0)
            buf736 = buf689; del buf689  # reuse
            buf737 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf735, rsqrt_25, buf736, buf737, 624, 196, stream=stream0)
            buf734 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_96.run(buf733, buf734, 624, 25088, stream=stream0)
            buf738 = reinterpret_tensor(buf717, (128, 624, 14, 14), (122304, 196, 14, 1), 0); del buf717  # reuse
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_97.run(buf733, cat_17, getitem_163, buf736, rsqrt_25, buf734, primals_200, buf738, 79872, 196, stream=stream0)
            del buf733
            del cat_17
            del getitem_163
            del primals_200
            del rsqrt_25
            buf739 = buf682; del buf682  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_98.run(buf738, buf739, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf740 = torch.ops.aten.convolution_backward.default(buf739, getitem_161, convert_element_type_152, [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_152
            del getitem_161
            buf741 = buf740[0]
            assert_size_stride(buf741, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf741, 16, 'torch.ops.aten.convolution_backward.default')
            buf742 = buf740[1]
            assert_size_stride(buf742, (156, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf742, 16, 'torch.ops.aten.convolution_backward.default')
            del buf740
            buf744 = buf739; del buf739  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_99.run(buf738, buf744, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf745 = torch.ops.aten.convolution_backward.default(buf744, getitem_156, convert_element_type_151, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_151
            del getitem_156
            buf746 = buf745[0]
            assert_size_stride(buf746, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf746, 16, 'torch.ops.aten.convolution_backward.default')
            buf747 = buf745[1]
            assert_size_stride(buf747, (156, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf747, 16, 'torch.ops.aten.convolution_backward.default')
            del buf745
            buf749 = buf744; del buf744  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_100.run(buf738, buf749, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf750 = torch.ops.aten.convolution_backward.default(buf749, getitem_151, convert_element_type_150, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 156, [True, True, False])
            del convert_element_type_150
            del getitem_151
            buf751 = buf750[0]
            assert_size_stride(buf751, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf751, 16, 'torch.ops.aten.convolution_backward.default')
            buf752 = buf750[1]
            assert_size_stride(buf752, (156, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf752, 16, 'torch.ops.aten.convolution_backward.default')
            del buf750
            buf754 = buf749; del buf749  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_101.run(buf738, buf754, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf755 = torch.ops.aten.convolution_backward.default(buf754, getitem_146, convert_element_type_149, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 156, [True, True, False])
            del buf754
            del convert_element_type_149
            del getitem_146
            buf756 = buf755[0]
            assert_size_stride(buf756, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf756, 16, 'torch.ops.aten.convolution_backward.default')
            buf757 = buf755[1]
            assert_size_stride(buf757, (156, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf757, 16, 'torch.ops.aten.convolution_backward.default')
            del buf755
            buf760 = buf738; del buf738  # reuse
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten.fill, aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_102.run(cat_16, getitem_141, rsqrt_24, primals_191, primals_192, buf756, buf751, buf746, buf741, buf760, 25088, 624, stream=stream0)
            del buf741
            del buf746
            del buf751
            del buf756
            del primals_192
            buf762 = buf735; del buf735  # reuse
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_103.run(buf760, cat_16, getitem_141, buf762, 624, 196, 128, stream=stream0)
            buf763 = buf736; del buf736  # reuse
            buf764 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_95.run(buf762, rsqrt_24, buf763, buf764, 624, 196, stream=stream0)
            del buf762
            buf761 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_104.run(buf760, buf761, 624, 25088, stream=stream0)
            buf765 = buf760; del buf760  # reuse
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_105.run(buf765, cat_16, getitem_141, buf763, rsqrt_24, buf761, primals_191, 79872, 196, stream=stream0)
            del cat_16
            del getitem_141
            del primals_191
            del rsqrt_24
            buf766 = buf714; del buf714  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_106.run(buf765, buf766, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf767 = torch.ops.aten.convolution_backward.default(buf766, getitem_139, convert_element_type_144, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_144
            del getitem_139
            buf768 = buf767[0]
            assert_size_stride(buf768, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf768, 16, 'torch.ops.aten.convolution_backward.default')
            buf769 = buf767[1]
            assert_size_stride(buf769, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf769, 16, 'torch.ops.aten.convolution_backward.default')
            del buf767
            buf771 = buf766; del buf766  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_107.run(buf765, buf771, 39936, 196, stream=stream0)
            del buf765
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf772 = torch.ops.aten.convolution_backward.default(buf771, getitem_138, convert_element_type_143, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf771
            del convert_element_type_143
            del getitem_138
            buf773 = buf772[0]
            assert_size_stride(buf773, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf773, 16, 'torch.ops.aten.convolution_backward.default')
            buf774 = buf772[1]
            assert_size_stride(buf774, (312, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf774, 16, 'torch.ops.aten.convolution_backward.default')
            del buf772
            buf776 = buf705; del buf705  # reuse
            buf778 = buf703; del buf703  # reuse
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_108.run(buf702, buf773, buf768, convolution_53, unsqueeze_642, buf776, buf778, 20384, 128, stream=stream0)
            buf777 = buf706; del buf706  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_88.run(buf776, buf777, 104, 196, stream=stream0)
            del buf776
            buf779 = empty_strided_cuda((104, ), (1, ), torch.float32)
            buf781 = empty_strided_cuda((104, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_89.run(buf778, squeeze_70, buf779, buf781, 104, 196, stream=stream0)
            del buf778
            buf782 = buf708; del buf708  # reuse
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_115.run(buf702, buf773, buf768, convolution_53, unsqueeze_642, buf779, squeeze_70, buf777, primals_184, buf782, 2609152, stream=stream0)
            del buf702
            del buf768
            del buf773
            del buf779
            del convolution_53
            del primals_184
            del squeeze_70
            del unsqueeze_642
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf783 = torch.ops.aten.convolution_backward.default(buf782, mul_180, convert_element_type_140, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf782
            del convert_element_type_140
            del mul_180
            buf784 = buf783[0]
            assert_size_stride(buf784, (128, 336, 14, 14), (65856, 1, 4704, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf784, 16, 'torch.ops.aten.convolution_backward.default')
            buf785 = buf783[1]
            assert_size_stride(buf785, (104, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf785, 16, 'torch.ops.aten.convolution_backward.default')
            del buf783
            buf787 = empty_strided_cuda((128, 336, 14, 14), (65856, 1, 4704, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_116.run(cat_15, getitem_135, rsqrt_22, primals_174, primals_175, buf787, 8429568, stream=stream0)
            del primals_175
            buf788 = empty_strided_cuda((128, 336, 1, 1), (336, 1, 43008, 43008), torch.float16)
            buf789 = reinterpret_tensor(buf788, (128, 336, 1, 1), (336, 1, 1, 1), 0); del buf788  # reuse
            # Topologically Sorted Source Nodes: [x_70, sigmoid_4], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_117.run(buf789, buf784, buf787, convolution_52, 43008, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf791 = torch.ops.aten.convolution_backward.default(buf789, convert_element_type_137, convert_element_type_139, [336], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_137
            del convert_element_type_139
            buf792 = buf791[0]
            assert_size_stride(buf792, (128, 14, 1, 1), (14, 1, 14, 14), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf792, 16, 'torch.ops.aten.convolution_backward.default')
            buf793 = buf791[1]
            assert_size_stride(buf793, (336, 14, 1, 1), (14, 1, 14, 14), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf793, 16, 'torch.ops.aten.convolution_backward.default')
            del buf791
            buf796 = reinterpret_tensor(buf792, (128, 14, 1, 1), (14, 1, 1, 1), 0); del buf792  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_118.run(buf796, convolution_51, 1792, stream=stream0)
            del convolution_51
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf798 = torch.ops.aten.convolution_backward.default(buf796, mean_4, convert_element_type_135, [14], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_135
            del mean_4
            buf799 = buf798[0]
            assert_size_stride(buf799, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf799, 16, 'torch.ops.aten.convolution_backward.default')
            buf800 = buf798[1]
            assert_size_stride(buf800, (14, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf800, 16, 'torch.ops.aten.convolution_backward.default')
            del buf798
            buf803 = empty_strided_cuda((336, 196), (1, 336), torch.float32)
            buf805 = empty_strided_cuda((336, 196), (1, 336), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_119.run(buf784, convolution_52, buf799, buf787, cat_15, getitem_135, buf803, buf805, 65856, 128, stream=stream0)
            buf804 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_4], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_120.run(buf803, buf804, 336, 196, stream=stream0)
            del buf803
            buf806 = empty_strided_cuda((336, ), (1, ), torch.float32)
            buf808 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_121.run(buf805, rsqrt_22, buf806, buf808, 336, 196, stream=stream0)
            del buf805
            buf807 = empty_strided_cuda((128, 336, 14, 14), (65856, 1, 4704, 336), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid_4, x_69], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_122.run(buf784, convolution_52, buf799, buf787, cat_15, getitem_135, buf806, rsqrt_22, buf804, buf807, 8429568, stream=stream0)
            del buf784
            del buf787
            del cat_15
            del convolution_52
            del getitem_135
            buf809 = empty_strided_cuda((128, 112, 14, 14), (21952, 1, 1568, 112), torch.float16)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_123.run(buf807, rsqrt_22, primals_174, buf809, 2809856, stream=stream0)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf810 = torch.ops.aten.convolution_backward.default(buf809, getitem_133, convert_element_type_129, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 112, [True, True, False])
            del convert_element_type_129
            del getitem_133
            buf811 = buf810[0]
            assert_size_stride(buf811, (128, 112, 28, 28), (87808, 1, 3136, 112), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf811, 16, 'torch.ops.aten.convolution_backward.default')
            buf812 = buf810[1]
            assert_size_stride(buf812, (112, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf812, 16, 'torch.ops.aten.convolution_backward.default')
            del buf810
            buf814 = buf809; del buf809  # reuse
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_124.run(buf807, rsqrt_22, primals_174, buf814, 2809856, stream=stream0)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf815 = torch.ops.aten.convolution_backward.default(buf814, getitem_129, convert_element_type_128, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 112, [True, True, False])
            del convert_element_type_128
            del getitem_129
            buf816 = buf815[0]
            assert_size_stride(buf816, (128, 112, 28, 28), (87808, 1, 3136, 112), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf816, 16, 'torch.ops.aten.convolution_backward.default')
            buf817 = buf815[1]
            assert_size_stride(buf817, (112, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf817, 16, 'torch.ops.aten.convolution_backward.default')
            del buf815
            buf819 = buf814; del buf814  # reuse
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_125.run(buf807, rsqrt_22, primals_174, buf819, 2809856, stream=stream0)
            del buf807
            del primals_174
            del rsqrt_22
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf820 = torch.ops.aten.convolution_backward.default(buf819, getitem_125, convert_element_type_127, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 112, [True, True, False])
            del convert_element_type_127
            del getitem_125
            buf821 = buf820[0]
            assert_size_stride(buf821, (128, 112, 28, 28), (87808, 1, 3136, 112), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf821, 16, 'torch.ops.aten.convolution_backward.default')
            buf822 = buf820[1]
            assert_size_stride(buf822, (112, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf822, 16, 'torch.ops.aten.convolution_backward.default')
            del buf820
            buf824 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            buf827 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_126.run(convolution_47, getitem_121, rsqrt_21, primals_166, primals_167, buf821, buf816, buf811, buf824, buf827, 33718272, stream=stream0)
            del primals_167
            buf825 = empty_strided_cuda((336, 392), (1, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_127.run(buf821, buf816, buf811, buf824, buf825, 131712, 256, stream=stream0)
            buf826 = buf806; del buf806  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.run(buf825, buf826, 336, 392, stream=stream0)
            buf828 = buf825; del buf825  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_129.run(buf827, buf828, 131712, 256, stream=stream0)
            buf829 = empty_strided_cuda((336, ), (1, ), torch.float32)
            buf831 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.run(buf828, rsqrt_21, buf829, buf831, 336, 392, stream=stream0)
            buf832 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.cat, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_fill_mul_native_batch_norm_backward_sigmoid_sub_131.run(buf821, buf816, buf811, buf824, convolution_47, getitem_121, buf829, rsqrt_21, buf826, primals_166, buf832, 33718272, stream=stream0)
            del buf811
            del buf816
            del buf821
            del buf824
            del convolution_47
            del getitem_121
            del primals_166
            del rsqrt_21
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf833 = torch.ops.aten.convolution_backward.default(buf832, add_109, convert_element_type_122, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_109
            del convert_element_type_122
            buf834 = buf833[0]
            assert_size_stride(buf834, (128, 56, 28, 28), (43904, 1, 1568, 56), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf834, 16, 'torch.ops.aten.convolution_backward.default')
            buf835 = buf833[1]
            assert_size_stride(buf835, (336, 56, 1, 1), (56, 1, 56, 56), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf835, 16, 'torch.ops.aten.convolution_backward.default')
            del buf833
            buf837 = empty_strided_cuda((56, 784), (1, 56), torch.float32)
            buf839 = empty_strided_cuda((56, 784), (1, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132.run(buf834, cat_14, unsqueeze_678, buf837, buf839, 43904, 128, stream=stream0)
            buf838 = empty_strided_cuda((56, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_133.run(buf837, buf838, 56, 784, stream=stream0)
            buf840 = empty_strided_cuda((56, ), (1, ), torch.float32)
            buf841 = empty_strided_cuda((56, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.run(buf839, squeeze_61, buf840, buf841, 56, 784, stream=stream0)
            buf842 = cat_14; del cat_14  # reuse
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135.run(buf842, buf834, unsqueeze_678, buf840, squeeze_61, buf838, primals_160, 5619712, stream=stream0)
            del primals_160
            del squeeze_61
            del unsqueeze_678
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf843 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf842, (128, 28, 28, 28), (43904, 1, 1568, 56), 28), getitem_117, convert_element_type_119, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_119
            del getitem_117
            buf844 = buf843[0]
            assert_size_stride(buf844, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            # buffer buf844 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf845 = buf843[1]
            assert_size_stride(buf845, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            # buffer buf845 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf843
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf847 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf842, (128, 28, 28, 28), (43904, 1, 1568, 56), 0), getitem_116, convert_element_type_118, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf842
            del convert_element_type_118
            del getitem_116
            buf848 = buf847[0]
            assert_size_stride(buf848, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf848, 16, 'torch.ops.aten.convolution_backward.default')
            buf849 = buf847[1]
            assert_size_stride(buf849, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf849, 16, 'torch.ops.aten.convolution_backward.default')
            del buf847
            buf851 = buf832; del buf832  # reuse
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_13, getitem_115, rsqrt_19, primals_149, primals_150, buf851, 33718272, stream=stream0)
            del primals_150
            buf852 = reinterpret_tensor(buf799, (128, 336, 1, 1), (336, 1, 43008, 43008), 0); del buf799  # reuse
            buf853 = reinterpret_tensor(buf852, (128, 336, 1, 1), (336, 1, 1, 1), 0); del buf852  # reuse
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137.run(buf853, buf848, buf844, buf851, convolution_44, 43008, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf855 = torch.ops.aten.convolution_backward.default(buf853, convert_element_type_115, convert_element_type_117, [336], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_115
            del convert_element_type_117
            buf856 = buf855[0]
            assert_size_stride(buf856, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf856, 16, 'torch.ops.aten.convolution_backward.default')
            buf857 = buf855[1]
            assert_size_stride(buf857, (336, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf857, 16, 'torch.ops.aten.convolution_backward.default')
            del buf855
            buf860 = reinterpret_tensor(buf856, (128, 28, 1, 1), (28, 1, 1, 1), 0); del buf856  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_138.run(buf860, convolution_43, 3584, stream=stream0)
            del convolution_43
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf862 = torch.ops.aten.convolution_backward.default(buf860, mean_3, convert_element_type_113, [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_113
            del mean_3
            buf863 = buf862[0]
            assert_size_stride(buf863, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf863, 16, 'torch.ops.aten.convolution_backward.default')
            buf864 = buf862[1]
            assert_size_stride(buf864, (28, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf864, 16, 'torch.ops.aten.convolution_backward.default')
            del buf862
            buf867 = reinterpret_tensor(buf827, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf827  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_3], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139.run(buf848, buf844, convolution_44, buf863, buf851, buf867, 100352, 336, stream=stream0)
            del buf844
            del convolution_44
            buf869 = reinterpret_tensor(buf828, (336, 392), (392, 1), 0); del buf828  # reuse
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140.run(buf867, cat_13, getitem_115, buf869, 336, 392, 256, stream=stream0)
            buf870 = buf829; del buf829  # reuse
            buf871 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141.run(buf869, rsqrt_19, buf870, buf871, 336, 392, stream=stream0)
            buf868 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_142.run(buf867, buf868, 336, 100352, stream=stream0)
            buf872 = reinterpret_tensor(buf851, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf851  # reuse
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143.run(buf867, cat_13, getitem_115, buf870, rsqrt_19, buf868, primals_149, buf872, 43008, 784, stream=stream0)
            del cat_13
            del getitem_115
            del primals_149
            del rsqrt_19
            buf873 = buf848; del buf848  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_144.run(buf872, buf873, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf874 = torch.ops.aten.convolution_backward.default(buf873, getitem_113, convert_element_type_107, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 168, [True, True, False])
            del convert_element_type_107
            del getitem_113
            buf875 = buf874[0]
            assert_size_stride(buf875, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf875, 16, 'torch.ops.aten.convolution_backward.default')
            buf876 = buf874[1]
            assert_size_stride(buf876, (168, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf876, 16, 'torch.ops.aten.convolution_backward.default')
            del buf874
            buf878 = buf873; del buf873  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_145.run(buf872, buf878, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf879 = torch.ops.aten.convolution_backward.default(buf878, getitem_110, convert_element_type_106, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 168, [True, True, False])
            del buf878
            del convert_element_type_106
            del getitem_110
            buf880 = buf879[0]
            assert_size_stride(buf880, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf880, 16, 'torch.ops.aten.convolution_backward.default')
            buf881 = buf879[1]
            assert_size_stride(buf881, (168, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf881, 16, 'torch.ops.aten.convolution_backward.default')
            del buf879
            buf883 = reinterpret_tensor(buf872, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf872  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_12, getitem_107, rsqrt_18, primals_142, primals_143, buf883, 33718272, stream=stream0)
            del primals_143
            buf884 = reinterpret_tensor(buf869, (336, 392), (1, 336), 0); del buf869  # reuse
            buf886 = empty_strided_cuda((336, 392), (1, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146.run(buf880, buf875, buf883, cat_12, getitem_107, buf884, buf886, 131712, 256, stream=stream0)
            buf885 = buf870; del buf870  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.run(buf884, buf885, 336, 392, stream=stream0)
            buf887 = empty_strided_cuda((336, ), (1, ), torch.float32)
            buf889 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.run(buf886, rsqrt_18, buf887, buf889, 336, 392, stream=stream0)
            buf888 = reinterpret_tensor(buf867, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf867  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147.run(buf880, buf875, buf883, cat_12, getitem_107, buf887, rsqrt_18, buf888, 33718272, stream=stream0)
            del buf875
            del cat_12
            del getitem_107
            buf890 = buf880; del buf880  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148.run(buf888, buf885, rsqrt_18, primals_142, buf890, 16859136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf891 = torch.ops.aten.convolution_backward.default(buf890, getitem_105, convert_element_type_101, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_101
            del getitem_105
            buf892 = buf891[0]
            assert_size_stride(buf892, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf892, 16, 'torch.ops.aten.convolution_backward.default')
            buf893 = buf891[1]
            assert_size_stride(buf893, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf893, 16, 'torch.ops.aten.convolution_backward.default')
            del buf891
            buf895 = buf890; del buf890  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149.run(buf888, buf885, rsqrt_18, primals_142, buf895, 16859136, stream=stream0)
            del primals_142
            del rsqrt_18
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf896 = torch.ops.aten.convolution_backward.default(buf895, getitem_104, convert_element_type_100, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf895
            del convert_element_type_100
            del getitem_104
            buf897 = buf896[0]
            assert_size_stride(buf897, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf897, 16, 'torch.ops.aten.convolution_backward.default')
            buf898 = buf896[1]
            assert_size_stride(buf898, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf898, 16, 'torch.ops.aten.convolution_backward.default')
            del buf896
            buf900 = buf839; del buf839  # reuse
            buf902 = buf837; del buf837  # reuse
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150.run(buf834, buf897, buf892, cat_11, unsqueeze_714, buf900, buf902, 43904, 128, stream=stream0)
            buf901 = buf840; del buf840  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_133.run(buf900, buf901, 56, 784, stream=stream0)
            buf903 = empty_strided_cuda((56, ), (1, ), torch.float32)
            buf905 = empty_strided_cuda((56, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.run(buf902, squeeze_52, buf903, buf905, 56, 784, stream=stream0)
            buf904 = empty_strided_cuda((128, 56, 28, 28), (43904, 1, 1568, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_151.run(buf834, buf897, buf892, cat_11, unsqueeze_714, buf903, squeeze_52, buf901, buf904, 5619712, stream=stream0)
            del cat_11
            del unsqueeze_714
            buf906 = reinterpret_tensor(buf819, (128, 28, 28, 28), (21952, 1, 784, 28), 0); del buf819  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_152.run(buf904, squeeze_52, primals_135, buf906, 2809856, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf907 = torch.ops.aten.convolution_backward.default(buf906, getitem_101, convert_element_type_97, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_97
            del getitem_101
            buf908 = buf907[0]
            assert_size_stride(buf908, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf908, 16, 'torch.ops.aten.convolution_backward.default')
            buf909 = buf907[1]
            assert_size_stride(buf909, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf909, 16, 'torch.ops.aten.convolution_backward.default')
            del buf907
            buf911 = buf906; del buf906  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_153.run(buf904, squeeze_52, primals_135, buf911, 2809856, stream=stream0)
            del buf904
            del primals_135
            del squeeze_52
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf912 = torch.ops.aten.convolution_backward.default(buf911, getitem_100, convert_element_type_96, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf911
            del convert_element_type_96
            del getitem_100
            buf913 = buf912[0]
            assert_size_stride(buf913, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf913, 16, 'torch.ops.aten.convolution_backward.default')
            buf914 = buf912[1]
            assert_size_stride(buf914, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf914, 16, 'torch.ops.aten.convolution_backward.default')
            del buf912
            buf916 = buf883; del buf883  # reuse
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_10, getitem_99, rsqrt_16, primals_124, primals_125, buf916, 33718272, stream=stream0)
            del primals_125
            buf917 = reinterpret_tensor(buf863, (128, 336, 1, 1), (336, 1, 43008, 43008), 0); del buf863  # reuse
            buf918 = reinterpret_tensor(buf917, (128, 336, 1, 1), (336, 1, 1, 1), 0); del buf917  # reuse
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_154.run(buf918, buf913, buf908, buf916, convolution_36, 43008, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf920 = torch.ops.aten.convolution_backward.default(buf918, convert_element_type_93, convert_element_type_95, [336], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_93
            del convert_element_type_95
            buf921 = buf920[0]
            assert_size_stride(buf921, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf921, 16, 'torch.ops.aten.convolution_backward.default')
            buf922 = buf920[1]
            assert_size_stride(buf922, (336, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf922, 16, 'torch.ops.aten.convolution_backward.default')
            del buf920
            buf925 = reinterpret_tensor(buf921, (128, 28, 1, 1), (28, 1, 1, 1), 0); del buf921  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_138.run(buf925, convolution_35, 3584, stream=stream0)
            del convolution_35
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf927 = torch.ops.aten.convolution_backward.default(buf925, mean_2, convert_element_type_91, [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_91
            del mean_2
            buf928 = buf927[0]
            assert_size_stride(buf928, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf928, 16, 'torch.ops.aten.convolution_backward.default')
            buf929 = buf927[1]
            assert_size_stride(buf929, (28, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf929, 16, 'torch.ops.aten.convolution_backward.default')
            del buf927
            buf932 = reinterpret_tensor(buf888, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf888  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_2], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_155.run(buf913, buf908, convolution_36, buf928, buf916, buf932, 100352, 336, stream=stream0)
            del buf908
            del convolution_36
            buf934 = reinterpret_tensor(buf886, (336, 392), (392, 1), 0); del buf886  # reuse
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140.run(buf932, cat_10, getitem_99, buf934, 336, 392, 256, stream=stream0)
            buf935 = buf887; del buf887  # reuse
            buf936 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141.run(buf934, rsqrt_16, buf935, buf936, 336, 392, stream=stream0)
            buf933 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_142.run(buf932, buf933, 336, 100352, stream=stream0)
            buf937 = reinterpret_tensor(buf916, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf916  # reuse
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143.run(buf932, cat_10, getitem_99, buf935, rsqrt_16, buf933, primals_124, buf937, 43008, 784, stream=stream0)
            del cat_10
            del getitem_99
            del primals_124
            del rsqrt_16
            buf938 = buf913; del buf913  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_144.run(buf937, buf938, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf939 = torch.ops.aten.convolution_backward.default(buf938, getitem_97, convert_element_type_85, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 168, [True, True, False])
            del convert_element_type_85
            del getitem_97
            buf940 = buf939[0]
            assert_size_stride(buf940, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf940, 16, 'torch.ops.aten.convolution_backward.default')
            buf941 = buf939[1]
            assert_size_stride(buf941, (168, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf941, 16, 'torch.ops.aten.convolution_backward.default')
            del buf939
            buf943 = buf938; del buf938  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_145.run(buf937, buf943, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf944 = torch.ops.aten.convolution_backward.default(buf943, getitem_94, convert_element_type_84, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 168, [True, True, False])
            del buf943
            del convert_element_type_84
            del getitem_94
            buf945 = buf944[0]
            assert_size_stride(buf945, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf945, 16, 'torch.ops.aten.convolution_backward.default')
            buf946 = buf944[1]
            assert_size_stride(buf946, (168, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf946, 16, 'torch.ops.aten.convolution_backward.default')
            del buf944
            buf948 = reinterpret_tensor(buf937, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf937  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_9, getitem_91, rsqrt_15, primals_117, primals_118, buf948, 33718272, stream=stream0)
            del primals_118
            buf949 = reinterpret_tensor(buf934, (336, 392), (1, 336), 0); del buf934  # reuse
            buf951 = buf884; del buf884  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146.run(buf945, buf940, buf948, cat_9, getitem_91, buf949, buf951, 131712, 256, stream=stream0)
            buf950 = buf935; del buf935  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.run(buf949, buf950, 336, 392, stream=stream0)
            buf952 = empty_strided_cuda((336, ), (1, ), torch.float32)
            buf954 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.run(buf951, rsqrt_15, buf952, buf954, 336, 392, stream=stream0)
            buf953 = reinterpret_tensor(buf932, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf932  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147.run(buf945, buf940, buf948, cat_9, getitem_91, buf952, rsqrt_15, buf953, 33718272, stream=stream0)
            del buf940
            del cat_9
            del getitem_91
            buf955 = buf945; del buf945  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148.run(buf953, buf950, rsqrt_15, primals_117, buf955, 16859136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf956 = torch.ops.aten.convolution_backward.default(buf955, getitem_89, convert_element_type_79, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_79
            del getitem_89
            buf957 = buf956[0]
            assert_size_stride(buf957, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf957, 16, 'torch.ops.aten.convolution_backward.default')
            buf958 = buf956[1]
            assert_size_stride(buf958, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf958, 16, 'torch.ops.aten.convolution_backward.default')
            del buf956
            buf960 = buf955; del buf955  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149.run(buf953, buf950, rsqrt_15, primals_117, buf960, 16859136, stream=stream0)
            del primals_117
            del rsqrt_15
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf961 = torch.ops.aten.convolution_backward.default(buf960, getitem_88, convert_element_type_78, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf960
            del convert_element_type_78
            del getitem_88
            buf962 = buf961[0]
            assert_size_stride(buf962, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf962, 16, 'torch.ops.aten.convolution_backward.default')
            buf963 = buf961[1]
            assert_size_stride(buf963, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf963, 16, 'torch.ops.aten.convolution_backward.default')
            del buf961
            buf965 = buf834; del buf834  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_156.run(buf965, buf897, buf892, buf962, buf957, 5619712, stream=stream0)
            del buf892
            del buf897
            del buf957
            del buf962
            buf966 = buf902; del buf902  # reuse
            buf968 = buf900; del buf900  # reuse
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_132.run(buf965, cat_8, unsqueeze_750, buf966, buf968, 43904, 128, stream=stream0)
            buf967 = buf903; del buf903  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_133.run(buf966, buf967, 56, 784, stream=stream0)
            buf969 = empty_strided_cuda((56, ), (1, ), torch.float32)
            buf970 = empty_strided_cuda((56, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.run(buf968, squeeze_43, buf969, buf970, 56, 784, stream=stream0)
            buf971 = cat_8; del cat_8  # reuse
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_135.run(buf971, buf965, unsqueeze_750, buf969, squeeze_43, buf967, primals_110, 5619712, stream=stream0)
            del primals_110
            del squeeze_43
            del unsqueeze_750
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf972 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf971, (128, 28, 28, 28), (43904, 1, 1568, 56), 28), getitem_85, convert_element_type_75, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_75
            del getitem_85
            buf973 = buf972[0]
            assert_size_stride(buf973, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            # buffer buf973 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf974 = buf972[1]
            assert_size_stride(buf974, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            # buffer buf974 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf972
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf976 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf971, (128, 28, 28, 28), (43904, 1, 1568, 56), 0), getitem_84, convert_element_type_74, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_74
            del getitem_84
            buf977 = buf976[0]
            assert_size_stride(buf977, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf977, 16, 'torch.ops.aten.convolution_backward.default')
            buf978 = buf976[1]
            assert_size_stride(buf978, (28, 168, 1, 1), (168, 1, 168, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf978, 16, 'torch.ops.aten.convolution_backward.default')
            del buf976
            buf980 = buf948; del buf948  # reuse
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_7, getitem_83, rsqrt_13, primals_99, primals_100, buf980, 33718272, stream=stream0)
            del primals_100
            buf981 = reinterpret_tensor(buf928, (128, 336, 1, 1), (336, 1, 43008, 43008), 0); del buf928  # reuse
            buf982 = reinterpret_tensor(buf981, (128, 336, 1, 1), (336, 1, 1, 1), 0); del buf981  # reuse
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1], Original ATen: [aten.cat, aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_mul_sigmoid_sigmoid_backward_silu_sum_137.run(buf982, buf977, buf973, buf980, convolution_28, 43008, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf984 = torch.ops.aten.convolution_backward.default(buf982, convert_element_type_71, convert_element_type_73, [336], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_71
            del convert_element_type_73
            buf985 = buf984[0]
            assert_size_stride(buf985, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf985, 16, 'torch.ops.aten.convolution_backward.default')
            buf986 = buf984[1]
            assert_size_stride(buf986, (336, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf986, 16, 'torch.ops.aten.convolution_backward.default')
            del buf984
            buf989 = reinterpret_tensor(buf985, (128, 28, 1, 1), (28, 1, 1, 1), 0); del buf985  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.sigmoid, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_138.run(buf989, convolution_27, 3584, stream=stream0)
            del convolution_27
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf991 = torch.ops.aten.convolution_backward.default(buf989, mean_1, convert_element_type_69, [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_69
            del mean_1
            buf992 = buf991[0]
            assert_size_stride(buf992, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf992, 16, 'torch.ops.aten.convolution_backward.default')
            buf993 = buf991[1]
            assert_size_stride(buf993, (28, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf993, 16, 'torch.ops.aten.convolution_backward.default')
            del buf991
            buf996 = reinterpret_tensor(buf953, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf953  # reuse
            # Topologically Sorted Source Nodes: [sigmoid_1], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_cat_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_139.run(buf977, buf973, convolution_28, buf992, buf980, buf996, 100352, 336, stream=stream0)
            del buf973
            del buf992
            del convolution_28
            buf998 = reinterpret_tensor(buf951, (336, 392), (392, 1), 0); del buf951  # reuse
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_140.run(buf996, cat_7, getitem_83, buf998, 336, 392, 256, stream=stream0)
            buf999 = buf952; del buf952  # reuse
            buf1000 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_141.run(buf998, rsqrt_13, buf999, buf1000, 336, 392, stream=stream0)
            buf997 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_142.run(buf996, buf997, 336, 100352, stream=stream0)
            buf1001 = reinterpret_tensor(buf980, (128, 336, 28, 28), (263424, 784, 28, 1), 0); del buf980  # reuse
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_143.run(buf996, cat_7, getitem_83, buf999, rsqrt_13, buf997, primals_99, buf1001, 43008, 784, stream=stream0)
            del cat_7
            del getitem_83
            del primals_99
            del rsqrt_13
            buf1002 = buf977; del buf977  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_144.run(buf1001, buf1002, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1003 = torch.ops.aten.convolution_backward.default(buf1002, getitem_81, convert_element_type_63, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 168, [True, True, False])
            del convert_element_type_63
            del getitem_81
            buf1004 = buf1003[0]
            assert_size_stride(buf1004, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1004, 16, 'torch.ops.aten.convolution_backward.default')
            buf1005 = buf1003[1]
            assert_size_stride(buf1005, (168, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1005, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1003
            buf1007 = buf1002; del buf1002  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_slice_145.run(buf1001, buf1007, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1008 = torch.ops.aten.convolution_backward.default(buf1007, getitem_78, convert_element_type_62, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 168, [True, True, False])
            del buf1007
            del convert_element_type_62
            del getitem_78
            buf1009 = buf1008[0]
            assert_size_stride(buf1009, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1009, 16, 'torch.ops.aten.convolution_backward.default')
            buf1010 = buf1008[1]
            assert_size_stride(buf1010, (168, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1010, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1008
            buf1012 = reinterpret_tensor(buf1001, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf1001  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_136.run(cat_6, getitem_75, rsqrt_12, primals_92, primals_93, buf1012, 33718272, stream=stream0)
            del primals_93
            buf1013 = reinterpret_tensor(buf998, (336, 392), (1, 336), 0); del buf998  # reuse
            buf1015 = buf949; del buf949  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_146.run(buf1009, buf1004, buf1012, cat_6, getitem_75, buf1013, buf1015, 131712, 256, stream=stream0)
            buf1014 = buf999; del buf999  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_128.run(buf1013, buf1014, 336, 392, stream=stream0)
            del buf1013
            buf1016 = empty_strided_cuda((336, ), (1, ), torch.float32)
            buf1018 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_130.run(buf1015, rsqrt_12, buf1016, buf1018, 336, 392, stream=stream0)
            del buf1015
            buf1017 = reinterpret_tensor(buf996, (128, 336, 28, 28), (263424, 1, 9408, 336), 0); del buf996  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.fill, aten.cat, aten.sigmoid, aten.sub, aten.mul, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_native_batch_norm_backward_sigmoid_sub_147.run(buf1009, buf1004, buf1012, cat_6, getitem_75, buf1016, rsqrt_12, buf1017, 33718272, stream=stream0)
            del buf1004
            del buf1012
            del cat_6
            del getitem_75
            buf1019 = buf1009; del buf1009  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_148.run(buf1017, buf1014, rsqrt_12, primals_92, buf1019, 16859136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf1020 = torch.ops.aten.convolution_backward.default(buf1019, getitem_73, convert_element_type_57, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_57
            del getitem_73
            buf1021 = buf1020[0]
            assert_size_stride(buf1021, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1021, 16, 'torch.ops.aten.convolution_backward.default')
            buf1022 = buf1020[1]
            assert_size_stride(buf1022, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1022, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1020
            buf1024 = buf1019; del buf1019  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_149.run(buf1017, buf1014, rsqrt_12, primals_92, buf1024, 16859136, stream=stream0)
            del buf1017
            del primals_92
            del rsqrt_12
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.slice, aten.convolution_backward]
            buf1025 = torch.ops.aten.convolution_backward.default(buf1024, getitem_72, convert_element_type_56, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1024
            del convert_element_type_56
            del getitem_72
            buf1026 = buf1025[0]
            assert_size_stride(buf1026, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1026, 16, 'torch.ops.aten.convolution_backward.default')
            buf1027 = buf1025[1]
            assert_size_stride(buf1027, (168, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1027, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1025
            buf1029 = buf968; del buf968  # reuse
            buf1031 = buf966; del buf966  # reuse
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_150.run(buf965, buf1026, buf1021, convolution_22, unsqueeze_786, buf1029, buf1031, 43904, 128, stream=stream0)
            buf1030 = buf969; del buf969  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_133.run(buf1029, buf1030, 56, 784, stream=stream0)
            del buf1029
            buf1032 = empty_strided_cuda((56, ), (1, ), torch.float32)
            buf1034 = empty_strided_cuda((56, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_134.run(buf1031, squeeze_34, buf1032, buf1034, 56, 784, stream=stream0)
            del buf1031
            buf1035 = buf971; del buf971  # reuse
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_157.run(buf965, buf1026, buf1021, convolution_22, unsqueeze_786, buf1032, squeeze_34, buf1030, primals_85, buf1035, 5619712, stream=stream0)
            del buf1021
            del buf1026
            del buf1032
            del buf965
            del convolution_22
            del primals_85
            del squeeze_34
            del unsqueeze_786
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf1036 = torch.ops.aten.convolution_backward.default(buf1035, mul_80, convert_element_type_53, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1035
            del convert_element_type_53
            del mul_80
            buf1037 = buf1036[0]
            assert_size_stride(buf1037, (128, 240, 28, 28), (188160, 1, 6720, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1037, 16, 'torch.ops.aten.convolution_backward.default')
            buf1038 = buf1036[1]
            assert_size_stride(buf1038, (56, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1038, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1036
            buf1040 = reinterpret_tensor(buf278, (128, 240, 28, 28), (188160, 1, 6720, 240), 0); del buf278  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_158.run(cat_5, getitem_69, rsqrt_10, primals_75, primals_76, buf1040, 24084480, stream=stream0)
            del primals_76
            buf1041 = empty_strided_cuda((128, 240, 1, 1), (240, 1, 30720, 30720), torch.float16)
            buf1042 = reinterpret_tensor(buf1041, (128, 240, 1, 1), (240, 1, 1, 1), 0); del buf1041  # reuse
            # Topologically Sorted Source Nodes: [x_31, sigmoid], Original ATen: [aten.silu, aten.mul, aten.sigmoid, aten.sum, aten.sigmoid_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_mul_sigmoid_sigmoid_backward_silu_sum_159.run(buf1042, buf1037, buf1040, convolution_21, 30720, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf1044 = torch.ops.aten.convolution_backward.default(buf1042, convert_element_type_50, convert_element_type_52, [240], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_50
            del convert_element_type_52
            buf1045 = buf1044[0]
            assert_size_stride(buf1045, (128, 20, 1, 1), (20, 1, 20, 20), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1045, 16, 'torch.ops.aten.convolution_backward.default')
            buf1046 = buf1044[1]
            assert_size_stride(buf1046, (240, 20, 1, 1), (20, 1, 20, 20), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1046, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1044
            buf1049 = reinterpret_tensor(buf1045, (128, 20, 1, 1), (20, 1, 1, 1), 0); del buf1045  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_fill_mul_sigmoid_sub_160.run(buf1049, convolution_20, 2560, stream=stream0)
            del convolution_20
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward]
            buf1051 = torch.ops.aten.convolution_backward.default(buf1049, mean, convert_element_type_48, [20], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_48
            del mean
            buf1052 = buf1051[0]
            assert_size_stride(buf1052, (128, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1052, 16, 'torch.ops.aten.convolution_backward.default')
            buf1053 = buf1051[1]
            assert_size_stride(buf1053, (20, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1053, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1051
            buf1056 = reinterpret_tensor(buf275, (240, 512), (1, 240), 0); del buf275  # reuse
            buf1058 = empty_strided_cuda((240, 512), (1, 240), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_161.run(buf1037, convolution_21, buf1052, buf1040, cat_5, getitem_69, buf1056, buf1058, 122880, 196, stream=stream0)
            buf1057 = empty_strided_cuda((240, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_162.run(buf1056, buf1057, 240, 512, stream=stream0)
            buf1059 = empty_strided_cuda((240, ), (1, ), torch.float32)
            buf1061 = empty_strided_cuda((240, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_163.run(buf1058, rsqrt_10, buf1059, buf1061, 240, 512, stream=stream0)
            buf1060 = empty_strided_cuda((128, 240, 28, 28), (188160, 1, 6720, 240), torch.float32)
            # Topologically Sorted Source Nodes: [sigmoid, x_30], Original ATen: [aten.sigmoid, aten.mul, aten.expand, aten.div, aten.add, aten.fill, aten.sub, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_div_expand_fill_mul_native_batch_norm_backward_sigmoid_sub_164.run(buf1037, convolution_21, buf1052, buf1040, cat_5, getitem_69, buf1059, rsqrt_10, buf1057, buf1060, 24084480, stream=stream0)
            del buf1037
            del buf1040
            del buf1052
            del cat_5
            del convolution_21
            del getitem_69
            buf1062 = reinterpret_tensor(buf500, (128, 60, 28, 28), (47040, 1, 1680, 60), 0); del buf500  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_165.run(buf1060, rsqrt_10, primals_75, buf1062, 6021120, stream=stream0)
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1063 = torch.ops.aten.convolution_backward.default(buf1062, getitem_67, convert_element_type_42, [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 60, [True, True, False])
            del convert_element_type_42
            del getitem_67
            buf1064 = buf1063[0]
            assert_size_stride(buf1064, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1064, 16, 'torch.ops.aten.convolution_backward.default')
            buf1065 = buf1063[1]
            assert_size_stride(buf1065, (60, 1, 9, 9), (81, 1, 9, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1065, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1063
            buf1067 = buf1062; del buf1062  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_166.run(buf1060, rsqrt_10, primals_75, buf1067, 6021120, stream=stream0)
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1068 = torch.ops.aten.convolution_backward.default(buf1067, getitem_62, convert_element_type_41, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 60, [True, True, False])
            del convert_element_type_41
            del getitem_62
            buf1069 = buf1068[0]
            assert_size_stride(buf1069, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1069, 16, 'torch.ops.aten.convolution_backward.default')
            buf1070 = buf1068[1]
            assert_size_stride(buf1070, (60, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1070, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1068
            buf1072 = buf1067; del buf1067  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_167.run(buf1060, rsqrt_10, primals_75, buf1072, 6021120, stream=stream0)
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1073 = torch.ops.aten.convolution_backward.default(buf1072, getitem_57, convert_element_type_40, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 60, [True, True, False])
            del convert_element_type_40
            del getitem_57
            buf1074 = buf1073[0]
            assert_size_stride(buf1074, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1074, 16, 'torch.ops.aten.convolution_backward.default')
            buf1075 = buf1073[1]
            assert_size_stride(buf1075, (60, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1075, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1073
            buf1077 = buf1072; del buf1072  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_slice_168.run(buf1060, rsqrt_10, primals_75, buf1077, 6021120, stream=stream0)
            del buf1060
            del primals_75
            del rsqrt_10
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1078 = torch.ops.aten.convolution_backward.default(buf1077, getitem_52, convert_element_type_39, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 60, [True, True, False])
            del buf1077
            del convert_element_type_39
            del getitem_52
            buf1079 = buf1078[0]
            assert_size_stride(buf1079, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1079, 16, 'torch.ops.aten.convolution_backward.default')
            buf1080 = buf1078[1]
            assert_size_stride(buf1080, (60, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1080, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1078
            buf1083 = empty_strided_cuda((128, 240, 56, 56), (752640, 3136, 56, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.cat, aten._native_batch_norm_legit_functional, aten.sigmoid, aten.fill, aten.sub, aten.mul, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_fill_mul_sigmoid_sub_169.run(convolution_15, getitem_47, rsqrt_9, primals_66, primals_67, buf1079, buf1074, buf1069, buf1064, buf1083, 401408, 240, stream=stream0)
            del buf1064
            del buf1069
            del buf1074
            del buf1079
            del primals_67
            buf1084 = reinterpret_tensor(buf276, (240, 4), (1, 240), 0); del buf276  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_170.run(buf1083, buf1084, 960, 100352, stream=stream0)
            buf1085 = buf1059; del buf1059  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_native_batch_norm_backward_171.run(buf1084, buf1085, 240, 4, stream=stream0)
            buf1086 = reinterpret_tensor(buf1058, (240, 512), (512, 1), 0); del buf1058  # reuse
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_172.run(buf1083, convolution_15, getitem_47, buf1086, 240, 512, 784, stream=stream0)
            buf1087 = empty_strided_cuda((240, ), (1, ), torch.float32)
            buf1088 = empty_strided_cuda((240, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_native_batch_norm_backward_173.run(buf1086, rsqrt_9, buf1087, buf1088, 240, 512, stream=stream0)
            buf1089 = convolution_15; del convolution_15  # reuse
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_174.run(buf1089, buf1083, getitem_47, buf1087, rsqrt_9, buf1085, primals_66, 401408, 240, stream=stream0)
            del buf1083
            del getitem_47
            del primals_66
            del rsqrt_9
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf1090 = torch.ops.aten.convolution_backward.default(buf1089, add_46, convert_element_type_34, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del add_46
            del buf1089
            del convert_element_type_34
            buf1091 = buf1090[0]
            assert_size_stride(buf1091, (128, 40, 56, 56), (125440, 1, 2240, 40), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1091, 16, 'torch.ops.aten.convolution_backward.default')
            buf1092 = buf1090[1]
            assert_size_stride(buf1092, (240, 40, 1, 1), (40, 1, 40, 40), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1092, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1090
            buf1094 = empty_strided_cuda((40, 1024), (1, 40), torch.float32)
            buf1096 = empty_strided_cuda((40, 1024), (1, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_175.run(buf1091, cat_4, unsqueeze_822, buf1094, buf1096, 40960, 392, stream=stream0)
            buf1095 = empty_strided_cuda((40, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_176.run(buf1094, buf1095, 40, 1024, stream=stream0)
            buf1097 = empty_strided_cuda((40, ), (1, ), torch.float32)
            buf1098 = empty_strided_cuda((40, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177.run(buf1096, squeeze_25, buf1097, buf1098, 40, 1024, stream=stream0)
            buf1099 = cat_4; del cat_4  # reuse
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_178.run(buf1099, buf1091, unsqueeze_822, buf1097, squeeze_25, buf1095, primals_60, 16056320, stream=stream0)
            del primals_60
            del squeeze_25
            del unsqueeze_822
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1100 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1099, (128, 20, 56, 56), (125440, 1, 2240, 40), 20), getitem_43, convert_element_type_31, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_31
            del getitem_43
            buf1101 = buf1100[0]
            assert_size_stride(buf1101, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            # buffer buf1101 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf1102 = buf1100[1]
            assert_size_stride(buf1102, (20, 60, 1, 1), (60, 1, 60, 60), 'torch.ops.aten.convolution_backward.default')
            # buffer buf1102 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf1100
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1104 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1099, (128, 20, 56, 56), (125440, 1, 2240, 40), 0), getitem_40, convert_element_type_30, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1099
            del convert_element_type_30
            del getitem_40
            buf1105 = buf1104[0]
            assert_size_stride(buf1105, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1105, 16, 'torch.ops.aten.convolution_backward.default')
            buf1106 = buf1104[1]
            assert_size_stride(buf1106, (20, 60, 1, 1), (60, 1, 60, 60), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1106, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1104
            buf1108 = empty_strided_cuda((128, 120, 56, 56), (376320, 1, 6720, 120), torch.float32)
            # Topologically Sorted Source Nodes: [x_21, x_22], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_179.run(convolution_12, getitem_37, rsqrt_7, primals_53, primals_54, buf1105, buf1101, buf1108, 48168960, stream=stream0)
            del buf1101
            del buf1105
            del primals_54
            buf1109 = reinterpret_tensor(buf1086, (120, 1024), (1, 120), 0); del buf1086  # reuse
            buf1111 = reinterpret_tensor(buf1056, (120, 1024), (1, 120), 0); del buf1056  # reuse
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_180.run(buf1108, convolution_12, getitem_37, buf1109, buf1111, 122880, 392, stream=stream0)
            buf1110 = empty_strided_cuda((120, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_181.run(buf1109, buf1110, 120, 1024, stream=stream0)
            buf1112 = empty_strided_cuda((120, ), (1, ), torch.float32)
            buf1113 = empty_strided_cuda((120, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_182.run(buf1111, rsqrt_7, buf1112, buf1113, 120, 1024, stream=stream0)
            buf1114 = convolution_12; del convolution_12  # reuse
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_183.run(buf1114, buf1108, getitem_37, buf1112, rsqrt_7, buf1110, primals_53, 48168960, stream=stream0)
            del buf1108
            del getitem_37
            del primals_53
            del rsqrt_7
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward, aten.convolution_backward]
            buf1115 = torch.ops.aten.convolution_backward.default(buf1114, relu_4, convert_element_type_27, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False])
            del buf1114
            del convert_element_type_27
            buf1116 = buf1115[0]
            assert_size_stride(buf1116, (128, 120, 56, 56), (376320, 1, 6720, 120), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1116, 16, 'torch.ops.aten.convolution_backward.default')
            buf1117 = buf1115[1]
            assert_size_stride(buf1117, (120, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1117, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1115
            buf1119 = buf1111; del buf1111  # reuse
            buf1121 = buf1109; del buf1109  # reuse
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_184.run(relu_4, buf1116, cat_3, unsqueeze_846, buf1119, buf1121, 122880, 392, stream=stream0)
            buf1120 = buf1112; del buf1112  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_181.run(buf1119, buf1120, 120, 1024, stream=stream0)
            del buf1119
            buf1122 = empty_strided_cuda((120, ), (1, ), torch.float32)
            buf1123 = empty_strided_cuda((120, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_185.run(buf1121, squeeze_19, buf1122, buf1123, 120, 1024, stream=stream0)
            del buf1121
            buf1124 = relu_4; del relu_4  # reuse
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_186.run(buf1124, buf1116, cat_3, unsqueeze_846, buf1122, squeeze_19, buf1120, primals_47, 48168960, stream=stream0)
            del buf1116
            del buf1122
            del cat_3
            del primals_47
            del squeeze_19
            del unsqueeze_846
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1125 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1124, (128, 60, 56, 56), (376320, 1, 6720, 120), 60), getitem_33, convert_element_type_24, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_24
            del getitem_33
            buf1126 = buf1125[0]
            assert_size_stride(buf1126, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution_backward.default')
            # buffer buf1126 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            buf1127 = buf1125[1]
            assert_size_stride(buf1127, (60, 20, 1, 1), (20, 1, 20, 20), 'torch.ops.aten.convolution_backward.default')
            # buffer buf1127 (op: torch.ops.aten.convolution_backward.default) is assumed to be not aligned
            del buf1125
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1129 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1124, (128, 60, 56, 56), (376320, 1, 6720, 120), 0), getitem_32, convert_element_type_23, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1124
            del convert_element_type_23
            del getitem_32
            buf1130 = buf1129[0]
            assert_size_stride(buf1130, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1130, 16, 'torch.ops.aten.convolution_backward.default')
            buf1131 = buf1129[1]
            assert_size_stride(buf1131, (60, 20, 1, 1), (20, 1, 20, 20), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1131, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1129
            buf1133 = buf1096; del buf1096  # reuse
            buf1135 = buf1094; del buf1094  # reuse
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_187.run(buf1091, buf1130, buf1126, cat_2, unsqueeze_858, buf1133, buf1135, 40960, 392, stream=stream0)
            buf1134 = buf1097; del buf1097  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_176.run(buf1133, buf1134, 40, 1024, stream=stream0)
            del buf1133
            buf1136 = empty_strided_cuda((40, ), (1, ), torch.float32)
            buf1138 = empty_strided_cuda((40, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_177.run(buf1135, squeeze_16, buf1136, buf1138, 40, 1024, stream=stream0)
            del buf1135
            buf1137 = empty_strided_cuda((128, 40, 56, 56), (125440, 1, 2240, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_188.run(buf1091, buf1130, buf1126, cat_2, unsqueeze_858, buf1136, squeeze_16, buf1134, buf1137, 16056320, stream=stream0)
            del buf1091
            del buf1126
            del buf1136
            del cat_2
            del unsqueeze_858
            buf1139 = buf1130; del buf1130  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_189.run(buf1137, squeeze_16, primals_40, buf1139, 8028160, stream=stream0)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1140 = torch.ops.aten.convolution_backward.default(buf1139, getitem_29, convert_element_type_20, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_20
            del getitem_29
            buf1141 = buf1140[0]
            assert_size_stride(buf1141, (128, 96, 56, 56), (301056, 1, 5376, 96), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1141, 16, 'torch.ops.aten.convolution_backward.default')
            buf1142 = buf1140[1]
            assert_size_stride(buf1142, (20, 96, 1, 1), (96, 1, 96, 96), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1142, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1140
            buf1144 = buf1139; del buf1139  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_backward_native_batch_norm_backward_slice_190.run(buf1137, squeeze_16, primals_40, buf1144, 8028160, stream=stream0)
            del buf1137
            del primals_40
            del squeeze_16
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.slice, aten.convolution_backward]
            buf1145 = torch.ops.aten.convolution_backward.default(buf1144, getitem_26, convert_element_type_19, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1144
            del convert_element_type_19
            del getitem_26
            buf1146 = buf1145[0]
            assert_size_stride(buf1146, (128, 96, 56, 56), (301056, 1, 5376, 96), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1146, 16, 'torch.ops.aten.convolution_backward.default')
            buf1147 = buf1145[1]
            assert_size_stride(buf1147, (20, 96, 1, 1), (96, 1, 96, 96), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1147, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1145
            buf1149 = empty_strided_cuda((128, 192, 56, 56), (602112, 1, 10752, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_13, x_14], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_relu_threshold_backward_191.run(cat_1, getitem_23, rsqrt_4, primals_33, primals_34, buf1146, buf1141, buf1149, 77070336, stream=stream0)
            del buf1141
            del buf1146
            del primals_34
            buf1150 = empty_strided_cuda((192, 512), (1, 192), torch.float32)
            buf1152 = empty_strided_cuda((192, 512), (1, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_192.run(buf1149, cat_1, getitem_23, buf1150, buf1152, 98304, 784, stream=stream0)
            buf1151 = empty_strided_cuda((192, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_193.run(buf1150, buf1151, 192, 512, stream=stream0)
            del buf1150
            buf1153 = empty_strided_cuda((192, ), (1, ), torch.float32)
            buf1154 = empty_strided_cuda((192, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_194.run(buf1152, rsqrt_4, buf1153, buf1154, 192, 512, stream=stream0)
            del buf1152
            buf1155 = cat_1; del cat_1  # reuse
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_195.run(buf1155, buf1149, getitem_23, buf1153, rsqrt_4, buf1151, primals_33, 77070336, stream=stream0)
            del buf1149
            del getitem_23
            del primals_33
            del rsqrt_4
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1156 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1155, (128, 64, 56, 56), (602112, 1, 10752, 192), 128), getitem_21, convert_element_type_16, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 64, [True, True, False])
            del convert_element_type_16
            del getitem_21
            buf1157 = buf1156[0]
            assert_size_stride(buf1157, (128, 64, 112, 112), (802816, 1, 7168, 64), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1157, 16, 'torch.ops.aten.convolution_backward.default')
            buf1158 = buf1156[1]
            assert_size_stride(buf1158, (64, 1, 7, 7), (49, 1, 7, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1158, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1156
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1160 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1155, (128, 64, 56, 56), (602112, 1, 10752, 192), 64), getitem_17, convert_element_type_15, [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 64, [True, True, False])
            del convert_element_type_15
            del getitem_17
            buf1161 = buf1160[0]
            assert_size_stride(buf1161, (128, 64, 112, 112), (802816, 1, 7168, 64), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1161, 16, 'torch.ops.aten.convolution_backward.default')
            buf1162 = buf1160[1]
            assert_size_stride(buf1162, (64, 1, 5, 5), (25, 1, 5, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1162, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1160
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1164 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1155, (128, 64, 56, 56), (602112, 1, 10752, 192), 0), getitem_13, convert_element_type_14, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 64, [True, True, False])
            del buf1155
            del convert_element_type_14
            del getitem_13
            buf1165 = buf1164[0]
            assert_size_stride(buf1165, (128, 64, 112, 112), (802816, 1, 7168, 64), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1165, 16, 'torch.ops.aten.convolution_backward.default')
            buf1166 = buf1164[1]
            assert_size_stride(buf1166, (64, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1166, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1164
            buf1168 = empty_strided_cuda((128, 192, 112, 112), (2408448, 1, 21504, 192), torch.float16)
            # Topologically Sorted Source Nodes: [x_10, x_11], Original ATen: [aten.threshold_backward, aten.cat, aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_cat_relu_threshold_backward_196.run(cat, getitem_9, rsqrt_3, primals_25, primals_26, buf1165, buf1161, buf1157, buf1168, 308281344, stream=stream0)
            del buf1157
            del buf1161
            del buf1165
            del primals_26
            buf1169 = empty_strided_cuda((192, 784), (1, 192), torch.float32)
            buf1171 = empty_strided_cuda((192, 784), (1, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_197.run(buf1168, cat, getitem_9, buf1169, buf1171, 150528, 2048, stream=stream0)
            buf1170 = buf1153; del buf1153  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_198.run(buf1169, buf1170, 192, 784, stream=stream0)
            del buf1169
            buf1172 = empty_strided_cuda((192, ), (1, ), torch.float32)
            buf1173 = empty_strided_cuda((192, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_199.run(buf1171, rsqrt_3, buf1172, buf1173, 192, 784, stream=stream0)
            del buf1171
            buf1174 = buf1168; del buf1168  # reuse
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_native_batch_norm_backward_200.run(buf1174, cat, getitem_9, buf1172, rsqrt_3, buf1170, primals_25, 308281344, stream=stream0)
            del buf1172
            del cat
            del getitem_9
            del primals_25
            del rsqrt_3
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1175 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1174, (128, 96, 112, 112), (2408448, 1, 21504, 192), 96), getitem_7, convert_element_type_11, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del convert_element_type_11
            del getitem_7
            buf1176 = buf1175[0]
            assert_size_stride(buf1176, (128, 16, 112, 112), (200704, 1, 1792, 16), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1176, 16, 'torch.ops.aten.convolution_backward.default')
            buf1177 = buf1175[1]
            assert_size_stride(buf1177, (96, 16, 1, 1), (16, 1, 16, 16), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1177, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1175
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.slice, aten.convolution_backward]
            buf1179 = torch.ops.aten.convolution_backward.default(reinterpret_tensor(buf1174, (128, 96, 112, 112), (2408448, 1, 21504, 192), 0), getitem_6, convert_element_type_10, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1174
            del convert_element_type_10
            del getitem_6
            buf1180 = buf1179[0]
            assert_size_stride(buf1180, (128, 16, 112, 112), (200704, 1, 1792, 16), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1180, 16, 'torch.ops.aten.convolution_backward.default')
            buf1181 = buf1179[1]
            assert_size_stride(buf1181, (96, 16, 1, 1), (16, 1, 16, 16), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1181, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1179
            buf1183 = empty_strided_cuda((32, 1024), (1024, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_cat_native_batch_norm_backward_201.run(buf1180, buf1176, buf1183, 32, 1024, 1568, stream=stream0)
            buf1184 = empty_strided_cuda((32, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.cat, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_per_fused_cat_native_batch_norm_backward_202.run(buf1183, buf1184, 32, 1024, stream=stream0)
            buf1185 = reinterpret_tensor(buf1183, (32, 1024), (1, 32), 0); del buf1183  # reuse
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_203.run(buf1180, buf1176, convolution_2, unsqueeze_894, buf1185, 32768, 1568, stream=stream0)
            buf1186 = empty_strided_cuda((32, ), (1, ), torch.float32)
            buf1188 = empty_strided_cuda((32, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204.run(buf1185, squeeze_7, buf1186, buf1188, 32, 1024, stream=stream0)
            buf1189 = empty_strided_cuda((128, 32, 112, 112), (401408, 1, 3584, 32), torch.float16)
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten.cat, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_cat_convolution_backward_native_batch_norm_backward_205.run(buf1180, buf1176, convolution_2, unsqueeze_894, buf1186, squeeze_7, buf1184, primals_18, buf1189, 51380224, stream=stream0)
            del convolution_2
            del primals_18
            del squeeze_7
            del unsqueeze_894
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf1190 = torch.ops.aten.convolution_backward.default(buf1189, relu_1, convert_element_type_7, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
            del buf1189
            del convert_element_type_7
            buf1191 = buf1190[0]
            assert_size_stride(buf1191, (128, 32, 112, 112), (401408, 1, 3584, 32), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1191, 16, 'torch.ops.aten.convolution_backward.default')
            buf1192 = buf1190[1]
            assert_size_stride(buf1192, (32, 32, 1, 1), (32, 1, 32, 32), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1192, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1190
            buf1194 = buf1185; del buf1185  # reuse
            buf1196 = empty_strided_cuda((32, 1024), (1, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_native_batch_norm_backward_threshold_backward_206.run(relu_1, buf1191, convolution_1, unsqueeze_906, buf1194, buf1196, 32768, 1568, stream=stream0)
            buf1195 = buf1186; del buf1186  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_threshold_backward_207.run(buf1194, buf1195, 32, 1024, stream=stream0)
            buf1197 = empty_strided_cuda((32, ), (1, ), torch.float32)
            buf1198 = empty_strided_cuda((32, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204.run(buf1196, squeeze_4, buf1197, buf1198, 32, 1024, stream=stream0)
            buf1199 = relu_1; del relu_1  # reuse
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_backward_native_batch_norm_backward_threshold_backward_208.run(buf1199, buf1191, convolution_1, unsqueeze_906, buf1197, squeeze_4, buf1195, primals_12, 51380224, stream=stream0)
            del buf1191
            del convolution_1
            del primals_12
            del squeeze_4
            del unsqueeze_906
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten.threshold_backward, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            buf1200 = torch.ops.aten.convolution_backward.default(buf1199, relu, convert_element_type_4, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 32, [True, True, False])
            del convert_element_type_4
            buf1201 = buf1200[0]
            assert_size_stride(buf1201, (128, 32, 112, 112), (401408, 1, 3584, 32), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1201, 16, 'torch.ops.aten.convolution_backward.default')
            buf1202 = buf1200[1]
            assert_size_stride(buf1202, (32, 1, 3, 3), (9, 1, 3, 1), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1202, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1200
            buf1204 = buf1196; del buf1196  # reuse
            buf1206 = buf1194; del buf1194  # reuse
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_add_cat_native_batch_norm_backward_threshold_backward_209.run(relu, buf1180, buf1176, buf1201, convolution, unsqueeze_918, buf1204, buf1206, 32768, 1568, stream=stream0)
            buf1205 = buf1197; del buf1197  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward]
            stream0 = get_raw_stream(0)
            triton_red_fused_native_batch_norm_backward_threshold_backward_207.run(buf1204, buf1205, 32, 1024, stream=stream0)
            del buf1204
            buf1207 = empty_strided_cuda((32, ), (1, ), torch.float32)
            buf1209 = empty_strided_cuda((32, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_cat_native_batch_norm_backward_204.run(buf1206, squeeze_1, buf1207, buf1209, 32, 1024, stream=stream0)
            del buf1206
            buf1210 = buf1199; del buf1199  # reuse
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten.threshold_backward, aten.cat, aten.add, aten.native_batch_norm_backward, aten._native_batch_norm_legit_functional, aten.convolution_backward]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_cat_convolution_backward_native_batch_norm_backward_threshold_backward_210.run(relu, buf1180, buf1176, buf1201, convolution, unsqueeze_918, buf1207, squeeze_1, buf1205, primals_6, buf1210, 51380224, stream=stream0)
            del buf1176
            del buf1180
            del buf1201
            del buf1207
            del convolution
            del primals_6
            del relu
            del squeeze_1
            del unsqueeze_918
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.convolution_backward]
            buf1211 = torch.ops.aten.convolution_backward.default(buf1210, convert_element_type_1, convert_element_type, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False])
            del buf1210
            del convert_element_type
            del convert_element_type_1
            buf1212 = buf1211[1]
            assert_size_stride(buf1212, (32, 3, 3, 3), (27, 1, 9, 3), 'torch.ops.aten.convolution_backward.default')
            assert_alignment(buf1212, 16, 'torch.ops.aten.convolution_backward.default')
            del buf1211
            buf1203 = empty_strided_cuda((32, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_211.run(buf1202, buf1203, 288, stream=stream0)
            del buf1202
            buf1081 = empty_strided_cuda((60, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_212.run(buf1080, buf1081, 540, stream=stream0)
            del buf1080
            buf1167 = empty_strided_cuda((64, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_213.run(buf1166, buf1167, 576, stream=stream0)
            del buf1166
            buf802 = empty_strided_cuda((14, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_214.run(buf796, buf802, 14, 128, stream=stream0)
            del buf796
            buf1213 = empty_strided_cuda((32, 3, 3, 3), (27, 1, 9, 3), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_215.run(buf1212, buf1213, 864, stream=stream0)
            del buf1212
            buf1055 = empty_strided_cuda((20, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_216.run(buf1049, buf1055, 20, 128, stream=stream0)
            del buf1049
            buf823 = empty_strided_cuda((112, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_217.run(buf822, buf823, 1008, stream=stream0)
            del buf822
            buf1193 = empty_strided_cuda((32, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_218.run(buf1192, buf1193, 1024, stream=stream0)
            del buf1192
            buf338 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_219.run(buf337, buf338, 1080, stream=stream0)
            del buf337
            buf413 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_219.run(buf412, buf413, 1080, stream=stream0)
            del buf412
            buf487 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_219.run(buf486, buf487, 1080, stream=stream0)
            del buf486
            buf1118 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_219.run(buf1117, buf1118, 1080, stream=stream0)
            del buf1117
            buf583 = empty_strided_cuda((26, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_220.run(buf577, buf583, 26, 128, stream=stream0)
            del buf577
            buf658 = empty_strided_cuda((26, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_220.run(buf652, buf658, 26, 128, stream=stream0)
            del buf652
            buf732 = empty_strided_cuda((26, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_220.run(buf726, buf732, 26, 128, stream=stream0)
            del buf726
            buf1103 = empty_strided_cuda((20, 60, 1, 1), (60, 1, 60, 60), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_221.run(buf1102, buf1103, 1200, stream=stream0)
            del buf1102
            buf1107 = empty_strided_cuda((20, 60, 1, 1), (60, 1, 60, 60), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_222.run(buf1106, buf1107, 1200, stream=stream0)
            del buf1106
            buf1128 = empty_strided_cuda((60, 20, 1, 1), (20, 1, 20, 20), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_223.run(buf1127, buf1128, 1200, stream=stream0)
            del buf1127
            buf1132 = empty_strided_cuda((60, 20, 1, 1), (20, 1, 20, 20), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_224.run(buf1131, buf1132, 1200, stream=stream0)
            del buf1131
            buf866 = empty_strided_cuda((28, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_225.run(buf860, buf866, 28, 128, stream=stream0)
            del buf860
            buf931 = empty_strided_cuda((28, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_225.run(buf925, buf931, 28, 128, stream=stream0)
            del buf925
            buf995 = empty_strided_cuda((28, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_225.run(buf989, buf995, 28, 128, stream=stream0)
            del buf989
            buf609 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(buf608, buf609, 1404, stream=stream0)
            del buf608
            buf684 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(buf683, buf684, 1404, stream=stream0)
            del buf683
            buf758 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(buf757, buf758, 1404, stream=stream0)
            del buf757
            buf1076 = empty_strided_cuda((60, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_227.run(buf1075, buf1076, 1500, stream=stream0)
            del buf1075
            buf882 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(buf881, buf882, 1512, stream=stream0)
            del buf881
            buf947 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(buf946, buf947, 1512, stream=stream0)
            del buf946
            buf1011 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(buf1010, buf1011, 1512, stream=stream0)
            del buf1010
            buf1178 = reinterpret_tensor(buf9, (96, 16, 1, 1), (16, 1, 16, 16), 0); del buf9  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_229.run(buf1177, buf1178, 1536, stream=stream0)
            del buf1177
            buf1182 = empty_strided_cuda((96, 16, 1, 1), (16, 1, 16, 16), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_229.run(buf1181, buf1182, 1536, stream=stream0)
            del buf1181
            buf1163 = empty_strided_cuda((64, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_230.run(buf1162, buf1163, 1600, stream=stream0)
            del buf1162
            buf1143 = empty_strided_cuda((20, 96, 1, 1), (96, 1, 96, 96), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_231.run(buf1142, buf1143, 1920, stream=stream0)
            del buf1142
            buf1148 = empty_strided_cuda((20, 96, 1, 1), (96, 1, 96, 96), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_231.run(buf1147, buf1148, 1920, stream=stream0)
            del buf1147
            buf271 = empty_strided_cuda((240, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_232.run(buf270, buf271, 2160, stream=stream0)
            del buf270
            buf531 = empty_strided_cuda((52, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_233.run(buf525, buf531, 52, 128, stream=stream0)
            del buf525
            buf818 = empty_strided_cuda((112, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_234.run(buf817, buf818, 2800, stream=stream0)
            del buf817
            buf1071 = empty_strided_cuda((60, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_235.run(buf1070, buf1071, 2940, stream=stream0)
            del buf1070
            buf333 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_236.run(buf332, buf333, 3000, stream=stream0)
            del buf332
            buf408 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_236.run(buf407, buf408, 3000, stream=stream0)
            del buf407
            buf482 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_236.run(buf481, buf482, 3000, stream=stream0)
            del buf481
            buf1159 = empty_strided_cuda((64, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_237.run(buf1158, buf1159, 3136, stream=stream0)
            del buf1158
            buf245 = empty_strided_cuda((80, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_238.run(buf239, buf245, 80, 128, stream=stream0)
            del buf239
            buf312 = empty_strided_cuda((80, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_238.run(buf306, buf312, 80, 128, stream=stream0)
            del buf306
            buf387 = empty_strided_cuda((80, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_238.run(buf381, buf387, 80, 128, stream=stream0)
            del buf381
            buf461 = empty_strided_cuda((80, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_238.run(buf455, buf461, 80, 128, stream=stream0)
            del buf455
            buf71 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_239.run(buf70, buf71, 3564, stream=stream0)
            del buf70
            buf138 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_239.run(buf137, buf138, 3564, stream=stream0)
            del buf137
            buf207 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_239.run(buf206, buf207, 3564, stream=stream0)
            del buf206
            buf604 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(buf603, buf604, 3900, stream=stream0)
            del buf603
            buf679 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(buf678, buf679, 3900, stream=stream0)
            del buf678
            buf753 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(buf752, buf753, 3900, stream=stream0)
            del buf752
            buf877 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_241.run(buf876, buf877, 4200, stream=stream0)
            del buf876
            buf942 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_241.run(buf941, buf942, 4200, stream=stream0)
            del buf941
            buf1006 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_241.run(buf1005, buf1006, 4200, stream=stream0)
            del buf1005
            buf794 = empty_strided_cuda((336, 14, 1, 1), (14, 1, 14, 14), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_242.run(buf793, buf794, 4704, stream=stream0)
            del buf793
            buf801 = empty_strided_cuda((14, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_243.run(buf800, buf801, 4704, stream=stream0)
            del buf800
            buf846 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_244.run(buf845, buf846, 4704, stream=stream0)
            del buf845
            buf850 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_245.run(buf849, buf850, 4704, stream=stream0)
            del buf849
            buf894 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf893, buf894, 4704, stream=stream0)
            del buf893
            buf899 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf898, buf899, 4704, stream=stream0)
            del buf898
            buf910 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_245.run(buf909, buf910, 4704, stream=stream0)
            del buf909
            buf915 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_245.run(buf914, buf915, 4704, stream=stream0)
            del buf914
            buf959 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf958, buf959, 4704, stream=stream0)
            del buf958
            buf964 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf963, buf964, 4704, stream=stream0)
            del buf963
            buf975 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_244.run(buf974, buf975, 4704, stream=stream0)
            del buf974
            buf979 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_245.run(buf978, buf979, 4704, stream=stream0)
            del buf978
            buf1023 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf1022, buf1023, 4704, stream=stream0)
            del buf1022
            buf1028 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_246.run(buf1027, buf1028, 4704, stream=stream0)
            del buf1027
            buf1047 = empty_strided_cuda((240, 20, 1, 1), (20, 1, 20, 20), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_247.run(buf1046, buf1047, 4800, stream=stream0)
            del buf1046
            buf1054 = empty_strided_cuda((20, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_248.run(buf1053, buf1054, 4800, stream=stream0)
            del buf1053
            buf1066 = empty_strided_cuda((60, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_249.run(buf1065, buf1066, 4860, stream=stream0)
            del buf1065
            buf813 = empty_strided_cuda((112, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_250.run(buf812, buf813, 5488, stream=stream0)
            del buf812
            buf542 = empty_strided_cuda((624, 1, 3, 3), (9, 1, 3, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_251.run(buf541, buf542, 5616, stream=stream0)
            del buf541
            buf45 = empty_strided_cuda((132, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_252.run(buf39, buf45, 132, 128, stream=stream0)
            del buf39
            buf112 = empty_strided_cuda((132, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_252.run(buf106, buf112, 132, 128, stream=stream0)
            del buf106
            buf181 = empty_strided_cuda((132, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_252.run(buf175, buf181, 132, 128, stream=stream0)
            del buf175
            buf328 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_253.run(buf327, buf328, 5880, stream=stream0)
            del buf327
            buf403 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_253.run(buf402, buf403, 5880, stream=stream0)
            del buf402
            buf477 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_253.run(buf476, buf477, 5880, stream=stream0)
            del buf476
            buf266 = empty_strided_cuda((240, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_254.run(buf265, buf266, 6000, stream=stream0)
            del buf265
            buf599 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_255.run(buf598, buf599, 7644, stream=stream0)
            del buf598
            buf674 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_255.run(buf673, buf674, 7644, stream=stream0)
            del buf673
            buf748 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_255.run(buf747, buf748, 7644, stream=stream0)
            del buf747
            buf858 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_256.run(buf857, buf858, 9408, stream=stream0)
            del buf857
            buf865 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_257.run(buf864, buf865, 9408, stream=stream0)
            del buf864
            buf923 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_256.run(buf922, buf923, 9408, stream=stream0)
            del buf922
            buf930 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_257.run(buf929, buf930, 9408, stream=stream0)
            del buf929
            buf987 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_256.run(buf986, buf987, 9408, stream=stream0)
            del buf986
            buf994 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_257.run(buf993, buf994, 9408, stream=stream0)
            del buf993
            buf1093 = empty_strided_cuda((240, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_258.run(buf1092, buf1093, 9600, stream=stream0)
            del buf1092
            buf323 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_259.run(buf322, buf323, 9720, stream=stream0)
            del buf322
            buf398 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_259.run(buf397, buf398, 9720, stream=stream0)
            del buf397
            buf472 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_259.run(buf471, buf472, 9720, stream=stream0)
            del buf471
            buf66 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_260.run(buf65, buf66, 9900, stream=stream0)
            del buf65
            buf133 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_260.run(buf132, buf133, 9900, stream=stream0)
            del buf132
            buf202 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_260.run(buf201, buf202, 9900, stream=stream0)
            del buf201
            buf1048 = buf1087; del buf1087  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_261.run(buf1042, buf1048, 240, 128, stream=stream0)
            del buf1042
            buf261 = empty_strided_cuda((240, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_262.run(buf260, buf261, 11760, stream=stream0)
            del buf260
            buf594 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_263.run(buf593, buf594, 12636, stream=stream0)
            del buf593
            buf669 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_263.run(buf668, buf669, 12636, stream=stream0)
            del buf668
            buf743 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_263.run(buf742, buf743, 12636, stream=stream0)
            del buf742
            buf1039 = empty_strided_cuda((56, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_264.run(buf1038, buf1039, 13440, stream=stream0)
            del buf1038
            buf795 = buf1016; del buf1016  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_265.run(buf789, buf795, 336, 128, stream=stream0)
            del buf789
            buf859 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_265.run(buf853, buf859, 336, 128, stream=stream0)
            del buf853
            buf924 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_265.run(buf918, buf924, 336, 128, stream=stream0)
            del buf918
            buf988 = empty_strided_cuda((336, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_265.run(buf982, buf988, 336, 128, stream=stream0)
            del buf982
            buf563 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_266.run(buf562, buf563, 16224, stream=stream0)
            del buf562
            buf567 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_267.run(buf566, buf567, 16224, stream=stream0)
            del buf566
            buf575 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_268.run(buf574, buf575, 16224, stream=stream0)
            del buf574
            buf582 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_269.run(buf581, buf582, 16224, stream=stream0)
            del buf581
            buf621 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf620, buf621, 16224, stream=stream0)
            del buf620
            buf626 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf625, buf626, 16224, stream=stream0)
            del buf625
            buf637 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_267.run(buf636, buf637, 16224, stream=stream0)
            del buf636
            buf642 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_267.run(buf641, buf642, 16224, stream=stream0)
            del buf641
            buf650 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_268.run(buf649, buf650, 16224, stream=stream0)
            del buf649
            buf657 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_269.run(buf656, buf657, 16224, stream=stream0)
            del buf656
            buf696 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf695, buf696, 16224, stream=stream0)
            del buf695
            buf701 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf700, buf701, 16224, stream=stream0)
            del buf700
            buf712 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_266.run(buf711, buf712, 16224, stream=stream0)
            del buf711
            buf716 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_267.run(buf715, buf716, 16224, stream=stream0)
            del buf715
            buf724 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_268.run(buf723, buf724, 16224, stream=stream0)
            del buf723
            buf731 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_269.run(buf730, buf731, 16224, stream=stream0)
            del buf730
            buf770 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf769, buf770, 16224, stream=stream0)
            del buf769
            buf775 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_270.run(buf774, buf775, 16224, stream=stream0)
            del buf774
            buf836 = empty_strided_cuda((336, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_271.run(buf835, buf836, 18816, stream=stream0)
            del buf835
            buf292 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf291, buf292, 19200, stream=stream0)
            del buf291
            buf296 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf295, buf296, 19200, stream=stream0)
            del buf295
            buf350 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf349, buf350, 19200, stream=stream0)
            del buf349
            buf355 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf354, buf355, 19200, stream=stream0)
            del buf354
            buf366 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf365, buf366, 19200, stream=stream0)
            del buf365
            buf371 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf370, buf371, 19200, stream=stream0)
            del buf370
            buf425 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf424, buf425, 19200, stream=stream0)
            del buf424
            buf430 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf429, buf430, 19200, stream=stream0)
            del buf429
            buf441 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf440, buf441, 19200, stream=stream0)
            del buf440
            buf445 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_272.run(buf444, buf445, 19200, stream=stream0)
            del buf444
            buf499 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf498, buf499, 19200, stream=stream0)
            del buf498
            buf504 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_273.run(buf503, buf504, 19200, stream=stream0)
            del buf503
            buf61 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_274.run(buf60, buf61, 19404, stream=stream0)
            del buf60
            buf128 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_274.run(buf127, buf128, 19404, stream=stream0)
            del buf127
            buf197 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_274.run(buf196, buf197, 19404, stream=stream0)
            del buf196
            buf256 = empty_strided_cuda((240, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_275.run(buf255, buf256, 19440, stream=stream0)
            del buf255
            buf305 = buf492; del buf492  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_276.run(buf299, buf305, 480, 128, stream=stream0)
            del buf299
            buf380 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_276.run(buf374, buf380, 480, 128, stream=stream0)
            del buf374
            buf454 = empty_strided_cuda((480, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_276.run(buf448, buf454, 480, 128, stream=stream0)
            del buf448
            buf524 = buf763; del buf763  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_277.run(buf518, buf524, 624, 128, stream=stream0)
            del buf518
            buf576 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_277.run(buf570, buf576, 624, 128, stream=stream0)
            del buf570
            buf651 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_277.run(buf645, buf651, 624, 128, stream=stream0)
            del buf645
            buf725 = empty_strided_cuda((624, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_277.run(buf719, buf725, 624, 128, stream=stream0)
            del buf719
            buf56 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_278.run(buf55, buf56, 32076, stream=stream0)
            del buf55
            buf123 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_278.run(buf122, buf123, 32076, stream=stream0)
            del buf122
            buf192 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_278.run(buf191, buf192, 32076, stream=stream0)
            del buf191
            buf523 = empty_strided_cuda((624, 52, 1, 1), (52, 1, 52, 52), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_279.run(buf522, buf523, 32448, stream=stream0)
            del buf522
            buf530 = empty_strided_cuda((52, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_280.run(buf529, buf530, 32448, stream=stream0)
            del buf529
            buf786 = empty_strided_cuda((104, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_281.run(buf785, buf786, 34944, stream=stream0)
            del buf785
            buf304 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_282.run(buf303, buf304, 38400, stream=stream0)
            del buf303
            buf311 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_283.run(buf310, buf311, 38400, stream=stream0)
            del buf310
            buf379 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_282.run(buf378, buf379, 38400, stream=stream0)
            del buf378
            buf386 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_283.run(buf385, buf386, 38400, stream=stream0)
            del buf385
            buf453 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_282.run(buf452, buf453, 38400, stream=stream0)
            del buf452
            buf460 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_283.run(buf459, buf460, 38400, stream=stream0)
            del buf459
            buf238 = reinterpret_tensor(buf1084, (960, ), (1, ), 0); del buf1084  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_284.run(buf232, buf238, 960, 128, stream=stream0)
            del buf232
            buf4 = empty_strided_cuda((1000, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.sum, aten.view, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_red_fused__to_copy_sum_view_285.run(tangents_1, buf4, 1000, 128, stream=stream0)
            buf553 = empty_strided_cuda((624, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_286.run(buf552, buf553, 64896, stream=stream0)
            del buf552
            buf38 = buf212; del buf212  # reuse
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_287.run(buf32, buf38, 1584, 128, stream=stream0)
            del buf32
            buf105 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_287.run(buf99, buf105, 1584, 128, stream=stream0)
            del buf99
            buf174 = empty_strided_cuda((1584, ), (1, ), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.convolution_backward, aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_per_fused__to_copy_convolution_backward_287.run(buf168, buf174, 1584, 128, stream=stream0)
            del buf168
            buf237 = empty_strided_cuda((960, 80, 1, 1), (80, 1, 80, 80), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_288.run(buf236, buf237, 76800, stream=stream0)
            del buf236
            buf244 = empty_strided_cuda((80, 960, 1, 1), (960, 1, 960, 960), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_289.run(buf243, buf244, 76800, stream=stream0)
            del buf243
            buf515 = empty_strided_cuda((160, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_290.run(buf514, buf515, 99840, stream=stream0)
            del buf514
            buf25 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_291.run(buf24, buf25, 104544, stream=stream0)
            del buf24
            buf29 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_292.run(buf28, buf29, 104544, stream=stream0)
            del buf28
            buf92 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_291.run(buf91, buf92, 104544, stream=stream0)
            del buf91
            buf96 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_292.run(buf95, buf96, 104544, stream=stream0)
            del buf95
            buf160 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_292.run(buf159, buf160, 104544, stream=stream0)
            del buf159
            buf165 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_292.run(buf164, buf165, 104544, stream=stream0)
            del buf164
            buf282 = empty_strided_cuda((960, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_293.run(buf281, buf282, 153600, stream=stream0)
            del buf281
            buf37 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_294.run(buf36, buf37, 209088, stream=stream0)
            del buf36
            buf44 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_295.run(buf43, buf44, 209088, stream=stream0)
            del buf43
            buf104 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_294.run(buf103, buf104, 209088, stream=stream0)
            del buf103
            buf111 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_295.run(buf110, buf111, 209088, stream=stream0)
            del buf110
            buf173 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_294.run(buf172, buf173, 209088, stream=stream0)
            del buf172
            buf180 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_295.run(buf179, buf180, 209088, stream=stream0)
            del buf179
            buf229 = empty_strided_cuda((264, 960, 1, 1), (960, 1, 960, 960), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_296.run(buf228, buf229, 253440, stream=stream0)
            del buf228
            buf15 = empty_strided_cuda((1536, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_297.run(buf14, buf15, 405504, stream=stream0)
            del buf14
            buf82 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_298.run(buf81, buf82, 418176, stream=stream0)
            del buf81
            buf149 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_298.run(buf148, buf149, 418176, stream=stream0)
            del buf148
            buf218 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_298.run(buf217, buf218, 418176, stream=stream0)
            del buf217
            buf1 = empty_strided_cuda((1000, 1536), (1536, 1), torch.float16)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten.t, aten.mm]
            extern_kernels.mm(reinterpret_tensor(tangents_1, (1000, 128), (1, 1000), 0), view, out=buf1)
            del tangents_1
            del view
            buf3 = empty_strided_cuda((1000, 1536), (1536, 1), torch.float32)
            # Topologically Sorted Source Nodes: [], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_299.run(buf1, buf3, 1536000, stream=stream0)
            del buf1
        return (buf1213, None, None, None, None, buf1209, buf1205, buf1203, None, None, None, buf1198, buf1195, buf1193, None, None, None, buf1188, buf1184, buf1182, buf1178, None, None, None, buf1173, buf1170, buf1167, buf1163, buf1159, None, None, None, buf1154, buf1151, buf1148, buf1143, None, None, None, buf1138, buf1134, buf1132, buf1128, None, None, None, buf1123, buf1120, buf1118, None, None, None, buf1113, buf1110, buf1107, buf1103, None, None, None, buf1098, buf1095, buf1093, None, None, None, buf1088, buf1085, buf1081, buf1076, buf1071, buf1066, None, None, None, buf1061, buf1057, buf1054, buf1055, buf1047, buf1048, buf1039, None, None, None, buf1034, buf1030, buf1028, buf1023, None, None, None, buf1018, buf1014, buf1011, buf1006, None, None, None, buf1000, buf997, buf994, buf995, buf987, buf988, buf979, buf975, None, None, None, buf970, buf967, buf964, buf959, None, None, None, buf954, buf950, buf947, buf942, None, None, None, buf936, buf933, buf930, buf931, buf923, buf924, buf915, buf910, None, None, None, buf905, buf901, buf899, buf894, None, None, None, buf889, buf885, buf882, buf877, None, None, None, buf871, buf868, buf865, buf866, buf858, buf859, buf850, buf846, None, None, None, buf841, buf838, buf836, None, None, None, buf831, buf826, buf823, buf818, buf813, None, None, None, buf808, buf804, buf801, buf802, buf794, buf795, buf786, None, None, None, buf781, buf777, buf775, buf770, None, None, None, buf764, buf761, buf758, buf753, buf748, buf743, None, None, None, buf737, buf734, buf731, buf732, buf724, buf725, buf716, buf712, None, None, None, buf707, buf704, buf701, buf696, None, None, None, buf690, buf687, buf684, buf679, buf674, buf669, None, None, None, buf663, buf660, buf657, buf658, buf650, buf651, buf642, buf637, None, None, None, buf632, buf628, buf626, buf621, None, None, None, buf615, buf612, buf609, buf604, buf599, buf594, None, None, None, buf588, buf585, buf582, buf583, buf575, buf576, buf567, buf563, None, None, None, buf558, buf555, buf553, None, None, None, buf548, buf545, buf542, None, None, None, buf537, buf533, buf530, buf531, buf523, buf524, buf515, None, None, None, buf510, buf506, buf504, buf499, None, None, None, buf493, buf490, buf487, buf482, buf477, buf472, None, None, None, buf466, buf463, buf460, buf461, buf453, buf454, buf445, buf441, None, None, None, buf436, buf433, buf430, buf425, None, None, None, buf419, buf416, buf413, buf408, buf403, buf398, None, None, None, buf392, buf389, buf386, buf387, buf379, buf380, buf371, buf366, None, None, None, buf361, buf357, buf355, buf350, None, None, None, buf344, buf341, buf338, buf333, buf328, buf323, None, None, None, buf317, buf314, buf311, buf312, buf304, buf305, buf296, buf292, None, None, None, buf287, buf284, buf282, None, None, None, buf277, buf274, buf271, buf266, buf261, buf256, None, None, None, buf251, buf247, buf244, buf245, buf237, buf238, buf229, None, None, None, buf224, buf220, buf218, None, None, None, buf213, buf210, buf207, buf202, buf197, buf192, None, None, None, buf186, buf183, buf180, buf181, buf173, buf174, buf165, buf160, None, None, None, buf155, buf151, buf149, None, None, None, buf144, buf141, buf138, buf133, buf128, buf123, None, None, None, buf117, buf114, buf111, buf112, buf104, buf105, buf96, buf92, None, None, None, buf87, buf84, buf82, None, None, None, buf77, buf74, buf71, buf66, buf61, buf56, None, None, None, buf50, buf47, buf44, buf45, buf37, buf38, buf29, buf25, None, None, None, buf20, buf17, buf15, None, None, None, buf10, buf7, buf3, buf4, )

runner = Runner(partitions=[])
call = runner.call
recursively_apply_fns = runner.recursively_apply_fns


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_6 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_12 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_18 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_25 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_26 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_33 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_34 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_40 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_47 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_53 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_54 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_60 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_66 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_67 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_75 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_76 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_85 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_92 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_93 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_99 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_100 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_110 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_117 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_118 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_124 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_125 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_135 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_142 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_143 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_149 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_150 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_160 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_166 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_167 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_174 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_175 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_184 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_191 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_192 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_200 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_201 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_211 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_218 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_219 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_227 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_228 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_238 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_245 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_246 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_254 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_255 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_265 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_271 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_272 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_277 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_278 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_287 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_294 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_295 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_303 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_304 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_314 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_321 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_322 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_330 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_331 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_341 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_348 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_349 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_357 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_358 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_368 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_374 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_375 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_383 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_384 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_393 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_399 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_400 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_408 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_409 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_419 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_425 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_426 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_434 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_435 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_445 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_451 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_452 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_460 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_461 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_471 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_477 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_478 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    convert_element_type = rand_strided((32, 3, 3, 3), (27, 1, 9, 3), device='cuda:0', dtype=torch.float16)
    convert_element_type_1 = rand_strided((128, 3, 224, 224), (150528, 1, 672, 3), device='cuda:0', dtype=torch.float16)
    convolution = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    squeeze_1 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    convert_element_type_4 = rand_strided((32, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    convolution_1 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    squeeze_4 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_1 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    convert_element_type_7 = rand_strided((32, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float16)
    convolution_2 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    squeeze_7 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_6 = rand_strided((128, 16, 112, 112), (401408, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    getitem_7 = rand_strided((128, 16, 112, 112), (401408, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_10 = rand_strided((96, 16, 1, 1), (16, 1, 16, 16), device='cuda:0', dtype=torch.float16)
    convert_element_type_11 = rand_strided((96, 16, 1, 1), (16, 1, 16, 16), device='cuda:0', dtype=torch.float16)
    cat = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    getitem_9 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    rsqrt_3 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    convert_element_type_14 = rand_strided((64, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_13 = rand_strided((128, 64, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_15 = rand_strided((64, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_17 = rand_strided((128, 64, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_16 = rand_strided((64, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_21 = rand_strided((128, 64, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    cat_1 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    getitem_23 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    rsqrt_4 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    convert_element_type_19 = rand_strided((20, 96, 1, 1), (96, 1, 96, 96), device='cuda:0', dtype=torch.float16)
    getitem_26 = rand_strided((128, 96, 56, 56), (602112, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_20 = rand_strided((20, 96, 1, 1), (96, 1, 96, 96), device='cuda:0', dtype=torch.float16)
    getitem_29 = rand_strided((128, 96, 56, 56), (602112, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    cat_2 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    squeeze_16 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_32 = rand_strided((128, 20, 56, 56), (62720, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    getitem_33 = rand_strided((128, 20, 56, 56), (62720, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_23 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    convert_element_type_24 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    cat_3 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    squeeze_19 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_4 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    convert_element_type_27 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    convolution_12 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    getitem_37 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    rsqrt_7 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    convert_element_type_30 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float16)
    getitem_40 = rand_strided((128, 60, 56, 56), (376320, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_31 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float16)
    getitem_43 = rand_strided((128, 60, 56, 56), (376320, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    cat_4 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    squeeze_25 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_46 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    convert_element_type_34 = rand_strided((240, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float16)
    convolution_15 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    getitem_47 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    rsqrt_9 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    convert_element_type_39 = rand_strided((60, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_52 = rand_strided((128, 60, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_40 = rand_strided((60, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_57 = rand_strided((128, 60, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_41 = rand_strided((60, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_62 = rand_strided((128, 60, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_42 = rand_strided((60, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_67 = rand_strided((128, 60, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    cat_5 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    getitem_69 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    rsqrt_10 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    mean = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convert_element_type_48 = rand_strided((20, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convolution_20 = rand_strided((128, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    convert_element_type_50 = rand_strided((128, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    convert_element_type_52 = rand_strided((240, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    convolution_21 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    mul_80 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    convert_element_type_53 = rand_strided((56, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convolution_22 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    squeeze_34 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_72 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_73 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_56 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_57 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    cat_6 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_75 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_12 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    convert_element_type_62 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_78 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_63 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_81 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    cat_7 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_83 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_13 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    mean_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convert_element_type_69 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convolution_27 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_71 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_73 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convolution_28 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    getitem_84 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_85 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_74 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    convert_element_type_75 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    cat_8 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    squeeze_43 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_88 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_89 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_78 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_79 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    cat_9 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_91 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_15 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    convert_element_type_84 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_94 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_85 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_97 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    cat_10 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_99 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_16 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    mean_2 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convert_element_type_91 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convolution_35 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_93 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_95 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convolution_36 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    getitem_100 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_101 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_96 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    convert_element_type_97 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    cat_11 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    squeeze_52 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_104 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_105 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_100 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_101 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    cat_12 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_107 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_18 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    convert_element_type_106 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_110 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_107 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_113 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    cat_13 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_115 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_19 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    mean_3 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convert_element_type_113 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convolution_43 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_115 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convert_element_type_117 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    convolution_44 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    getitem_116 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    getitem_117 = rand_strided((128, 168, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_118 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    convert_element_type_119 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    cat_14 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    squeeze_61 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_109 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    convert_element_type_122 = rand_strided((336, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float16)
    convolution_47 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    getitem_121 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_21 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    convert_element_type_127 = rand_strided((112, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_125 = rand_strided((128, 112, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_128 = rand_strided((112, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_129 = rand_strided((128, 112, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_129 = rand_strided((112, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_133 = rand_strided((128, 112, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    cat_15 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    getitem_135 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    rsqrt_22 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    mean_4 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convert_element_type_135 = rand_strided((14, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convolution_51 = rand_strided((128, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    convert_element_type_137 = rand_strided((128, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    convert_element_type_139 = rand_strided((336, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    convolution_52 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    mul_180 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    convert_element_type_140 = rand_strided((104, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    convolution_53 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    squeeze_70 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_138 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_139 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_143 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convert_element_type_144 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    cat_16 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_141 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_24 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    convert_element_type_149 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_146 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_150 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_151 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_151 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_156 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_152 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_161 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_17 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_163 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_25 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    mean_5 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_158 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convolution_60 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_160 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_162 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convolution_61 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    getitem_164 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_165 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_163 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    convert_element_type_164 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    cat_18 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    squeeze_79 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_168 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_169 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_167 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convert_element_type_168 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    cat_19 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_171 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_27 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    convert_element_type_173 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_176 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_174 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_181 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_175 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_186 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_176 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_191 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_20 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_193 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_28 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    mean_6 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_182 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convolution_70 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_184 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_186 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convolution_71 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    getitem_194 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_195 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_187 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    convert_element_type_188 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    cat_21 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    squeeze_88 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_198 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_199 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_191 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convert_element_type_192 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    cat_22 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_201 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_30 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    convert_element_type_197 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_206 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_198 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_211 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_199 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_216 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_200 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_221 = rand_strided((128, 156, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_23 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_223 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_31 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    mean_7 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_206 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convolution_80 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_208 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convert_element_type_210 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    convolution_81 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    getitem_224 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_225 = rand_strided((128, 312, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_211 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    convert_element_type_212 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    cat_24 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    squeeze_97 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_172 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    convert_element_type_215 = rand_strided((624, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float16)
    convolution_84 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_229 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_33 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    convert_element_type_219 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_220 = rand_strided((624, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    convolution_85 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    getitem_231 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    rsqrt_34 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    mean_8 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_226 = rand_strided((52, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convolution_86 = rand_strided((128, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convert_element_type_228 = rand_strided((128, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convert_element_type_230 = rand_strided((624, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    convolution_87 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    mul_280 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    convert_element_type_231 = rand_strided((160, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    convolution_88 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    squeeze_106 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_234 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_235 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_234 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_235 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    cat_25 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_237 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_36 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    convert_element_type_240 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_242 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_241 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_247 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_242 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_252 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_243 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_257 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_26 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_259 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_37 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    mean_9 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convert_element_type_249 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convolution_95 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_251 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_253 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convolution_96 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    getitem_260 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_261 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_254 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convert_element_type_255 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    cat_27 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    squeeze_115 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_264 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_265 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_258 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_259 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    cat_28 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_267 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_39 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    convert_element_type_264 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_272 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_265 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_277 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_266 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_282 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_267 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_287 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_29 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_289 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_40 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    mean_10 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convert_element_type_273 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convolution_105 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_275 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_277 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convolution_106 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    getitem_290 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_291 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_278 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convert_element_type_279 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    cat_30 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    squeeze_124 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_294 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_295 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_282 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_283 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    cat_31 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_297 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_42 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    convert_element_type_288 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_302 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_289 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_307 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_290 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_312 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_291 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_317 = rand_strided((128, 120, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_32 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    getitem_319 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    rsqrt_43 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    mean_11 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convert_element_type_297 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    convolution_115 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_299 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_301 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convolution_116 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    getitem_320 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    getitem_321 = rand_strided((128, 240, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_302 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    convert_element_type_303 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    cat_33 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    squeeze_133 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_235 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    convert_element_type_306 = rand_strided((960, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float16)
    convolution_119 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    getitem_325 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    rsqrt_45 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    convert_element_type_311 = rand_strided((240, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_330 = rand_strided((128, 240, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_312 = rand_strided((240, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_335 = rand_strided((128, 240, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_313 = rand_strided((240, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_340 = rand_strided((128, 240, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_314 = rand_strided((240, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_345 = rand_strided((128, 240, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    cat_34 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    getitem_347 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    rsqrt_46 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    mean_12 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    convert_element_type_320 = rand_strided((80, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    convolution_124 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_322 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convert_element_type_324 = rand_strided((960, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    convolution_125 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    mul_380 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    convert_element_type_325 = rand_strided((264, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    convolution_126 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    squeeze_142 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    convert_element_type_327 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    convert_element_type_328 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    convolution_127 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_351 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_48 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    convert_element_type_333 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_356 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_334 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_361 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_335 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_366 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_336 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_371 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    cat_35 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_373 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_49 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    mean_13 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convert_element_type_342 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convolution_132 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_344 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_346 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convolution_133 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    getitem_374 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_375 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_347 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    convert_element_type_348 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    cat_36 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    squeeze_151 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_266 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    convert_element_type_351 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    convolution_136 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_379 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_51 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    convert_element_type_356 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_384 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_357 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_389 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_358 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_394 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_359 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_399 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    cat_37 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_401 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_52 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    mean_14 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convert_element_type_365 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convolution_141 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_367 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_369 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convolution_142 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    getitem_402 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_403 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_370 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    convert_element_type_371 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    cat_38 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    squeeze_160 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_282 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    convert_element_type_374 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    convolution_145 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_407 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_54 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    convert_element_type_379 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    getitem_412 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_380 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    getitem_417 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_381 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_422 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_382 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    getitem_427 = rand_strided((128, 396, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    cat_39 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    getitem_429 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    rsqrt_55 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    mean_15 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convert_element_type_388 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    convolution_150 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_390 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convert_element_type_392 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    convolution_151 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    getitem_430 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    getitem_431 = rand_strided((128, 792, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    convert_element_type_393 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    convert_element_type_394 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    cat_40 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    squeeze_169 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_298 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    convert_element_type_397 = rand_strided((1536, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    convolution_154 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    getitem_435 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    rsqrt_57 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    view = rand_strided((128, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    permute_1 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    unsqueeze_246 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_282 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_318 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_354 = rand_strided((1, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_390 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_426 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_462 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_498 = rand_strided((1, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_534 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_570 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_606 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_642 = rand_strided((1, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_678 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_714 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_750 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_786 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_822 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_846 = rand_strided((1, 120, 1, 1), (120, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_858 = rand_strided((1, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_894 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_906 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_918 = rand_strided((1, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    tangents_1 = rand_strided((128, 1000), (1000, 1), device='cuda:0', dtype=torch.float16)
    fn = lambda: call([primals_6, primals_12, primals_18, primals_25, primals_26, primals_33, primals_34, primals_40, primals_47, primals_53, primals_54, primals_60, primals_66, primals_67, primals_75, primals_76, primals_85, primals_92, primals_93, primals_99, primals_100, primals_110, primals_117, primals_118, primals_124, primals_125, primals_135, primals_142, primals_143, primals_149, primals_150, primals_160, primals_166, primals_167, primals_174, primals_175, primals_184, primals_191, primals_192, primals_200, primals_201, primals_211, primals_218, primals_219, primals_227, primals_228, primals_238, primals_245, primals_246, primals_254, primals_255, primals_265, primals_271, primals_272, primals_277, primals_278, primals_287, primals_294, primals_295, primals_303, primals_304, primals_314, primals_321, primals_322, primals_330, primals_331, primals_341, primals_348, primals_349, primals_357, primals_358, primals_368, primals_374, primals_375, primals_383, primals_384, primals_393, primals_399, primals_400, primals_408, primals_409, primals_419, primals_425, primals_426, primals_434, primals_435, primals_445, primals_451, primals_452, primals_460, primals_461, primals_471, primals_477, primals_478, convert_element_type, convert_element_type_1, convolution, squeeze_1, relu, convert_element_type_4, convolution_1, squeeze_4, relu_1, convert_element_type_7, convolution_2, squeeze_7, getitem_6, getitem_7, convert_element_type_10, convert_element_type_11, cat, getitem_9, rsqrt_3, convert_element_type_14, getitem_13, convert_element_type_15, getitem_17, convert_element_type_16, getitem_21, cat_1, getitem_23, rsqrt_4, convert_element_type_19, getitem_26, convert_element_type_20, getitem_29, cat_2, squeeze_16, getitem_32, getitem_33, convert_element_type_23, convert_element_type_24, cat_3, squeeze_19, relu_4, convert_element_type_27, convolution_12, getitem_37, rsqrt_7, convert_element_type_30, getitem_40, convert_element_type_31, getitem_43, cat_4, squeeze_25, add_46, convert_element_type_34, convolution_15, getitem_47, rsqrt_9, convert_element_type_39, getitem_52, convert_element_type_40, getitem_57, convert_element_type_41, getitem_62, convert_element_type_42, getitem_67, cat_5, getitem_69, rsqrt_10, mean, convert_element_type_48, convolution_20, convert_element_type_50, convert_element_type_52, convolution_21, mul_80, convert_element_type_53, convolution_22, squeeze_34, getitem_72, getitem_73, convert_element_type_56, convert_element_type_57, cat_6, getitem_75, rsqrt_12, convert_element_type_62, getitem_78, convert_element_type_63, getitem_81, cat_7, getitem_83, rsqrt_13, mean_1, convert_element_type_69, convolution_27, convert_element_type_71, convert_element_type_73, convolution_28, getitem_84, getitem_85, convert_element_type_74, convert_element_type_75, cat_8, squeeze_43, getitem_88, getitem_89, convert_element_type_78, convert_element_type_79, cat_9, getitem_91, rsqrt_15, convert_element_type_84, getitem_94, convert_element_type_85, getitem_97, cat_10, getitem_99, rsqrt_16, mean_2, convert_element_type_91, convolution_35, convert_element_type_93, convert_element_type_95, convolution_36, getitem_100, getitem_101, convert_element_type_96, convert_element_type_97, cat_11, squeeze_52, getitem_104, getitem_105, convert_element_type_100, convert_element_type_101, cat_12, getitem_107, rsqrt_18, convert_element_type_106, getitem_110, convert_element_type_107, getitem_113, cat_13, getitem_115, rsqrt_19, mean_3, convert_element_type_113, convolution_43, convert_element_type_115, convert_element_type_117, convolution_44, getitem_116, getitem_117, convert_element_type_118, convert_element_type_119, cat_14, squeeze_61, add_109, convert_element_type_122, convolution_47, getitem_121, rsqrt_21, convert_element_type_127, getitem_125, convert_element_type_128, getitem_129, convert_element_type_129, getitem_133, cat_15, getitem_135, rsqrt_22, mean_4, convert_element_type_135, convolution_51, convert_element_type_137, convert_element_type_139, convolution_52, mul_180, convert_element_type_140, convolution_53, squeeze_70, getitem_138, getitem_139, convert_element_type_143, convert_element_type_144, cat_16, getitem_141, rsqrt_24, convert_element_type_149, getitem_146, convert_element_type_150, getitem_151, convert_element_type_151, getitem_156, convert_element_type_152, getitem_161, cat_17, getitem_163, rsqrt_25, mean_5, convert_element_type_158, convolution_60, convert_element_type_160, convert_element_type_162, convolution_61, getitem_164, getitem_165, convert_element_type_163, convert_element_type_164, cat_18, squeeze_79, getitem_168, getitem_169, convert_element_type_167, convert_element_type_168, cat_19, getitem_171, rsqrt_27, convert_element_type_173, getitem_176, convert_element_type_174, getitem_181, convert_element_type_175, getitem_186, convert_element_type_176, getitem_191, cat_20, getitem_193, rsqrt_28, mean_6, convert_element_type_182, convolution_70, convert_element_type_184, convert_element_type_186, convolution_71, getitem_194, getitem_195, convert_element_type_187, convert_element_type_188, cat_21, squeeze_88, getitem_198, getitem_199, convert_element_type_191, convert_element_type_192, cat_22, getitem_201, rsqrt_30, convert_element_type_197, getitem_206, convert_element_type_198, getitem_211, convert_element_type_199, getitem_216, convert_element_type_200, getitem_221, cat_23, getitem_223, rsqrt_31, mean_7, convert_element_type_206, convolution_80, convert_element_type_208, convert_element_type_210, convolution_81, getitem_224, getitem_225, convert_element_type_211, convert_element_type_212, cat_24, squeeze_97, add_172, convert_element_type_215, convolution_84, getitem_229, rsqrt_33, convert_element_type_219, convert_element_type_220, convolution_85, getitem_231, rsqrt_34, mean_8, convert_element_type_226, convolution_86, convert_element_type_228, convert_element_type_230, convolution_87, mul_280, convert_element_type_231, convolution_88, squeeze_106, getitem_234, getitem_235, convert_element_type_234, convert_element_type_235, cat_25, getitem_237, rsqrt_36, convert_element_type_240, getitem_242, convert_element_type_241, getitem_247, convert_element_type_242, getitem_252, convert_element_type_243, getitem_257, cat_26, getitem_259, rsqrt_37, mean_9, convert_element_type_249, convolution_95, convert_element_type_251, convert_element_type_253, convolution_96, getitem_260, getitem_261, convert_element_type_254, convert_element_type_255, cat_27, squeeze_115, getitem_264, getitem_265, convert_element_type_258, convert_element_type_259, cat_28, getitem_267, rsqrt_39, convert_element_type_264, getitem_272, convert_element_type_265, getitem_277, convert_element_type_266, getitem_282, convert_element_type_267, getitem_287, cat_29, getitem_289, rsqrt_40, mean_10, convert_element_type_273, convolution_105, convert_element_type_275, convert_element_type_277, convolution_106, getitem_290, getitem_291, convert_element_type_278, convert_element_type_279, cat_30, squeeze_124, getitem_294, getitem_295, convert_element_type_282, convert_element_type_283, cat_31, getitem_297, rsqrt_42, convert_element_type_288, getitem_302, convert_element_type_289, getitem_307, convert_element_type_290, getitem_312, convert_element_type_291, getitem_317, cat_32, getitem_319, rsqrt_43, mean_11, convert_element_type_297, convolution_115, convert_element_type_299, convert_element_type_301, convolution_116, getitem_320, getitem_321, convert_element_type_302, convert_element_type_303, cat_33, squeeze_133, add_235, convert_element_type_306, convolution_119, getitem_325, rsqrt_45, convert_element_type_311, getitem_330, convert_element_type_312, getitem_335, convert_element_type_313, getitem_340, convert_element_type_314, getitem_345, cat_34, getitem_347, rsqrt_46, mean_12, convert_element_type_320, convolution_124, convert_element_type_322, convert_element_type_324, convolution_125, mul_380, convert_element_type_325, convolution_126, squeeze_142, convert_element_type_327, convert_element_type_328, convolution_127, getitem_351, rsqrt_48, convert_element_type_333, getitem_356, convert_element_type_334, getitem_361, convert_element_type_335, getitem_366, convert_element_type_336, getitem_371, cat_35, getitem_373, rsqrt_49, mean_13, convert_element_type_342, convolution_132, convert_element_type_344, convert_element_type_346, convolution_133, getitem_374, getitem_375, convert_element_type_347, convert_element_type_348, cat_36, squeeze_151, add_266, convert_element_type_351, convolution_136, getitem_379, rsqrt_51, convert_element_type_356, getitem_384, convert_element_type_357, getitem_389, convert_element_type_358, getitem_394, convert_element_type_359, getitem_399, cat_37, getitem_401, rsqrt_52, mean_14, convert_element_type_365, convolution_141, convert_element_type_367, convert_element_type_369, convolution_142, getitem_402, getitem_403, convert_element_type_370, convert_element_type_371, cat_38, squeeze_160, add_282, convert_element_type_374, convolution_145, getitem_407, rsqrt_54, convert_element_type_379, getitem_412, convert_element_type_380, getitem_417, convert_element_type_381, getitem_422, convert_element_type_382, getitem_427, cat_39, getitem_429, rsqrt_55, mean_15, convert_element_type_388, convolution_150, convert_element_type_390, convert_element_type_392, convolution_151, getitem_430, getitem_431, convert_element_type_393, convert_element_type_394, cat_40, squeeze_169, add_298, convert_element_type_397, convolution_154, getitem_435, rsqrt_57, view, permute_1, unsqueeze_246, unsqueeze_282, unsqueeze_318, unsqueeze_354, unsqueeze_390, unsqueeze_426, unsqueeze_462, unsqueeze_498, unsqueeze_534, unsqueeze_570, unsqueeze_606, unsqueeze_642, unsqueeze_678, unsqueeze_714, unsqueeze_750, unsqueeze_786, unsqueeze_822, unsqueeze_846, unsqueeze_858, unsqueeze_894, unsqueeze_906, unsqueeze_918, tangents_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('mixnet_l', benchmark_compiled_module)