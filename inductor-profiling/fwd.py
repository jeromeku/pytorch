# AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
from torch._C import _cuda_getCurrentRawStream as get_raw_stream

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
assert_alignment = torch._C._dynamo.guards.assert_alignment
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_jeromeku/7l/c7lmpzfrg6zn4uqddxksm7djzzrsj5ldqibp3ld7gvkcbcsw5xye.py
# Topologically Sorted Source Nodes: [x], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x => convert_element_type
# Graph fragment:
#   %primals_1 : Tensor "f32[32, 3, 3, 3][27, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_1]
#   %convert_element_type : Tensor "f16[32, 3, 3, 3][27, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_1, torch.float16), kwargs = {})
#   return %convert_element_type
triton_poi_fused__to_copy_0 = async_compile.triton('triton_poi_fused__to_copy_0', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 128, 'x': 16}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_0', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 3456, 'x': 3456}, 'kernel_num_gb': 5.184e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_0(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 96
    xnumel = 9
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 3)
    y1 = yindex // 3
    tmp0 = tl.load(in_ptr0 + (x2 + 9*y3), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 3*x2 + 27*y1), tmp1, xmask & ymask)


def get_args():
    arg_0 = rand_strided((32, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32, 3, 3, 3), (27, 1, 9, 3), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 96, 9,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_0.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_0.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.184e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/as/casd3wh4pztumb7qytbdtwi3ciofldjnbffltfwhatpiizimcpkz.py
# Topologically Sorted Source Nodes: [x], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x => convert_element_type_1
# Graph fragment:
#   %primals_2 : Tensor "f32[128, 3, 224, 224][150528, 50176, 224, 1]cuda:0" = PlaceHolder[target=primals_2]
#   %convert_element_type_1 : Tensor "f16[128, 3, 224, 224][150528, 50176, 224, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_2, torch.float16), kwargs = {})
#   return %convert_element_type_1
triton_poi_fused__to_copy_1 = async_compile.triton('triton_poi_fused__to_copy_1', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 512, 'x': 65536}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_1', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 77070336, 'x': 77070336}, 'kernel_num_gb': 0.115605504, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_1(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 384
    xnumel = 50176
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 3)
    y1 = yindex // 3
    tmp0 = tl.load(in_ptr0 + (x2 + 50176*y3), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (y0 + 3*x2 + 150528*y1), tmp1, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 3, 224, 224), (150528, 50176, 224, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 3, 224, 224), (150528, 1, 672, 3), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 384, 50176,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_1.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_1.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.115605504
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hj/chj25y7pzz342x3xdigoqxxp4lch6zdj7euc2gsz3lb24q4k6a3d.py
# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_1 => convert_element_type_2, var_mean
# Graph fragment:
#   %convolution : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution]
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %var_mean : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_2, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf3,%buf4,%buf5
triton_red_fused__native_batch_norm_legit_functional_2 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_2', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_2', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 103546880, 'r0_': 0}, 'kernel_num_gb': 0.103153664, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_2(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 32768
    r0_numel = 1568
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_2 + 50176*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 32768, 1568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_2.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_2.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.103153664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6z/c6zy24exuyjl2kxjdngdbdr72zmiujvgdzghms5izkxbgtkcnvd5.py
# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_1 => convert_element_type_2, var_mean
# Graph fragment:
#   %buf3 : Tensor "f32[1, 32, 1, 1, 1024][32768, 1, 32768, 32768, 32]cuda:0" = PlaceHolder[target=buf3]
#   %buf4 : Tensor "f32[1, 32, 1, 1, 1024][32768, 1, 32768, 32768, 32]cuda:0" = PlaceHolder[target=buf4]
#   %buf5 : Tensor "f32[1, 32, 1, 1, 1024][32768, 1, 32768, 32768, 32]cuda:0" = PlaceHolder[target=buf5]
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %var_mean : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_2, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf6,%buf7,%buf8
triton_red_fused__native_batch_norm_legit_functional_3 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_3', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_3', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 399360, 'r0_': 0}, 'kernel_num_gb': 0.000396288, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_3(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 256
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_2 + 4096*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 32*r0_2 + 4096*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 32*r0_2 + 4096*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 256, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_3.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_3.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000396288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4v/c4vsgm3notglq4smbnstflwc4ykegz6ip4n5noat7zkmcauofbvc.py
# Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_1 => add_1, add_2, add_3, convert_element_type_2, mul_1, mul_2, mul_3, mul_4, mul_5, rsqrt, squeeze, squeeze_2, var_mean
# Graph fragment:
#   %buf6 : Tensor "f32[1, 32, 1, 1, 8][256, 1, 256, 256, 32]cuda:0" = PlaceHolder[target=buf6]
#   %buf7 : Tensor "f32[1, 32, 1, 1, 8][256, 1, 256, 256, 32]cuda:0" = PlaceHolder[target=buf7]
#   %buf8 : Tensor "f32[1, 32, 1, 1, 8][256, 1, 256, 256, 32]cuda:0" = PlaceHolder[target=buf8]
#   %buf10 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=buf10]
#   %getitem_1 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=getitem_1]
#   %copy__1 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=copy__1]
#   %add_2 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=add_2]
#   %copy__2 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=copy__2]
#   %add_3 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=add_3]
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %var_mean : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_2, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_1 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem, 1e-05), kwargs = {})
#   %rsqrt : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1,), kwargs = {})
#   %squeeze : Tensor "f32[32][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_1, [0, 2, 3]), kwargs = {})
#   %mul_1 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, 0.1), kwargs = {})
#   %mul_2 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_4, 0.9), kwargs = {})
#   %add_2 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1, %mul_2), kwargs = {})
#   %squeeze_2 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem, [0, 2, 3]), kwargs = {})
#   %mul_3 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, 1.0000006228081046), kwargs = {})
#   %mul_4 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_3, 0.1), kwargs = {})
#   %mul_5 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_5, 0.9), kwargs = {})
#   %add_3 : Tensor "f32[32][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_4, %mul_5), kwargs = {})
#   %copy__1 : Tensor "f32[32][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_4, %add_2), kwargs = {})
#   %copy__2 : Tensor "f32[32][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_5, %add_3), kwargs = {})
#   return %getitem_1,%buf10,%rsqrt,%add_2,%buf1197,%add_3,%buf1200
triton_per_fused__native_batch_norm_legit_functional_copy__4 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__4', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 32, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__4', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4608, 'r0_': 0}, 'kernel_num_gb': 3.968e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__4(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 32
    r0_numel = 8
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 32*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 32*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 32*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 1605632.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000006228081046
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 32, 8,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__4.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__4.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.968e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/x7/cx7wocvsvtyogvc6dszn3icqc7ue36nqnhut2vbhutrj7enronc3.py
# Topologically Sorted Source Nodes: [x_1, x_2], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_1 => add_1, add_4, convert_element_type_2, convert_element_type_3, mul, mul_6, rsqrt, sub, unsqueeze, unsqueeze_1, unsqueeze_2, unsqueeze_3, var_mean
#   x_2 => relu
# Graph fragment:
#   %convolution : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution]
#   %getitem_1 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=getitem_1]
#   %buf10 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=buf10]
#   %primals_6 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_6]
#   %primals_7 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_7]
#   %convert_element_type_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution, torch.float32), kwargs = {})
#   %var_mean : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_2, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_1 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem, 1e-05), kwargs = {})
#   %rsqrt : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1,), kwargs = {})
#   %sub : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution, %getitem_1), kwargs = {})
#   %mul : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, %rsqrt), kwargs = {})
#   %unsqueeze : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_6, -1), kwargs = {})
#   %unsqueeze_1 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, -1), kwargs = {})
#   %mul_6 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %unsqueeze_1), kwargs = {})
#   %unsqueeze_2 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_7, -1), kwargs = {})
#   %unsqueeze_3 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_2, -1), kwargs = {})
#   %add_4 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_6, %unsqueeze_3), kwargs = {})
#   %convert_element_type_3 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_4, torch.float16), kwargs = {})
#   %relu : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_3,), kwargs = {})
#   return %relu
triton_poi_fused__native_batch_norm_legit_functional_relu_5 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_relu_5', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_relu_5', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 308281856}, 'kernel_num_gb': 0.205521408, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_relu_5(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 51380224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 32)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 1605632.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp16 = tl.full([1], 0, tl.int32)
    tmp17 = triton_helpers.maximum(tmp16, tmp15)
    tl.store(out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 51380224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_relu_5.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_relu_5.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.205521408
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/56/c56g4swah4nhmpjpadjomiie3tygngrsy2tk6u46ni7q4thqrqd4.py
# Topologically Sorted Source Nodes: [x_3], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_3 => convert_element_type_4
# Graph fragment:
#   %primals_8 : Tensor "f32[32, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_8]
#   %convert_element_type_4 : Tensor "f16[32, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_8, torch.float16), kwargs = {})
#   return %convert_element_type_4
triton_poi_fused__to_copy_6 = async_compile.triton('triton_poi_fused__to_copy_6', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 512}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_6', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2304}, 'kernel_num_gb': 1.728e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_6(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 288
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((32, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 288,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_6.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_6.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.728e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zw/czw2uckkqs24mzr3s22ckcfkozq3eko5y5iv2nrf5ochhirjqjkd.py
# Topologically Sorted Source Nodes: [x_6], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_6 => convert_element_type_7
# Graph fragment:
#   %primals_14 : Tensor "f32[32, 32, 1, 1][32, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_14]
#   %convert_element_type_7 : Tensor "f16[32, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_14, torch.float16), kwargs = {})
#   return %convert_element_type_7
triton_poi_fused__to_copy_7 = async_compile.triton('triton_poi_fused__to_copy_7', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_7', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8192}, 'kernel_num_gb': 6.144e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_7(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((32, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((32, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1024,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_7.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_7.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.144e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qo/cqozcjernj7i7nvi5t2qmeld33kvvp7yqapoefyxmhnzfvbth5m3.py
# Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_7 => add_11, add_14, convert_element_type_8, convert_element_type_9, mul_14, mul_20, rsqrt_2, sub_2, unsqueeze_10, unsqueeze_11, unsqueeze_8, unsqueeze_9, var_mean_2
#   x_8 => add_15
# Graph fragment:
#   %convolution_2 : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=convolution_2]
#   %getitem_5 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=getitem_5]
#   %buf36 : Tensor "f32[1, 32, 1, 1][32, 1, 32, 32]cuda:0" = PlaceHolder[target=buf36]
#   %primals_18 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_18]
#   %primals_19 : Tensor "f32[32][1]cuda:0" = PlaceHolder[target=primals_19]
#   %relu : Tensor "f16[128, 32, 112, 112][401408, 1, 3584, 32]cuda:0" = PlaceHolder[target=relu]
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %var_mean_2 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_8, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_11 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_4, 1e-05), kwargs = {})
#   %rsqrt_2 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_11,), kwargs = {})
#   %sub_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_2, %getitem_5), kwargs = {})
#   %mul_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_2, %rsqrt_2), kwargs = {})
#   %unsqueeze_8 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_18, -1), kwargs = {})
#   %unsqueeze_9 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_8, -1), kwargs = {})
#   %mul_20 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_14, %unsqueeze_9), kwargs = {})
#   %unsqueeze_10 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_19, -1), kwargs = {})
#   %unsqueeze_11 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_10, -1), kwargs = {})
#   %add_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_20, %unsqueeze_11), kwargs = {})
#   %convert_element_type_9 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_14, torch.float16), kwargs = {})
#   %add_15 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_9, %relu), kwargs = {})
#   return %add_15
triton_poi_fused__native_batch_norm_legit_functional_add_8 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_8', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 2097152, 'x': 32}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_8', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 205520896, 'x': 205521408}, 'kernel_num_gb': 0.308281856, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_8(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1605632
    xnumel = 32
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = (yindex % 12544)
    y1 = yindex // 12544
    tmp0 = tl.load(in_ptr0 + (x2 + 32*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x2 + 32*y3), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 1605632.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(out_ptr0 + (y0 + 12544*x2 + 401408*y1), tmp17, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 32, 1, 1), (32, 1, 32, 32), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((32,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 32, 112, 112), (401408, 1, 3584, 32), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 32, 112, 112), (401408, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 1605632, 32,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_8.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_8.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.308281856
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/tj/ctja52k52h3ntcdwat7ne3ak5fe3pciwfilfqgneflidyc3i4lcq.py
# Topologically Sorted Source Nodes: [conv2d_3], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_3 => convert_element_type_10
# Graph fragment:
#   %primals_20 : Tensor "f32[96, 16, 1, 1][16, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_20]
#   %convert_element_type_10 : Tensor "f16[96, 16, 1, 1][16, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_20, torch.float16), kwargs = {})
#   return %convert_element_type_10
triton_poi_fused__to_copy_9 = async_compile.triton('triton_poi_fused__to_copy_9', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_9', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12288}, 'kernel_num_gb': 9.216e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_9(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1536
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((96, 16, 1, 1), (16, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((96, 16, 1, 1), (16, 1, 16, 16), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1536,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_9.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_9.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.216e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5d/c5dmbwpdllpmjdnvinozlujgyoi7c3nd3mbxodxf33aaaygcvnby.py
# Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_3], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_3 => convolution_3
#   split => split_with_sizes
#   x_7 => add_11, add_14, convert_element_type_8, convert_element_type_9, mul_14, mul_20, rsqrt_2, sub_2, unsqueeze_10, unsqueeze_11, unsqueeze_8, unsqueeze_9, var_mean_2
#   x_8 => add_15
# Graph fragment:
#   %add_15 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0" = PlaceHolder[target=add_15]
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %var_mean_2 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_8, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_11 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_4, 1e-05), kwargs = {})
#   %rsqrt_2 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_11,), kwargs = {})
#   %sub_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_2, %getitem_5), kwargs = {})
#   %mul_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_2, %rsqrt_2), kwargs = {})
#   %unsqueeze_8 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_18, -1), kwargs = {})
#   %unsqueeze_9 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_8, -1), kwargs = {})
#   %mul_20 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_14, %unsqueeze_9), kwargs = {})
#   %unsqueeze_10 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_19, -1), kwargs = {})
#   %unsqueeze_11 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_10, -1), kwargs = {})
#   %add_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_20, %unsqueeze_11), kwargs = {})
#   %convert_element_type_9 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_14, torch.float16), kwargs = {})
#   %add_15 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_9, %relu), kwargs = {})
#   %split_with_sizes : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%add_15, [16, 16], 1), kwargs = {})
#   %convolution_3 : Tensor "f16[128, 96, 112, 112][1204224, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_6, %convert_element_type_10, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf41
triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 2048, 'x': 16384}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 102760448, 'x': 51380224}, 'kernel_num_gb': 0.102760448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2048
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 16)
    y1 = yindex // 16
    tmp0 = tl.load(in_ptr0 + (x2 + 12544*y0 + 401408*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 16*x2 + 200704*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2048, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.102760448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wg/cwgiznnfesak3cwsnnb7q4qbimwxrgwnn5f5cz6wnm2r5r6ev57a.py
# Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_4], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_4 => convolution_4
#   split => split_with_sizes
#   x_7 => add_11, add_14, convert_element_type_8, convert_element_type_9, mul_14, mul_20, rsqrt_2, sub_2, unsqueeze_10, unsqueeze_11, unsqueeze_8, unsqueeze_9, var_mean_2
#   x_8 => add_15
# Graph fragment:
#   %add_15 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0" = PlaceHolder[target=add_15]
#   %convert_element_type_8 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_2, torch.float32), kwargs = {})
#   %var_mean_2 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_8, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_11 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_4, 1e-05), kwargs = {})
#   %rsqrt_2 : Tensor "f32[1, 32, 1, 1][32, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_11,), kwargs = {})
#   %sub_2 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_2, %getitem_5), kwargs = {})
#   %mul_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_2, %rsqrt_2), kwargs = {})
#   %unsqueeze_8 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_18, -1), kwargs = {})
#   %unsqueeze_9 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_8, -1), kwargs = {})
#   %mul_20 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_14, %unsqueeze_9), kwargs = {})
#   %unsqueeze_10 : Tensor "f32[32, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_19, -1), kwargs = {})
#   %unsqueeze_11 : Tensor "f32[32, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_10, -1), kwargs = {})
#   %add_14 : Tensor "f32[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_20, %unsqueeze_11), kwargs = {})
#   %convert_element_type_9 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_14, torch.float16), kwargs = {})
#   %add_15 : Tensor "f16[128, 32, 112, 112][401408, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_9, %relu), kwargs = {})
#   %split_with_sizes : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%add_15, [16, 16], 1), kwargs = {})
#   %convolution_4 : Tensor "f16[128, 96, 112, 112][1204224, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_7, %convert_element_type_11, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf44
triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 2048, 'x': 16384}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 102760448, 'x': 51380224}, 'kernel_num_gb': 0.102760448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2048
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 16)
    y1 = yindex // 16
    tmp0 = tl.load(in_ptr0 + (200704 + x2 + 12544*y0 + 401408*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 16*x2 + 200704*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 32, 112, 112), (401408, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 16, 112, 112), (200704, 1, 1792, 16), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2048, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.102760448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3e/c3e42gbtn2p2etehixsrwcxyjyyz3gjn622lbcggnu5mphuj7qgg.py
# Topologically Sorted Source Nodes: [x_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_9 => cat
# Graph fragment:
#   %convolution_3 : Tensor "f16[128, 96, 112, 112][1204224, 1, 10752, 96]cuda:0" = PlaceHolder[target=convolution_3]
#   %convolution_4 : Tensor "f16[128, 96, 112, 112][1204224, 1, 10752, 96]cuda:0" = PlaceHolder[target=convolution_4]
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_3, %convolution_4], 1), kwargs = {})
#   return %cat
triton_poi_fused_cat_12 = async_compile.triton('triton_poi_fused_cat_12', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 536870912}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_12', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2466250752}, 'kernel_num_gb': 1.233125376, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_12(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 308281344
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 192)
    x1 = xindex // 192
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 96, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (96*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 192, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (96*x1 + ((-96) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 96, 112, 112), (1204224, 1, 10752, 96), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 96, 112, 112), (1204224, 1, 10752, 96), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 308281344,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_12.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_12.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.233125376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7a/c7afktj6kjhivv7vorlsrkelc3by2htredezjju4j2yxj36dcgjc.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_10 => convert_element_type_12, var_mean_3
# Graph fragment:
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=cat]
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %var_mean_3 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_12, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf47,%buf48,%buf49
triton_red_fused__native_batch_norm_legit_functional_13 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_13', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 2048},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_13', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 620175360, 'r0_': 0}, 'kernel_num_gb': 0.618369024, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_13(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 150528
    r0_numel = 2048
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 393216*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 150528, 2048,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_13.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_13.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.618369024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4r/c4rpe4xrgzrvadkmhu2cgkpjs7sfr6y2qna5p2dsobnx5aygli5y.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_10 => convert_element_type_12, var_mean_3
# Graph fragment:
#   %buf47 : Tensor "f32[1, 192, 1, 1, 784][150528, 1, 150528, 150528, 192]cuda:0" = PlaceHolder[target=buf47]
#   %buf48 : Tensor "f32[1, 192, 1, 1, 784][150528, 1, 150528, 150528, 192]cuda:0" = PlaceHolder[target=buf48]
#   %buf49 : Tensor "f32[1, 192, 1, 1, 784][150528, 1, 150528, 150528, 192]cuda:0" = PlaceHolder[target=buf49]
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %var_mean_3 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_12, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf50,%buf51,%buf52
triton_red_fused__native_batch_norm_legit_functional_14 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_14', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 2048, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_14', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1838592, 'r0_': 0}, 'kernel_num_gb': 0.001822464, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_14(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1344
    r0_numel = 112
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 21504*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 192*r0_2 + 21504*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 192*r0_2 + 21504*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 1344, 112,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_14.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_14.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001822464
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wa/cwaxg2oyo3offbkufwpbajdzayc44rpj7b62plwsvjtnclafiho7.py
# Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_10 => add_17, add_18, add_19, convert_element_type_12, mul_22, mul_23, mul_24, mul_25, mul_26, rsqrt_3, squeeze_11, squeeze_9, var_mean_3
# Graph fragment:
#   %buf50 : Tensor "f32[1, 192, 1, 1, 7][1344, 1, 1344, 1344, 192]cuda:0" = PlaceHolder[target=buf50]
#   %buf51 : Tensor "f32[1, 192, 1, 1, 7][1344, 1, 1344, 1344, 192]cuda:0" = PlaceHolder[target=buf51]
#   %buf52 : Tensor "f32[1, 192, 1, 1, 7][1344, 1, 1344, 1344, 192]cuda:0" = PlaceHolder[target=buf52]
#   %buf54 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=buf54]
#   %getitem_9 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_9]
#   %copy__10 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=copy__10]
#   %add_18 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=add_18]
#   %copy__11 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=copy__11]
#   %add_19 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=add_19]
#   %convert_element_type_12 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat, torch.float32), kwargs = {})
#   %var_mean_3 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_12, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_17 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_8, 1e-05), kwargs = {})
#   %rsqrt_3 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_17,), kwargs = {})
#   %squeeze_9 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_9, [0, 2, 3]), kwargs = {})
#   %mul_22 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_9, 0.1), kwargs = {})
#   %mul_23 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_23, 0.9), kwargs = {})
#   %add_18 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_22, %mul_23), kwargs = {})
#   %squeeze_11 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_8, [0, 2, 3]), kwargs = {})
#   %mul_24 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_11, 1.0000006228081046), kwargs = {})
#   %mul_25 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_24, 0.1), kwargs = {})
#   %mul_26 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_24, 0.9), kwargs = {})
#   %add_19 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_25, %mul_26), kwargs = {})
#   %copy__10 : Tensor "f32[192][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_23, %add_18), kwargs = {})
#   %copy__11 : Tensor "f32[192][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_24, %add_19), kwargs = {})
#   return %getitem_9,%buf54,%rsqrt_3,%add_18,%buf1221,%add_19,%buf1224
triton_per_fused__native_batch_norm_legit_functional_copy__15 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__15', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__15', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 23808, 'r0_': 0}, 'kernel_num_gb': 2.0736e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__15(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 7
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 192*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 192*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 1605632.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000006228081046
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 192, 7,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__15.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__15.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.0736e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ds/cdsxbg236szyl3ty5xm3pxlultdo4lyb6uj3p7uaqhsjdmjyjr3c.py
# Topologically Sorted Source Nodes: [conv2d_5], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_5 => convert_element_type_14
# Graph fragment:
#   %primals_27 : Tensor "f32[64, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_27]
#   %convert_element_type_14 : Tensor "f16[64, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_27, torch.float16), kwargs = {})
#   return %convert_element_type_14
triton_poi_fused__to_copy_16 = async_compile.triton('triton_poi_fused__to_copy_16', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_16', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4608}, 'kernel_num_gb': 3.456e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_16(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 576
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((64, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 576,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_16.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_16.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.456e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/r6/cr6vdqjvdriqanavzptohd35ot4j3ntf5uh2p27nvbuasftnecro.py
# Topologically Sorted Source Nodes: [x_10, x_11], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_10 => add_20, convert_element_type_13, mul_21, mul_27, sub_3, unsqueeze_12, unsqueeze_13, unsqueeze_14, unsqueeze_15
#   x_11 => relu_2
# Graph fragment:
#   %cat : Tensor "f16[128, 192, 112, 112][2408448, 1, 21504, 192]cuda:0" = PlaceHolder[target=cat]
#   %getitem_9 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_9]
#   %rsqrt_3 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_3]
#   %primals_25 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_25]
#   %primals_26 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_26]
#   %sub_3 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat, %getitem_9), kwargs = {})
#   %mul_21 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_3, %rsqrt_3), kwargs = {})
#   %unsqueeze_12 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_25, -1), kwargs = {})
#   %unsqueeze_13 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, -1), kwargs = {})
#   %mul_27 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_21, %unsqueeze_13), kwargs = {})
#   %unsqueeze_14 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_26, -1), kwargs = {})
#   %unsqueeze_15 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, -1), kwargs = {})
#   %add_20 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_27, %unsqueeze_15), kwargs = {})
#   %convert_element_type_13 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_20, torch.float16), kwargs = {})
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_13,), kwargs = {})
#   return %relu_2
triton_poi_fused__native_batch_norm_legit_functional_relu_17 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_relu_17', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 16384}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_relu_17', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 616565760, 'x': 1233125376}, 'kernel_num_gb': 1.233128448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_relu_17(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 24576
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 192)
    y1 = yindex // 192
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 192*x2 + 2408448*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (y0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1, 1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tl.store(out_ptr0 + (x2 + 12544*y3), tmp12, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 1, 21504, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 192, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 24576, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_relu_17.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_relu_17.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.233128448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sr/csrhbh4zjn6us6nggajgyrzikbcxszn5v5ydbppf3z3lj4j7aql4.py
# Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_5 => convolution_5, split_with_sizes_2
#   x_10 => add_20, convert_element_type_13, mul_21, mul_27, sub_3, unsqueeze_12, unsqueeze_13, unsqueeze_14, unsqueeze_15
#   x_11 => relu_2
# Graph fragment:
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0" = PlaceHolder[target=relu_2]
#   %sub_3 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat, %getitem_9), kwargs = {})
#   %mul_21 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_3, %rsqrt_3), kwargs = {})
#   %unsqueeze_12 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_25, -1), kwargs = {})
#   %unsqueeze_13 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, -1), kwargs = {})
#   %mul_27 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_21, %unsqueeze_13), kwargs = {})
#   %unsqueeze_14 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_26, -1), kwargs = {})
#   %unsqueeze_15 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, -1), kwargs = {})
#   %add_20 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_27, %unsqueeze_15), kwargs = {})
#   %convert_element_type_13 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_20, torch.float16), kwargs = {})
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_13,), kwargs = {})
#   %split_with_sizes_2 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_2, [64, 64, 64], 1), kwargs = {})
#   %convolution_5 : Tensor "f16[128, 64, 56, 56][200704, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_13, %convert_element_type_14, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 64), kwargs = {})
#   return %buf59
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 16384}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 411041792, 'x': 205520896}, 'kernel_num_gb': 0.411041792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8192
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 64)
    y1 = yindex // 64
    tmp0 = tl.load(in_ptr0 + (x2 + 12544*y0 + 2408448*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 64*x2 + 802816*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 8192, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.411041792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6o/c6o3g2k5qmfpeyhnclwqpsuimybs26kwdm55zblcnygfx7cbmkmh.py
# Topologically Sorted Source Nodes: [conv2d_6], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_6 => convert_element_type_15
# Graph fragment:
#   %primals_28 : Tensor "f32[64, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_28]
#   %convert_element_type_15 : Tensor "f16[64, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_28, torch.float16), kwargs = {})
#   return %convert_element_type_15
triton_poi_fused__to_copy_19 = async_compile.triton('triton_poi_fused__to_copy_19', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_19', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12800}, 'kernel_num_gb': 9.6e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_19(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((64, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_19.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_19.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.6e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/np/cnpydr23ngtsdyjahowtuolm6o44vp5touybd4nymujy2ldtdrm6.py
# Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_6], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_5 => split_with_sizes_2
#   conv2d_6 => convolution_6
#   x_10 => add_20, convert_element_type_13, mul_21, mul_27, sub_3, unsqueeze_12, unsqueeze_13, unsqueeze_14, unsqueeze_15
#   x_11 => relu_2
# Graph fragment:
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0" = PlaceHolder[target=relu_2]
#   %sub_3 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat, %getitem_9), kwargs = {})
#   %mul_21 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_3, %rsqrt_3), kwargs = {})
#   %unsqueeze_12 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_25, -1), kwargs = {})
#   %unsqueeze_13 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, -1), kwargs = {})
#   %mul_27 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_21, %unsqueeze_13), kwargs = {})
#   %unsqueeze_14 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_26, -1), kwargs = {})
#   %unsqueeze_15 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, -1), kwargs = {})
#   %add_20 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_27, %unsqueeze_15), kwargs = {})
#   %convert_element_type_13 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_20, torch.float16), kwargs = {})
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_13,), kwargs = {})
#   %split_with_sizes_2 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_2, [64, 64, 64], 1), kwargs = {})
#   %convolution_6 : Tensor "f16[128, 64, 56, 56][200704, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_17, %convert_element_type_15, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 64), kwargs = {})
#   return %buf62
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 16384}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 411041792, 'x': 205520896}, 'kernel_num_gb': 0.411041792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8192
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 64)
    y1 = yindex // 64
    tmp0 = tl.load(in_ptr0 + (802816 + x2 + 12544*y0 + 2408448*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 64*x2 + 802816*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 8192, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.411041792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/df/cdfanhwzjbfldkwpfx5ygvealtzssqu7ojbnzhzzi6h7qrdya467.py
# Topologically Sorted Source Nodes: [conv2d_7], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_7 => convert_element_type_16
# Graph fragment:
#   %primals_29 : Tensor "f32[64, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_29]
#   %convert_element_type_16 : Tensor "f16[64, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_29, torch.float16), kwargs = {})
#   return %convert_element_type_16
triton_poi_fused__to_copy_21 = async_compile.triton('triton_poi_fused__to_copy_21', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_21', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 25088}, 'kernel_num_gb': 1.8816e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_21(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3136
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((64, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((64, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_21.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_21.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.8816e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ac/cacwfq5h3cki3vqnkxnrcqcqbto6atv6ssa3fwx5mzjqblxwia76.py
# Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_7], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_5 => split_with_sizes_2
#   conv2d_7 => convolution_7
#   x_10 => add_20, convert_element_type_13, mul_21, mul_27, sub_3, unsqueeze_12, unsqueeze_13, unsqueeze_14, unsqueeze_15
#   x_11 => relu_2
# Graph fragment:
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0" = PlaceHolder[target=relu_2]
#   %sub_3 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat, %getitem_9), kwargs = {})
#   %mul_21 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_3, %rsqrt_3), kwargs = {})
#   %unsqueeze_12 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_25, -1), kwargs = {})
#   %unsqueeze_13 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, -1), kwargs = {})
#   %mul_27 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_21, %unsqueeze_13), kwargs = {})
#   %unsqueeze_14 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_26, -1), kwargs = {})
#   %unsqueeze_15 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, -1), kwargs = {})
#   %add_20 : Tensor "f32[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_27, %unsqueeze_15), kwargs = {})
#   %convert_element_type_13 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_20, torch.float16), kwargs = {})
#   %relu_2 : Tensor "f16[128, 192, 112, 112][2408448, 12544, 112, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_13,), kwargs = {})
#   %split_with_sizes_2 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_2, [64, 64, 64], 1), kwargs = {})
#   %convolution_7 : Tensor "f16[128, 64, 56, 56][200704, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_21, %convert_element_type_16, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 64), kwargs = {})
#   return %buf65
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 16384}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 411041792, 'x': 205520896}, 'kernel_num_gb': 0.411041792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8192
    xnumel = 12544
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 64)
    y1 = yindex // 64
    tmp0 = tl.load(in_ptr0 + (1605632 + x2 + 12544*y0 + 2408448*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 64*x2 + 802816*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 112, 112), (2408448, 12544, 112, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 8192, 12544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.411041792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xg/cxgyomensfyc7skyo6oxzpuzia3ifp24cfzkfylclxuknbknabts.py
# Topologically Sorted Source Nodes: [x_12], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_12 => cat_1
# Graph fragment:
#   %convolution_5 : Tensor "f16[128, 64, 56, 56][200704, 1, 3584, 64]cuda:0" = PlaceHolder[target=convolution_5]
#   %convolution_6 : Tensor "f16[128, 64, 56, 56][200704, 1, 3584, 64]cuda:0" = PlaceHolder[target=convolution_6]
#   %convolution_7 : Tensor "f16[128, 64, 56, 56][200704, 1, 3584, 64]cuda:0" = PlaceHolder[target=convolution_7]
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_5, %convolution_6, %convolution_7], 1), kwargs = {})
#   return %cat_1
triton_poi_fused_cat_23 = async_compile.triton('triton_poi_fused_cat_23', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 134217728}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_23', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 770703360}, 'kernel_num_gb': 0.308281344, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_23(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 77070336
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 192)
    x1 = xindex // 192
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 64, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (64*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 128, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (64*x1 + ((-64) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 192, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tl.load(in_ptr2 + (64*x1 + ((-128) + x0)), tmp11, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp9, tmp10, tmp14)
    tmp16 = tl.where(tmp4, tmp5, tmp15)
    tl.store(out_ptr0 + (x2), tmp16, None)


def get_args():
    arg_0 = rand_strided((128, 64, 56, 56), (200704, 1, 3584, 64), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 64, 56, 56), (200704, 1, 3584, 64), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 64, 56, 56), (200704, 1, 3584, 64), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 77070336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_23.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_23.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.308281344
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qu/cqua5wkdaqmsbcqlmunawiagqednzppjgskjmmy7lei6wsd5kcqh.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_13 => convert_element_type_17, var_mean_4
# Graph fragment:
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=cat_1]
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %var_mean_4 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_17, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf68,%buf69,%buf70
triton_red_fused__native_batch_norm_legit_functional_24 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_24', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_24', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 156499968, 'r0_': 0}, 'kernel_num_gb': 0.15532032, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_24(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 98304
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 150528*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 98304, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_24.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_24.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.15532032
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/66/c66pblcswsk7ssirjhjechnroedwanweq76pbd65kf7hrkk6oqdf.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_13 => convert_element_type_17, var_mean_4
# Graph fragment:
#   %buf68 : Tensor "f32[1, 192, 1, 1, 512][98304, 1, 98304, 98304, 192]cuda:0" = PlaceHolder[target=buf68]
#   %buf69 : Tensor "f32[1, 192, 1, 1, 512][98304, 1, 98304, 98304, 192]cuda:0" = PlaceHolder[target=buf69]
#   %buf70 : Tensor "f32[1, 192, 1, 1, 512][98304, 1, 98304, 98304, 192]cuda:0" = PlaceHolder[target=buf70]
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %var_mean_4 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_17, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf71,%buf72,%buf73
triton_red_fused__native_batch_norm_legit_functional_25 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_25', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_25', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1198080, 'r0_': 0}, 'kernel_num_gb': 0.001188864, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_25(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 768
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_2 + 24576*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 192*r0_2 + 24576*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 192*r0_2 + 24576*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 768, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_25.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_25.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001188864
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kk/ckkdmkg6xkbek5ms2syuijuqxzcjt2r5ebp5pyktib2rnvxxa4vj.py
# Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_13 => add_22, add_23, add_24, convert_element_type_17, mul_29, mul_30, mul_31, mul_32, mul_33, rsqrt_4, squeeze_12, squeeze_14, var_mean_4
# Graph fragment:
#   %buf71 : Tensor "f32[1, 192, 1, 1, 4][768, 1, 768, 768, 192]cuda:0" = PlaceHolder[target=buf71]
#   %buf72 : Tensor "f32[1, 192, 1, 1, 4][768, 1, 768, 768, 192]cuda:0" = PlaceHolder[target=buf72]
#   %buf73 : Tensor "f32[1, 192, 1, 1, 4][768, 1, 768, 768, 192]cuda:0" = PlaceHolder[target=buf73]
#   %buf75 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=buf75]
#   %getitem_23 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_23]
#   %copy__13 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=copy__13]
#   %add_23 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=add_23]
#   %copy__14 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=copy__14]
#   %add_24 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=add_24]
#   %convert_element_type_17 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_1, torch.float32), kwargs = {})
#   %var_mean_4 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_17, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_22 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_22, 1e-05), kwargs = {})
#   %rsqrt_4 : Tensor "f32[1, 192, 1, 1][192, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_22,), kwargs = {})
#   %squeeze_12 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_23, [0, 2, 3]), kwargs = {})
#   %mul_29 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_12, 0.1), kwargs = {})
#   %mul_30 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_31, 0.9), kwargs = {})
#   %add_23 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_29, %mul_30), kwargs = {})
#   %squeeze_14 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_22, [0, 2, 3]), kwargs = {})
#   %mul_31 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_14, 1.0000024912370735), kwargs = {})
#   %mul_32 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_31, 0.1), kwargs = {})
#   %mul_33 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_32, 0.9), kwargs = {})
#   %add_24 : Tensor "f32[192][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_32, %mul_33), kwargs = {})
#   %copy__13 : Tensor "f32[192][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_31, %add_23), kwargs = {})
#   %copy__14 : Tensor "f32[192][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_32, %add_24), kwargs = {})
#   return %getitem_23,%buf75,%rsqrt_4,%add_23,%buf1229,%add_24,%buf1232
triton_per_fused__native_batch_norm_legit_functional_copy__26 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__26', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 4},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__26', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16896, 'r0_': 0}, 'kernel_num_gb': 1.3824e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__26(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 192
    r0_numel = 4
    R0_BLOCK: tl.constexpr = 4
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 192*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 192*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 192*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 401408.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000024912370735
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 192, 4,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__26.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__26.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.3824e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xg/cxgxzz5txadrftufmn5kfpp2dcrls7xgto7dckzgxojdlruyaczq.py
# Topologically Sorted Source Nodes: [conv2d_8], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_8 => convert_element_type_19
# Graph fragment:
#   %primals_35 : Tensor "f32[20, 96, 1, 1][96, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_35]
#   %convert_element_type_19 : Tensor "f16[20, 96, 1, 1][96, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_35, torch.float16), kwargs = {})
#   return %convert_element_type_19
triton_poi_fused__to_copy_27 = async_compile.triton('triton_poi_fused__to_copy_27', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_27', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15360}, 'kernel_num_gb': 1.152e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_27(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1920
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((20, 96, 1, 1), (96, 1, 96, 96), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1920,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_27.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_27.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.152e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lr/clrwpt3or7337m6rsppph4yznxv4liddbq6cpwjjqpv4l6hqizji.py
# Topologically Sorted Source Nodes: [x_13, x_14], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_13 => add_25, convert_element_type_18, mul_28, mul_34, sub_4, unsqueeze_16, unsqueeze_17, unsqueeze_18, unsqueeze_19
#   x_14 => relu_3
# Graph fragment:
#   %cat_1 : Tensor "f16[128, 192, 56, 56][602112, 1, 10752, 192]cuda:0" = PlaceHolder[target=cat_1]
#   %getitem_23 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=getitem_23]
#   %rsqrt_4 : Tensor "f32[1, 192, 1, 1][192, 1, 192, 192]cuda:0" = PlaceHolder[target=rsqrt_4]
#   %primals_33 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_33]
#   %primals_34 : Tensor "f32[192][1]cuda:0" = PlaceHolder[target=primals_34]
#   %sub_4 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_1, %getitem_23), kwargs = {})
#   %mul_28 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_4, %rsqrt_4), kwargs = {})
#   %unsqueeze_16 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_33, -1), kwargs = {})
#   %unsqueeze_17 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_16, -1), kwargs = {})
#   %mul_34 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_28, %unsqueeze_17), kwargs = {})
#   %unsqueeze_18 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_34, -1), kwargs = {})
#   %unsqueeze_19 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_18, -1), kwargs = {})
#   %add_25 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_34, %unsqueeze_19), kwargs = {})
#   %convert_element_type_18 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_25, torch.float16), kwargs = {})
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_18,), kwargs = {})
#   return %relu_3
triton_poi_fused__native_batch_norm_legit_functional_relu_28 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_relu_28', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 4096}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_relu_28', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 154143744, 'x': 308281344}, 'kernel_num_gb': 0.308284416, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_relu_28(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 24576
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 192)
    y1 = yindex // 192
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 192*x2 + 602112*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (y0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1, 1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tl.store(out_ptr0 + (x2 + 3136*y3), tmp12, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 1, 10752, 192), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 192, 1, 1), (192, 1, 192, 192), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((192,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 192, 56, 56), (602112, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 24576, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_relu_28.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_relu_28.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.308284416
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2c/c2c4b4uuvp5taikdpwyau326v2nh7ip5cv77zflyahdjfjy7js3k.py
# Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_8 => convolution_8, split_with_sizes_6
#   x_13 => add_25, convert_element_type_18, mul_28, mul_34, sub_4, unsqueeze_16, unsqueeze_17, unsqueeze_18, unsqueeze_19
#   x_14 => relu_3
# Graph fragment:
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0" = PlaceHolder[target=relu_3]
#   %sub_4 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_1, %getitem_23), kwargs = {})
#   %mul_28 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_4, %rsqrt_4), kwargs = {})
#   %unsqueeze_16 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_33, -1), kwargs = {})
#   %unsqueeze_17 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_16, -1), kwargs = {})
#   %mul_34 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_28, %unsqueeze_17), kwargs = {})
#   %unsqueeze_18 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_34, -1), kwargs = {})
#   %unsqueeze_19 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_18, -1), kwargs = {})
#   %add_25 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_34, %unsqueeze_19), kwargs = {})
#   %convert_element_type_18 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_25, torch.float16), kwargs = {})
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_18,), kwargs = {})
#   %split_with_sizes_6 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_3, [96, 96], 1), kwargs = {})
#   %convolution_8 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_26, %convert_element_type_19, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf80
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 154140672, 'x': 77070336}, 'kernel_num_gb': 0.154140672, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 12288
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 96)
    y1 = yindex // 96
    tmp0 = tl.load(in_ptr0 + (x2 + 3136*y0 + 602112*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 96*x2 + 301056*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 96, 56, 56), (301056, 1, 5376, 96), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 12288, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.154140672
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/be/cbe3ljrknuukcj7jxxcu4k7syctyr5jbdfovklvhsekbzxifod4r.py
# Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8, conv2d_9], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_8 => split_with_sizes_6
#   conv2d_9 => convolution_9
#   x_13 => add_25, convert_element_type_18, mul_28, mul_34, sub_4, unsqueeze_16, unsqueeze_17, unsqueeze_18, unsqueeze_19
#   x_14 => relu_3
# Graph fragment:
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0" = PlaceHolder[target=relu_3]
#   %sub_4 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_1, %getitem_23), kwargs = {})
#   %mul_28 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_4, %rsqrt_4), kwargs = {})
#   %unsqueeze_16 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_33, -1), kwargs = {})
#   %unsqueeze_17 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_16, -1), kwargs = {})
#   %mul_34 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_28, %unsqueeze_17), kwargs = {})
#   %unsqueeze_18 : Tensor "f32[192, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_34, -1), kwargs = {})
#   %unsqueeze_19 : Tensor "f32[192, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_18, -1), kwargs = {})
#   %add_25 : Tensor "f32[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_34, %unsqueeze_19), kwargs = {})
#   %convert_element_type_18 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_25, torch.float16), kwargs = {})
#   %relu_3 : Tensor "f16[128, 192, 56, 56][602112, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_18,), kwargs = {})
#   %split_with_sizes_6 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_3, [96, 96], 1), kwargs = {})
#   %convolution_9 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_29, %convert_element_type_20, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf83
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 154140672, 'x': 77070336}, 'kernel_num_gb': 0.154140672, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 12288
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 96)
    y1 = yindex // 96
    tmp0 = tl.load(in_ptr0 + (301056 + x2 + 3136*y0 + 602112*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 96*x2 + 301056*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 192, 56, 56), (602112, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 96, 56, 56), (301056, 1, 5376, 96), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 12288, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.154140672
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bg/cbgumeltnkl4cwnorvv7tcx4osfh3yaovmzfy6xwhrl77ctrjjmo.py
# Topologically Sorted Source Nodes: [x_15], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_15 => cat_2
# Graph fragment:
#   %convolution_8 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=convolution_8]
#   %convolution_9 : Tensor "f16[128, 20, 56, 56][62720, 1, 1120, 20]cuda:0" = PlaceHolder[target=convolution_9]
#   %cat_2 : Tensor "f16[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_8, %convolution_9], 1), kwargs = {})
#   return %cat_2
triton_poi_fused_cat_31 = async_compile.triton('triton_poi_fused_cat_31', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_31', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 128450560}, 'kernel_num_gb': 0.06422528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_31(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16056320
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 40)
    x1 = xindex // 40
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 20, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (20*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 40, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (20*x1 + ((-20) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 16056320,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_31.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_31.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06422528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kb/ckbtm6xj7ev66mtfxib4wkxjwfy2bz2rb67croghxzbq72clqz75.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_16 => convert_element_type_21, var_mean_5
# Graph fragment:
#   %cat_2 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_2]
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %var_mean_5 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_21, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf86,%buf87,%buf88
triton_red_fused__native_batch_norm_legit_functional_32 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_32', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_32', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33095680, 'r0_': 0}, 'kernel_num_gb': 0.03260416, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_32(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 40960
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 40)
    x1 = xindex // 40
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_2 + 15680*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 40960, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_32.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_32.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03260416
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kq/ckqyy3ed5uyn4uj3dau372dtw52dgo64nm3kbe5x45bq3zl2ziuh.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_16 => convert_element_type_21, var_mean_5
# Graph fragment:
#   %buf86 : Tensor "f32[1, 40, 1, 1, 1024][40960, 1, 40960, 40960, 40]cuda:0" = PlaceHolder[target=buf86]
#   %buf87 : Tensor "f32[1, 40, 1, 1, 1024][40960, 1, 40960, 40960, 40]cuda:0" = PlaceHolder[target=buf87]
#   %buf88 : Tensor "f32[1, 40, 1, 1, 1024][40960, 1, 40960, 40960, 40]cuda:0" = PlaceHolder[target=buf88]
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %var_mean_5 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_21, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf89,%buf90,%buf91
triton_red_fused__native_batch_norm_legit_functional_33 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_33', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_33', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 499200, 'r0_': 0}, 'kernel_num_gb': 0.00049536, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_33(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 320
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 40)
    x1 = xindex // 40
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_2 + 5120*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 40*r0_2 + 5120*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 40*r0_2 + 5120*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 320, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_33.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_33.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00049536
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/a6/ca6rvnhgrutypdx4dc3zxgfreq2hftvyjhlpf6bde3n2dlswe7fb.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_16 => add_27, add_28, add_29, convert_element_type_21, mul_36, mul_37, mul_38, mul_39, mul_40, rsqrt_5, squeeze_15, squeeze_17, var_mean_5
# Graph fragment:
#   %buf89 : Tensor "f32[1, 40, 1, 1, 8][320, 1, 320, 320, 40]cuda:0" = PlaceHolder[target=buf89]
#   %buf90 : Tensor "f32[1, 40, 1, 1, 8][320, 1, 320, 320, 40]cuda:0" = PlaceHolder[target=buf90]
#   %buf91 : Tensor "f32[1, 40, 1, 1, 8][320, 1, 320, 320, 40]cuda:0" = PlaceHolder[target=buf91]
#   %buf93 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=buf93]
#   %getitem_31 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=getitem_31]
#   %copy__16 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=copy__16]
#   %add_28 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=add_28]
#   %copy__17 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=copy__17]
#   %add_29 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=add_29]
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %var_mean_5 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_21, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_27 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_30, 1e-05), kwargs = {})
#   %rsqrt_5 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_27,), kwargs = {})
#   %squeeze_15 : Tensor "f32[40][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_31, [0, 2, 3]), kwargs = {})
#   %mul_36 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_15, 0.1), kwargs = {})
#   %mul_37 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_38, 0.9), kwargs = {})
#   %add_28 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_36, %mul_37), kwargs = {})
#   %squeeze_17 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_30, [0, 2, 3]), kwargs = {})
#   %mul_38 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_17, 1.0000024912370735), kwargs = {})
#   %mul_39 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_38, 0.1), kwargs = {})
#   %mul_40 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_39, 0.9), kwargs = {})
#   %add_29 : Tensor "f32[40][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_39, %mul_40), kwargs = {})
#   %copy__16 : Tensor "f32[40][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_38, %add_28), kwargs = {})
#   %copy__17 : Tensor "f32[40][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_39, %add_29), kwargs = {})
#   return %getitem_31,%buf93,%rsqrt_5,%add_28,%buf1237,%add_29,%buf1240
triton_per_fused__native_batch_norm_legit_functional_copy__34 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__34', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 64, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__34', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5760, 'r0_': 0}, 'kernel_num_gb': 4.96e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__34(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 40
    r0_numel = 8
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 40*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 40*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 40*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 401408.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000024912370735
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 40, 8,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__34.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__34.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 4.96e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6e/c6e7c3u37caxqb6wjkpqxocodewser4fxcsfwhf2ld7hh5hgg6ad.py
# Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_16 => add_27, add_30, convert_element_type_21, convert_element_type_22, mul_35, mul_41, rsqrt_5, sub_5, unsqueeze_20, unsqueeze_21, unsqueeze_22, unsqueeze_23, var_mean_5
# Graph fragment:
#   %cat_2 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_2]
#   %getitem_31 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=getitem_31]
#   %buf93 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=buf93]
#   %primals_40 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_40]
#   %primals_41 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_41]
#   %convert_element_type_21 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_2, torch.float32), kwargs = {})
#   %var_mean_5 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_21, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_27 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_30, 1e-05), kwargs = {})
#   %rsqrt_5 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_27,), kwargs = {})
#   %sub_5 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_2, %getitem_31), kwargs = {})
#   %mul_35 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_5, %rsqrt_5), kwargs = {})
#   %unsqueeze_20 : Tensor "f32[40, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_40, -1), kwargs = {})
#   %unsqueeze_21 : Tensor "f32[40, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_20, -1), kwargs = {})
#   %mul_41 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_35, %unsqueeze_21), kwargs = {})
#   %unsqueeze_22 : Tensor "f32[40, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_41, -1), kwargs = {})
#   %unsqueeze_23 : Tensor "f32[40, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_22, -1), kwargs = {})
#   %add_30 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_41, %unsqueeze_23), kwargs = {})
#   %convert_element_type_22 : Tensor "f16[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_30, torch.float16), kwargs = {})
#   return %convert_element_type_22
triton_poi_fused__native_batch_norm_legit_functional_35 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_35', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_35', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96338560}, 'kernel_num_gb': 0.06422592, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_35(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16056320
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 40)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 401408.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 16056320,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_35.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_35.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.06422592
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bl/cbls5eiyqzapmgholcdcngljjhdaev4laeutfdklr3gaskbp45pg.py
# Topologically Sorted Source Nodes: [split_3, conv2d_10], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_10 => convolution_10
#   split_3 => getitem_32, split_with_sizes_8
# Graph fragment:
#   %convert_element_type_22 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=convert_element_type_22]
#   %getitem_32 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0" = PlaceHolder[target=getitem_32]
#   %split_with_sizes_8 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_22, [20, 20], 1), kwargs = {})
#   %getitem_32 : Tensor "f16[128, 20, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_8, 0), kwargs = {})
#   %convolution_10 : Tensor "f16[128, 60, 56, 56][188160, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_32, %convert_element_type_23, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_32,%buf100
triton_poi_fused_convolution_split_with_sizes_36 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_36', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 4096, 'x': 4096}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_36', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 32112640}, 'kernel_num_gb': 0.04816896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_36(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2560
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 20)
    y1 = yindex // 20
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 40*x2 + 125440*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 3136*y3), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 20*x2 + 62720*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 56, 56), (62720, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 2560, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_36.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_36.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/s2/cs2qhzisqizwbdglpeqpstcul2yyzalcnz6kkmdw67oipoetu6oh.py
# Topologically Sorted Source Nodes: [split_3, conv2d_11], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_11 => convolution_11
#   split_3 => getitem_33, split_with_sizes_8
# Graph fragment:
#   %convert_element_type_22 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=convert_element_type_22]
#   %getitem_33 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0" = PlaceHolder[target=getitem_33]
#   %split_with_sizes_8 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_22, [20, 20], 1), kwargs = {})
#   %getitem_33 : Tensor "f16[128, 20, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_8, 1), kwargs = {})
#   %convolution_11 : Tensor "f16[128, 60, 56, 56][188160, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_33, %convert_element_type_24, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_33,%buf103
triton_poi_fused_convolution_split_with_sizes_37 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_37', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 4096, 'x': 4096}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_37', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 32112640}, 'kernel_num_gb': 0.04816896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_37(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2560
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 20)
    y1 = yindex // 20
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (20 + y0 + 40*x2 + 125440*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 3136*y3), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 20*x2 + 62720*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 20, 56, 56), (62720, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 20, 56, 56), (62720, 1, 1120, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 2560, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_37.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_37.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4t/c4thp3zrdnc5w4tnz3cfkwmxham3cfzbqdoftbuwvmbstfhpz5jd.py
# Topologically Sorted Source Nodes: [conv2d_10], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_10 => convert_element_type_23
# Graph fragment:
#   %primals_42 : Tensor "f32[60, 20, 1, 1][20, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_42]
#   %convert_element_type_23 : Tensor "f16[60, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_42, torch.float16), kwargs = {})
#   return %convert_element_type_23
triton_poi_fused__to_copy_38 = async_compile.triton('triton_poi_fused__to_copy_38', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_38', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 9600}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_38(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((60, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_38.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_38.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/oy/coyvjuxdr5ka3im7yslbeuy2t2lw3e5m24cjjohbqr4aqbglnbww.py
# Topologically Sorted Source Nodes: [x_17], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_17 => cat_3
# Graph fragment:
#   %convolution_10 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=convolution_10]
#   %convolution_11 : Tensor "f16[128, 60, 56, 56][188160, 1, 3360, 60]cuda:0" = PlaceHolder[target=convolution_11]
#   %cat_3 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_10, %convolution_11], 1), kwargs = {})
#   return %cat_3
triton_poi_fused_cat_39 = async_compile.triton('triton_poi_fused_cat_39', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_39', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 385351680}, 'kernel_num_gb': 0.19267584, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_39(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 48168960
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 120)
    x1 = xindex // 120
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 60, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (60*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 120, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (60*x1 + ((-60) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 48168960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_39.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_39.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.19267584
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fv/cfvgfnvulwbzlpcxosuseqnsxmiulxbjw3trzscyqwqsganbe3ia.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_18 => convert_element_type_25, var_mean_6
# Graph fragment:
#   %cat_3 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=cat_3]
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %var_mean_6 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_25, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf106,%buf107,%buf108
triton_red_fused__native_batch_norm_legit_functional_40 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_40', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 512},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_40', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 99287040, 'r0_': 0}, 'kernel_num_gb': 0.09781248, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_40(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 392
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 120)
    x1 = xindex // 120
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122880, 392,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_40.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_40.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09781248
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xb/cxbdyy6vcdv72uwkmgviwmpratnuxhcbxijlzb5t7tpln33wp5rh.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_18 => convert_element_type_25, var_mean_6
# Graph fragment:
#   %buf106 : Tensor "f32[1, 120, 1, 1, 1024][122880, 1, 122880, 122880, 120]cuda:0" = PlaceHolder[target=buf106]
#   %buf107 : Tensor "f32[1, 120, 1, 1, 1024][122880, 1, 122880, 122880, 120]cuda:0" = PlaceHolder[target=buf107]
#   %buf108 : Tensor "f32[1, 120, 1, 1, 1024][122880, 1, 122880, 122880, 120]cuda:0" = PlaceHolder[target=buf108]
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %var_mean_6 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_25, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf109,%buf110,%buf111
triton_red_fused__native_batch_norm_legit_functional_41 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_41', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_41', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1497600, 'r0_': 0}, 'kernel_num_gb': 0.00148608, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_41(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 120)
    x1 = xindex // 120
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_2 + 15360*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 120*r0_2 + 15360*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 120*r0_2 + 15360*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 960, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_41.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_41.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00148608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ty/ctymcfqqbc6h7bxamn6ij7coqz7isrnxpcnvtiiljayi7nbadcjc.py
# Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_18 => add_32, add_33, add_34, convert_element_type_25, mul_43, mul_44, mul_45, mul_46, mul_47, rsqrt_6, squeeze_18, squeeze_20, var_mean_6
# Graph fragment:
#   %buf109 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf109]
#   %buf110 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf110]
#   %buf111 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf111]
#   %buf113 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=buf113]
#   %getitem_35 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_35]
#   %copy__19 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=copy__19]
#   %add_33 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=add_33]
#   %copy__20 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=copy__20]
#   %add_34 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=add_34]
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %var_mean_6 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_25, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_32 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_34, 1e-05), kwargs = {})
#   %rsqrt_6 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_32,), kwargs = {})
#   %squeeze_18 : Tensor "f32[120][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_35, [0, 2, 3]), kwargs = {})
#   %mul_43 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_18, 0.1), kwargs = {})
#   %mul_44 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_45, 0.9), kwargs = {})
#   %add_33 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_43, %mul_44), kwargs = {})
#   %squeeze_20 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_34, [0, 2, 3]), kwargs = {})
#   %mul_45 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_20, 1.0000024912370735), kwargs = {})
#   %mul_46 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_45, 0.1), kwargs = {})
#   %mul_47 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_46, 0.9), kwargs = {})
#   %add_34 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_46, %mul_47), kwargs = {})
#   %copy__19 : Tensor "f32[120][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_45, %add_33), kwargs = {})
#   %copy__20 : Tensor "f32[120][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_46, %add_34), kwargs = {})
#   return %getitem_35,%buf113,%rsqrt_6,%add_33,%buf1245,%add_34,%buf1248
triton_per_fused__native_batch_norm_legit_functional_copy__42 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__42', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 128, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__42', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 17280, 'r0_': 0}, 'kernel_num_gb': 1.488e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__42(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 120
    r0_numel = 8
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 401408.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000024912370735
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 120, 8,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__42.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__42.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.488e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/v2/cv2zvx6oqffa66ko4vsn5jwl4vyw6qcyqg76l3p32j5bwxjxbamb.py
# Topologically Sorted Source Nodes: [x_18, x_19], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_18 => add_32, add_35, convert_element_type_25, convert_element_type_26, mul_42, mul_48, rsqrt_6, sub_6, unsqueeze_24, unsqueeze_25, unsqueeze_26, unsqueeze_27, var_mean_6
#   x_19 => relu_4
# Graph fragment:
#   %cat_3 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=cat_3]
#   %getitem_35 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_35]
#   %buf113 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=buf113]
#   %primals_47 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_47]
#   %primals_48 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_48]
#   %convert_element_type_25 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_3, torch.float32), kwargs = {})
#   %var_mean_6 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_25, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_32 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_34, 1e-05), kwargs = {})
#   %rsqrt_6 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_32,), kwargs = {})
#   %sub_6 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_3, %getitem_35), kwargs = {})
#   %mul_42 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_6, %rsqrt_6), kwargs = {})
#   %unsqueeze_24 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_47, -1), kwargs = {})
#   %unsqueeze_25 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_24, -1), kwargs = {})
#   %mul_48 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_42, %unsqueeze_25), kwargs = {})
#   %unsqueeze_26 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_48, -1), kwargs = {})
#   %unsqueeze_27 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_26, -1), kwargs = {})
#   %add_35 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_48, %unsqueeze_27), kwargs = {})
#   %convert_element_type_26 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_35, torch.float16), kwargs = {})
#   %relu_4 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_26,), kwargs = {})
#   return %relu_4
triton_poi_fused__native_batch_norm_legit_functional_relu_43 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_relu_43', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_relu_43', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 289015680}, 'kernel_num_gb': 0.19267776, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_relu_43(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 48168960
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 120)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 401408.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp16 = tl.full([1], 0, tl.int32)
    tmp17 = triton_helpers.maximum(tmp16, tmp15)
    tl.store(out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 48168960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_relu_43.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_relu_43.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.19267776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/n6/cn6e6bhkj3bicofudsleo62egogs2skyir63d25bjwpigueqce7p.py
# Topologically Sorted Source Nodes: [x_20], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_20 => convert_element_type_27
# Graph fragment:
#   %primals_49 : Tensor "f32[120, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_49]
#   %convert_element_type_27 : Tensor "f16[120, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_49, torch.float16), kwargs = {})
#   return %convert_element_type_27
triton_poi_fused__to_copy_44 = async_compile.triton('triton_poi_fused__to_copy_44', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_44', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8640}, 'kernel_num_gb': 6.48e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_44(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_44.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_44.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.48e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/tg/ctgvcsu5bhucw7qzgyxjz4px3g5n4hlzhkwea2ja7pkmbrszmjtv.py
# Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_21 => add_37, add_38, add_39, convert_element_type_28, mul_50, mul_51, mul_52, mul_53, mul_54, rsqrt_7, squeeze_21, squeeze_23, var_mean_7
# Graph fragment:
#   %buf122 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf122]
#   %buf123 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf123]
#   %buf124 : Tensor "f32[1, 120, 1, 1, 8][960, 1, 960, 960, 120]cuda:0" = PlaceHolder[target=buf124]
#   %buf126 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=buf126]
#   %getitem_37 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_37]
#   %copy__22 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=copy__22]
#   %add_38 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=add_38]
#   %copy__23 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=copy__23]
#   %add_39 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=add_39]
#   %convert_element_type_28 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_12, torch.float32), kwargs = {})
#   %var_mean_7 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_28, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_37 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_36, 1e-05), kwargs = {})
#   %rsqrt_7 : Tensor "f32[1, 120, 1, 1][120, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_37,), kwargs = {})
#   %squeeze_21 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_37, [0, 2, 3]), kwargs = {})
#   %mul_50 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_21, 0.1), kwargs = {})
#   %mul_51 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_51, 0.9), kwargs = {})
#   %add_38 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_50, %mul_51), kwargs = {})
#   %squeeze_23 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_36, [0, 2, 3]), kwargs = {})
#   %mul_52 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_23, 1.0000024912370735), kwargs = {})
#   %mul_53 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_52, 0.1), kwargs = {})
#   %mul_54 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_52, 0.9), kwargs = {})
#   %add_39 : Tensor "f32[120][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_53, %mul_54), kwargs = {})
#   %copy__22 : Tensor "f32[120][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_51, %add_38), kwargs = {})
#   %copy__23 : Tensor "f32[120][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_52, %add_39), kwargs = {})
#   return %getitem_37,%buf126,%rsqrt_7,%add_38,%buf1253,%add_39,%buf1256
triton_per_fused__native_batch_norm_legit_functional_copy__45 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__45', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 128, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__45', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 16320, 'r0_': 0}, 'kernel_num_gb': 1.44e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__45(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 120
    r0_numel = 8
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 120*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 401408.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000024912370735
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 120, 8,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__45.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__45.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.44e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qt/cqtxsxr7srrfxp2d7bhuztomvk3esmtkwwoa4qfb22i5kzqydytz.py
# Topologically Sorted Source Nodes: [conv2d_13], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_13 => convert_element_type_30
# Graph fragment:
#   %primals_55 : Tensor "f32[20, 60, 1, 1][60, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_55]
#   %convert_element_type_30 : Tensor "f16[20, 60, 1, 1][60, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_55, torch.float16), kwargs = {})
#   return %convert_element_type_30
triton_poi_fused__to_copy_46 = async_compile.triton('triton_poi_fused__to_copy_46', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_46', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 9600}, 'kernel_num_gb': 7.2e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_46(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 60, 1, 1), (60, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((20, 60, 1, 1), (60, 1, 60, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_46.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_46.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.2e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ef/cefg6xaqbtfoiszpjaywojjkmqk6cqidc5vor4tv5ssrnqlbcfvo.py
# Topologically Sorted Source Nodes: [x_21, x_22], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
# Source node to ATen node mapping:
#   x_21 => add_40, convert_element_type_29, mul_49, mul_55, sub_7, unsqueeze_28, unsqueeze_29, unsqueeze_30, unsqueeze_31
#   x_22 => relu_5
# Graph fragment:
#   %convolution_12 : Tensor "f16[128, 120, 56, 56][376320, 1, 6720, 120]cuda:0" = PlaceHolder[target=convolution_12]
#   %getitem_37 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=getitem_37]
#   %rsqrt_7 : Tensor "f32[1, 120, 1, 1][120, 1, 120, 120]cuda:0" = PlaceHolder[target=rsqrt_7]
#   %primals_53 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_53]
#   %primals_54 : Tensor "f32[120][1]cuda:0" = PlaceHolder[target=primals_54]
#   %sub_7 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_12, %getitem_37), kwargs = {})
#   %mul_49 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_7, %rsqrt_7), kwargs = {})
#   %unsqueeze_28 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_53, -1), kwargs = {})
#   %unsqueeze_29 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_28, -1), kwargs = {})
#   %mul_55 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_49, %unsqueeze_29), kwargs = {})
#   %unsqueeze_30 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_54, -1), kwargs = {})
#   %unsqueeze_31 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_30, -1), kwargs = {})
#   %add_40 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_55, %unsqueeze_31), kwargs = {})
#   %convert_element_type_29 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_40, torch.float16), kwargs = {})
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_29,), kwargs = {})
#   return %relu_5
triton_poi_fused__native_batch_norm_legit_functional_relu_47 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_relu_47', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 4096}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_relu_47', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96339840, 'x': 192675840}, 'kernel_num_gb': 0.19267776, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_relu_47(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 120*x2 + 376320*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (y0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (y0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (y0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1, 1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tl.store(out_ptr0 + (x2 + 3136*y3), tmp12, xmask)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 1, 6720, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 120, 1, 1), (120, 1, 120, 120), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((120,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 120, 56, 56), (376320, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 15360, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_relu_47.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_relu_47.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.19267776
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2v/c2vnz733j2soqahmy6i5dviuga5etqsmns62gwcolhnyzffvbqbz.py
# Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_13 => convolution_13, split_with_sizes_10
#   x_21 => add_40, convert_element_type_29, mul_49, mul_55, sub_7, unsqueeze_28, unsqueeze_29, unsqueeze_30, unsqueeze_31
#   x_22 => relu_5
# Graph fragment:
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0" = PlaceHolder[target=relu_5]
#   %sub_7 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_12, %getitem_37), kwargs = {})
#   %mul_49 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_7, %rsqrt_7), kwargs = {})
#   %unsqueeze_28 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_53, -1), kwargs = {})
#   %unsqueeze_29 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_28, -1), kwargs = {})
#   %mul_55 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_49, %unsqueeze_29), kwargs = {})
#   %unsqueeze_30 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_54, -1), kwargs = {})
#   %unsqueeze_31 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_30, -1), kwargs = {})
#   %add_40 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_55, %unsqueeze_31), kwargs = {})
#   %convert_element_type_29 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_40, torch.float16), kwargs = {})
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_29,), kwargs = {})
#   %split_with_sizes_10 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_5, [60, 60], 1), kwargs = {})
#   %convolution_13 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_40, %convert_element_type_30, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf131
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (x2 + 3136*y0 + 376320*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dc/cdcpecvja6rfvr6nya2i62upevxlp533uvdkhzid6fhaxvy2meig.py
# Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13, conv2d_14], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_13 => split_with_sizes_10
#   conv2d_14 => convolution_14
#   x_21 => add_40, convert_element_type_29, mul_49, mul_55, sub_7, unsqueeze_28, unsqueeze_29, unsqueeze_30, unsqueeze_31
#   x_22 => relu_5
# Graph fragment:
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0" = PlaceHolder[target=relu_5]
#   %sub_7 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_12, %getitem_37), kwargs = {})
#   %mul_49 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_7, %rsqrt_7), kwargs = {})
#   %unsqueeze_28 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_53, -1), kwargs = {})
#   %unsqueeze_29 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_28, -1), kwargs = {})
#   %mul_55 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_49, %unsqueeze_29), kwargs = {})
#   %unsqueeze_30 : Tensor "f32[120, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_54, -1), kwargs = {})
#   %unsqueeze_31 : Tensor "f32[120, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_30, -1), kwargs = {})
#   %add_40 : Tensor "f32[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_55, %unsqueeze_31), kwargs = {})
#   %convert_element_type_29 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_40, torch.float16), kwargs = {})
#   %relu_5 : Tensor "f16[128, 120, 56, 56][376320, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_29,), kwargs = {})
#   %split_with_sizes_10 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%relu_5, [60, 60], 1), kwargs = {})
#   %convolution_14 : Tensor "f16[128, 20, 56, 56][62720, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_43, %convert_element_type_31, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf134
triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (188160 + x2 + 3136*y0 + 376320*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 120, 56, 56), (376320, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bd/cbdn2vy4errsi52yufdyezugc4cj2vpiezthbnud53ta6fi6nl2v.py
# Topologically Sorted Source Nodes: [x_24, x_25], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_24 => add_42, add_45, convert_element_type_32, convert_element_type_33, mul_56, mul_62, rsqrt_8, sub_8, unsqueeze_32, unsqueeze_33, unsqueeze_34, unsqueeze_35, var_mean_8
#   x_25 => add_46
# Graph fragment:
#   %cat_4 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=cat_4]
#   %getitem_45 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=getitem_45]
#   %buf144 : Tensor "f32[1, 40, 1, 1][40, 1, 40, 40]cuda:0" = PlaceHolder[target=buf144]
#   %primals_60 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_60]
#   %primals_61 : Tensor "f32[40][1]cuda:0" = PlaceHolder[target=primals_61]
#   %convert_element_type_22 : Tensor "f16[128, 40, 56, 56][125440, 1, 2240, 40]cuda:0" = PlaceHolder[target=convert_element_type_22]
#   %convert_element_type_32 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_4, torch.float32), kwargs = {})
#   %var_mean_8 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_32, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_42 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_44, 1e-05), kwargs = {})
#   %rsqrt_8 : Tensor "f32[1, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_42,), kwargs = {})
#   %sub_8 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_4, %getitem_45), kwargs = {})
#   %mul_56 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_8, %rsqrt_8), kwargs = {})
#   %unsqueeze_32 : Tensor "f32[40, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_60, -1), kwargs = {})
#   %unsqueeze_33 : Tensor "f32[40, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_32, -1), kwargs = {})
#   %mul_62 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_56, %unsqueeze_33), kwargs = {})
#   %unsqueeze_34 : Tensor "f32[40, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_61, -1), kwargs = {})
#   %unsqueeze_35 : Tensor "f32[40, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_34, -1), kwargs = {})
#   %add_45 : Tensor "f32[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_62, %unsqueeze_35), kwargs = {})
#   %convert_element_type_33 : Tensor "f16[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_45, torch.float16), kwargs = {})
#   %add_46 : Tensor "f16[128, 40, 56, 56][125440, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_33, %convert_element_type_22), kwargs = {})
#   return %add_46
triton_poi_fused__native_batch_norm_legit_functional_add_50 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_50', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_50', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 128451200}, 'kernel_num_gb': 0.09633856, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_50(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16056320
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 40)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 401408.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(in_out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 40, 56, 56), (125440, 1, 2240, 40), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((40,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 16056320,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_50.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_50.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633856
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wl/cwlwyohlvh64rd4ucsd57n3fvjy6gmqlxx4aqagpnmlghl4climo.py
# Topologically Sorted Source Nodes: [x_26], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_26 => convert_element_type_34
# Graph fragment:
#   %primals_62 : Tensor "f32[240, 40, 1, 1][40, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_62]
#   %convert_element_type_34 : Tensor "f16[240, 40, 1, 1][40, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_62, torch.float16), kwargs = {})
#   return %convert_element_type_34
triton_poi_fused__to_copy_51 = async_compile.triton('triton_poi_fused__to_copy_51', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_51', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 76800}, 'kernel_num_gb': 5.76e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_51(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 40, 1, 1), (40, 1, 40, 40), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 9600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_51.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_51.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.76e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/u2/cu27w5xcurxzcpbbrsunsutflvgwvwh6c3qv3kkxqxhivxxgiaao.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_27 => convert_element_type_35, var_mean_9
# Graph fragment:
#   %convolution_15 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convolution_15]
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %var_mean_9 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_35, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf150,%buf151,%buf152
triton_red_fused__native_batch_norm_legit_functional_52 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_52', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 1024},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_52', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 195624960, 'r0_': 0}, 'kernel_num_gb': 0.1941504, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_52(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 188160*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122880, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_52.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_52.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.1941504
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ok/cokoixiot2642icu4q7ukg6czolhz6dxjf2aawqtvjwezid7ld3r.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_27 => convert_element_type_35, var_mean_9
# Graph fragment:
#   %buf150 : Tensor "f32[1, 240, 1, 1, 512][122880, 1, 122880, 122880, 240]cuda:0" = PlaceHolder[target=buf150]
#   %buf151 : Tensor "f32[1, 240, 1, 1, 512][122880, 1, 122880, 122880, 240]cuda:0" = PlaceHolder[target=buf151]
#   %buf152 : Tensor "f32[1, 240, 1, 1, 512][122880, 1, 122880, 122880, 240]cuda:0" = PlaceHolder[target=buf152]
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %var_mean_9 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_35, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf153,%buf154,%buf155
triton_red_fused__native_batch_norm_legit_functional_53 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_53', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_53', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1497600, 'r0_': 0}, 'kernel_num_gb': 0.00148608, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_53(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 30720*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 240*r0_2 + 30720*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 240*r0_2 + 30720*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 960, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_53.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_53.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00148608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/k6/ck6fmhybtglnqxxhz6szafdujptmznc5x4ldbvvlelvbb6rthsmh.py
# Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_27 => add_48, add_49, add_50, convert_element_type_35, mul_64, mul_65, mul_66, mul_67, mul_68, rsqrt_9, squeeze_27, squeeze_29, var_mean_9
# Graph fragment:
#   %buf153 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf153]
#   %buf154 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf154]
#   %buf155 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf155]
#   %buf157 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=buf157]
#   %getitem_47 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_47]
#   %copy__28 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=copy__28]
#   %add_49 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=add_49]
#   %copy__29 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=copy__29]
#   %add_50 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=add_50]
#   %convert_element_type_35 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_15, torch.float32), kwargs = {})
#   %var_mean_9 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_35, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_48 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_46, 1e-05), kwargs = {})
#   %rsqrt_9 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_48,), kwargs = {})
#   %squeeze_27 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_47, [0, 2, 3]), kwargs = {})
#   %mul_64 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_27, 0.1), kwargs = {})
#   %mul_65 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_64, 0.9), kwargs = {})
#   %add_49 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_64, %mul_65), kwargs = {})
#   %squeeze_29 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_46, [0, 2, 3]), kwargs = {})
#   %mul_66 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_29, 1.0000024912370735), kwargs = {})
#   %mul_67 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_66, 0.1), kwargs = {})
#   %mul_68 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_65, 0.9), kwargs = {})
#   %add_50 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_67, %mul_68), kwargs = {})
#   %copy__28 : Tensor "f32[240][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_64, %add_49), kwargs = {})
#   %copy__29 : Tensor "f32[240][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_65, %add_50), kwargs = {})
#   return %getitem_47,%buf157,%rsqrt_9,%add_49,%buf1269,%add_50,%buf1272
triton_per_fused__native_batch_norm_legit_functional_copy__54 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__54', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 4},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__54', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21120, 'r0_': 0}, 'kernel_num_gb': 1.728e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__54(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 4
    R0_BLOCK: tl.constexpr = 4
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 401408.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000024912370735
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 240, 4,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__54.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__54.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.728e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7t/c7t3miapqrt6dcyw3cyermo7uflcwnha5jmwq7dxcw7a7xs7ojav.py
# Topologically Sorted Source Nodes: [x_27, x_28], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_27 => add_51, convert_element_type_36, mul_63, mul_69, sub_9, unsqueeze_36, unsqueeze_37, unsqueeze_38, unsqueeze_39
#   x_28 => convert_element_type_37, convert_element_type_38, mul_70, sigmoid
# Graph fragment:
#   %convolution_15 : Tensor "f16[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convolution_15]
#   %getitem_47 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_47]
#   %rsqrt_9 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_9]
#   %primals_66 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_66]
#   %primals_67 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_67]
#   %convert_element_type_37 : Tensor "f32[128, 240, 56, 56][752640, 1, 13440, 240]cuda:0" = PlaceHolder[target=convert_element_type_37]
#   %sub_9 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_15, %getitem_47), kwargs = {})
#   %mul_63 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_9, %rsqrt_9), kwargs = {})
#   %unsqueeze_36 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_66, -1), kwargs = {})
#   %unsqueeze_37 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_36, -1), kwargs = {})
#   %mul_69 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_63, %unsqueeze_37), kwargs = {})
#   %unsqueeze_38 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_67, -1), kwargs = {})
#   %unsqueeze_39 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_38, -1), kwargs = {})
#   %add_51 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_69, %unsqueeze_39), kwargs = {})
#   %convert_element_type_36 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_51, torch.float16), kwargs = {})
#   %convert_element_type_37 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_36, torch.float32), kwargs = {})
#   %sigmoid : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_37,), kwargs = {})
#   %mul_70 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_37, %sigmoid), kwargs = {})
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_70, torch.float16), kwargs = {})
#   return %convert_element_type_37,%convert_element_type_38
triton_poi_fused__native_batch_norm_legit_functional_silu_55 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_55', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 524288, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_55', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 385351680, 'x': 192679680}, 'kernel_num_gb': 0.38535552, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_55(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 401408
    xnumel = 240
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 3136)
    y3 = yindex // 3136
    tmp0 = tl.load(in_ptr0 + (x1 + 240*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 3136*x1 + 752640*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 1, 13440, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 401408, 240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_55.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_55.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.38535552
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/js/cjsnopqg5vg753v6ebsnztfxxzcptn2bzzrxw64zve4hh3bg2ykd.py
# Topologically Sorted Source Nodes: [conv2d_16], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_16 => convert_element_type_39
# Graph fragment:
#   %primals_68 : Tensor "f32[60, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_68]
#   %convert_element_type_39 : Tensor "f16[60, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_68, torch.float16), kwargs = {})
#   return %convert_element_type_39
triton_poi_fused__to_copy_56 = async_compile.triton('triton_poi_fused__to_copy_56', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_56', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 4320}, 'kernel_num_gb': 3.24e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_56(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 540
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((60, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 540,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_56.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_56.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.24e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fn/cfnpb5i26whsjlia75n4y5xjequ5bxlgyjj2yh34td3ttzbjydb3.py
# Topologically Sorted Source Nodes: [x_28, conv2d_16], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_16 => convolution_16, split_with_sizes_13
#   x_28 => convert_element_type_38, mul_70, sigmoid
# Graph fragment:
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=convert_element_type_38]
#   %sigmoid : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_37,), kwargs = {})
#   %mul_70 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_37, %sigmoid), kwargs = {})
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_70, torch.float16), kwargs = {})
#   %split_with_sizes_13 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_38, [60, 60, 60, 60], 1), kwargs = {})
#   %convolution_16 : Tensor "f16[128, 60, 28, 28][47040, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_52, %convert_element_type_39, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 60), kwargs = {})
#   return %buf163
triton_poi_fused_convolution_silu_split_with_sizes_57 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_57', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_57', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_57(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (x2 + 3136*y0 + 752640*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_57.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_57.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b4/cb4o3shbsk63e56lgrtclbj2etbrjhdhcugwip2lynhy5kckegfb.py
# Topologically Sorted Source Nodes: [conv2d_17], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_17 => convert_element_type_40
# Graph fragment:
#   %primals_69 : Tensor "f32[60, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_69]
#   %convert_element_type_40 : Tensor "f16[60, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_69, torch.float16), kwargs = {})
#   return %convert_element_type_40
triton_poi_fused__to_copy_58 = async_compile.triton('triton_poi_fused__to_copy_58', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_58', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12000}, 'kernel_num_gb': 9e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_58(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1500
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((60, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1500,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_58.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_58.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ln/cln3fjqk7poufgj3bjq55tn7cxvbnhy2qmu23r46xmew63ytz7co.py
# Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_17], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_16 => split_with_sizes_13
#   conv2d_17 => convolution_17
#   x_28 => convert_element_type_38, mul_70, sigmoid
# Graph fragment:
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=convert_element_type_38]
#   %sigmoid : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_37,), kwargs = {})
#   %mul_70 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_37, %sigmoid), kwargs = {})
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_70, torch.float16), kwargs = {})
#   %split_with_sizes_13 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_38, [60, 60, 60, 60], 1), kwargs = {})
#   %convolution_17 : Tensor "f16[128, 60, 28, 28][47040, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_57, %convert_element_type_40, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 60), kwargs = {})
#   return %buf166
triton_poi_fused_convolution_silu_split_with_sizes_59 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_59', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_59', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_59(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (188160 + x2 + 3136*y0 + 752640*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_59.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_59.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fb/cfbgaudgxnepzhtlklwh32f2du3fkioevlqpeazcti4amp6v7hry.py
# Topologically Sorted Source Nodes: [conv2d_18], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_18 => convert_element_type_41
# Graph fragment:
#   %primals_70 : Tensor "f32[60, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_70]
#   %convert_element_type_41 : Tensor "f16[60, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_70, torch.float16), kwargs = {})
#   return %convert_element_type_41
triton_poi_fused__to_copy_60 = async_compile.triton('triton_poi_fused__to_copy_60', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_60', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 23520}, 'kernel_num_gb': 1.764e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_60(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2940
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((60, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2940,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_60.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_60.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.764e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/st/cstzf2xxyamfastpvs6bfbaxnzlabmytbsuz5dabpeuurfxcceao.py
# Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_18], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_16 => split_with_sizes_13
#   conv2d_18 => convolution_18
#   x_28 => convert_element_type_38, mul_70, sigmoid
# Graph fragment:
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=convert_element_type_38]
#   %sigmoid : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_37,), kwargs = {})
#   %mul_70 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_37, %sigmoid), kwargs = {})
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_70, torch.float16), kwargs = {})
#   %split_with_sizes_13 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_38, [60, 60, 60, 60], 1), kwargs = {})
#   %convolution_18 : Tensor "f16[128, 60, 28, 28][47040, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_62, %convert_element_type_41, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 60), kwargs = {})
#   return %buf169
triton_poi_fused_convolution_silu_split_with_sizes_61 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_61', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_61', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_61(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (376320 + x2 + 3136*y0 + 752640*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_61.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_61.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/da/cdatpdc7dtwlhdnvbnocsff4z32ihn7xaycczwbt4zi57w5efujm.py
# Topologically Sorted Source Nodes: [conv2d_19], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_19 => convert_element_type_42
# Graph fragment:
#   %primals_71 : Tensor "f32[60, 1, 9, 9][81, 81, 9, 1]cuda:0" = PlaceHolder[target=primals_71]
#   %convert_element_type_42 : Tensor "f16[60, 1, 9, 9][81, 81, 9, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_71, torch.float16), kwargs = {})
#   return %convert_element_type_42
triton_poi_fused__to_copy_62 = async_compile.triton('triton_poi_fused__to_copy_62', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_62', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 38880}, 'kernel_num_gb': 2.916e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_62(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4860
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((60, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((60, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4860,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_62.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_62.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.916e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yo/cyoighjsaot73wkjyvnkzdinetq22jrzlkhtxubvluyvwyz7eef3.py
# Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_19], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_16 => split_with_sizes_13
#   conv2d_19 => convolution_19
#   x_28 => convert_element_type_38, mul_70, sigmoid
# Graph fragment:
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0" = PlaceHolder[target=convert_element_type_38]
#   %sigmoid : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_37,), kwargs = {})
#   %mul_70 : Tensor "f32[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_37, %sigmoid), kwargs = {})
#   %convert_element_type_38 : Tensor "f16[128, 240, 56, 56][752640, 3136, 56, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_70, torch.float16), kwargs = {})
#   %split_with_sizes_13 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_38, [60, 60, 60, 60], 1), kwargs = {})
#   %convolution_19 : Tensor "f16[128, 60, 28, 28][47040, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_67, %convert_element_type_42, None, [2, 2], [4, 4], [1, 1], False, [0, 0], 60), kwargs = {})
#   return %buf172
triton_poi_fused_convolution_silu_split_with_sizes_63 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_63', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 4096}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_63', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48168960}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_63(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 7680
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 60)
    y1 = yindex // 60
    tmp0 = tl.load(in_ptr0 + (564480 + x2 + 3136*y0 + 752640*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 60*x2 + 188160*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 240, 56, 56), (752640, 3136, 56, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 56, 56), (188160, 1, 3360, 60), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7680, 3136,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_63.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_63.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kl/ckloplc7oneoh7aabf524s3zah2wlh3vygx4p42sqh2pa64rlffz.py
# Topologically Sorted Source Nodes: [x_29], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_29 => cat_5
# Graph fragment:
#   %convolution_16 : Tensor "f16[128, 60, 28, 28][47040, 1, 1680, 60]cuda:0" = PlaceHolder[target=convolution_16]
#   %convolution_17 : Tensor "f16[128, 60, 28, 28][47040, 1, 1680, 60]cuda:0" = PlaceHolder[target=convolution_17]
#   %convolution_18 : Tensor "f16[128, 60, 28, 28][47040, 1, 1680, 60]cuda:0" = PlaceHolder[target=convolution_18]
#   %convolution_19 : Tensor "f16[128, 60, 28, 28][47040, 1, 1680, 60]cuda:0" = PlaceHolder[target=convolution_19]
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_16, %convolution_17, %convolution_18, %convolution_19], 1), kwargs = {})
#   return %cat_5
triton_poi_fused_cat_64 = async_compile.triton('triton_poi_fused_cat_64', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_64', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 289013760}, 'kernel_num_gb': 0.09633792, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_64(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 24084480
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 240)
    x1 = xindex // 240
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 60, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (60*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 120, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (60*x1 + ((-60) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 180, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (60*x1 + ((-120) + x0)), tmp14, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 240, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (60*x1 + ((-180) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, None)


def get_args():
    arg_0 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 60, 28, 28), (47040, 1, 1680, 60), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 24084480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_64.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_64.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09633792
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mn/cmngwconftd5ykosraz2gxe6mkplrfdw4hmwzfzrly5j6yhdh2ec.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_30 => convert_element_type_43, var_mean_10
# Graph fragment:
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=cat_5]
#   %convert_element_type_43 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_5, torch.float32), kwargs = {})
#   %var_mean_10 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_43, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf175,%buf176,%buf177
triton_red_fused__native_batch_norm_legit_functional_65 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_65', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_65', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 51118080, 'r0_': 0}, 'kernel_num_gb': 0.04964352, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_65(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 47040*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122880, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_65.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_65.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04964352
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bw/cbwihnpnjlj5hdn36zznfz7o6ta4rw4tdiejdofiprpersyxusix.py
# Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_30 => add_53, add_54, add_55, convert_element_type_43, mul_72, mul_73, mul_74, mul_75, mul_76, rsqrt_10, squeeze_30, squeeze_32, var_mean_10
# Graph fragment:
#   %buf178 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf178]
#   %buf179 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf179]
#   %buf180 : Tensor "f32[1, 240, 1, 1, 4][960, 1, 960, 960, 240]cuda:0" = PlaceHolder[target=buf180]
#   %buf182 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=buf182]
#   %getitem_69 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_69]
#   %copy__31 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=copy__31]
#   %add_54 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=add_54]
#   %copy__32 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=copy__32]
#   %add_55 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=add_55]
#   %convert_element_type_43 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_5, torch.float32), kwargs = {})
#   %var_mean_10 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_43, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_53 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_68, 1e-05), kwargs = {})
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_53,), kwargs = {})
#   %squeeze_30 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_69, [0, 2, 3]), kwargs = {})
#   %mul_72 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_30, 0.1), kwargs = {})
#   %mul_73 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_73, 0.9), kwargs = {})
#   %add_54 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_72, %mul_73), kwargs = {})
#   %squeeze_32 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_68, [0, 2, 3]), kwargs = {})
#   %mul_74 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_32, 1.00000996502277), kwargs = {})
#   %mul_75 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_74, 0.1), kwargs = {})
#   %mul_76 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_74, 0.9), kwargs = {})
#   %add_55 : Tensor "f32[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_75, %mul_76), kwargs = {})
#   %copy__31 : Tensor "f32[240][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_73, %add_54), kwargs = {})
#   %copy__32 : Tensor "f32[240][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_74, %add_55), kwargs = {})
#   return %getitem_69,%buf182,%rsqrt_10,%add_54,%buf1277,%add_55,%buf1280
triton_per_fused__native_batch_norm_legit_functional_copy__66 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__66', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 4},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__66', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21120, 'r0_': 0}, 'kernel_num_gb': 1.728e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__66(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 240
    r0_numel = 4
    R0_BLOCK: tl.constexpr = 4
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 240*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 100352.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.00000996502277
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1, 4), (960, 1, 960, 960, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 240, 4,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__66.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__66.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.728e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3w/c3wuf4frsar7w65r4wlliragw4tncemocn2ky2obim3nomojkiup.py
# Topologically Sorted Source Nodes: [x_30, x_31], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_30 => add_56, convert_element_type_44, mul_71, mul_77, sub_10, unsqueeze_40, unsqueeze_41, unsqueeze_42, unsqueeze_43
#   x_31 => convert_element_type_45
# Graph fragment:
#   %cat_5 : Tensor "f16[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=cat_5]
#   %getitem_69 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=getitem_69]
#   %rsqrt_10 : Tensor "f32[1, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=rsqrt_10]
#   %primals_75 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_75]
#   %primals_76 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_76]
#   %sub_10 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_5, %getitem_69), kwargs = {})
#   %mul_71 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_10, %rsqrt_10), kwargs = {})
#   %unsqueeze_40 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_75, -1), kwargs = {})
#   %unsqueeze_41 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_40, -1), kwargs = {})
#   %mul_77 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_71, %unsqueeze_41), kwargs = {})
#   %unsqueeze_42 : Tensor "f32[240, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_76, -1), kwargs = {})
#   %unsqueeze_43 : Tensor "f32[240, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_42, -1), kwargs = {})
#   %add_56 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_77, %unsqueeze_43), kwargs = {})
#   %convert_element_type_44 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_56, torch.float16), kwargs = {})
#   %convert_element_type_45 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_44, torch.float32), kwargs = {})
#   return %convert_element_type_45
triton_poi_fused__native_batch_norm_legit_functional_silu_67 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_67', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_67', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 240848640}, 'kernel_num_gb': 0.14451072, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_67(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 24084480
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 240)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 24084480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_67.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_67.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.14451072
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2q/c2qwcsaajzm6vpm3ywn3hpo4kuvop5evcf4zgczq4i5i3ki7dcrl.py
# Topologically Sorted Source Nodes: [x_31, x_se], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_31 => convert_element_type_46, mul_78, sigmoid_1
#   x_se => mean
# Graph fragment:
#   %convert_element_type_45 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=convert_element_type_45]
#   %buf186 : Tensor "f32[128, 240, 1, 1][240, 1, 30720, 30720]cuda:0" = PlaceHolder[target=buf186]
#   %sigmoid_1 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_45,), kwargs = {})
#   %mul_78 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_45, %sigmoid_1), kwargs = {})
#   %convert_element_type_46 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_78, torch.float16), kwargs = {})
#   %mean : Tensor "f16[128, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_46, [2, 3], True), kwargs = {})
#   return %buf186,%mean
triton_red_fused_mean_silu_68 = async_compile.triton('triton_red_fused_mean_silu_68', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 1024},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_silu_68', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96460800, 'r0_': 0}, 'kernel_num_gb': 0.09639936, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mean_silu_68(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 30720
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 240)
    x1 = xindex // 240
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 240*r0_2 + 188160*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.sigmoid(tmp0)
        tmp2 = tmp0 * tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp3.to(tl.float32)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = 784.0
    tmp9 = (tmp6 / tmp8)
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_silu_68.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mean_silu_68.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09639936
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gf/cgfucrinx6yvh2v7dtzosw7b6eorvs473c6efwwyvl6bgb2642to.py
# Topologically Sorted Source Nodes: [x_se_1], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_1 => convert_element_type_48
# Graph fragment:
#   %primals_77 : Tensor "f32[20, 240, 1, 1][240, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_77]
#   %convert_element_type_48 : Tensor "f16[20, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_77, torch.float16), kwargs = {})
#   return %convert_element_type_48
triton_poi_fused__to_copy_69 = async_compile.triton('triton_poi_fused__to_copy_69', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_69', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 38400}, 'kernel_num_gb': 2.88e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_69(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((20, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((20, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_69.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_69.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.88e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/s4/cs43rc6efsmoa2f46agjbhys24vrarof3laakdmhauvt7vxghtql.py
# Topologically Sorted Source Nodes: [x_se_1, x_se_2], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_1 => convert_element_type_47, convolution_20
#   x_se_2 => convert_element_type_49, convert_element_type_50, mul_79, sigmoid_2
# Graph fragment:
#   %buf189 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=buf189]
#   %primals_78 : Tensor "f32[20][1]cuda:0" = PlaceHolder[target=primals_78]
#   %convolution_20 : Tensor "f16[128, 20, 1, 1][20, 1, 20, 20]cuda:0" = PlaceHolder[target=convolution_20]
#   %convert_element_type_47 : Tensor "f16[20][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_78, torch.float16), kwargs = {})
#   %convolution_20 : Tensor "f16[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean, %convert_element_type_48, %convert_element_type_47, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_49 : Tensor "f32[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_20, torch.float32), kwargs = {})
#   %sigmoid_2 : Tensor "f32[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_49,), kwargs = {})
#   %mul_79 : Tensor "f32[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_49, %sigmoid_2), kwargs = {})
#   %convert_element_type_50 : Tensor "f16[128, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_79, torch.float16), kwargs = {})
#   return %convolution_20,%convert_element_type_50
triton_poi_fused__to_copy_convolution_silu_70 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_70', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_70', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 25680}, 'kernel_num_gb': 1.544e-05, 'kernel_flop': 1228800},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_70(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2560
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 20)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((20,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 2560,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_70.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_70.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.544e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6u/c6uvbfmbde66hkxysafh3kidvo5pustssecwmvp3b6pi23hicixm.py
# Topologically Sorted Source Nodes: [x_se_3], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_3 => convert_element_type_52
# Graph fragment:
#   %primals_79 : Tensor "f32[240, 20, 1, 1][20, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_79]
#   %convert_element_type_52 : Tensor "f16[240, 20, 1, 1][20, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_79, torch.float16), kwargs = {})
#   return %convert_element_type_52
triton_poi_fused__to_copy_71 = async_compile.triton('triton_poi_fused__to_copy_71', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_71', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 38400}, 'kernel_num_gb': 2.88e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_71(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 20, 1, 1), (20, 1, 20, 20), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_71.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_71.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.88e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wr/cwr6lnvw3vjo5rrz4pio2ynh7gssaehrsuhch647knlqr3yknoog.py
# Topologically Sorted Source Nodes: [x_se_3], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_3 => convert_element_type_51, convolution_21
# Graph fragment:
#   %buf193 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=buf193]
#   %primals_80 : Tensor "f32[240][1]cuda:0" = PlaceHolder[target=primals_80]
#   %convert_element_type_51 : Tensor "f16[240][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_80, torch.float16), kwargs = {})
#   %convolution_21 : Tensor "f16[128, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_50, %convert_element_type_52, %convert_element_type_51, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_21
triton_poi_fused__to_copy_convolution_72 = async_compile.triton('triton_poi_fused__to_copy_convolution_72', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_72', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 185280}, 'kernel_num_gb': 0.00012384, 'kernel_flop': 1228800},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_72(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 30720
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 240)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((240,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 30720,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_72.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_72.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00012384
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dx/cdxfonfgv64wpfyxwpvq2gfedabtcrg654b3cnjmrmzwfj2pk2bi.py
# Topologically Sorted Source Nodes: [x_31, sigmoid, x_32], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid => sigmoid_3
#   x_31 => convert_element_type_46, mul_78, sigmoid_1
#   x_32 => mul_80
# Graph fragment:
#   %convert_element_type_45 : Tensor "f32[128, 240, 28, 28][188160, 1, 6720, 240]cuda:0" = PlaceHolder[target=convert_element_type_45]
#   %convolution_21 : Tensor "f16[128, 240, 1, 1][240, 1, 240, 240]cuda:0" = PlaceHolder[target=convolution_21]
#   %sigmoid_1 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_45,), kwargs = {})
#   %mul_78 : Tensor "f32[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_45, %sigmoid_1), kwargs = {})
#   %convert_element_type_46 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_78, torch.float16), kwargs = {})
#   %sigmoid_3 : Tensor "f16[128, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_21,), kwargs = {})
#   %mul_80 : Tensor "f16[128, 240, 28, 28][188160, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_46, %sigmoid_3), kwargs = {})
#   return %mul_80
triton_poi_fused_mul_sigmoid_silu_73 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_73', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 33554432}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_73', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 192737280}, 'kernel_num_gb': 0.14456832, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_73(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 24084480
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 240)
    x2 = xindex // 188160
    tmp0 = tl.load(in_ptr0 + (x3), None)
    tmp4 = tl.load(in_ptr1 + (x0 + 240*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (x3), tmp6, None)


def get_args():
    arg_0 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 28, 28), (188160, 1, 6720, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 24084480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_73.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_73.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.14456832
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2d/c2doi3ac6fqnzyruivzat3o7ddrlnn4qd4m255uvlvqzhpoe4vqt.py
# Topologically Sorted Source Nodes: [x_33], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_33 => convert_element_type_53
# Graph fragment:
#   %primals_81 : Tensor "f32[56, 240, 1, 1][240, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_81]
#   %convert_element_type_53 : Tensor "f16[56, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_81, torch.float16), kwargs = {})
#   return %convert_element_type_53
triton_poi_fused__to_copy_74 = async_compile.triton('triton_poi_fused__to_copy_74', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_74', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 107520}, 'kernel_num_gb': 8.064e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_74(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 13440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((56, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((56, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 13440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_74.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_74.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.064e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5h/c5hn6twbl23njyult34xw46lwmxatzv3bwsa4oupoefegcveu46f.py
# Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_34 => convert_element_type_54, var_mean_11
# Graph fragment:
#   %convolution_22 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convolution_22]
#   %convert_element_type_54 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_22, torch.float32), kwargs = {})
#   %var_mean_11 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_54, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf198,%buf199,%buf200
triton_red_fused__native_batch_norm_legit_functional_75 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_75', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_75', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12293120, 'r0_': 0}, 'kernel_num_gb': 0.011766272, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_75(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43904
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 56)
    x1 = xindex // 56
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_2 + 7168*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 43904, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_75.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_75.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.011766272
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/r6/cr6b7ws7b5zy6xeebmhl5su2y6jpc4kcg7ldoqcrvsyoiljoquas.py
# Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_34 => convert_element_type_54, var_mean_11
# Graph fragment:
#   %buf198 : Tensor "f32[1, 56, 1, 1, 784][43904, 1, 43904, 43904, 56]cuda:0" = PlaceHolder[target=buf198]
#   %buf199 : Tensor "f32[1, 56, 1, 1, 784][43904, 1, 43904, 43904, 56]cuda:0" = PlaceHolder[target=buf199]
#   %buf200 : Tensor "f32[1, 56, 1, 1, 784][43904, 1, 43904, 43904, 56]cuda:0" = PlaceHolder[target=buf200]
#   %convert_element_type_54 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_22, torch.float32), kwargs = {})
#   %var_mean_11 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_54, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf201,%buf202,%buf203
triton_red_fused__native_batch_norm_legit_functional_76 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_76', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_76', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 536256, 'r0_': 0}, 'kernel_num_gb': 0.000531552, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_76(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 392
    r0_numel = 112
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 56)
    x1 = xindex // 56
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_2 + 6272*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 56*r0_2 + 6272*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 56*r0_2 + 6272*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 392, 112,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_76.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_76.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000531552
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/na/cnaihwalwz7hur3ll7dsgdsbztnaxzlk33jp7eqzszcioxuwcya4.py
# Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_34 => add_58, add_59, add_60, convert_element_type_54, mul_82, mul_83, mul_84, mul_85, mul_86, rsqrt_11, squeeze_33, squeeze_35, var_mean_11
# Graph fragment:
#   %buf201 : Tensor "f32[1, 56, 1, 1, 7][392, 1, 392, 392, 56]cuda:0" = PlaceHolder[target=buf201]
#   %buf202 : Tensor "f32[1, 56, 1, 1, 7][392, 1, 392, 392, 56]cuda:0" = PlaceHolder[target=buf202]
#   %buf203 : Tensor "f32[1, 56, 1, 1, 7][392, 1, 392, 392, 56]cuda:0" = PlaceHolder[target=buf203]
#   %buf205 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=buf205]
#   %getitem_71 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=getitem_71]
#   %copy__34 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=copy__34]
#   %add_59 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=add_59]
#   %copy__35 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=copy__35]
#   %add_60 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=add_60]
#   %convert_element_type_54 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_22, torch.float32), kwargs = {})
#   %var_mean_11 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_54, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_58 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_70, 1e-05), kwargs = {})
#   %rsqrt_11 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_58,), kwargs = {})
#   %squeeze_33 : Tensor "f32[56][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_71, [0, 2, 3]), kwargs = {})
#   %mul_82 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_33, 0.1), kwargs = {})
#   %mul_83 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_83, 0.9), kwargs = {})
#   %add_59 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_82, %mul_83), kwargs = {})
#   %squeeze_35 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_70, [0, 2, 3]), kwargs = {})
#   %mul_84 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_35, 1.00000996502277), kwargs = {})
#   %mul_85 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_84, 0.1), kwargs = {})
#   %mul_86 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_84, 0.9), kwargs = {})
#   %add_60 : Tensor "f32[56][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_85, %mul_86), kwargs = {})
#   %copy__34 : Tensor "f32[56][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_83, %add_59), kwargs = {})
#   %copy__35 : Tensor "f32[56][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_84, %add_60), kwargs = {})
#   return %getitem_71,%buf205,%rsqrt_11,%add_59,%buf1285,%add_60,%buf1288
triton_per_fused__native_batch_norm_legit_functional_copy__77 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__77', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 64, 'r0_': 8},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__77', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 7392, 'r0_': 0}, 'kernel_num_gb': 6.272e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__77(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 56
    r0_numel = 7
    R0_BLOCK: tl.constexpr = 8
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 56*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 56*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 56*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 100352.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.00000996502277
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 56, 7,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__77.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__77.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.272e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/r6/cr6pw7d6o2y7fjyzuhgywfwzdh5lmpfzw6qoeqfvnl6764b5kprj.py
# Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_34 => add_58, add_61, convert_element_type_54, convert_element_type_55, mul_81, mul_87, rsqrt_11, sub_11, unsqueeze_44, unsqueeze_45, unsqueeze_46, unsqueeze_47, var_mean_11
# Graph fragment:
#   %convolution_22 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convolution_22]
#   %getitem_71 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=getitem_71]
#   %buf205 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=buf205]
#   %primals_85 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_85]
#   %primals_86 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_86]
#   %convert_element_type_54 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_22, torch.float32), kwargs = {})
#   %var_mean_11 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_54, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_58 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_70, 1e-05), kwargs = {})
#   %rsqrt_11 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_58,), kwargs = {})
#   %sub_11 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_22, %getitem_71), kwargs = {})
#   %mul_81 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_11, %rsqrt_11), kwargs = {})
#   %unsqueeze_44 : Tensor "f32[56, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_85, -1), kwargs = {})
#   %unsqueeze_45 : Tensor "f32[56, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_44, -1), kwargs = {})
#   %mul_87 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_81, %unsqueeze_45), kwargs = {})
#   %unsqueeze_46 : Tensor "f32[56, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_86, -1), kwargs = {})
#   %unsqueeze_47 : Tensor "f32[56, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_46, -1), kwargs = {})
#   %add_61 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_87, %unsqueeze_47), kwargs = {})
#   %convert_element_type_55 : Tensor "f16[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_61, torch.float16), kwargs = {})
#   return %convert_element_type_55
triton_poi_fused__native_batch_norm_legit_functional_78 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_78', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_78', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33719168}, 'kernel_num_gb': 0.022479744, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_78(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 100352.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_78.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_78.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.022479744
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ga/cgafgtajrptrpycwrrw5aylg6d2fxbreeq4qhfgqb6ztlo53mvuy.py
# Topologically Sorted Source Nodes: [split_6, conv2d_23], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_23 => convolution_23
#   split_6 => getitem_72, split_with_sizes_17
# Graph fragment:
#   %convert_element_type_55 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convert_element_type_55]
#   %getitem_72 : Tensor "f16[128, 28, 28, 28][21952, 784, 28, 1]cuda:0" = PlaceHolder[target=getitem_72]
#   %split_with_sizes_17 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_55, [28, 28], 1), kwargs = {})
#   %getitem_72 : Tensor "f16[128, 28, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_17, 0), kwargs = {})
#   %convolution_23 : Tensor "f16[128, 168, 28, 28][131712, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_72, %convert_element_type_56, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_72,%buf212
triton_poi_fused_convolution_split_with_sizes_79 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_79', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 4096, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_79', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 16859136, 'x': 11239424}, 'kernel_num_gb': 0.016859136, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_79(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3584
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 28)
    y1 = yindex // 28
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 56*x2 + 43904*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 784*y3), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 28*x2 + 21952*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 3584, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_79.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_79.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016859136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yd/cydvmhz5zubtzp5vf535crkahpnnxkjlqqd52opcpyvsdqqvrpb7.py
# Topologically Sorted Source Nodes: [split_6, conv2d_24], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_24 => convolution_24
#   split_6 => getitem_73, split_with_sizes_17
# Graph fragment:
#   %convert_element_type_55 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convert_element_type_55]
#   %getitem_73 : Tensor "f16[128, 28, 28, 28][21952, 784, 28, 1]cuda:0" = PlaceHolder[target=getitem_73]
#   %split_with_sizes_17 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_55, [28, 28], 1), kwargs = {})
#   %getitem_73 : Tensor "f16[128, 28, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_17, 1), kwargs = {})
#   %convolution_24 : Tensor "f16[128, 168, 28, 28][131712, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_73, %convert_element_type_57, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_73,%buf215
triton_poi_fused_convolution_split_with_sizes_80 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_80', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 4096, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_80', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 16859136, 'x': 11239424}, 'kernel_num_gb': 0.016859136, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_80(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3584
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 28)
    y1 = yindex // 28
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (28 + y0 + 56*x2 + 43904*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 784*y3), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 28*x2 + 21952*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 3584, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_80.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_80.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.016859136
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/v7/cv7lcsqs3spkgvfiyd4da6dmn5z5gn253acc2vaoe6ltkosdsv5w.py
# Topologically Sorted Source Nodes: [conv2d_23], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_23 => convert_element_type_56
# Graph fragment:
#   %primals_87 : Tensor "f32[168, 28, 1, 1][28, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_87]
#   %convert_element_type_56 : Tensor "f16[168, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_87, torch.float16), kwargs = {})
#   return %convert_element_type_56
triton_poi_fused__to_copy_81 = async_compile.triton('triton_poi_fused__to_copy_81', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_81', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 37632}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_81(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((168, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_81.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_81.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/xi/cxiiwjyihhzo4fxlvkf24mmxx52qdxgopspvvkpuw6wkk6j7lfpb.py
# Topologically Sorted Source Nodes: [x_35], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_35 => cat_6
# Graph fragment:
#   %convolution_23 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=convolution_23]
#   %convolution_24 : Tensor "f16[128, 168, 28, 28][131712, 1, 4704, 168]cuda:0" = PlaceHolder[target=convolution_24]
#   %cat_6 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_23, %convolution_24], 1), kwargs = {})
#   return %cat_6
triton_poi_fused_cat_82 = async_compile.triton('triton_poi_fused_cat_82', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_82', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 269746176}, 'kernel_num_gb': 0.134873088, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_82(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 336)
    x1 = xindex // 336
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 168, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (168*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 336, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (168*x1 + ((-168) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_82.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_82.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.134873088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gx/cgxyd5l6n3rhsamginb5g6c6y6xcz7p24rvplxjfg4kgpbehx6j5.py
# Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_36 => convert_element_type_58, var_mean_12
# Graph fragment:
#   %cat_6 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_6]
#   %convert_element_type_58 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_6, torch.float32), kwargs = {})
#   %var_mean_12 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_58, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf218,%buf219,%buf220
triton_red_fused__native_batch_norm_legit_functional_83 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_83', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 262144, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_83', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 70597632, 'r0_': 0}, 'kernel_num_gb': 0.069017088, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_83(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 131712
    r0_numel = 256
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 86016*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 131712, 256,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_83.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_83.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.069017088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ly/clywbuebzuqnrjveowoknz4kgydi6oyn6v6kj4j53j54iw643jvz.py
# Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_36 => convert_element_type_58, var_mean_12
# Graph fragment:
#   %buf218 : Tensor "f32[1, 336, 1, 1, 392][131712, 1, 131712, 131712, 336]cuda:0" = PlaceHolder[target=buf218]
#   %buf219 : Tensor "f32[1, 336, 1, 1, 392][131712, 1, 131712, 131712, 336]cuda:0" = PlaceHolder[target=buf219]
#   %buf220 : Tensor "f32[1, 336, 1, 1, 392][131712, 1, 131712, 131712, 336]cuda:0" = PlaceHolder[target=buf220]
#   %convert_element_type_58 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_6, torch.float32), kwargs = {})
#   %var_mean_12 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_58, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf221,%buf222,%buf223
triton_red_fused__native_batch_norm_legit_functional_84 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_84', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 2048, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_84', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1612800, 'r0_': 0}, 'kernel_num_gb': 0.001596672, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_84(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1344
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 1344, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_84.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_84.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001596672
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qa/cqavaazroe7kk7jncdrrng4v3ras5v6ctfs3v5iyu5dsjycc4zlj.py
# Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_36 => add_63, add_64, add_65, convert_element_type_58, mul_89, mul_90, mul_91, mul_92, mul_93, rsqrt_12, squeeze_36, squeeze_38, var_mean_12
# Graph fragment:
#   %buf221 : Tensor "f32[1, 336, 1, 1, 4][1344, 1, 1344, 1344, 336]cuda:0" = PlaceHolder[target=buf221]
#   %buf222 : Tensor "f32[1, 336, 1, 1, 4][1344, 1, 1344, 1344, 336]cuda:0" = PlaceHolder[target=buf222]
#   %buf223 : Tensor "f32[1, 336, 1, 1, 4][1344, 1, 1344, 1344, 336]cuda:0" = PlaceHolder[target=buf223]
#   %buf225 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=buf225]
#   %getitem_75 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_75]
#   %copy__37 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=copy__37]
#   %add_64 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=add_64]
#   %copy__38 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=copy__38]
#   %add_65 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=add_65]
#   %convert_element_type_58 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_6, torch.float32), kwargs = {})
#   %var_mean_12 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_58, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_63 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_74, 1e-05), kwargs = {})
#   %rsqrt_12 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_63,), kwargs = {})
#   %squeeze_36 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_75, [0, 2, 3]), kwargs = {})
#   %mul_89 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_36, 0.1), kwargs = {})
#   %mul_90 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_90, 0.9), kwargs = {})
#   %add_64 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_89, %mul_90), kwargs = {})
#   %squeeze_38 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_74, [0, 2, 3]), kwargs = {})
#   %mul_91 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_38, 1.00000996502277), kwargs = {})
#   %mul_92 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_91, 0.1), kwargs = {})
#   %mul_93 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_91, 0.9), kwargs = {})
#   %add_65 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_92, %mul_93), kwargs = {})
#   %copy__37 : Tensor "f32[336][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_90, %add_64), kwargs = {})
#   %copy__38 : Tensor "f32[336][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_91, %add_65), kwargs = {})
#   return %getitem_75,%buf225,%rsqrt_12,%add_64,%buf1293,%add_65,%buf1296
triton_per_fused__native_batch_norm_legit_functional_copy__85 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__85', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 4},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__85', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 29568, 'r0_': 0}, 'kernel_num_gb': 2.4192e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__85(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 4
    R0_BLOCK: tl.constexpr = 4
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 100352.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.00000996502277
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 336, 4,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__85.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__85.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.4192e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vg/cvg5ddglzucywxoaharoekslgy5l7vhzde4bg27gdxvkf4g3zfmb.py
# Topologically Sorted Source Nodes: [x_36, x_37], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_36 => add_66, convert_element_type_59, mul_88, mul_94, sub_12, unsqueeze_48, unsqueeze_49, unsqueeze_50, unsqueeze_51
#   x_37 => convert_element_type_60, convert_element_type_61, mul_95, sigmoid_4
# Graph fragment:
#   %cat_6 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_6]
#   %getitem_75 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_75]
#   %rsqrt_12 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_12]
#   %primals_92 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_92]
#   %primals_93 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_93]
#   %convert_element_type_60 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_60]
#   %sub_12 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_6, %getitem_75), kwargs = {})
#   %mul_88 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_12, %rsqrt_12), kwargs = {})
#   %unsqueeze_48 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_92, -1), kwargs = {})
#   %unsqueeze_49 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_48, -1), kwargs = {})
#   %mul_94 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_88, %unsqueeze_49), kwargs = {})
#   %unsqueeze_50 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_93, -1), kwargs = {})
#   %unsqueeze_51 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_50, -1), kwargs = {})
#   %add_66 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_94, %unsqueeze_51), kwargs = {})
#   %convert_element_type_59 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_66, torch.float16), kwargs = {})
#   %convert_element_type_60 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_59, torch.float32), kwargs = {})
#   %sigmoid_4 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_60,), kwargs = {})
#   %mul_95 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_60, %sigmoid_4), kwargs = {})
#   %convert_element_type_61 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_95, torch.float16), kwargs = {})
#   return %convert_element_type_60,%convert_element_type_61
triton_poi_fused__native_batch_norm_legit_functional_silu_86 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_86', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_86', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 134873088, 'x': 67441920}, 'kernel_num_gb': 0.134878464, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_86(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 100352
    xnumel = 336
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 784)
    y3 = yindex // 784
    tmp0 = tl.load(in_ptr0 + (x1 + 336*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 784*x1 + 263424*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 100352, 336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_86.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_86.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.134878464
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ao/caosuj64wwfe3qrikp5se5hmtgclcc4ow73fjiplvtqsws5tvjmq.py
# Topologically Sorted Source Nodes: [conv2d_25], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_25 => convert_element_type_62
# Graph fragment:
#   %primals_94 : Tensor "f32[168, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_94]
#   %convert_element_type_62 : Tensor "f16[168, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_94, torch.float16), kwargs = {})
#   return %convert_element_type_62
triton_poi_fused__to_copy_87 = async_compile.triton('triton_poi_fused__to_copy_87', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_87', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12096}, 'kernel_num_gb': 9.072e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_87(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((168, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1512,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_87.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_87.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.072e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d6/cd6nyqfmil6cukkx2sqam3uljljmiab2y6if44krctoog5bthcvz.py
# Topologically Sorted Source Nodes: [x_37, conv2d_25], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_25 => convolution_25, split_with_sizes_19
#   x_37 => convert_element_type_61, mul_95, sigmoid_4
# Graph fragment:
#   %convert_element_type_61 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_61]
#   %sigmoid_4 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_60,), kwargs = {})
#   %mul_95 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_60, %sigmoid_4), kwargs = {})
#   %convert_element_type_61 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_95, torch.float16), kwargs = {})
#   %split_with_sizes_19 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_61, [168, 168], 1), kwargs = {})
#   %convolution_25 : Tensor "f16[128, 168, 28, 28][131712, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_78, %convert_element_type_62, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 168), kwargs = {})
#   return %buf231
triton_poi_fused_convolution_silu_split_with_sizes_88 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_88', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_88', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67436544, 'x': 33718272}, 'kernel_num_gb': 0.067436544, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_88(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 21504
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 168)
    y1 = yindex // 168
    tmp0 = tl.load(in_ptr0 + (x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 168*x2 + 131712*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 21504, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_88.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_88.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.067436544
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hj/chj5vdhiitzj5rhp6v7l5gqud4x74uqyvcomtxunovlev32yqdo3.py
# Topologically Sorted Source Nodes: [conv2d_26], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_26 => convert_element_type_63
# Graph fragment:
#   %primals_95 : Tensor "f32[168, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_95]
#   %convert_element_type_63 : Tensor "f16[168, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_95, torch.float16), kwargs = {})
#   return %convert_element_type_63
triton_poi_fused__to_copy_89 = async_compile.triton('triton_poi_fused__to_copy_89', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_89', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33600}, 'kernel_num_gb': 2.52e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_89(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((168, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((168, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_89.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_89.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.52e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/v4/cv4akydhhzygzi4ohkpoip3jixn4m4eertqcvijesxvgihfpdefi.py
# Topologically Sorted Source Nodes: [x_37, conv2d_25, conv2d_26], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_25 => split_with_sizes_19
#   conv2d_26 => convolution_26
#   x_37 => convert_element_type_61, mul_95, sigmoid_4
# Graph fragment:
#   %convert_element_type_61 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_61]
#   %sigmoid_4 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_60,), kwargs = {})
#   %mul_95 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_60, %sigmoid_4), kwargs = {})
#   %convert_element_type_61 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_95, torch.float16), kwargs = {})
#   %split_with_sizes_19 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_61, [168, 168], 1), kwargs = {})
#   %convolution_26 : Tensor "f16[128, 168, 28, 28][131712, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_81, %convert_element_type_63, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 168), kwargs = {})
#   return %buf234
triton_poi_fused_convolution_silu_split_with_sizes_90 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_90', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_90', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 67436544, 'x': 33718272}, 'kernel_num_gb': 0.067436544, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_90(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 21504
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 168)
    y1 = yindex // 168
    tmp0 = tl.load(in_ptr0 + (131712 + x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 168*x2 + 131712*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 168, 28, 28), (131712, 1, 4704, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 21504, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_90.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_90.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.067436544
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5p/c5pnx4rlszn6ml42wiluo3nnvxtsxqlwkqnuuvzqwuur4fix7njj.py
# Topologically Sorted Source Nodes: [x_39, x_40], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_39 => add_71, convert_element_type_65, mul_102, mul_96, sub_13, unsqueeze_52, unsqueeze_53, unsqueeze_54, unsqueeze_55
#   x_40 => convert_element_type_66
# Graph fragment:
#   %cat_7 : Tensor "f16[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=cat_7]
#   %getitem_83 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_83]
#   %rsqrt_13 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_13]
#   %primals_99 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_99]
#   %primals_100 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_100]
#   %sub_13 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_7, %getitem_83), kwargs = {})
#   %mul_96 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_13, %rsqrt_13), kwargs = {})
#   %unsqueeze_52 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_99, -1), kwargs = {})
#   %unsqueeze_53 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_52, -1), kwargs = {})
#   %mul_102 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_96, %unsqueeze_53), kwargs = {})
#   %unsqueeze_54 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_100, -1), kwargs = {})
#   %unsqueeze_55 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_54, -1), kwargs = {})
#   %add_71 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_102, %unsqueeze_55), kwargs = {})
#   %convert_element_type_65 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_71, torch.float16), kwargs = {})
#   %convert_element_type_66 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_65, torch.float32), kwargs = {})
#   return %convert_element_type_66
triton_poi_fused__native_batch_norm_legit_functional_silu_91 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_91', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 67108864}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_91', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 337188096}, 'kernel_num_gb': 0.202315008, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_91(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 33718272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 33718272,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_91.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_91.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.202315008
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ea/ceaz4zly5neb3jeid3a6ovm3q632jeofmks2hcteei6nvqnnzwjq.py
# Topologically Sorted Source Nodes: [x_40, x_se_4], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_40 => convert_element_type_67, mul_103, sigmoid_5
#   x_se_4 => mean_1
# Graph fragment:
#   %convert_element_type_66 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_66]
#   %buf248 : Tensor "f32[128, 336, 1, 1][336, 1, 43008, 43008]cuda:0" = PlaceHolder[target=buf248]
#   %sigmoid_5 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_66,), kwargs = {})
#   %mul_103 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_66, %sigmoid_5), kwargs = {})
#   %convert_element_type_67 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_103, torch.float16), kwargs = {})
#   %mean_1 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_67, [2, 3], True), kwargs = {})
#   return %buf248,%mean_1
triton_red_fused_mean_silu_92 = async_compile.triton('triton_red_fused_mean_silu_92', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 1024},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_silu_92', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 135045120, 'r0_': 0}, 'kernel_num_gb': 0.134959104, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mean_silu_92(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43008
    r0_numel = 784
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 263424*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.sigmoid(tmp0)
        tmp2 = tmp0 * tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp3.to(tl.float32)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = 784.0
    tmp9 = (tmp6 / tmp8)
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 43008, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_silu_92.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mean_silu_92.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.134959104
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rc/crcob5rb6tbdwwxvdori6ni4pc7k3fcvg2jc5ta2mvdreljixt3c.py
# Topologically Sorted Source Nodes: [x_se_5], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_5 => convert_element_type_69
# Graph fragment:
#   %primals_101 : Tensor "f32[28, 336, 1, 1][336, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_101]
#   %convert_element_type_69 : Tensor "f16[28, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_101, torch.float16), kwargs = {})
#   return %convert_element_type_69
triton_poi_fused__to_copy_93 = async_compile.triton('triton_poi_fused__to_copy_93', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_93', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 75264}, 'kernel_num_gb': 5.6448e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_93(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9408
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((28, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((28, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 9408,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_93.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_93.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.6448e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/q3/cq3tx4llp3f2jojyr5rhifdbu6pzm2fy3buhpzgwczdmde6lglxd.py
# Topologically Sorted Source Nodes: [x_se_5, x_se_6], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_5 => convert_element_type_68, convolution_27
#   x_se_6 => convert_element_type_70, convert_element_type_71, mul_104, sigmoid_6
# Graph fragment:
#   %buf251 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=buf251]
#   %primals_102 : Tensor "f32[28][1]cuda:0" = PlaceHolder[target=primals_102]
#   %convolution_27 : Tensor "f16[128, 28, 1, 1][28, 1, 28, 28]cuda:0" = PlaceHolder[target=convolution_27]
#   %convert_element_type_68 : Tensor "f16[28][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_102, torch.float16), kwargs = {})
#   %convolution_27 : Tensor "f16[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_1, %convert_element_type_69, %convert_element_type_68, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_70 : Tensor "f32[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_27, torch.float32), kwargs = {})
#   %sigmoid_6 : Tensor "f32[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_70,), kwargs = {})
#   %mul_104 : Tensor "f32[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_70, %sigmoid_6), kwargs = {})
#   %convert_element_type_71 : Tensor "f16[128, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_104, torch.float16), kwargs = {})
#   return %convolution_27,%convert_element_type_71
triton_poi_fused__to_copy_convolution_silu_94 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_94', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_94', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 35952}, 'kernel_num_gb': 2.1616e-05, 'kernel_flop': 2408448},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_94(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3584
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 28)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((28,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 3584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_94.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_94.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.1616e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5r/c5rq7rybxzqf77yyjn7cs6jjddtdc3vdtx3lrbjwzlj3tv5ucfxx.py
# Topologically Sorted Source Nodes: [x_se_7], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_7 => convert_element_type_73
# Graph fragment:
#   %primals_103 : Tensor "f32[336, 28, 1, 1][28, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_103]
#   %convert_element_type_73 : Tensor "f16[336, 28, 1, 1][28, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_103, torch.float16), kwargs = {})
#   return %convert_element_type_73
triton_poi_fused__to_copy_95 = async_compile.triton('triton_poi_fused__to_copy_95', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_95', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 75264}, 'kernel_num_gb': 5.6448e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_95(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9408
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336, 28, 1, 1), (28, 1, 28, 28), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 9408,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_95.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_95.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.6448e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ai/cai6iqrtkvfl5vguqhdcocor34svh2noankvtpq2prjveds2jp7l.py
# Topologically Sorted Source Nodes: [x_se_7], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_7 => convert_element_type_72, convolution_28
# Graph fragment:
#   %buf255 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=buf255]
#   %primals_104 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_104]
#   %convert_element_type_72 : Tensor "f16[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_104, torch.float16), kwargs = {})
#   %convolution_28 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_71, %convert_element_type_73, %convert_element_type_72, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_28
triton_poi_fused__to_copy_convolution_96 = async_compile.triton('triton_poi_fused__to_copy_convolution_96', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_96', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 259392}, 'kernel_num_gb': 0.000173376, 'kernel_flop': 2408448},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_96(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 43008
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 43008,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_96.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_96.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000173376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pj/cpjhqo3t4rmz6gepqdbltamcec2cjbm4t2qfxq5hfmq4zercvcoo.py
# Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_1 => sigmoid_7
#   x_40 => convert_element_type_67, mul_103, sigmoid_5
#   x_41 => mul_105
# Graph fragment:
#   %convert_element_type_66 : Tensor "f32[128, 336, 28, 28][263424, 1, 9408, 336]cuda:0" = PlaceHolder[target=convert_element_type_66]
#   %convolution_28 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_28]
#   %sigmoid_5 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_66,), kwargs = {})
#   %mul_103 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_66, %sigmoid_5), kwargs = {})
#   %convert_element_type_67 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_103, torch.float16), kwargs = {})
#   %sigmoid_7 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_28,), kwargs = {})
#   %mul_105 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_67, %sigmoid_7), kwargs = {})
#   return %mul_105
triton_poi_fused_mul_sigmoid_silu_97 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_97', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_97', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 134873088, 'x': 134959104}, 'kernel_num_gb': 0.202395648, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_97(in_ptr0, in_ptr1, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 100352
    xnumel = 336
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 784
    y0 = (yindex % 784)
    tmp0 = tl.load(in_ptr0 + (x2 + 336*y3), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr1 + (x2 + 336*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (y0 + 784*x2 + 263424*y1), tmp6, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 1, 9408, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 100352, 336,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_97.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_97.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.202395648
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fj/cfjkyvg5qziixgv5txmn7qosheolx4sui6ism5hxepfgfpwq5gqi.py
# Topologically Sorted Source Nodes: [conv2d_29], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_29 => convert_element_type_74
# Graph fragment:
#   %primals_105 : Tensor "f32[28, 168, 1, 1][168, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_105]
#   %convert_element_type_74 : Tensor "f16[28, 168, 1, 1][168, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_105, torch.float16), kwargs = {})
#   return %convert_element_type_74
triton_poi_fused__to_copy_98 = async_compile.triton('triton_poi_fused__to_copy_98', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_98', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 37632}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_98(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((28, 168, 1, 1), (168, 1, 168, 168), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_98.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_98.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cj/ccjwg55uzzlgbdiforu5svwuhzbk5cfolkypbvfck5dkfcqq6kdo.py
# Topologically Sorted Source Nodes: [x_42], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_42 => cat_8
# Graph fragment:
#   %convolution_29 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=convolution_29]
#   %convolution_30 : Tensor "f16[128, 28, 28, 28][21952, 1, 784, 28]cuda:0" = PlaceHolder[target=convolution_30]
#   %cat_8 : Tensor "f16[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_29, %convolution_30], 1), kwargs = {})
#   return %cat_8
triton_poi_fused_cat_99 = async_compile.triton('triton_poi_fused_cat_99', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_99', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 44957696}, 'kernel_num_gb': 0.022478848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_99(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 56)
    x1 = xindex // 56
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 28, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (28*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 56, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (28*x1 + ((-28) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_99.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_99.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.022478848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ni/cnig5u7dgclzocv62d7lncewalgk3v2lb4e64ksaqabbuo6ujyms.py
# Topologically Sorted Source Nodes: [x_43, x_44], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_43 => add_73, add_76, convert_element_type_76, convert_element_type_77, mul_106, mul_112, rsqrt_14, sub_14, unsqueeze_56, unsqueeze_57, unsqueeze_58, unsqueeze_59, var_mean_14
#   x_44 => add_77
# Graph fragment:
#   %cat_8 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=cat_8]
#   %getitem_87 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=getitem_87]
#   %buf272 : Tensor "f32[1, 56, 1, 1][56, 1, 56, 56]cuda:0" = PlaceHolder[target=buf272]
#   %primals_110 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_110]
#   %primals_111 : Tensor "f32[56][1]cuda:0" = PlaceHolder[target=primals_111]
#   %convert_element_type_55 : Tensor "f16[128, 56, 28, 28][43904, 1, 1568, 56]cuda:0" = PlaceHolder[target=convert_element_type_55]
#   %convert_element_type_76 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_8, torch.float32), kwargs = {})
#   %var_mean_14 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_76, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_73 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_86, 1e-05), kwargs = {})
#   %rsqrt_14 : Tensor "f32[1, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_73,), kwargs = {})
#   %sub_14 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_8, %getitem_87), kwargs = {})
#   %mul_106 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_14, %rsqrt_14), kwargs = {})
#   %unsqueeze_56 : Tensor "f32[56, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_110, -1), kwargs = {})
#   %unsqueeze_57 : Tensor "f32[56, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_56, -1), kwargs = {})
#   %mul_112 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_106, %unsqueeze_57), kwargs = {})
#   %unsqueeze_58 : Tensor "f32[56, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_111, -1), kwargs = {})
#   %unsqueeze_59 : Tensor "f32[56, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_58, -1), kwargs = {})
#   %add_76 : Tensor "f32[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_112, %unsqueeze_59), kwargs = {})
#   %convert_element_type_77 : Tensor "f16[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_76, torch.float16), kwargs = {})
#   %add_77 : Tensor "f16[128, 56, 28, 28][43904, 784, 28, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_77, %convert_element_type_55), kwargs = {})
#   return %add_77
triton_poi_fused__native_batch_norm_legit_functional_add_100 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_100', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_100', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 44958592}, 'kernel_num_gb': 0.033719168, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_100(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5619712
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 56)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 100352.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(in_out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 56, 28, 28), (43904, 1, 1568, 56), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((56,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 5619712,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_100.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_100.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033719168
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/sd/csd2eqzq6rrf6oq2jneyvfgm3ui5b2jydjii24k6ijvozkvsayoc.py
# Topologically Sorted Source Nodes: [x_65], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_65 => convert_element_type_122
# Graph fragment:
#   %primals_162 : Tensor "f32[336, 56, 1, 1][56, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_162]
#   %convert_element_type_122 : Tensor "f16[336, 56, 1, 1][56, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_162, torch.float16), kwargs = {})
#   return %convert_element_type_122
triton_poi_fused__to_copy_101 = async_compile.triton('triton_poi_fused__to_copy_101', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_101', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 150528}, 'kernel_num_gb': 0.000112896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_101(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 18816
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336, 56, 1, 1), (56, 1, 56, 56), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 18816,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_101.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_101.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000112896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d4/cd4yodypl2cc7wvxdr54yjbf5g24eomrdh33hdmjqeitgwgkqtqr.py
# Topologically Sorted Source Nodes: [conv2d_48], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_48 => convert_element_type_127
# Graph fragment:
#   %primals_168 : Tensor "f32[112, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_168]
#   %convert_element_type_127 : Tensor "f16[112, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_168, torch.float16), kwargs = {})
#   return %convert_element_type_127
triton_poi_fused__to_copy_102 = async_compile.triton('triton_poi_fused__to_copy_102', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_102', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8064}, 'kernel_num_gb': 6.048e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_102(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1008
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((112, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1008,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_102.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_102.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.048e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/27/c27pjohi5xlasvnqzjjh3pphqwg656snjmdr566xlji7ebaviwku.py
# Topologically Sorted Source Nodes: [x_67, conv2d_48], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_48 => convolution_48, split_with_sizes_33
#   x_67 => convert_element_type_126, mul_170, sigmoid_16
# Graph fragment:
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_126]
#   %sigmoid_16 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_125,), kwargs = {})
#   %mul_170 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_125, %sigmoid_16), kwargs = {})
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_170, torch.float16), kwargs = {})
#   %split_with_sizes_33 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_126, [112, 112, 112], 1), kwargs = {})
#   %convolution_48 : Tensor "f16[128, 112, 14, 14][21952, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_125, %convert_element_type_127, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 112), kwargs = {})
#   return %buf425
triton_poi_fused_convolution_silu_split_with_sizes_103 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_103', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_103', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 44957696, 'x': 22478848}, 'kernel_num_gb': 0.044957696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_103(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14336
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 112)
    y1 = yindex // 112
    tmp0 = tl.load(in_ptr0 + (x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 112*x2 + 87808*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 14336, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_103.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_103.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.044957696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g7/cg7ouvv2alyqakszbxpg2km2zgylaada4rnw3aywqnpwq4l22lr3.py
# Topologically Sorted Source Nodes: [conv2d_49], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_49 => convert_element_type_128
# Graph fragment:
#   %primals_169 : Tensor "f32[112, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_169]
#   %convert_element_type_128 : Tensor "f16[112, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_169, torch.float16), kwargs = {})
#   return %convert_element_type_128
triton_poi_fused__to_copy_104 = async_compile.triton('triton_poi_fused__to_copy_104', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_104', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 22400}, 'kernel_num_gb': 1.68e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_104(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((112, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_104.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_104.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.68e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/75/c75tilgpq6le3yu3bqxdjizwwsw3bebhtfua6s4dwgpfsayx7tag.py
# Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_49], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_48 => split_with_sizes_33
#   conv2d_49 => convolution_49
#   x_67 => convert_element_type_126, mul_170, sigmoid_16
# Graph fragment:
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_126]
#   %sigmoid_16 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_125,), kwargs = {})
#   %mul_170 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_125, %sigmoid_16), kwargs = {})
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_170, torch.float16), kwargs = {})
#   %split_with_sizes_33 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_126, [112, 112, 112], 1), kwargs = {})
#   %convolution_49 : Tensor "f16[128, 112, 14, 14][21952, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_129, %convert_element_type_128, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 112), kwargs = {})
#   return %buf428
triton_poi_fused_convolution_silu_split_with_sizes_105 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_105', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_105', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 44957696, 'x': 22478848}, 'kernel_num_gb': 0.044957696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_105(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14336
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 112)
    y1 = yindex // 112
    tmp0 = tl.load(in_ptr0 + (87808 + x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 112*x2 + 87808*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 14336, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_105.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_105.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.044957696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zb/czbvcex63uocog3qc4mtqkmhjbfrwdkbiinr4jcecb7ocelosiyl.py
# Topologically Sorted Source Nodes: [conv2d_50], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_50 => convert_element_type_129
# Graph fragment:
#   %primals_170 : Tensor "f32[112, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_170]
#   %convert_element_type_129 : Tensor "f16[112, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_170, torch.float16), kwargs = {})
#   return %convert_element_type_129
triton_poi_fused__to_copy_106 = async_compile.triton('triton_poi_fused__to_copy_106', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_106', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 43904}, 'kernel_num_gb': 3.2928e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_106(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5488
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((112, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((112, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 5488,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_106.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_106.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.2928e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b5/cb5o2kt2pzrzuit4sizcmb5o2d2rj4geqr4oc7attjkwt6p3xs7r.py
# Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_50], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_48 => split_with_sizes_33
#   conv2d_50 => convolution_50
#   x_67 => convert_element_type_126, mul_170, sigmoid_16
# Graph fragment:
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0" = PlaceHolder[target=convert_element_type_126]
#   %sigmoid_16 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_125,), kwargs = {})
#   %mul_170 : Tensor "f32[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_125, %sigmoid_16), kwargs = {})
#   %convert_element_type_126 : Tensor "f16[128, 336, 28, 28][263424, 784, 28, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_170, torch.float16), kwargs = {})
#   %split_with_sizes_33 : [num_users=3] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_126, [112, 112, 112], 1), kwargs = {})
#   %convolution_50 : Tensor "f16[128, 112, 14, 14][21952, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_133, %convert_element_type_129, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 112), kwargs = {})
#   return %buf431
triton_poi_fused_convolution_silu_split_with_sizes_107 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_107', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 1024}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_107', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 44957696, 'x': 22478848}, 'kernel_num_gb': 0.044957696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_107(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14336
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 112)
    y1 = yindex // 112
    tmp0 = tl.load(in_ptr0 + (175616 + x2 + 784*y0 + 263424*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 112*x2 + 87808*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 28, 28), (263424, 784, 28, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 28, 28), (87808, 1, 3136, 112), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 14336, 784,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_107.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_107.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.044957696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/la/cla732rjziujoeiqr5a7pa4trav4ibqbrolvi3xg56fudos6wrhc.py
# Topologically Sorted Source Nodes: [x_68], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_68 => cat_15
# Graph fragment:
#   %convolution_48 : Tensor "f16[128, 112, 14, 14][21952, 1, 1568, 112]cuda:0" = PlaceHolder[target=convolution_48]
#   %convolution_49 : Tensor "f16[128, 112, 14, 14][21952, 1, 1568, 112]cuda:0" = PlaceHolder[target=convolution_49]
#   %convolution_50 : Tensor "f16[128, 112, 14, 14][21952, 1, 1568, 112]cuda:0" = PlaceHolder[target=convolution_50]
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_48, %convolution_49, %convolution_50], 1), kwargs = {})
#   return %cat_15
triton_poi_fused_cat_108 = async_compile.triton('triton_poi_fused_cat_108', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_108', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 84295680}, 'kernel_num_gb': 0.033718272, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_108(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8429568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 336)
    x1 = xindex // 336
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 112, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (112*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 224, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (112*x1 + ((-112) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 336, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tl.load(in_ptr2 + (112*x1 + ((-224) + x0)), tmp11, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tl.where(tmp9, tmp10, tmp14)
    tmp16 = tl.where(tmp4, tmp5, tmp15)
    tl.store(out_ptr0 + (x2), tmp16, None)


def get_args():
    arg_0 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 112, 14, 14), (21952, 1, 1568, 112), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, 8429568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_108.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_108.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033718272
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/67/c67lviy67a67nklxn7iuzqa7cdqu4fv5zy7vjwuaji4srgl7xsae.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_69 => convert_element_type_130, var_mean_22
# Graph fragment:
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=cat_15]
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %var_mean_22 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_130, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf434,%buf435,%buf436
triton_red_fused__native_batch_norm_legit_functional_109 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_109', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_109', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 18439680, 'r0_': 0}, 'kernel_num_gb': 0.017649408, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_109(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 65856
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 43008*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 65856, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_109.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_109.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.017649408
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nr/cnrslfgc356yhorquz3oyzciuwuuvvovt2uwjimd7ldyzkmmwl5m.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_69 => convert_element_type_130, var_mean_22
# Graph fragment:
#   %buf434 : Tensor "f32[1, 336, 1, 1, 196][65856, 1, 65856, 65856, 336]cuda:0" = PlaceHolder[target=buf434]
#   %buf435 : Tensor "f32[1, 336, 1, 1, 196][65856, 1, 65856, 65856, 336]cuda:0" = PlaceHolder[target=buf435]
#   %buf436 : Tensor "f32[1, 336, 1, 1, 196][65856, 1, 65856, 65856, 336]cuda:0" = PlaceHolder[target=buf436]
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %var_mean_22 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_130, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf437,%buf438,%buf439
triton_red_fused__native_batch_norm_legit_functional_110 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_110', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_110', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 806400, 'r0_': 0}, 'kernel_num_gb': 0.000798336, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_110(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 672
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 336*r0_2 + 32928*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 672, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_110.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_110.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000798336
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5v/c5vin7v5ljlw5teucgenftd455uvjjdgqbplbkovsfxwti5tfqlu.py
# Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_69 => add_116, add_117, add_118, convert_element_type_130, mul_172, mul_173, mul_174, mul_175, mul_176, rsqrt_22, squeeze_66, squeeze_68, var_mean_22
# Graph fragment:
#   %buf437 : Tensor "f32[1, 336, 1, 1, 2][672, 1, 672, 672, 336]cuda:0" = PlaceHolder[target=buf437]
#   %buf438 : Tensor "f32[1, 336, 1, 1, 2][672, 1, 672, 672, 336]cuda:0" = PlaceHolder[target=buf438]
#   %buf439 : Tensor "f32[1, 336, 1, 1, 2][672, 1, 672, 672, 336]cuda:0" = PlaceHolder[target=buf439]
#   %buf441 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=buf441]
#   %getitem_135 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_135]
#   %copy__67 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=copy__67]
#   %add_117 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=add_117]
#   %copy__68 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=copy__68]
#   %add_118 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=add_118]
#   %convert_element_type_130 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_15, torch.float32), kwargs = {})
#   %var_mean_22 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_130, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_116 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_134, 1e-05), kwargs = {})
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_116,), kwargs = {})
#   %squeeze_66 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_135, [0, 2, 3]), kwargs = {})
#   %mul_172 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_66, 0.1), kwargs = {})
#   %mul_173 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_172, 0.9), kwargs = {})
#   %add_117 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_172, %mul_173), kwargs = {})
#   %squeeze_68 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_134, [0, 2, 3]), kwargs = {})
#   %mul_174 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_68, 1.0000398612827361), kwargs = {})
#   %mul_175 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_174, 0.1), kwargs = {})
#   %mul_176 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_173, 0.9), kwargs = {})
#   %add_118 : Tensor "f32[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_175, %mul_176), kwargs = {})
#   %copy__67 : Tensor "f32[336][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_172, %add_117), kwargs = {})
#   %copy__68 : Tensor "f32[336][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_173, %add_118), kwargs = {})
#   return %getitem_135,%buf441,%rsqrt_22,%add_117,%buf1373,%add_118,%buf1376
triton_per_fused__native_batch_norm_legit_functional_copy__111 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__111', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 2},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__111', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21504, 'r0_': 0}, 'kernel_num_gb': 1.6128e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__111(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 336
    r0_numel = 2
    R0_BLOCK: tl.constexpr = 2
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 336*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 25088.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000398612827361
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 336, 2,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__111.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__111.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.6128e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jq/cjqsei72hqjgdynehjgynbo5djqqikqupdfmytu2wup7ntikfowi.py
# Topologically Sorted Source Nodes: [x_69, x_70], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_69 => add_119, convert_element_type_131, mul_171, mul_177, sub_22, unsqueeze_88, unsqueeze_89, unsqueeze_90, unsqueeze_91
#   x_70 => convert_element_type_132
# Graph fragment:
#   %cat_15 : Tensor "f16[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=cat_15]
#   %getitem_135 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=getitem_135]
#   %rsqrt_22 : Tensor "f32[1, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=rsqrt_22]
#   %primals_174 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_174]
#   %primals_175 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_175]
#   %sub_22 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_15, %getitem_135), kwargs = {})
#   %mul_171 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_22, %rsqrt_22), kwargs = {})
#   %unsqueeze_88 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_174, -1), kwargs = {})
#   %unsqueeze_89 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_88, -1), kwargs = {})
#   %mul_177 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_171, %unsqueeze_89), kwargs = {})
#   %unsqueeze_90 : Tensor "f32[336, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_175, -1), kwargs = {})
#   %unsqueeze_91 : Tensor "f32[336, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_90, -1), kwargs = {})
#   %add_119 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_177, %unsqueeze_91), kwargs = {})
#   %convert_element_type_131 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_119, torch.float16), kwargs = {})
#   %convert_element_type_132 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_131, torch.float32), kwargs = {})
#   return %convert_element_type_132
triton_poi_fused__native_batch_norm_legit_functional_silu_112 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_112', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_112', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 84301056}, 'kernel_num_gb': 0.050582784, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_112(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8429568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 8429568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_112.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_112.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.050582784
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/hk/chkfvlnzkp7k6umi4ziqivxqzvdsvxqvjibmhp55rxpovy6k6lhl.py
# Topologically Sorted Source Nodes: [x_70, x_se_16], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_70 => convert_element_type_133, mul_178, sigmoid_17
#   x_se_16 => mean_4
# Graph fragment:
#   %convert_element_type_132 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=convert_element_type_132]
#   %buf445 : Tensor "f32[128, 336, 1, 1][336, 1, 43008, 43008]cuda:0" = PlaceHolder[target=buf445]
#   %sigmoid_17 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_132,), kwargs = {})
#   %mul_178 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_132, %sigmoid_17), kwargs = {})
#   %convert_element_type_133 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_178, torch.float16), kwargs = {})
#   %mean_4 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_133, [2, 3], True), kwargs = {})
#   return %buf445,%mean_4
triton_red_fused_mean_silu_113 = async_compile.triton('triton_red_fused_mean_silu_113', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_silu_113', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33890304, 'r0_': 0}, 'kernel_num_gb': 0.033804288, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mean_silu_113(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 43008
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 336)
    x1 = xindex // 336
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 336*r0_2 + 65856*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.sigmoid(tmp0)
        tmp2 = tmp0 * tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp3.to(tl.float32)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = 196.0
    tmp9 = (tmp6 / tmp8)
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 43008, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_silu_113.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mean_silu_113.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.033804288
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nx/cnxh3ubef57uya5gqqvc4vkq6mewbaylwpalvynxobirjnjkcf7v.py
# Topologically Sorted Source Nodes: [x_se_17], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_17 => convert_element_type_135
# Graph fragment:
#   %primals_176 : Tensor "f32[14, 336, 1, 1][336, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_176]
#   %convert_element_type_135 : Tensor "f16[14, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_176, torch.float16), kwargs = {})
#   return %convert_element_type_135
triton_poi_fused__to_copy_114 = async_compile.triton('triton_poi_fused__to_copy_114', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_114', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 37632}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_114(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((14, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((14, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_114.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_114.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ij/cij4u2wenm24mq47xttogwcvors47rj4v6uuo6ifz3nyt5c2xrqr.py
# Topologically Sorted Source Nodes: [x_se_17, x_se_18], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_17 => convert_element_type_134, convolution_51
#   x_se_18 => convert_element_type_136, convert_element_type_137, mul_179, sigmoid_18
# Graph fragment:
#   %buf448 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0" = PlaceHolder[target=buf448]
#   %primals_177 : Tensor "f32[14][1]cuda:0" = PlaceHolder[target=primals_177]
#   %convolution_51 : Tensor "f16[128, 14, 1, 1][14, 1, 14, 14]cuda:0" = PlaceHolder[target=convolution_51]
#   %convert_element_type_134 : Tensor "f16[14][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_177, torch.float16), kwargs = {})
#   %convolution_51 : Tensor "f16[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_4, %convert_element_type_135, %convert_element_type_134, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_136 : Tensor "f32[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_51, torch.float32), kwargs = {})
#   %sigmoid_18 : Tensor "f32[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_136,), kwargs = {})
#   %mul_179 : Tensor "f32[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_136, %sigmoid_18), kwargs = {})
#   %convert_element_type_137 : Tensor "f16[128, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_179, torch.float16), kwargs = {})
#   return %convolution_51,%convert_element_type_137
triton_poi_fused__to_copy_convolution_silu_115 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_115', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_115', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 17976}, 'kernel_num_gb': 1.0808e-05, 'kernel_flop': 1204224},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_115(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1792
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 14)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((14,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 1792,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_115.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_115.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.0808e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nu/cnu56gta3rdxnbjjit2tw5asixq6inoxyynlghfycmarc7vm4aiz.py
# Topologically Sorted Source Nodes: [x_se_19], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_19 => convert_element_type_139
# Graph fragment:
#   %primals_178 : Tensor "f32[336, 14, 1, 1][14, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_178]
#   %convert_element_type_139 : Tensor "f16[336, 14, 1, 1][14, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_178, torch.float16), kwargs = {})
#   return %convert_element_type_139
triton_poi_fused__to_copy_116 = async_compile.triton('triton_poi_fused__to_copy_116', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_116', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 37632}, 'kernel_num_gb': 2.8224e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_116(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4704
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((336, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((336, 14, 1, 1), (14, 1, 14, 14), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 4704,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_116.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_116.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.8224e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/67/c67d4biusxendejsnmmdoyt6fg5xp4p3jjzxiphcgoenw66pz6dt.py
# Topologically Sorted Source Nodes: [x_se_19], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_19 => convert_element_type_138, convolution_52
# Graph fragment:
#   %buf452 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=buf452]
#   %primals_179 : Tensor "f32[336][1]cuda:0" = PlaceHolder[target=primals_179]
#   %convert_element_type_138 : Tensor "f16[336][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_179, torch.float16), kwargs = {})
#   %convolution_52 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_137, %convert_element_type_139, %convert_element_type_138, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_52
triton_poi_fused__to_copy_convolution_117 = async_compile.triton('triton_poi_fused__to_copy_convolution_117', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_117', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 259392}, 'kernel_num_gb': 0.000173376, 'kernel_flop': 1204224},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_117(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 43008
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 336)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((336,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 43008,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_117.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_117.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000173376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/is/cise52stgon7jd5ew5t5vi2rep3qq7anokmdmh3fh5tzugp2c6zd.py
# Topologically Sorted Source Nodes: [x_70, sigmoid_4, x_71], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_4 => sigmoid_19
#   x_70 => convert_element_type_133, mul_178, sigmoid_17
#   x_71 => mul_180
# Graph fragment:
#   %convert_element_type_132 : Tensor "f32[128, 336, 14, 14][65856, 1, 4704, 336]cuda:0" = PlaceHolder[target=convert_element_type_132]
#   %convolution_52 : Tensor "f16[128, 336, 1, 1][336, 1, 336, 336]cuda:0" = PlaceHolder[target=convolution_52]
#   %sigmoid_17 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_132,), kwargs = {})
#   %mul_178 : Tensor "f32[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_132, %sigmoid_17), kwargs = {})
#   %convert_element_type_133 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_178, torch.float16), kwargs = {})
#   %sigmoid_19 : Tensor "f16[128, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_52,), kwargs = {})
#   %mul_180 : Tensor "f16[128, 336, 14, 14][65856, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_133, %sigmoid_19), kwargs = {})
#   return %mul_180
triton_poi_fused_mul_sigmoid_silu_118 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_118', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_118', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 67522560}, 'kernel_num_gb': 0.050663424, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_118(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 8429568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 336)
    x2 = xindex // 65856
    tmp0 = tl.load(in_ptr0 + (x3), None)
    tmp4 = tl.load(in_ptr1 + (x0 + 336*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (x3), tmp6, None)


def get_args():
    arg_0 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 336, 14, 14), (65856, 1, 4704, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 8429568,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_118.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_118.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.050663424
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lh/clhed2wbxn7n6fsmbadgulkfmeaa66fneaawfoycgs2nl4u4lele.py
# Topologically Sorted Source Nodes: [x_72], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_72 => convert_element_type_140
# Graph fragment:
#   %primals_180 : Tensor "f32[104, 336, 1, 1][336, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_180]
#   %convert_element_type_140 : Tensor "f16[104, 336, 1, 1][336, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_180, torch.float16), kwargs = {})
#   return %convert_element_type_140
triton_poi_fused__to_copy_119 = async_compile.triton('triton_poi_fused__to_copy_119', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_119', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 279552}, 'kernel_num_gb': 0.000209664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_119(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 34944
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((104, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((104, 336, 1, 1), (336, 1, 336, 336), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 34944,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_119.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_119.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000209664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/i2/ci2inlkknk3yp64j5vohxjxkxwhlt663jlucrh2uzbohr3mv2y7f.py
# Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_73 => convert_element_type_141, var_mean_23
# Graph fragment:
#   %convolution_53 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convolution_53]
#   %convert_element_type_141 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_53, torch.float32), kwargs = {})
#   %var_mean_23 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_141, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf457,%buf458,%buf459
triton_red_fused__native_batch_norm_legit_functional_120 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_120', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_120', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 5707520, 'r0_': 0}, 'kernel_num_gb': 0.005462912, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_120(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 20384
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 104)
    x1 = xindex // 104
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_2 + 13312*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 20384, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_120.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_120.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.005462912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rr/crrj2efvbztoysvlsqfrxov6btfw4e6kcjivn4onnaxzfcsxhhx3.py
# Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_73 => convert_element_type_141, var_mean_23
# Graph fragment:
#   %buf457 : Tensor "f32[1, 104, 1, 1, 196][20384, 1, 20384, 20384, 104]cuda:0" = PlaceHolder[target=buf457]
#   %buf458 : Tensor "f32[1, 104, 1, 1, 196][20384, 1, 20384, 20384, 104]cuda:0" = PlaceHolder[target=buf458]
#   %buf459 : Tensor "f32[1, 104, 1, 1, 196][20384, 1, 20384, 20384, 104]cuda:0" = PlaceHolder[target=buf459]
#   %convert_element_type_141 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_53, torch.float32), kwargs = {})
#   %var_mean_23 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_141, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf460,%buf461,%buf462
triton_red_fused__native_batch_norm_legit_functional_121 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_121', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 256, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_121', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 249600, 'r0_': 0}, 'kernel_num_gb': 0.000247104, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_121(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 208
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 104)
    x1 = xindex // 104
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_2 + 10192*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 104*r0_2 + 10192*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 104*r0_2 + 10192*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 208, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_121.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_121.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000247104
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2p/c2piuv2scruuyssinlf3zek46si24bcekfvlu7qmbdiikbwmct74.py
# Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_73 => add_121, add_122, add_123, convert_element_type_141, mul_182, mul_183, mul_184, mul_185, mul_186, rsqrt_23, squeeze_69, squeeze_71, var_mean_23
# Graph fragment:
#   %buf460 : Tensor "f32[1, 104, 1, 1, 2][208, 1, 208, 208, 104]cuda:0" = PlaceHolder[target=buf460]
#   %buf461 : Tensor "f32[1, 104, 1, 1, 2][208, 1, 208, 208, 104]cuda:0" = PlaceHolder[target=buf461]
#   %buf462 : Tensor "f32[1, 104, 1, 1, 2][208, 1, 208, 208, 104]cuda:0" = PlaceHolder[target=buf462]
#   %buf464 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=buf464]
#   %getitem_137 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=getitem_137]
#   %copy__70 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=copy__70]
#   %add_122 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=add_122]
#   %copy__71 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=copy__71]
#   %add_123 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=add_123]
#   %convert_element_type_141 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_53, torch.float32), kwargs = {})
#   %var_mean_23 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_141, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_121 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_136, 1e-05), kwargs = {})
#   %rsqrt_23 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_121,), kwargs = {})
#   %squeeze_69 : Tensor "f32[104][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_137, [0, 2, 3]), kwargs = {})
#   %mul_182 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_69, 0.1), kwargs = {})
#   %mul_183 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_182, 0.9), kwargs = {})
#   %add_122 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_182, %mul_183), kwargs = {})
#   %squeeze_71 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_136, [0, 2, 3]), kwargs = {})
#   %mul_184 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_71, 1.0000398612827361), kwargs = {})
#   %mul_185 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_184, 0.1), kwargs = {})
#   %mul_186 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_183, 0.9), kwargs = {})
#   %add_123 : Tensor "f32[104][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_185, %mul_186), kwargs = {})
#   %copy__70 : Tensor "f32[104][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_182, %add_122), kwargs = {})
#   %copy__71 : Tensor "f32[104][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_183, %add_123), kwargs = {})
#   return %getitem_137,%buf464,%rsqrt_23,%add_122,%buf1381,%add_123,%buf1384
triton_per_fused__native_batch_norm_legit_functional_copy__122 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__122', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 128, 'r0_': 2},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__122', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 7488, 'r0_': 0}, 'kernel_num_gb': 5.408e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__122(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 104
    r0_numel = 2
    R0_BLOCK: tl.constexpr = 2
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 104*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 104*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 104*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 25088.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000398612827361
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 104, 2,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__122.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__122.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.408e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zz/czzqy4ag4jfv3zcnsnidkiz4dakgg5uk3mx4pxqj2xeirxknqvre.py
# Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_73 => add_121, add_124, convert_element_type_141, convert_element_type_142, mul_181, mul_187, rsqrt_23, sub_23, unsqueeze_92, unsqueeze_93, unsqueeze_94, unsqueeze_95, var_mean_23
# Graph fragment:
#   %convolution_53 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convolution_53]
#   %getitem_137 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=getitem_137]
#   %buf464 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=buf464]
#   %primals_184 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_184]
#   %primals_185 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_185]
#   %convert_element_type_141 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_53, torch.float32), kwargs = {})
#   %var_mean_23 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_141, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_121 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_136, 1e-05), kwargs = {})
#   %rsqrt_23 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_121,), kwargs = {})
#   %sub_23 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_53, %getitem_137), kwargs = {})
#   %mul_181 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_23, %rsqrt_23), kwargs = {})
#   %unsqueeze_92 : Tensor "f32[104, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_184, -1), kwargs = {})
#   %unsqueeze_93 : Tensor "f32[104, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_92, -1), kwargs = {})
#   %mul_187 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_181, %unsqueeze_93), kwargs = {})
#   %unsqueeze_94 : Tensor "f32[104, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_185, -1), kwargs = {})
#   %unsqueeze_95 : Tensor "f32[104, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_94, -1), kwargs = {})
#   %add_124 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_187, %unsqueeze_95), kwargs = {})
#   %convert_element_type_142 : Tensor "f16[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_124, torch.float16), kwargs = {})
#   return %convert_element_type_142
triton_poi_fused__native_batch_norm_legit_functional_123 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_123', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_123', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 15656576}, 'kernel_num_gb': 0.010438272, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_123(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 25088.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_123.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_123.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.010438272
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wg/cwgkqi74ne2ojnaj7vhq63g4gb2b5fzbv7qqoadmlwdnpeba2tme.py
# Topologically Sorted Source Nodes: [split_16, conv2d_54], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_54 => convolution_54
#   split_16 => getitem_138, split_with_sizes_36
# Graph fragment:
#   %convert_element_type_142 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convert_element_type_142]
#   %getitem_138 : Tensor "f16[128, 52, 14, 14][10240, 196, 14, 1]cuda:0" = PlaceHolder[target=getitem_138]
#   %split_with_sizes_36 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_142, [52, 52], 1), kwargs = {})
#   %getitem_138 : Tensor "f16[128, 52, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_36, 0), kwargs = {})
#   %convolution_54 : Tensor "f16[128, 312, 14, 14][61152, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_138, %convert_element_type_143, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_138,%buf471
triton_poi_fused_convolution_split_with_sizes_124 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_124', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_124', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 7827456, 'x': 5218304}, 'kernel_num_gb': 0.007827456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_124(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6656
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 52)
    y1 = yindex // 52
    tmp0 = tl.load(in_ptr0 + (y0 + 104*x2 + 20384*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y0 + 10240*y1), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 52*x2 + 10192*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 6656, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_124.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_124.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.007827456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3x/c3x6oitwktxl6exaaqeu3e2b6sknyf47fzm2x3dacbuwoocizrqo.py
# Topologically Sorted Source Nodes: [split_16, conv2d_55], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_55 => convolution_55
#   split_16 => getitem_139, split_with_sizes_36
# Graph fragment:
#   %convert_element_type_142 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convert_element_type_142]
#   %getitem_139 : Tensor "f16[128, 52, 14, 14][10240, 196, 14, 1]cuda:0" = PlaceHolder[target=getitem_139]
#   %split_with_sizes_36 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_142, [52, 52], 1), kwargs = {})
#   %getitem_139 : Tensor "f16[128, 52, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_36, 1), kwargs = {})
#   %convolution_55 : Tensor "f16[128, 312, 14, 14][61152, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_139, %convert_element_type_144, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_139,%buf474
triton_poi_fused_convolution_split_with_sizes_125 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_125', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_125', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 7827456, 'x': 5218304}, 'kernel_num_gb': 0.007827456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_125(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6656
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 52)
    y1 = yindex // 52
    tmp0 = tl.load(in_ptr0 + (52 + y0 + 104*x2 + 20384*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y0 + 10240*y1), tmp0, xmask & ymask)
    tl.store(out_ptr1 + (y0 + 52*x2 + 10192*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10240, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 6656, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_125.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_125.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.007827456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/23/c23g52yqc6wdyamyfn3gqj6ksoya3l4i37sxkqiodgouf3n5pec4.py
# Topologically Sorted Source Nodes: [conv2d_54], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_54 => convert_element_type_143
# Graph fragment:
#   %primals_186 : Tensor "f32[312, 52, 1, 1][52, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_186]
#   %convert_element_type_143 : Tensor "f16[312, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_186, torch.float16), kwargs = {})
#   return %convert_element_type_143
triton_poi_fused__to_copy_126 = async_compile.triton('triton_poi_fused__to_copy_126', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_126', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 129792}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_126(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((312, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_126.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_126.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6n/c6n3oo4jwndnniqd2zcgg7z55vn7vfzylbmjmbt3iu4xvl7gl3ph.py
# Topologically Sorted Source Nodes: [x_74], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_74 => cat_16
# Graph fragment:
#   %convolution_54 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=convolution_54]
#   %convolution_55 : Tensor "f16[128, 312, 14, 14][61152, 1, 4368, 312]cuda:0" = PlaceHolder[target=convolution_55]
#   %cat_16 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_54, %convolution_55], 1), kwargs = {})
#   return %cat_16
triton_poi_fused_cat_127 = async_compile.triton('triton_poi_fused_cat_127', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_127', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 125239296}, 'kernel_num_gb': 0.062619648, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_127(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 624)
    x1 = xindex // 624
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 312, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (312*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 624, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (312*x1 + ((-312) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_127.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_127.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062619648
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/md/cmdk4tt2dhxguxhqfn3vnj5wqmvugtrltusizrj45cn3jxyz7asc.py
# Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_75 => convert_element_type_145, var_mean_24
# Graph fragment:
#   %cat_16 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_16]
#   %convert_element_type_145 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_16, torch.float32), kwargs = {})
#   %var_mean_24 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_145, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf477,%buf478,%buf479
triton_red_fused__native_batch_norm_legit_functional_128 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_128', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_128', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 34245120, 'r0_': 0}, 'kernel_num_gb': 0.032777472, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_128(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122304
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 79872*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122304, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_128.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_128.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.032777472
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/tn/ctnkrvjm7knvipo72wvibscaxvpk37otxv3z6zhq3jttz5n5w5lb.py
# Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_75 => convert_element_type_145, var_mean_24
# Graph fragment:
#   %buf477 : Tensor "f32[1, 624, 1, 1, 196][122304, 1, 122304, 122304, 624]cuda:0" = PlaceHolder[target=buf477]
#   %buf478 : Tensor "f32[1, 624, 1, 1, 196][122304, 1, 122304, 122304, 624]cuda:0" = PlaceHolder[target=buf478]
#   %buf479 : Tensor "f32[1, 624, 1, 1, 196][122304, 1, 122304, 122304, 624]cuda:0" = PlaceHolder[target=buf479]
#   %convert_element_type_145 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_16, torch.float32), kwargs = {})
#   %var_mean_24 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_145, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf480,%buf481,%buf482
triton_red_fused__native_batch_norm_legit_functional_129 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_129', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 2048, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_129', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1497600, 'r0_': 0}, 'kernel_num_gb': 0.001482624, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_129(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 1248
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 61152*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 624*r0_2 + 61152*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 624*r0_2 + 61152*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 1248, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_129.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_129.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001482624
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3j/c3jd475ekpycysr3qykcomcseioj5m46jwofdsvhpsi2hklxqg7p.py
# Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_75 => add_126, add_127, add_128, convert_element_type_145, mul_189, mul_190, mul_191, mul_192, mul_193, rsqrt_24, squeeze_72, squeeze_74, var_mean_24
# Graph fragment:
#   %buf480 : Tensor "f32[1, 624, 1, 1, 2][1248, 1, 1248, 1248, 624]cuda:0" = PlaceHolder[target=buf480]
#   %buf481 : Tensor "f32[1, 624, 1, 1, 2][1248, 1, 1248, 1248, 624]cuda:0" = PlaceHolder[target=buf481]
#   %buf482 : Tensor "f32[1, 624, 1, 1, 2][1248, 1, 1248, 1248, 624]cuda:0" = PlaceHolder[target=buf482]
#   %buf484 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=buf484]
#   %getitem_141 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_141]
#   %copy__73 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=copy__73]
#   %add_127 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=add_127]
#   %copy__74 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=copy__74]
#   %add_128 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=add_128]
#   %convert_element_type_145 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_16, torch.float32), kwargs = {})
#   %var_mean_24 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_145, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_126 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_140, 1e-05), kwargs = {})
#   %rsqrt_24 : Tensor "f32[1, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_126,), kwargs = {})
#   %squeeze_72 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_141, [0, 2, 3]), kwargs = {})
#   %mul_189 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_72, 0.1), kwargs = {})
#   %mul_190 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_189, 0.9), kwargs = {})
#   %add_127 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_189, %mul_190), kwargs = {})
#   %squeeze_74 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_140, [0, 2, 3]), kwargs = {})
#   %mul_191 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_74, 1.0000398612827361), kwargs = {})
#   %mul_192 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_191, 0.1), kwargs = {})
#   %mul_193 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_190, 0.9), kwargs = {})
#   %add_128 : Tensor "f32[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_192, %mul_193), kwargs = {})
#   %copy__73 : Tensor "f32[624][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_189, %add_127), kwargs = {})
#   %copy__74 : Tensor "f32[624][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_190, %add_128), kwargs = {})
#   return %getitem_141,%buf484,%rsqrt_24,%add_127,%buf1389,%add_128,%buf1392
triton_per_fused__native_batch_norm_legit_functional_copy__130 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__130', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 2},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__130', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 39936, 'r0_': 0}, 'kernel_num_gb': 2.9952e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__130(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 624
    r0_numel = 2
    R0_BLOCK: tl.constexpr = 2
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 624*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 624*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 25088.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000398612827361
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 624, 2,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__130.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__130.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.9952e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2y/c2yc7lfwv2aaervzsfsfjqrsss3qtepewud767kxlqi4fzadxiae.py
# Topologically Sorted Source Nodes: [x_75, x_76], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_75 => add_129, convert_element_type_146, mul_188, mul_194, sub_24, unsqueeze_96, unsqueeze_97, unsqueeze_98, unsqueeze_99
#   x_76 => convert_element_type_147, convert_element_type_148, mul_195, sigmoid_20
# Graph fragment:
#   %cat_16 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_16]
#   %getitem_141 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_141]
#   %rsqrt_24 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_24]
#   %primals_191 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_191]
#   %primals_192 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_192]
#   %convert_element_type_147 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_147]
#   %sub_24 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_16, %getitem_141), kwargs = {})
#   %mul_188 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_24, %rsqrt_24), kwargs = {})
#   %unsqueeze_96 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_191, -1), kwargs = {})
#   %unsqueeze_97 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_96, -1), kwargs = {})
#   %mul_194 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_188, %unsqueeze_97), kwargs = {})
#   %unsqueeze_98 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_192, -1), kwargs = {})
#   %unsqueeze_99 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_98, -1), kwargs = {})
#   %add_129 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_194, %unsqueeze_99), kwargs = {})
#   %convert_element_type_146 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_129, torch.float16), kwargs = {})
#   %convert_element_type_147 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_146, torch.float32), kwargs = {})
#   %sigmoid_20 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_147,), kwargs = {})
#   %mul_195 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_147, %sigmoid_20), kwargs = {})
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_195, torch.float16), kwargs = {})
#   return %convert_element_type_147,%convert_element_type_148
triton_poi_fused__native_batch_norm_legit_functional_silu_131 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_131', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_131', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 62619648, 'x': 31319808}, 'kernel_num_gb': 0.062629632, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_131(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 624
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 624*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 196*x1 + 122304*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 624,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_131.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_131.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062629632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ct/cctszfcgmfxh2mkdcevs3erdokw6ahhbiyy4absztehczzyhjzsx.py
# Topologically Sorted Source Nodes: [conv2d_56], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_56 => convert_element_type_149
# Graph fragment:
#   %primals_193 : Tensor "f32[156, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_193]
#   %convert_element_type_149 : Tensor "f16[156, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_193, torch.float16), kwargs = {})
#   return %convert_element_type_149
triton_poi_fused__to_copy_132 = async_compile.triton('triton_poi_fused__to_copy_132', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2048}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_132', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 11232}, 'kernel_num_gb': 8.424e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_132(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1404
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((156, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1404,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_132.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_132.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.424e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3p/c3poxkxkxau235yrf2eqgnnxh6thppr4z4safeuvmxfcw2dxk6u5.py
# Topologically Sorted Source Nodes: [x_76, conv2d_56], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_56 => convolution_56, split_with_sizes_38
#   x_76 => convert_element_type_148, mul_195, sigmoid_20
# Graph fragment:
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_148]
#   %sigmoid_20 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_147,), kwargs = {})
#   %mul_195 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_147, %sigmoid_20), kwargs = {})
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_195, torch.float16), kwargs = {})
#   %split_with_sizes_38 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_148, [156, 156, 156, 156], 1), kwargs = {})
#   %convolution_56 : Tensor "f16[128, 156, 14, 14][30576, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_146, %convert_element_type_149, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 156), kwargs = {})
#   return %buf490
triton_poi_fused_convolution_silu_split_with_sizes_133 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_133', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_133', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_133(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_133.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_133.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/47/c47ghrnykhpcwcb6wzecupsl75irwof2nfgen3gfq3xuder6xq3r.py
# Topologically Sorted Source Nodes: [conv2d_57], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_57 => convert_element_type_150
# Graph fragment:
#   %primals_194 : Tensor "f32[156, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_194]
#   %convert_element_type_150 : Tensor "f16[156, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_194, torch.float16), kwargs = {})
#   return %convert_element_type_150
triton_poi_fused__to_copy_134 = async_compile.triton('triton_poi_fused__to_copy_134', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_134', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 31200}, 'kernel_num_gb': 2.34e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_134(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3900
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((156, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3900,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_134.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_134.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.34e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rf/crf5qaodtc5retvs2sj6xhimzxlga4pyo4m7gqc66q67lvrv7dja.py
# Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_57], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_56 => split_with_sizes_38
#   conv2d_57 => convolution_57
#   x_76 => convert_element_type_148, mul_195, sigmoid_20
# Graph fragment:
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_148]
#   %sigmoid_20 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_147,), kwargs = {})
#   %mul_195 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_147, %sigmoid_20), kwargs = {})
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_195, torch.float16), kwargs = {})
#   %split_with_sizes_38 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_148, [156, 156, 156, 156], 1), kwargs = {})
#   %convolution_57 : Tensor "f16[128, 156, 14, 14][30576, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_151, %convert_element_type_150, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 156), kwargs = {})
#   return %buf493
triton_poi_fused_convolution_silu_split_with_sizes_135 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_135', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_135', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_135(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (30576 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_135.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_135.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6c/c6cscubnrr46io3odddm2zna4occcwcdhz4ixo75zrxffugmov2i.py
# Topologically Sorted Source Nodes: [conv2d_58], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_58 => convert_element_type_151
# Graph fragment:
#   %primals_195 : Tensor "f32[156, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_195]
#   %convert_element_type_151 : Tensor "f16[156, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_195, torch.float16), kwargs = {})
#   return %convert_element_type_151
triton_poi_fused__to_copy_136 = async_compile.triton('triton_poi_fused__to_copy_136', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_136', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 61152}, 'kernel_num_gb': 4.5864e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_136(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 7644
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((156, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 7644,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_136.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_136.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 4.5864e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lh/clhjpzcs7avnubfdql34izddbgd7o276qzgpucfugmvnpkqbon5j.py
# Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_58], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_56 => split_with_sizes_38
#   conv2d_58 => convolution_58
#   x_76 => convert_element_type_148, mul_195, sigmoid_20
# Graph fragment:
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_148]
#   %sigmoid_20 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_147,), kwargs = {})
#   %mul_195 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_147, %sigmoid_20), kwargs = {})
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_195, torch.float16), kwargs = {})
#   %split_with_sizes_38 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_148, [156, 156, 156, 156], 1), kwargs = {})
#   %convolution_58 : Tensor "f16[128, 156, 14, 14][30576, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_156, %convert_element_type_151, None, [1, 1], [3, 3], [1, 1], False, [0, 0], 156), kwargs = {})
#   return %buf496
triton_poi_fused_convolution_silu_split_with_sizes_137 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_137', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_137', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_137(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (61152 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_137.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_137.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/t7/ct7nvwgqscp6udx5j5zoi55lqzovr25ywovac4tx2frvi3gwnn72.py
# Topologically Sorted Source Nodes: [conv2d_59], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_59 => convert_element_type_152
# Graph fragment:
#   %primals_196 : Tensor "f32[156, 1, 9, 9][81, 81, 9, 1]cuda:0" = PlaceHolder[target=primals_196]
#   %convert_element_type_152 : Tensor "f16[156, 1, 9, 9][81, 81, 9, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_196, torch.float16), kwargs = {})
#   return %convert_element_type_152
triton_poi_fused__to_copy_138 = async_compile.triton('triton_poi_fused__to_copy_138', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_138', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 101088}, 'kernel_num_gb': 7.5816e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_138(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12636
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((156, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((156, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 12636,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_138.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_138.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.5816e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/bi/cbik2mrzyaybnzj247gb6yzts7ricnvi4qhyi46ns5mfoxzjkqyt.py
# Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_59], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_56 => split_with_sizes_38
#   conv2d_59 => convolution_59
#   x_76 => convert_element_type_148, mul_195, sigmoid_20
# Graph fragment:
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_148]
#   %sigmoid_20 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_147,), kwargs = {})
#   %mul_195 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_147, %sigmoid_20), kwargs = {})
#   %convert_element_type_148 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_195, torch.float16), kwargs = {})
#   %split_with_sizes_38 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_148, [156, 156, 156, 156], 1), kwargs = {})
#   %convolution_59 : Tensor "f16[128, 156, 14, 14][30576, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_161, %convert_element_type_152, None, [1, 1], [4, 4], [1, 1], False, [0, 0], 156), kwargs = {})
#   return %buf499
triton_poi_fused_convolution_silu_split_with_sizes_139 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_139', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_139', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 15654912, 'x': 7827456}, 'kernel_num_gb': 0.015654912, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_139(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 19968
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 156)
    y1 = yindex // 156
    tmp0 = tl.load(in_ptr0 + (91728 + x2 + 196*y0 + 122304*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 156*x2 + 30576*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19968, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_139.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_139.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015654912
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g4/cg43ssj7hnkhinkbhzhyralcxcm672k246yy5gjusqmlj7bq2kyj.py
# Topologically Sorted Source Nodes: [x_77], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_77 => cat_17
# Graph fragment:
#   %convolution_56 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=convolution_56]
#   %convolution_57 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=convolution_57]
#   %convolution_58 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=convolution_58]
#   %convolution_59 : Tensor "f16[128, 156, 14, 14][30576, 1, 2184, 156]cuda:0" = PlaceHolder[target=convolution_59]
#   %cat_17 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_56, %convolution_57, %convolution_58, %convolution_59], 1), kwargs = {})
#   return %cat_17
triton_poi_fused_cat_140 = async_compile.triton('triton_poi_fused_cat_140', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_140', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 187858944}, 'kernel_num_gb': 0.062619648, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_140(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 624)
    x1 = xindex // 624
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 156, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (156*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 312, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (156*x1 + ((-156) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 468, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (156*x1 + ((-312) + x0)), tmp14, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 624, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (156*x1 + ((-468) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, None)


def get_args():
    arg_0 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 156, 14, 14), (30576, 1, 2184, 156), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_140.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_140.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062619648
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/23/c237meaqye7m6jcyfbrk7lz5cd6tvew6se66yl7esq3jfjdf33ma.py
# Topologically Sorted Source Nodes: [x_78, x_79], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_78 => add_134, convert_element_type_154, mul_196, mul_202, sub_25, unsqueeze_100, unsqueeze_101, unsqueeze_102, unsqueeze_103
#   x_79 => convert_element_type_155
# Graph fragment:
#   %cat_17 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=cat_17]
#   %getitem_163 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_163]
#   %rsqrt_25 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_25]
#   %primals_200 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_200]
#   %primals_201 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_201]
#   %sub_25 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_17, %getitem_163), kwargs = {})
#   %mul_196 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_25, %rsqrt_25), kwargs = {})
#   %unsqueeze_100 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_200, -1), kwargs = {})
#   %unsqueeze_101 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_100, -1), kwargs = {})
#   %mul_202 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_196, %unsqueeze_101), kwargs = {})
#   %unsqueeze_102 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_201, -1), kwargs = {})
#   %unsqueeze_103 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_102, -1), kwargs = {})
#   %add_134 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_202, %unsqueeze_103), kwargs = {})
#   %convert_element_type_154 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_134, torch.float16), kwargs = {})
#   %convert_element_type_155 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_154, torch.float32), kwargs = {})
#   return %convert_element_type_155
triton_poi_fused__native_batch_norm_legit_functional_silu_141 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_141', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_141', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 156559104}, 'kernel_num_gb': 0.093939456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_141(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_141.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_141.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.093939456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vr/cvrwukzcajhtqnbktgsubs4mmmobah2tk6avohgorzbrsjqe7eyt.py
# Topologically Sorted Source Nodes: [x_79, x_se_20], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_79 => convert_element_type_156, mul_203, sigmoid_21
#   x_se_20 => mean_5
# Graph fragment:
#   %convert_element_type_155 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_155]
#   %buf513 : Tensor "f32[128, 624, 1, 1][624, 1, 79872, 79872]cuda:0" = PlaceHolder[target=buf513]
#   %sigmoid_21 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_155,), kwargs = {})
#   %mul_203 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_155, %sigmoid_21), kwargs = {})
#   %convert_element_type_156 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_203, torch.float16), kwargs = {})
#   %mean_5 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_156, [2, 3], True), kwargs = {})
#   return %buf513,%mean_5
triton_red_fused_mean_silu_142 = async_compile.triton('triton_red_fused_mean_silu_142', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_silu_142', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 62939136, 'r0_': 0}, 'kernel_num_gb': 0.062779392, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mean_silu_142(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 79872
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 624)
    x1 = xindex // 624
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 624*r0_2 + 122304*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.sigmoid(tmp0)
        tmp2 = tmp0 * tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp3.to(tl.float32)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = 196.0
    tmp9 = (tmp6 / tmp8)
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 79872, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_silu_142.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mean_silu_142.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062779392
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vn/cvnbev7r2ezryau774kr7kglduklv4qdo3pfyrsexrwohcfi2ipx.py
# Topologically Sorted Source Nodes: [x_se_21], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_21 => convert_element_type_158
# Graph fragment:
#   %primals_202 : Tensor "f32[26, 624, 1, 1][624, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_202]
#   %convert_element_type_158 : Tensor "f16[26, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_202, torch.float16), kwargs = {})
#   return %convert_element_type_158
triton_poi_fused__to_copy_143 = async_compile.triton('triton_poi_fused__to_copy_143', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_143', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 129792}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_143(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((26, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((26, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_143.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_143.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ww/cwwmdvbwoefvbqm7tvqaagbwzfdw6uetniebr5stylbqxsdznbys.py
# Topologically Sorted Source Nodes: [x_se_21, x_se_22], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_21 => convert_element_type_157, convolution_60
#   x_se_22 => convert_element_type_159, convert_element_type_160, mul_204, sigmoid_22
# Graph fragment:
#   %buf516 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0" = PlaceHolder[target=buf516]
#   %primals_203 : Tensor "f32[26][1]cuda:0" = PlaceHolder[target=primals_203]
#   %convolution_60 : Tensor "f16[128, 26, 1, 1][26, 1, 26, 26]cuda:0" = PlaceHolder[target=convolution_60]
#   %convert_element_type_157 : Tensor "f16[26][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_203, torch.float16), kwargs = {})
#   %convolution_60 : Tensor "f16[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_5, %convert_element_type_158, %convert_element_type_157, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_159 : Tensor "f32[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_60, torch.float32), kwargs = {})
#   %sigmoid_22 : Tensor "f32[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_159,), kwargs = {})
#   %mul_204 : Tensor "f32[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_159, %sigmoid_22), kwargs = {})
#   %convert_element_type_160 : Tensor "f16[128, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_204, torch.float16), kwargs = {})
#   return %convolution_60,%convert_element_type_160
triton_poi_fused__to_copy_convolution_silu_144 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_144', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_144', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 33384}, 'kernel_num_gb': 2.0072e-05, 'kernel_flop': 4153344},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_144(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3328
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 26)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((26,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 3328,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_144.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_144.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.0072e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2e/c2egp46ib5lksxjggmvahv2o6yknfprh2teb65pisrvyfrrmux2h.py
# Topologically Sorted Source Nodes: [x_se_23], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_23 => convert_element_type_162
# Graph fragment:
#   %primals_204 : Tensor "f32[624, 26, 1, 1][26, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_204]
#   %convert_element_type_162 : Tensor "f16[624, 26, 1, 1][26, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_204, torch.float16), kwargs = {})
#   return %convert_element_type_162
triton_poi_fused__to_copy_145 = async_compile.triton('triton_poi_fused__to_copy_145', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_145', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 129792}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_145(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624, 26, 1, 1), (26, 1, 26, 26), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_145.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_145.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ye/cye74owry7jyuvu4353tz677nri23pwx3ezxq3i2lzymtmggn3nc.py
# Topologically Sorted Source Nodes: [x_se_23], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_23 => convert_element_type_161, convolution_61
# Graph fragment:
#   %buf520 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=buf520]
#   %primals_205 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_205]
#   %convert_element_type_161 : Tensor "f16[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_205, torch.float16), kwargs = {})
#   %convolution_61 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_160, %convert_element_type_162, %convert_element_type_161, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_61
triton_poi_fused__to_copy_convolution_146 = async_compile.triton('triton_poi_fused__to_copy_convolution_146', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_146', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 481728}, 'kernel_num_gb': 0.000321984, 'kernel_flop': 4153344},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_146(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 79872
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 79872,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_146.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_146.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000321984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/dk/cdki4nwwdxh26taamnil7rdc2gj463lt37vmdh5rv2xelhql22lg.py
# Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_5 => sigmoid_23
#   x_79 => convert_element_type_156, mul_203, sigmoid_21
#   x_80 => mul_205
# Graph fragment:
#   %convert_element_type_155 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_155]
#   %convolution_61 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_61]
#   %sigmoid_21 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_155,), kwargs = {})
#   %mul_203 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_155, %sigmoid_21), kwargs = {})
#   %convert_element_type_156 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_203, torch.float16), kwargs = {})
#   %sigmoid_23 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_61,), kwargs = {})
#   %mul_205 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_156, %sigmoid_23), kwargs = {})
#   return %mul_205
triton_poi_fused_mul_sigmoid_silu_147 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_147', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_147', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 62619648, 'x': 62779392}, 'kernel_num_gb': 0.094089216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_147(in_ptr0, in_ptr1, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 624
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 196
    y0 = (yindex % 196)
    tmp0 = tl.load(in_ptr0 + (x2 + 624*y3), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr1 + (x2 + 624*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (y0 + 196*x2 + 122304*y1), tmp6, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 25088, 624,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_147.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_147.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.094089216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nd/cnddbwqjetjsecc6xpan5q7gbj3rijwcy3gmqztecosphj4kb67t.py
# Topologically Sorted Source Nodes: [conv2d_62], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_62 => convert_element_type_163
# Graph fragment:
#   %primals_206 : Tensor "f32[52, 312, 1, 1][312, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_206]
#   %convert_element_type_163 : Tensor "f16[52, 312, 1, 1][312, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_206, torch.float16), kwargs = {})
#   return %convert_element_type_163
triton_poi_fused__to_copy_148 = async_compile.triton('triton_poi_fused__to_copy_148', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_148', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 129792}, 'kernel_num_gb': 9.7344e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_148(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16224
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((52, 312, 1, 1), (312, 1, 312, 312), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 16224,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_148.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_148.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 9.7344e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3m/c3mcfnswhldu7b22drsgv5bq7ieu664wwiixbdrvkuifuk3bkv3o.py
# Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_62], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_62 => convolution_62
#   sigmoid_5 => sigmoid_23
#   split_18 => split_with_sizes_42
#   x_79 => convert_element_type_156, mul_203, sigmoid_21
#   x_80 => mul_205
# Graph fragment:
#   %mul_205 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_205]
#   %sigmoid_21 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_155,), kwargs = {})
#   %mul_203 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_155, %sigmoid_21), kwargs = {})
#   %convert_element_type_156 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_203, torch.float16), kwargs = {})
#   %sigmoid_23 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_61,), kwargs = {})
#   %mul_205 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_156, %sigmoid_23), kwargs = {})
#   %split_with_sizes_42 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_205, [312, 312], 1), kwargs = {})
#   %convolution_62 : Tensor "f16[128, 52, 14, 14][10192, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_164, %convert_element_type_163, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf524
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31309824, 'x': 15654912}, 'kernel_num_gb': 0.031309824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 39936
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 312)
    y1 = yindex // 312
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 122304*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 312*x2 + 61152*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 39936, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.031309824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/wv/cwvzd57u2ey2bf47j4auvsm33oz5gljtmavso42tzuk7kdxdrkom.py
# Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_63], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_63 => convolution_63
#   sigmoid_5 => sigmoid_23
#   split_18 => split_with_sizes_42
#   x_79 => convert_element_type_156, mul_203, sigmoid_21
#   x_80 => mul_205
# Graph fragment:
#   %mul_205 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_205]
#   %sigmoid_21 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_155,), kwargs = {})
#   %mul_203 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_155, %sigmoid_21), kwargs = {})
#   %convert_element_type_156 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_203, torch.float16), kwargs = {})
#   %sigmoid_23 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_61,), kwargs = {})
#   %mul_205 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_156, %sigmoid_23), kwargs = {})
#   %split_with_sizes_42 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_205, [312, 312], 1), kwargs = {})
#   %convolution_63 : Tensor "f16[128, 52, 14, 14][10192, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_165, %convert_element_type_164, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf527
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 31309824, 'x': 15654912}, 'kernel_num_gb': 0.031309824, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 39936
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 312)
    y1 = yindex // 312
    tmp0 = tl.load(in_ptr0 + (61152 + x2 + 196*y0 + 122304*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 312*x2 + 61152*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 312, 14, 14), (61152, 1, 4368, 312), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 39936, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.031309824
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/55/c55p45nzj7tngzkcmem732mlauv6m23vtsn75fjdfe447rxrd2zs.py
# Topologically Sorted Source Nodes: [x_81], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_81 => cat_18
# Graph fragment:
#   %convolution_62 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=convolution_62]
#   %convolution_63 : Tensor "f16[128, 52, 14, 14][10192, 1, 728, 52]cuda:0" = PlaceHolder[target=convolution_63]
#   %cat_18 : Tensor "f16[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_62, %convolution_63], 1), kwargs = {})
#   return %cat_18
triton_poi_fused_cat_151 = async_compile.triton('triton_poi_fused_cat_151', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_151', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20873216}, 'kernel_num_gb': 0.010436608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_151(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 104)
    x1 = xindex // 104
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 52, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (52*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 104, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (52*x1 + ((-52) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 52, 14, 14), (10192, 1, 728, 52), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_151.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_151.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.010436608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ja/cjazxodzgxf2tzlbh5swon5touovmhzcqfxvdo62g44kppvvuto5.py
# Topologically Sorted Source Nodes: [x_82, x_83], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_82 => add_136, add_139, convert_element_type_165, convert_element_type_166, mul_206, mul_212, rsqrt_26, sub_26, unsqueeze_104, unsqueeze_105, unsqueeze_106, unsqueeze_107, var_mean_26
#   x_83 => add_140
# Graph fragment:
#   %cat_18 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=cat_18]
#   %getitem_167 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=getitem_167]
#   %buf537 : Tensor "f32[1, 104, 1, 1][104, 1, 104, 104]cuda:0" = PlaceHolder[target=buf537]
#   %primals_211 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_211]
#   %primals_212 : Tensor "f32[104][1]cuda:0" = PlaceHolder[target=primals_212]
#   %convert_element_type_142 : Tensor "f16[128, 104, 14, 14][20384, 1, 1456, 104]cuda:0" = PlaceHolder[target=convert_element_type_142]
#   %convert_element_type_165 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_18, torch.float32), kwargs = {})
#   %var_mean_26 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_165, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_136 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_166, 1e-05), kwargs = {})
#   %rsqrt_26 : Tensor "f32[1, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_136,), kwargs = {})
#   %sub_26 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_18, %getitem_167), kwargs = {})
#   %mul_206 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_26, %rsqrt_26), kwargs = {})
#   %unsqueeze_104 : Tensor "f32[104, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_211, -1), kwargs = {})
#   %unsqueeze_105 : Tensor "f32[104, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_104, -1), kwargs = {})
#   %mul_212 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_206, %unsqueeze_105), kwargs = {})
#   %unsqueeze_106 : Tensor "f32[104, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_212, -1), kwargs = {})
#   %unsqueeze_107 : Tensor "f32[104, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_106, -1), kwargs = {})
#   %add_139 : Tensor "f32[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_212, %unsqueeze_107), kwargs = {})
#   %convert_element_type_166 : Tensor "f16[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_139, torch.float16), kwargs = {})
#   %add_140 : Tensor "f16[128, 104, 14, 14][20384, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_166, %convert_element_type_142), kwargs = {})
#   return %add_140
triton_poi_fused__native_batch_norm_legit_functional_add_152 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_152', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_152', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20874880}, 'kernel_num_gb': 0.015656576, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_152(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2609152
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 104)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 25088.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(in_out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 104, 14, 14), (20384, 1, 1456, 104), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((104,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 2609152,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_152.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_152.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.015656576
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/lc/clcf3zjiz27benpzsl5kmvuwr7tzgzp4u2qltk4tpf2zim4e43j2.py
# Topologically Sorted Source Nodes: [x_104], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_104 => convert_element_type_215
# Graph fragment:
#   %primals_267 : Tensor "f32[624, 104, 1, 1][104, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_267]
#   %convert_element_type_215 : Tensor "f16[624, 104, 1, 1][104, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_267, torch.float16), kwargs = {})
#   return %convert_element_type_215
triton_poi_fused__to_copy_153 = async_compile.triton('triton_poi_fused__to_copy_153', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_153', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 519168}, 'kernel_num_gb': 0.000389376, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_153(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 64896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624, 104, 1, 1), (104, 1, 104, 104), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 64896,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_153.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_153.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000389376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5h/c5hte47ea6qotooz7kyu4fqszvgwqzgjexlqsgwmt3muoq2majyu.py
# Topologically Sorted Source Nodes: [x_105, x_106], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_105 => add_177, convert_element_type_217, mul_263, mul_269, sub_33, unsqueeze_132, unsqueeze_133, unsqueeze_134, unsqueeze_135
#   x_106 => convert_element_type_218, convert_element_type_219, mul_270, sigmoid_32
# Graph fragment:
#   %convolution_84 : Tensor "f16[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convolution_84]
#   %getitem_229 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=getitem_229]
#   %rsqrt_33 : Tensor "f32[1, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=rsqrt_33]
#   %primals_271 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_271]
#   %primals_272 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_272]
#   %convert_element_type_218 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_218]
#   %sub_33 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_84, %getitem_229), kwargs = {})
#   %mul_263 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_33, %rsqrt_33), kwargs = {})
#   %unsqueeze_132 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_271, -1), kwargs = {})
#   %unsqueeze_133 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_132, -1), kwargs = {})
#   %mul_269 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_263, %unsqueeze_133), kwargs = {})
#   %unsqueeze_134 : Tensor "f32[624, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_272, -1), kwargs = {})
#   %unsqueeze_135 : Tensor "f32[624, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_134, -1), kwargs = {})
#   %add_177 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_269, %unsqueeze_135), kwargs = {})
#   %convert_element_type_217 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_177, torch.float16), kwargs = {})
#   %convert_element_type_218 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_217, torch.float32), kwargs = {})
#   %sigmoid_32 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_218,), kwargs = {})
#   %mul_270 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_218, %sigmoid_32), kwargs = {})
#   %convert_element_type_219 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_270, torch.float16), kwargs = {})
#   return %convert_element_type_218,%convert_element_type_219
triton_poi_fused__native_batch_norm_legit_functional_silu_154 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_154', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_154', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 93939456}, 'kernel_num_gb': 0.062629632, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_154(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (x2), tmp14, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_154.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_154.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.062629632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/pk/cpk6cn5eelhx6jgqwpeg7mhkq3rdfzqyec2bytpyyivy3wjitry5.py
# Topologically Sorted Source Nodes: [x_107], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_107 => convert_element_type_220
# Graph fragment:
#   %primals_273 : Tensor "f32[624, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_273]
#   %convert_element_type_220 : Tensor "f16[624, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_273, torch.float16), kwargs = {})
#   return %convert_element_type_220
triton_poi_fused__to_copy_155 = async_compile.triton('triton_poi_fused__to_copy_155', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_155', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 44928}, 'kernel_num_gb': 3.3696e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_155(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5616
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 5616,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_155.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_155.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.3696e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ak/cak7zkwphjmyiub33k6srypkgx3wlzaw6i6nyxtzbr7pxdrauipx.py
# Topologically Sorted Source Nodes: [x_se_33], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_33 => convert_element_type_226
# Graph fragment:
#   %primals_279 : Tensor "f32[52, 624, 1, 1][624, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_279]
#   %convert_element_type_226 : Tensor "f16[52, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_279, torch.float16), kwargs = {})
#   return %convert_element_type_226
triton_poi_fused__to_copy_156 = async_compile.triton('triton_poi_fused__to_copy_156', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_156', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 259584}, 'kernel_num_gb': 0.000194688, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_156(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32448
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((52, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((52, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 32448,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_156.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_156.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000194688
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/l2/cl2g6rkykilhqgovunsd5evcvvybridcjtld6pfxtyuw2leoujna.py
# Topologically Sorted Source Nodes: [x_se_33, x_se_34], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_33 => convert_element_type_225, convolution_86
#   x_se_34 => convert_element_type_227, convert_element_type_228, mul_279, sigmoid_34
# Graph fragment:
#   %buf717 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=buf717]
#   %primals_280 : Tensor "f32[52][1]cuda:0" = PlaceHolder[target=primals_280]
#   %convolution_86 : Tensor "f16[128, 52, 1, 1][52, 1, 52, 52]cuda:0" = PlaceHolder[target=convolution_86]
#   %convert_element_type_225 : Tensor "f16[52][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_280, torch.float16), kwargs = {})
#   %convolution_86 : Tensor "f16[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_8, %convert_element_type_226, %convert_element_type_225, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_227 : Tensor "f32[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_86, torch.float32), kwargs = {})
#   %sigmoid_34 : Tensor "f32[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_227,), kwargs = {})
#   %mul_279 : Tensor "f32[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_227, %sigmoid_34), kwargs = {})
#   %convert_element_type_228 : Tensor "f16[128, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_279, torch.float16), kwargs = {})
#   return %convolution_86,%convert_element_type_228
triton_poi_fused__to_copy_convolution_silu_157 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_157', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_157', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 66768}, 'kernel_num_gb': 4.0144e-05, 'kernel_flop': 8306688},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_157(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6656
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 52)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((52,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 6656,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_157.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_157.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 4.0144e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/og/cogouxpv2lldyszimwf2wjla4nj2geltpxtva7gk6y5vi626rmtm.py
# Topologically Sorted Source Nodes: [x_se_35], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_35 => convert_element_type_230
# Graph fragment:
#   %primals_281 : Tensor "f32[624, 52, 1, 1][52, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_281]
#   %convert_element_type_230 : Tensor "f16[624, 52, 1, 1][52, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_281, torch.float16), kwargs = {})
#   return %convert_element_type_230
triton_poi_fused__to_copy_158 = async_compile.triton('triton_poi_fused__to_copy_158', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_158', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 259584}, 'kernel_num_gb': 0.000194688, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_158(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32448
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((624, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((624, 52, 1, 1), (52, 1, 52, 52), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 32448,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_158.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_158.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000194688
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/g4/cg4iqbvja7pxfkmwxqoc2ipbz4vhelzdk4jhlzkmgfa4mkpo6qne.py
# Topologically Sorted Source Nodes: [x_se_35], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_35 => convert_element_type_229, convolution_87
# Graph fragment:
#   %buf721 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=buf721]
#   %primals_282 : Tensor "f32[624][1]cuda:0" = PlaceHolder[target=primals_282]
#   %convert_element_type_229 : Tensor "f16[624][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_282, torch.float16), kwargs = {})
#   %convolution_87 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_228, %convert_element_type_230, %convert_element_type_229, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_87
triton_poi_fused__to_copy_convolution_159 = async_compile.triton('triton_poi_fused__to_copy_convolution_159', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_159', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 481728}, 'kernel_num_gb': 0.000321984, 'kernel_flop': 8306688},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_159(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 79872
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 624)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((624,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 79872,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_159.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_159.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000321984
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ps/cpscost3wyyqiro7avy7meuktoklfzv6kv3gbsub6jbkmfmuum37.py
# Topologically Sorted Source Nodes: [x_109, sigmoid_8, x_110], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_8 => sigmoid_35
#   x_109 => convert_element_type_224, mul_278, sigmoid_33
#   x_110 => mul_280
# Graph fragment:
#   %convert_element_type_223 : Tensor "f32[128, 624, 14, 14][122304, 1, 8736, 624]cuda:0" = PlaceHolder[target=convert_element_type_223]
#   %convolution_87 : Tensor "f16[128, 624, 1, 1][624, 1, 624, 624]cuda:0" = PlaceHolder[target=convolution_87]
#   %sigmoid_33 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_223,), kwargs = {})
#   %mul_278 : Tensor "f32[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_223, %sigmoid_33), kwargs = {})
#   %convert_element_type_224 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_278, torch.float16), kwargs = {})
#   %sigmoid_35 : Tensor "f16[128, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_87,), kwargs = {})
#   %mul_280 : Tensor "f16[128, 624, 14, 14][122304, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_224, %sigmoid_35), kwargs = {})
#   return %mul_280
triton_poi_fused_mul_sigmoid_silu_160 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_160', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_160', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 125399040}, 'kernel_num_gb': 0.094089216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_160(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 15654912
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 624)
    x2 = xindex // 122304
    tmp0 = tl.load(in_ptr0 + (x3), None)
    tmp4 = tl.load(in_ptr1 + (x0 + 624*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (x3), tmp6, None)


def get_args():
    arg_0 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 624, 14, 14), (122304, 1, 8736, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 15654912,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_160.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_160.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.094089216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/m5/cm5p5hwfl35nryitnt263pbhwy7avf3gbfbt4fuqhak75wbccnrv.py
# Topologically Sorted Source Nodes: [x_111], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_111 => convert_element_type_231
# Graph fragment:
#   %primals_283 : Tensor "f32[160, 624, 1, 1][624, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_283]
#   %convert_element_type_231 : Tensor "f16[160, 624, 1, 1][624, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_283, torch.float16), kwargs = {})
#   return %convert_element_type_231
triton_poi_fused__to_copy_161 = async_compile.triton('triton_poi_fused__to_copy_161', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_161', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 798720}, 'kernel_num_gb': 0.00059904, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_161(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 99840
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((160, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((160, 624, 1, 1), (624, 1, 624, 624), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 99840,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_161.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_161.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00059904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/oe/coe5ipfszq522726gizinxs2bo42i5l66xlvnzf4ufbxy2vnw2gj.py
# Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_112 => convert_element_type_232, var_mean_35
# Graph fragment:
#   %convolution_88 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convolution_88]
#   %convert_element_type_232 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_88, torch.float32), kwargs = {})
#   %var_mean_35 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_232, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf726,%buf727,%buf728
triton_red_fused__native_batch_norm_legit_functional_162 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_162', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 32768, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_162', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8780800, 'r0_': 0}, 'kernel_num_gb': 0.00840448, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_162(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 31360
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 160)
    x1 = xindex // 160
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_2 + 20480*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 31360, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_162.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_162.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00840448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kv/ckv64ieo6vzt2f66ldq6enyancsrgfpaufyvxoxjewckvxfpfhca.py
# Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_112 => convert_element_type_232, var_mean_35
# Graph fragment:
#   %buf726 : Tensor "f32[1, 160, 1, 1, 196][31360, 1, 31360, 31360, 160]cuda:0" = PlaceHolder[target=buf726]
#   %buf727 : Tensor "f32[1, 160, 1, 1, 196][31360, 1, 31360, 31360, 160]cuda:0" = PlaceHolder[target=buf727]
#   %buf728 : Tensor "f32[1, 160, 1, 1, 196][31360, 1, 31360, 31360, 160]cuda:0" = PlaceHolder[target=buf728]
#   %convert_element_type_232 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_88, torch.float32), kwargs = {})
#   %var_mean_35 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_232, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf729,%buf730,%buf731
triton_red_fused__native_batch_norm_legit_functional_163 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_163', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 512, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_163', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 384000, 'r0_': 0}, 'kernel_num_gb': 0.00038016, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_163(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 320
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 160)
    x1 = xindex // 160
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_2 + 15680*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 160*r0_2 + 15680*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 160*r0_2 + 15680*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 320, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_163.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_163.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00038016
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4x/c4x3kkpshd5mgvujttuf24z5mdemnztmq2niulndselimqdqrgwy.py
# Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_112 => add_184, add_185, add_186, convert_element_type_232, mul_282, mul_283, mul_284, mul_285, mul_286, rsqrt_35, squeeze_105, squeeze_107, var_mean_35
# Graph fragment:
#   %buf729 : Tensor "f32[1, 160, 1, 1, 2][320, 1, 320, 320, 160]cuda:0" = PlaceHolder[target=buf729]
#   %buf730 : Tensor "f32[1, 160, 1, 1, 2][320, 1, 320, 320, 160]cuda:0" = PlaceHolder[target=buf730]
#   %buf731 : Tensor "f32[1, 160, 1, 1, 2][320, 1, 320, 320, 160]cuda:0" = PlaceHolder[target=buf731]
#   %buf733 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=buf733]
#   %getitem_233 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=getitem_233]
#   %copy__106 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=copy__106]
#   %add_185 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=add_185]
#   %copy__107 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=copy__107]
#   %add_186 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=add_186]
#   %convert_element_type_232 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_88, torch.float32), kwargs = {})
#   %var_mean_35 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_232, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_184 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_232, 1e-05), kwargs = {})
#   %rsqrt_35 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_184,), kwargs = {})
#   %squeeze_105 : Tensor "f32[160][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_233, [0, 2, 3]), kwargs = {})
#   %mul_282 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_105, 0.1), kwargs = {})
#   %mul_283 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_285, 0.9), kwargs = {})
#   %add_185 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_282, %mul_283), kwargs = {})
#   %squeeze_107 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_232, [0, 2, 3]), kwargs = {})
#   %mul_284 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_107, 1.0000398612827361), kwargs = {})
#   %mul_285 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_284, 0.1), kwargs = {})
#   %mul_286 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_286, 0.9), kwargs = {})
#   %add_186 : Tensor "f32[160][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_285, %mul_286), kwargs = {})
#   %copy__106 : Tensor "f32[160][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_285, %add_185), kwargs = {})
#   %copy__107 : Tensor "f32[160][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_286, %add_186), kwargs = {})
#   return %getitem_233,%buf733,%rsqrt_35,%add_185,%buf1477,%add_186,%buf1480
triton_per_fused__native_batch_norm_legit_functional_copy__164 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__164', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 256, 'r0_': 2},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__164', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 11520, 'r0_': 0}, 'kernel_num_gb': 8.32e-06, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__164(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 160
    r0_numel = 2
    R0_BLOCK: tl.constexpr = 2
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 160*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 160*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 160*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 25088.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000398612827361
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 160, 1, 1, 2), (320, 1, 320, 320, 160), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 160, 2,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__164.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__164.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 8.32e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zv/czv454ntxflflrr7xi5zvoc4fxtw5upwrzlmmk74zftl2qeprv5h.py
# Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_112 => add_184, add_187, convert_element_type_232, convert_element_type_233, mul_281, mul_287, rsqrt_35, sub_35, unsqueeze_140, unsqueeze_141, unsqueeze_142, unsqueeze_143, var_mean_35
# Graph fragment:
#   %convolution_88 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convolution_88]
#   %getitem_233 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=getitem_233]
#   %buf733 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=buf733]
#   %primals_287 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_287]
#   %primals_288 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_288]
#   %convert_element_type_232 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_88, torch.float32), kwargs = {})
#   %var_mean_35 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_232, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_184 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_232, 1e-05), kwargs = {})
#   %rsqrt_35 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_184,), kwargs = {})
#   %sub_35 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_88, %getitem_233), kwargs = {})
#   %mul_281 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_35, %rsqrt_35), kwargs = {})
#   %unsqueeze_140 : Tensor "f32[160, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_287, -1), kwargs = {})
#   %unsqueeze_141 : Tensor "f32[160, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_140, -1), kwargs = {})
#   %mul_287 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_281, %unsqueeze_141), kwargs = {})
#   %unsqueeze_142 : Tensor "f32[160, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_288, -1), kwargs = {})
#   %unsqueeze_143 : Tensor "f32[160, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_142, -1), kwargs = {})
#   %add_187 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_287, %unsqueeze_143), kwargs = {})
#   %convert_element_type_233 : Tensor "f16[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_187, torch.float16), kwargs = {})
#   return %convert_element_type_233
triton_poi_fused__native_batch_norm_legit_functional_165 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_165', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_165', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 24087040}, 'kernel_num_gb': 0.01605888, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_165(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 25088.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp15, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_165.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_165.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01605888
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nf/cnfyn63ki3jop4xgvasaolssn5u2zztolndakv5synuabugmwe73.py
# Topologically Sorted Source Nodes: [split_25, conv2d_89], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_89 => convolution_89
#   split_25 => getitem_234, split_with_sizes_57
# Graph fragment:
#   %convert_element_type_233 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convert_element_type_233]
#   %getitem_234 : Tensor "f16[128, 80, 14, 14][15680, 196, 14, 1]cuda:0" = PlaceHolder[target=getitem_234]
#   %split_with_sizes_57 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_233, [80, 80], 1), kwargs = {})
#   %getitem_234 : Tensor "f16[128, 80, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_57, 0), kwargs = {})
#   %convolution_89 : Tensor "f16[128, 240, 14, 14][47040, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_234, %convert_element_type_234, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_234,%buf740
triton_poi_fused_convolution_split_with_sizes_166 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_166', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_166', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 8028160}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_166(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 10240
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 80)
    y1 = yindex // 80
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + 160*x2 + 31360*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y3), tmp0, xmask)
    tl.store(out_ptr1 + (y0 + 80*x2 + 15680*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 10240, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_166.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_166.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/q4/cq4ax4htot6kgv45bwy37c2u2hh7iwenducsbj2ss3tzz4voahny.py
# Topologically Sorted Source Nodes: [split_25, conv2d_90], Original ATen: [aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_90 => convolution_90
#   split_25 => getitem_235, split_with_sizes_57
# Graph fragment:
#   %convert_element_type_233 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convert_element_type_233]
#   %getitem_235 : Tensor "f16[128, 80, 14, 14][15680, 196, 14, 1]cuda:0" = PlaceHolder[target=getitem_235]
#   %split_with_sizes_57 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_233, [80, 80], 1), kwargs = {})
#   %getitem_235 : Tensor "f16[128, 80, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=operator.getitem](args = (%split_with_sizes_57, 1), kwargs = {})
#   %convolution_90 : Tensor "f16[128, 240, 14, 14][47040, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_235, %convert_element_type_235, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %getitem_235,%buf743
triton_poi_fused_convolution_split_with_sizes_167 = async_compile.triton('triton_poi_fused_convolution_split_with_sizes_167', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_split_with_sizes_167', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 8028160}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_split_with_sizes_167(in_ptr0, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 10240
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 80)
    y1 = yindex // 80
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (80 + y0 + 160*x2 + 31360*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (x2 + 196*y3), tmp0, xmask)
    tl.store(out_ptr1 + (y0 + 80*x2 + 15680*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 10240, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_split_with_sizes_167.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_split_with_sizes_167.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/to/ctocse3yg6ldnsnm4nx3yh36a7bwlw72d5ofpdozxl4sk36volyt.py
# Topologically Sorted Source Nodes: [conv2d_89], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_89 => convert_element_type_234
# Graph fragment:
#   %primals_289 : Tensor "f32[240, 80, 1, 1][80, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_289]
#   %convert_element_type_234 : Tensor "f16[240, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_289, torch.float16), kwargs = {})
#   return %convert_element_type_234
triton_poi_fused__to_copy_168 = async_compile.triton('triton_poi_fused__to_copy_168', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_168', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 153600}, 'kernel_num_gb': 0.0001152, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_168(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_168.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_168.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rx/crxeeiilefgt3huvxuvoptoxah7yhjccaz4cnwraegnesqk74su2.py
# Topologically Sorted Source Nodes: [x_113], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_113 => cat_25
# Graph fragment:
#   %convolution_89 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=convolution_89]
#   %convolution_90 : Tensor "f16[128, 240, 14, 14][47040, 1, 3360, 240]cuda:0" = PlaceHolder[target=convolution_90]
#   %cat_25 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_89, %convolution_90], 1), kwargs = {})
#   return %cat_25
triton_poi_fused_cat_169 = async_compile.triton('triton_poi_fused_cat_169', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_169', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 96337920}, 'kernel_num_gb': 0.04816896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_169(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12042240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 480)
    x1 = xindex // 480
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 240, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (240*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 480, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (240*x1 + ((-240) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 12042240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_169.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_169.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ko/ckogdy2ujg5mhdb2qa35ahcuou2cbcdilzfi42qj2xgq5gjr526c.py
# Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_114 => convert_element_type_236, var_mean_36
# Graph fragment:
#   %cat_25 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_25]
#   %convert_element_type_236 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_25, torch.float32), kwargs = {})
#   %var_mean_36 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_236, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf746,%buf747,%buf748
triton_red_fused__native_batch_norm_legit_functional_170 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_170', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_170', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 26342400, 'r0_': 0}, 'kernel_num_gb': 0.02521344, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_170(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 94080
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 480)
    x1 = xindex // 480
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 480*r0_2 + 61440*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 94080, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_170.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_170.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02521344
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/es/cesggxj2qtf3rog7f4offkwpf5hyjqte7ou3cbaa6r2c3476muw6.py
# Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_114 => convert_element_type_236, var_mean_36
# Graph fragment:
#   %buf746 : Tensor "f32[1, 480, 1, 1, 196][94080, 1, 94080, 94080, 480]cuda:0" = PlaceHolder[target=buf746]
#   %buf747 : Tensor "f32[1, 480, 1, 1, 196][94080, 1, 94080, 94080, 480]cuda:0" = PlaceHolder[target=buf747]
#   %buf748 : Tensor "f32[1, 480, 1, 1, 196][94080, 1, 94080, 94080, 480]cuda:0" = PlaceHolder[target=buf748]
#   %convert_element_type_236 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_25, torch.float32), kwargs = {})
#   %var_mean_36 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_236, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf749,%buf750,%buf751
triton_red_fused__native_batch_norm_legit_functional_171 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_171', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_171', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1152000, 'r0_': 0}, 'kernel_num_gb': 0.00114048, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_171(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 98
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 480)
    x1 = xindex // 480
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 480*r0_2 + 47040*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 480*r0_2 + 47040*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 480*r0_2 + 47040*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tl.store(out_ptr1 + (x3), tmp10, xmask)
    tl.store(out_ptr2 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 960, 98,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_171.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_171.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00114048
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qn/cqnrzs3nzdx3xpbouinpad2blk3x2kytnywwlipltowv37reydfv.py
# Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_114 => add_189, add_190, add_191, convert_element_type_236, mul_289, mul_290, mul_291, mul_292, mul_293, rsqrt_36, squeeze_108, squeeze_110, var_mean_36
# Graph fragment:
#   %buf749 : Tensor "f32[1, 480, 1, 1, 2][960, 1, 960, 960, 480]cuda:0" = PlaceHolder[target=buf749]
#   %buf750 : Tensor "f32[1, 480, 1, 1, 2][960, 1, 960, 960, 480]cuda:0" = PlaceHolder[target=buf750]
#   %buf751 : Tensor "f32[1, 480, 1, 1, 2][960, 1, 960, 960, 480]cuda:0" = PlaceHolder[target=buf751]
#   %buf753 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=buf753]
#   %getitem_237 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_237]
#   %copy__109 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=copy__109]
#   %add_190 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=add_190]
#   %copy__110 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=copy__110]
#   %add_191 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=add_191]
#   %convert_element_type_236 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_25, torch.float32), kwargs = {})
#   %var_mean_36 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_236, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_189 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_236, 1e-05), kwargs = {})
#   %rsqrt_36 : Tensor "f32[1, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_189,), kwargs = {})
#   %squeeze_108 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_237, [0, 2, 3]), kwargs = {})
#   %mul_289 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_108, 0.1), kwargs = {})
#   %mul_290 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_292, 0.9), kwargs = {})
#   %add_190 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_289, %mul_290), kwargs = {})
#   %squeeze_110 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_236, [0, 2, 3]), kwargs = {})
#   %mul_291 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_110, 1.0000398612827361), kwargs = {})
#   %mul_292 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_291, 0.1), kwargs = {})
#   %mul_293 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_293, 0.9), kwargs = {})
#   %add_191 : Tensor "f32[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_292, %mul_293), kwargs = {})
#   %copy__109 : Tensor "f32[480][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_292, %add_190), kwargs = {})
#   %copy__110 : Tensor "f32[480][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_293, %add_191), kwargs = {})
#   return %getitem_237,%buf753,%rsqrt_36,%add_190,%buf1485,%add_191,%buf1488
triton_per_fused__native_batch_norm_legit_functional_copy__172 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__172', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 2},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__172', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 30720, 'r0_': 0}, 'kernel_num_gb': 2.304e-05, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__172(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 480
    r0_numel = 2
    R0_BLOCK: tl.constexpr = 2
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 480*r0_1), xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 480*r0_1), xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 480*r0_1), xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(xmask, tmp3, 0)
    tmp8 = tl.where(xmask, tmp4, 0)
    tmp9 = tl.where(xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 25088.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0000398612827361
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1, 2), (960, 1, 960, 960, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 480, 2,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__172.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__172.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.304e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4o/c4oki6e45p3zvro65ovuklfuecnwybqeps2x4yfuijupsalxzu7e.py
# Topologically Sorted Source Nodes: [x_114, x_115], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_114 => add_192, convert_element_type_237, mul_288, mul_294, sub_36, unsqueeze_144, unsqueeze_145, unsqueeze_146, unsqueeze_147
#   x_115 => convert_element_type_238, convert_element_type_239, mul_295, sigmoid_36
# Graph fragment:
#   %cat_25 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_25]
#   %getitem_237 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_237]
#   %rsqrt_36 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_36]
#   %primals_294 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_294]
#   %primals_295 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_295]
#   %convert_element_type_238 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_238]
#   %sub_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_25, %getitem_237), kwargs = {})
#   %mul_288 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_36, %rsqrt_36), kwargs = {})
#   %unsqueeze_144 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_294, -1), kwargs = {})
#   %unsqueeze_145 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_144, -1), kwargs = {})
#   %mul_294 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_288, %unsqueeze_145), kwargs = {})
#   %unsqueeze_146 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_295, -1), kwargs = {})
#   %unsqueeze_147 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_146, -1), kwargs = {})
#   %add_192 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_294, %unsqueeze_147), kwargs = {})
#   %convert_element_type_237 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_192, torch.float16), kwargs = {})
#   %convert_element_type_238 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_237, torch.float32), kwargs = {})
#   %sigmoid_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_238,), kwargs = {})
#   %mul_295 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_238, %sigmoid_36), kwargs = {})
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_295, torch.float16), kwargs = {})
#   return %convert_element_type_238,%convert_element_type_239
triton_poi_fused__native_batch_norm_legit_functional_silu_173 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_173', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_173', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 24092160}, 'kernel_num_gb': 0.04817664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_173(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 480
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 480*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 196*x1 + 94080*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_173.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_173.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04817664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6f/c6fwldr4xxuzubpl4k7frruio7sah73x3rs4ecztv62wajk3g36q.py
# Topologically Sorted Source Nodes: [x_115, conv2d_91], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_91 => convolution_91, split_with_sizes_59
#   x_115 => convert_element_type_239, mul_295, sigmoid_36
# Graph fragment:
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_239]
#   %sigmoid_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_238,), kwargs = {})
#   %mul_295 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_238, %sigmoid_36), kwargs = {})
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_295, torch.float16), kwargs = {})
#   %split_with_sizes_59 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_239, [120, 120, 120, 120], 1), kwargs = {})
#   %convolution_91 : Tensor "f16[128, 120, 14, 14][23520, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_242, %convert_element_type_240, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 120), kwargs = {})
#   return %buf759
triton_poi_fused_convolution_silu_split_with_sizes_174 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_174', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_174', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_174(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_174.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_174.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/a6/ca64d6gv523qkrmesv4donn5daj7dgnquclaqc5yb647vtivrzbi.py
# Topologically Sorted Source Nodes: [conv2d_92], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_92 => convert_element_type_241
# Graph fragment:
#   %primals_297 : Tensor "f32[120, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_297]
#   %convert_element_type_241 : Tensor "f16[120, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_297, torch.float16), kwargs = {})
#   return %convert_element_type_241
triton_poi_fused__to_copy_175 = async_compile.triton('triton_poi_fused__to_copy_175', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_175', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 24000}, 'kernel_num_gb': 1.8e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_175(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_175.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_175.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.8e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/aw/cawxy2tsslb245ywyec3akycgr7kdibnsmztnl4slzstjf35i7be.py
# Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_92], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_91 => split_with_sizes_59
#   conv2d_92 => convolution_92
#   x_115 => convert_element_type_239, mul_295, sigmoid_36
# Graph fragment:
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_239]
#   %sigmoid_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_238,), kwargs = {})
#   %mul_295 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_238, %sigmoid_36), kwargs = {})
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_295, torch.float16), kwargs = {})
#   %split_with_sizes_59 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_239, [120, 120, 120, 120], 1), kwargs = {})
#   %convolution_92 : Tensor "f16[128, 120, 14, 14][23520, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_247, %convert_element_type_241, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), kwargs = {})
#   return %buf762
triton_poi_fused_convolution_silu_split_with_sizes_176 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_176', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_176', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_176(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (23520 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_176.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_176.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/re/cretor5sq3pmwmiswsapzw4j7ens643audcnlp5ntpi2yq6yh6kd.py
# Topologically Sorted Source Nodes: [conv2d_93], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_93 => convert_element_type_242
# Graph fragment:
#   %primals_298 : Tensor "f32[120, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_298]
#   %convert_element_type_242 : Tensor "f16[120, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_298, torch.float16), kwargs = {})
#   return %convert_element_type_242
triton_poi_fused__to_copy_177 = async_compile.triton('triton_poi_fused__to_copy_177', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_177', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 47040}, 'kernel_num_gb': 3.528e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_177(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 5880
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 5880,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_177.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_177.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.528e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/64/c644kxwoul6ezk2urf53e42vg2534rxymtxar7psoofv43edjol3.py
# Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_93], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_91 => split_with_sizes_59
#   conv2d_93 => convolution_93
#   x_115 => convert_element_type_239, mul_295, sigmoid_36
# Graph fragment:
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_239]
#   %sigmoid_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_238,), kwargs = {})
#   %mul_295 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_238, %sigmoid_36), kwargs = {})
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_295, torch.float16), kwargs = {})
#   %split_with_sizes_59 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_239, [120, 120, 120, 120], 1), kwargs = {})
#   %convolution_93 : Tensor "f16[128, 120, 14, 14][23520, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_252, %convert_element_type_242, None, [1, 1], [3, 3], [1, 1], False, [0, 0], 120), kwargs = {})
#   return %buf765
triton_poi_fused_convolution_silu_split_with_sizes_178 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_178', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_178', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_178(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (47040 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_178.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_178.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4r/c4rqk53rlkygmmzuy5ibuhr3cfbilwlxtwrlon3hpmr2cst5uloh.py
# Topologically Sorted Source Nodes: [conv2d_94], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_94 => convert_element_type_243
# Graph fragment:
#   %primals_299 : Tensor "f32[120, 1, 9, 9][81, 81, 9, 1]cuda:0" = PlaceHolder[target=primals_299]
#   %convert_element_type_243 : Tensor "f16[120, 1, 9, 9][81, 81, 9, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_299, torch.float16), kwargs = {})
#   return %convert_element_type_243
triton_poi_fused__to_copy_179 = async_compile.triton('triton_poi_fused__to_copy_179', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_179', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 77760}, 'kernel_num_gb': 5.832e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_179(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9720
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((120, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((120, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 9720,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_179.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_179.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.832e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/4e/c4eicrpmz222tpmxs7oceitko43krmrkl3jgos2gukvxzla3fo7p.py
# Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_94], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_91 => split_with_sizes_59
#   conv2d_94 => convolution_94
#   x_115 => convert_element_type_239, mul_295, sigmoid_36
# Graph fragment:
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_239]
#   %sigmoid_36 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_238,), kwargs = {})
#   %mul_295 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_238, %sigmoid_36), kwargs = {})
#   %convert_element_type_239 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_295, torch.float16), kwargs = {})
#   %split_with_sizes_59 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_239, [120, 120, 120, 120], 1), kwargs = {})
#   %convolution_94 : Tensor "f16[128, 120, 14, 14][23520, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_257, %convert_element_type_243, None, [1, 1], [4, 4], [1, 1], False, [0, 0], 120), kwargs = {})
#   return %buf768
triton_poi_fused_convolution_silu_split_with_sizes_180 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_180', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 16384, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_180', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 12042240, 'x': 6021120}, 'kernel_num_gb': 0.01204224, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_180(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 15360
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 120)
    y1 = yindex // 120
    tmp0 = tl.load(in_ptr0 + (70560 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 120*x2 + 23520*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 15360, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_180.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_180.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01204224
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b7/cb7qljqyphu267dvntvzwafpv2mszz7rt7isqkxdolq4oe76u2yu.py
# Topologically Sorted Source Nodes: [x_116], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_116 => cat_26
# Graph fragment:
#   %convolution_91 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=convolution_91]
#   %convolution_92 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=convolution_92]
#   %convolution_93 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=convolution_93]
#   %convolution_94 : Tensor "f16[128, 120, 14, 14][23520, 1, 1680, 120]cuda:0" = PlaceHolder[target=convolution_94]
#   %cat_26 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_91, %convolution_92, %convolution_93, %convolution_94], 1), kwargs = {})
#   return %cat_26
triton_poi_fused_cat_181 = async_compile.triton('triton_poi_fused_cat_181', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_181', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 144506880}, 'kernel_num_gb': 0.04816896, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_181(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12042240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 480)
    x1 = xindex // 480
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 120, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (120*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 240, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (120*x1 + ((-120) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 360, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (120*x1 + ((-240) + x0)), tmp14, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 480, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (120*x1 + ((-360) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, None)


def get_args():
    arg_0 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 120, 14, 14), (23520, 1, 1680, 120), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 12042240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_181.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_181.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04816896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ju/cju4gohka2rnbzfkx6234zb246od2j3kgjmn4mrkj7yzn5tqwnt7.py
# Topologically Sorted Source Nodes: [x_117, x_118], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_117 => add_197, convert_element_type_245, mul_296, mul_302, sub_37, unsqueeze_148, unsqueeze_149, unsqueeze_150, unsqueeze_151
#   x_118 => convert_element_type_246
# Graph fragment:
#   %cat_26 : Tensor "f16[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=cat_26]
#   %getitem_259 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=getitem_259]
#   %rsqrt_37 : Tensor "f32[1, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=rsqrt_37]
#   %primals_303 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_303]
#   %primals_304 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_304]
#   %sub_37 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_26, %getitem_259), kwargs = {})
#   %mul_296 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_37, %rsqrt_37), kwargs = {})
#   %unsqueeze_148 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_303, -1), kwargs = {})
#   %unsqueeze_149 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_148, -1), kwargs = {})
#   %mul_302 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_296, %unsqueeze_149), kwargs = {})
#   %unsqueeze_150 : Tensor "f32[480, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_304, -1), kwargs = {})
#   %unsqueeze_151 : Tensor "f32[480, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_150, -1), kwargs = {})
#   %add_197 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_302, %unsqueeze_151), kwargs = {})
#   %convert_element_type_245 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_197, torch.float16), kwargs = {})
#   %convert_element_type_246 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_245, torch.float32), kwargs = {})
#   return %convert_element_type_246
triton_poi_fused__native_batch_norm_legit_functional_silu_182 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_182', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_182', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 120430080}, 'kernel_num_gb': 0.07226112, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_182(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 12042240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 480)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 12042240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_182.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_182.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.07226112
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d2/cd2wjznvlbn3onkx5bhll77lik6hwwqzmaxlnomwb7sfb4okwool.py
# Topologically Sorted Source Nodes: [x_118, x_se_36], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_118 => convert_element_type_247, mul_303, sigmoid_37
#   x_se_36 => mean_9
# Graph fragment:
#   %convert_element_type_246 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_246]
#   %buf782 : Tensor "f32[128, 480, 1, 1][480, 1, 61440, 61440]cuda:0" = PlaceHolder[target=buf782]
#   %sigmoid_37 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_246,), kwargs = {})
#   %mul_303 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_246, %sigmoid_37), kwargs = {})
#   %convert_element_type_247 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_303, torch.float16), kwargs = {})
#   %mean_9 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_247, [2, 3], True), kwargs = {})
#   return %buf782,%mean_9
triton_red_fused_mean_silu_183 = async_compile.triton('triton_red_fused_mean_silu_183', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 256},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mean_silu_183', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48414720, 'r0_': 0}, 'kernel_num_gb': 0.04829184, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused_mean_silu_183(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 61440
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 480)
    x1 = xindex // 480
    _tmp6 = tl.full([XBLOCK, R0_BLOCK], 0, tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 480*r0_2 + 94080*x1), r0_mask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.sigmoid(tmp0)
        tmp2 = tmp0 * tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tmp3.to(tl.float32)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(r0_mask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tmp8 = 196.0
    tmp9 = (tmp6 / tmp8)
    tmp10 = tmp9.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 61440, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused_mean_silu_183.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused_mean_silu_183.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04829184
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/aj/cajzp2g6hx3atznl7jbxumxyib4xjvvt2mcvzuu4anx6lpkaewaq.py
# Topologically Sorted Source Nodes: [x_se_37], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_37 => convert_element_type_249
# Graph fragment:
#   %primals_305 : Tensor "f32[80, 480, 1, 1][480, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_305]
#   %convert_element_type_249 : Tensor "f16[80, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_305, torch.float16), kwargs = {})
#   return %convert_element_type_249
triton_poi_fused__to_copy_184 = async_compile.triton('triton_poi_fused__to_copy_184', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_184', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 307200}, 'kernel_num_gb': 0.0002304, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_184(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 38400
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((80, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 38400,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_184.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_184.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0002304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/b2/cb2n6j5lxojmayiuacptjhhxqqc3pot2tclvuaz27gqejv4n2zra.py
# Topologically Sorted Source Nodes: [x_se_37, x_se_38], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_37 => convert_element_type_248, convolution_95
#   x_se_38 => convert_element_type_250, convert_element_type_251, mul_304, sigmoid_38
# Graph fragment:
#   %buf785 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=buf785]
#   %primals_306 : Tensor "f32[80][1]cuda:0" = PlaceHolder[target=primals_306]
#   %convolution_95 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=convolution_95]
#   %convert_element_type_248 : Tensor "f16[80][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_306, torch.float16), kwargs = {})
#   %convolution_95 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_9, %convert_element_type_249, %convert_element_type_248, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_250 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_95, torch.float32), kwargs = {})
#   %sigmoid_38 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_250,), kwargs = {})
#   %mul_304 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_250, %sigmoid_38), kwargs = {})
#   %convert_element_type_251 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_304, torch.float16), kwargs = {})
#   return %convolution_95,%convert_element_type_251
triton_poi_fused__to_copy_convolution_silu_185 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_185', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_185', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 102720}, 'kernel_num_gb': 6.176e-05, 'kernel_flop': 9830400},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_185(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 10240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 80)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 10240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_185.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_185.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.176e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qt/cqtj4e3p2s5boa7imuie7amt7ooq6kvbpwic47innkama6o75rk4.py
# Topologically Sorted Source Nodes: [x_se_39], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_39 => convert_element_type_253
# Graph fragment:
#   %primals_307 : Tensor "f32[480, 80, 1, 1][80, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_307]
#   %convert_element_type_253 : Tensor "f16[480, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_307, torch.float16), kwargs = {})
#   return %convert_element_type_253
triton_poi_fused__to_copy_186 = async_compile.triton('triton_poi_fused__to_copy_186', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_186', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 307200}, 'kernel_num_gb': 0.0002304, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_186(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 38400
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((480, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((480, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 38400,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_186.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_186.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0002304
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2u/c2u5mfg6onexnk6qo5jui3ktjfh2g2o3cqa2e42f7z556dg3hur7.py
# Topologically Sorted Source Nodes: [x_se_39], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_39 => convert_element_type_252, convolution_96
# Graph fragment:
#   %buf789 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=buf789]
#   %primals_308 : Tensor "f32[480][1]cuda:0" = PlaceHolder[target=primals_308]
#   %convert_element_type_252 : Tensor "f16[480][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_308, torch.float16), kwargs = {})
#   %convolution_96 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_251, %convert_element_type_253, %convert_element_type_252, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_96
triton_poi_fused__to_copy_convolution_187 = async_compile.triton('triton_poi_fused__to_copy_convolution_187', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 65536}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_187', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 370560}, 'kernel_num_gb': 0.00024768, 'kernel_flop': 9830400},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_187(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 61440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 480)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, None)


def get_args():
    arg_0 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((480,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 61440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_187.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_187.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00024768
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d4/cd4tm3dimljebb36yj32m3t4b7xjd7klds7wa7lluq37sjo4ninp.py
# Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_9 => sigmoid_39
#   x_118 => convert_element_type_247, mul_303, sigmoid_37
#   x_119 => mul_305
# Graph fragment:
#   %convert_element_type_246 : Tensor "f32[128, 480, 14, 14][94080, 1, 6720, 480]cuda:0" = PlaceHolder[target=convert_element_type_246]
#   %convolution_96 : Tensor "f16[128, 480, 1, 1][480, 1, 480, 480]cuda:0" = PlaceHolder[target=convolution_96]
#   %sigmoid_37 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_246,), kwargs = {})
#   %mul_303 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_246, %sigmoid_37), kwargs = {})
#   %convert_element_type_247 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_303, torch.float16), kwargs = {})
#   %sigmoid_39 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_96,), kwargs = {})
#   %mul_305 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_247, %sigmoid_39), kwargs = {})
#   return %mul_305
triton_poi_fused_mul_sigmoid_silu_188 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_188', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 512}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_188', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 48168960, 'x': 48291840}, 'kernel_num_gb': 0.07237632, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_188(in_ptr0, in_ptr1, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 480
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 196
    y0 = (yindex % 196)
    tmp0 = tl.load(in_ptr0 + (x2 + 480*y3), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr1 + (x2 + 480*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (y0 + 196*x2 + 94080*y1), tmp6, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 1, 6720, 480), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 480, 1, 1), (480, 1, 480, 480), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 25088, 480,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_188.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_188.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.07237632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mf/cmfelmavgvu4hin7xqzf5nybzto5fpr3ulwnug5oofov4qco6ppp.py
# Topologically Sorted Source Nodes: [conv2d_97], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_97 => convert_element_type_254
# Graph fragment:
#   %primals_309 : Tensor "f32[80, 240, 1, 1][240, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_309]
#   %convert_element_type_254 : Tensor "f16[80, 240, 1, 1][240, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_309, torch.float16), kwargs = {})
#   return %convert_element_type_254
triton_poi_fused__to_copy_189 = async_compile.triton('triton_poi_fused__to_copy_189', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_189', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 153600}, 'kernel_num_gb': 0.0001152, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_189(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19200
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((80, 240, 1, 1), (240, 1, 240, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19200,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_189.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_189.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0001152
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rg/crgmgwfmug4yavrmrn6ussubzgtmw5k526fncanu5wenk7f74dvr.py
# Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_97], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_97 => convolution_97
#   sigmoid_9 => sigmoid_39
#   split_27 => split_with_sizes_63
#   x_118 => convert_element_type_247, mul_303, sigmoid_37
#   x_119 => mul_305
# Graph fragment:
#   %mul_305 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_305]
#   %sigmoid_37 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_246,), kwargs = {})
#   %mul_303 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_246, %sigmoid_37), kwargs = {})
#   %convert_element_type_247 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_303, torch.float16), kwargs = {})
#   %sigmoid_39 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_96,), kwargs = {})
#   %mul_305 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_247, %sigmoid_39), kwargs = {})
#   %split_with_sizes_63 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_305, [240, 240], 1), kwargs = {})
#   %convolution_97 : Tensor "f16[128, 80, 14, 14][15680, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_260, %convert_element_type_254, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf793
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/if/cifyi34ympuuhwp5okxwioyhcwr6e2b2olo6jfdapamqtadeoajf.py
# Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_98], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_98 => convolution_98
#   sigmoid_9 => sigmoid_39
#   split_27 => split_with_sizes_63
#   x_118 => convert_element_type_247, mul_303, sigmoid_37
#   x_119 => mul_305
# Graph fragment:
#   %mul_305 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0" = PlaceHolder[target=mul_305]
#   %sigmoid_37 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_246,), kwargs = {})
#   %mul_303 : Tensor "f32[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_246, %sigmoid_37), kwargs = {})
#   %convert_element_type_247 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_303, torch.float16), kwargs = {})
#   %sigmoid_39 : Tensor "f16[128, 480, 1, 1][480, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_96,), kwargs = {})
#   %mul_305 : Tensor "f16[128, 480, 14, 14][94080, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_247, %sigmoid_39), kwargs = {})
#   %split_with_sizes_63 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_305, [240, 240], 1), kwargs = {})
#   %convolution_98 : Tensor "f16[128, 80, 14, 14][15680, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_261, %convert_element_type_255, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf796
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (47040 + x2 + 196*y0 + 94080*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 480, 14, 14), (94080, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mh/cmh4a67xqk4dljeqzksr23azqnvaoxoimjkcbs7b4ysxsde6ethy.py
# Topologically Sorted Source Nodes: [x_120], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_120 => cat_27
# Graph fragment:
#   %convolution_97 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=convolution_97]
#   %convolution_98 : Tensor "f16[128, 80, 14, 14][15680, 1, 1120, 80]cuda:0" = PlaceHolder[target=convolution_98]
#   %cat_27 : Tensor "f16[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_97, %convolution_98], 1), kwargs = {})
#   return %cat_27
triton_poi_fused_cat_192 = async_compile.triton('triton_poi_fused_cat_192', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_192', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 32112640}, 'kernel_num_gb': 0.01605632, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_192(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 160)
    x1 = xindex // 160
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 80, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (80*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 160, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (80*x1 + ((-80) + x0)), tmp6, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, None)


def get_args():
    arg_0 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 80, 14, 14), (15680, 1, 1120, 80), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_192.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_192.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01605632
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/js/cjsbn67vzezjt5toi2zzxxji6e7aqw5wvhvjgfn4buprd6h4vd2n.py
# Topologically Sorted Source Nodes: [x_121, x_122], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_121 => add_199, add_202, convert_element_type_256, convert_element_type_257, mul_306, mul_312, rsqrt_38, sub_38, unsqueeze_152, unsqueeze_153, unsqueeze_154, unsqueeze_155, var_mean_38
#   x_122 => add_203
# Graph fragment:
#   %cat_27 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=cat_27]
#   %getitem_263 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=getitem_263]
#   %buf806 : Tensor "f32[1, 160, 1, 1][160, 1, 160, 160]cuda:0" = PlaceHolder[target=buf806]
#   %primals_314 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_314]
#   %primals_315 : Tensor "f32[160][1]cuda:0" = PlaceHolder[target=primals_315]
#   %convert_element_type_233 : Tensor "f16[128, 160, 14, 14][31360, 1, 2240, 160]cuda:0" = PlaceHolder[target=convert_element_type_233]
#   %convert_element_type_256 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_27, torch.float32), kwargs = {})
#   %var_mean_38 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_256, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_199 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_262, 1e-05), kwargs = {})
#   %rsqrt_38 : Tensor "f32[1, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_199,), kwargs = {})
#   %sub_38 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_27, %getitem_263), kwargs = {})
#   %mul_306 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_38, %rsqrt_38), kwargs = {})
#   %unsqueeze_152 : Tensor "f32[160, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_314, -1), kwargs = {})
#   %unsqueeze_153 : Tensor "f32[160, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_152, -1), kwargs = {})
#   %mul_312 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_306, %unsqueeze_153), kwargs = {})
#   %unsqueeze_154 : Tensor "f32[160, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_315, -1), kwargs = {})
#   %unsqueeze_155 : Tensor "f32[160, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_154, -1), kwargs = {})
#   %add_202 : Tensor "f32[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_312, %unsqueeze_155), kwargs = {})
#   %convert_element_type_257 : Tensor "f16[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_202, torch.float16), kwargs = {})
#   %add_203 : Tensor "f16[128, 160, 14, 14][31360, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_257, %convert_element_type_233), kwargs = {})
#   return %add_203
triton_poi_fused__native_batch_norm_legit_functional_add_193 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_193', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4194304}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_193', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 32115200}, 'kernel_num_gb': 0.02408704, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_193(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4014080
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 160)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 25088.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(in_out_ptr0 + (x2), tmp17, None)


def get_args():
    arg_0 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 160, 14, 14), (31360, 1, 2240, 160), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((160,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 4014080,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_193.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_193.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408704
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yu/cyuva2ucs4r5onabumhw5hnlcd3ia3k33jwopoqzalwkalynfjbw.py
# Topologically Sorted Source Nodes: [x_143], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_143 => convert_element_type_306
# Graph fragment:
#   %primals_370 : Tensor "f32[960, 160, 1, 1][160, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_370]
#   %convert_element_type_306 : Tensor "f16[960, 160, 1, 1][160, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_370, torch.float16), kwargs = {})
#   return %convert_element_type_306
triton_poi_fused__to_copy_194 = async_compile.triton('triton_poi_fused__to_copy_194', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_194', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1228800}, 'kernel_num_gb': 0.0009216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_194(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 153600
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((960, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((960, 160, 1, 1), (160, 1, 160, 160), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 153600,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_194.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_194.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0009216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/l3/cl3gdhrmbffitgvkw755im736k3u26luc5k2ya3fv4unrw553gcv.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_144 => convert_element_type_307, var_mean_45
# Graph fragment:
#   %convolution_119 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convolution_119]
#   %convert_element_type_307 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_119, torch.float32), kwargs = {})
#   %var_mean_45 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_307, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf958,%buf959,%buf960
triton_red_fused__native_batch_norm_legit_functional_195 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_195', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 256},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_195', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 51118080, 'r0_': 0}, 'kernel_num_gb': 0.04964352, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_195(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 196
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 960)
    x1 = xindex // 960
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_2 + 188160*x1), r0_mask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, None)
    tl.store(out_ptr1 + (x3), tmp7, None)
    tl.store(out_ptr2 + (x3), tmp8, None)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 122880, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_195.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_195.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.04964352
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zf/czfo2rogl6bm2ctwrffhtzussyafwisedxxxudw32y7v5leno66n.py
# Topologically Sorted Source Nodes: [x_144], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_144 => add_237, add_238, add_239, convert_element_type_307, mul_364, mul_365, mul_366, mul_367, mul_368, rsqrt_45, squeeze_135, squeeze_137, var_mean_45
# Graph fragment:
#   %buf958 : Tensor "f32[1, 960, 1, 1, 128][122880, 1, 122880, 122880, 960]cuda:0" = PlaceHolder[target=buf958]
#   %buf959 : Tensor "f32[1, 960, 1, 1, 128][122880, 1, 122880, 122880, 960]cuda:0" = PlaceHolder[target=buf959]
#   %buf960 : Tensor "f32[1, 960, 1, 1, 128][122880, 1, 122880, 122880, 960]cuda:0" = PlaceHolder[target=buf960]
#   %buf962 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=buf962]
#   %getitem_325 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_325]
#   %copy__136 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=copy__136]
#   %add_238 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=add_238]
#   %copy__137 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=copy__137]
#   %add_239 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=add_239]
#   %convert_element_type_307 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_119, torch.float32), kwargs = {})
#   %var_mean_45 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_307, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_237 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_324, 1e-05), kwargs = {})
#   %rsqrt_45 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_237,), kwargs = {})
#   %squeeze_135 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_325, [0, 2, 3]), kwargs = {})
#   %mul_364 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_135, 0.1), kwargs = {})
#   %mul_365 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_372, 0.9), kwargs = {})
#   %add_238 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_364, %mul_365), kwargs = {})
#   %squeeze_137 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_324, [0, 2, 3]), kwargs = {})
#   %mul_366 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_137, 1.0000398612827361), kwargs = {})
#   %mul_367 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_366, 0.1), kwargs = {})
#   %mul_368 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_373, 0.9), kwargs = {})
#   %add_239 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_367, %mul_368), kwargs = {})
#   %copy__136 : Tensor "f32[960][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_372, %add_238), kwargs = {})
#   %copy__137 : Tensor "f32[960][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_373, %add_239), kwargs = {})
#   return %getitem_325,%buf962,%rsqrt_45,%add_238,%buf1557,%add_239,%buf1560
triton_red_fused__native_batch_norm_legit_functional_copy__196 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_copy__196', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 1024, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]], (10,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_copy__196', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1512960, 'r0_': 0}, 'kernel_num_gb': 0.0014976, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_copy__196(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = xindex
    tmp6_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp6_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_1 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + 960*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tl.load(in_ptr2 + (x0 + 960*r0_1), r0_mask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_combine(
            tmp6_mean, tmp6_m2, tmp6_weight,
            tmp3, tmp4, tmp5
        )
        tmp6_mean = tl.where(r0_mask & xmask, tmp6_mean_next, tmp6_mean)
        tmp6_m2 = tl.where(r0_mask & xmask, tmp6_m2_next, tmp6_m2)
        tmp6_weight = tl.where(r0_mask & xmask, tmp6_weight_next, tmp6_weight)
    tmp7, tmp8, tmp9 = triton_helpers.welford(tmp6_mean, tmp6_m2, tmp6_weight, 1)
    tmp6 = tmp7[:, None]
    tmp10 = tmp8[:, None]
    tmp11 = tmp9[:, None]
    tl.store(out_ptr0 + (x0), tmp6, xmask)
    tmp19 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp26 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = 25088.0
    tmp13 = (tmp10 / tmp12)
    tmp14 = 1e-05
    tmp15 = tmp13 + tmp14
    tmp16 = libdevice.rsqrt(tmp15)
    tmp17 = 0.1
    tmp18 = tmp6 * tmp17
    tmp20 = 0.9
    tmp21 = tmp19 * tmp20
    tmp22 = tmp18 + tmp21
    tmp23 = 1.0000398612827361
    tmp24 = tmp13 * tmp23
    tmp25 = tmp24 * tmp17
    tmp27 = tmp26 * tmp20
    tmp28 = tmp25 + tmp27
    tl.store(out_ptr2 + (x0), tmp16, xmask)
    tl.store(out_ptr4 + (x0), tmp22, xmask)
    tl.store(out_ptr6 + (x0), tmp28, xmask)


def get_args():
    arg_0 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 960, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_copy__196.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_copy__196.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0014976
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2v/c2vuxignodptobl5env3auq365ujllwfuneoartcxzbogssdw7dz.py
# Topologically Sorted Source Nodes: [x_144, x_145], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_144 => add_240, convert_element_type_308, mul_363, mul_369, sub_45, unsqueeze_180, unsqueeze_181, unsqueeze_182, unsqueeze_183
#   x_145 => convert_element_type_309, convert_element_type_310, mul_370, sigmoid_48
# Graph fragment:
#   %convolution_119 : Tensor "f16[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convolution_119]
#   %getitem_325 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_325]
#   %rsqrt_45 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_45]
#   %primals_374 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_374]
#   %primals_375 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_375]
#   %convert_element_type_309 : Tensor "f32[128, 960, 14, 14][188160, 1, 13440, 960]cuda:0" = PlaceHolder[target=convert_element_type_309]
#   %sub_45 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_119, %getitem_325), kwargs = {})
#   %mul_363 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_45, %rsqrt_45), kwargs = {})
#   %unsqueeze_180 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_374, -1), kwargs = {})
#   %unsqueeze_181 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_180, -1), kwargs = {})
#   %mul_369 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_363, %unsqueeze_181), kwargs = {})
#   %unsqueeze_182 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_375, -1), kwargs = {})
#   %unsqueeze_183 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_182, -1), kwargs = {})
#   %add_240 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_369, %unsqueeze_183), kwargs = {})
#   %convert_element_type_308 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_240, torch.float16), kwargs = {})
#   %convert_element_type_309 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_308, torch.float32), kwargs = {})
#   %sigmoid_48 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_309,), kwargs = {})
#   %mul_370 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_309, %sigmoid_48), kwargs = {})
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_370, torch.float16), kwargs = {})
#   return %convert_element_type_309,%convert_element_type_310
triton_poi_fused__native_batch_norm_legit_functional_silu_197 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_197', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 1024}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_197', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 96337920, 'x': 48184320}, 'kernel_num_gb': 0.09635328, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_197(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 960
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 196)
    y3 = yindex // 196
    tmp0 = tl.load(in_ptr0 + (x1 + 960*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 196*x1 + 188160*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 1, 13440, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 25088, 960,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_197.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_197.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.09635328
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/iy/ciyidffoeliosfa7d7thi27a4nyv5jrms2ephb46esy3tmv77lnf.py
# Topologically Sorted Source Nodes: [conv2d_120], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_120 => convert_element_type_311
# Graph fragment:
#   %primals_376 : Tensor "f32[240, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_376]
#   %convert_element_type_311 : Tensor "f16[240, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_376, torch.float16), kwargs = {})
#   return %convert_element_type_311
triton_poi_fused__to_copy_198 = async_compile.triton('triton_poi_fused__to_copy_198', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_198', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 17280}, 'kernel_num_gb': 1.296e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_198(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 2160
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 2160,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_198.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_198.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.296e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qk/cqkdcdcui64cirsg3uxrt5bd6mubvhzws6fov64guvwu3cvhe2yq.py
# Topologically Sorted Source Nodes: [x_145, conv2d_120], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_120 => convolution_120, split_with_sizes_79
#   x_145 => convert_element_type_310, mul_370, sigmoid_48
# Graph fragment:
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_310]
#   %sigmoid_48 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_309,), kwargs = {})
#   %mul_370 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_309, %sigmoid_48), kwargs = {})
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_370, torch.float16), kwargs = {})
#   %split_with_sizes_79 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_310, [240, 240, 240, 240], 1), kwargs = {})
#   %convolution_120 : Tensor "f16[128, 240, 7, 7][11760, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_330, %convert_element_type_311, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), kwargs = {})
#   return %buf968
triton_poi_fused_convolution_silu_split_with_sizes_199 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_199', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_199', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_199(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (x2 + 196*y0 + 188160*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_199.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_199.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w5/cw5fmdeotutw6vh27xaknzfnz5upiqu7vcjkwzb66k5qbfbyhril.py
# Topologically Sorted Source Nodes: [conv2d_121], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_121 => convert_element_type_312
# Graph fragment:
#   %primals_377 : Tensor "f32[240, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_377]
#   %convert_element_type_312 : Tensor "f16[240, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_377, torch.float16), kwargs = {})
#   return %convert_element_type_312
triton_poi_fused__to_copy_200 = async_compile.triton('triton_poi_fused__to_copy_200', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8192}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_200', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48000}, 'kernel_num_gb': 3.6e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_200(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 6000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_200.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_200.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 3.6e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cg/ccg422l2eca34ko3o3dgf2oc3u5mjm6clcoqjfhqnsu6zi2tudnj.py
# Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_121], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_120 => split_with_sizes_79
#   conv2d_121 => convolution_121
#   x_145 => convert_element_type_310, mul_370, sigmoid_48
# Graph fragment:
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_310]
#   %sigmoid_48 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_309,), kwargs = {})
#   %mul_370 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_309, %sigmoid_48), kwargs = {})
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_370, torch.float16), kwargs = {})
#   %split_with_sizes_79 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_310, [240, 240, 240, 240], 1), kwargs = {})
#   %convolution_121 : Tensor "f16[128, 240, 7, 7][11760, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_335, %convert_element_type_312, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 240), kwargs = {})
#   return %buf971
triton_poi_fused_convolution_silu_split_with_sizes_201 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_201', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_201', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_201(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (47040 + x2 + 196*y0 + 188160*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_201.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_201.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/re/crezsja3cw7rftpnoxvwxhfmohbozkxq2yh2a6dmogy2gqx7tsno.py
# Topologically Sorted Source Nodes: [conv2d_122], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_122 => convert_element_type_313
# Graph fragment:
#   %primals_378 : Tensor "f32[240, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_378]
#   %convert_element_type_313 : Tensor "f16[240, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_378, torch.float16), kwargs = {})
#   return %convert_element_type_313
triton_poi_fused__to_copy_202 = async_compile.triton('triton_poi_fused__to_copy_202', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_202', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 94080}, 'kernel_num_gb': 7.056e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_202(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 11760
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 11760,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_202.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_202.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 7.056e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/e2/ce27j2xrxqkvv6fmaviowmdpkbabqdoqcjx5lgowltplakhusg4q.py
# Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_122], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_120 => split_with_sizes_79
#   conv2d_122 => convolution_122
#   x_145 => convert_element_type_310, mul_370, sigmoid_48
# Graph fragment:
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_310]
#   %sigmoid_48 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_309,), kwargs = {})
#   %mul_370 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_309, %sigmoid_48), kwargs = {})
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_370, torch.float16), kwargs = {})
#   %split_with_sizes_79 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_310, [240, 240, 240, 240], 1), kwargs = {})
#   %convolution_122 : Tensor "f16[128, 240, 7, 7][11760, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_340, %convert_element_type_313, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 240), kwargs = {})
#   return %buf974
triton_poi_fused_convolution_silu_split_with_sizes_203 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_203', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_203', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_203(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (94080 + x2 + 196*y0 + 188160*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_203.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_203.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uk/cukbyxmu2slqrx7s6okavsoetrvqbtktxsx7ddydafegxfpe6pik.py
# Topologically Sorted Source Nodes: [conv2d_123], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_123 => convert_element_type_314
# Graph fragment:
#   %primals_379 : Tensor "f32[240, 1, 9, 9][81, 81, 9, 1]cuda:0" = PlaceHolder[target=primals_379]
#   %convert_element_type_314 : Tensor "f16[240, 1, 9, 9][81, 81, 9, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_379, torch.float16), kwargs = {})
#   return %convert_element_type_314
triton_poi_fused__to_copy_204 = async_compile.triton('triton_poi_fused__to_copy_204', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_204', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 155520}, 'kernel_num_gb': 0.00011664, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_204(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((240, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((240, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_204.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_204.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00011664
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ft/cfta2qpbuuytcnc3ua37b3r7we2twicgfcsml4qjik6it6hjsbkg.py
# Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_123], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_120 => split_with_sizes_79
#   conv2d_123 => convolution_123
#   x_145 => convert_element_type_310, mul_370, sigmoid_48
# Graph fragment:
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0" = PlaceHolder[target=convert_element_type_310]
#   %sigmoid_48 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_309,), kwargs = {})
#   %mul_370 : Tensor "f32[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_309, %sigmoid_48), kwargs = {})
#   %convert_element_type_310 : Tensor "f16[128, 960, 14, 14][188160, 196, 14, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_370, torch.float16), kwargs = {})
#   %split_with_sizes_79 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_310, [240, 240, 240, 240], 1), kwargs = {})
#   %convolution_123 : Tensor "f16[128, 240, 7, 7][11760, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_345, %convert_element_type_314, None, [2, 2], [4, 4], [1, 1], False, [0, 0], 240), kwargs = {})
#   return %buf977
triton_poi_fused_convolution_silu_split_with_sizes_205 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_205', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 32768, 'x': 256}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_205', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 24084480, 'x': 12042240}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_205(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 30720
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = tl.full([YBLOCK, XBLOCK], True, tl.int1)
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 240)
    y1 = yindex // 240
    tmp0 = tl.load(in_ptr0 + (141120 + x2 + 196*y0 + 188160*y1), xmask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 240*x2 + 47040*y1), tmp0, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 14, 14), (188160, 196, 14, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 14, 14), (47040, 1, 3360, 240), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 30720, 196,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_205.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_205.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/qc/cqc2zrgwiyyja45rynaso2yvyvrr7ygqwxxcqeratjl2irnvlw6w.py
# Topologically Sorted Source Nodes: [x_146], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_146 => cat_34
# Graph fragment:
#   %convolution_120 : Tensor "f16[128, 240, 7, 7][11760, 1, 1680, 240]cuda:0" = PlaceHolder[target=convolution_120]
#   %convolution_121 : Tensor "f16[128, 240, 7, 7][11760, 1, 1680, 240]cuda:0" = PlaceHolder[target=convolution_121]
#   %convolution_122 : Tensor "f16[128, 240, 7, 7][11760, 1, 1680, 240]cuda:0" = PlaceHolder[target=convolution_122]
#   %convolution_123 : Tensor "f16[128, 240, 7, 7][11760, 1, 1680, 240]cuda:0" = PlaceHolder[target=convolution_123]
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_120, %convolution_121, %convolution_122, %convolution_123], 1), kwargs = {})
#   return %cat_34
triton_poi_fused_cat_206 = async_compile.triton('triton_poi_fused_cat_206', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_206', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 72253440}, 'kernel_num_gb': 0.02408448, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_206(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 960)
    x1 = xindex // 960
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 240, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (240*x1 + (x0)), tmp4, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 480, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (240*x1 + ((-240) + x0)), tmp9, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 720, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (240*x1 + ((-480) + x0)), tmp14, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 960, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (240*x1 + ((-720) + x0)), tmp16, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, None)


def get_args():
    arg_0 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 240, 7, 7), (11760, 1, 1680, 240), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_206.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_206.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02408448
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ho/choqaypbj2yw3qwxz35z3decwbfadydmickzmukda4iccrackskf.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_147 => convert_element_type_315, var_mean_46
# Graph fragment:
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=cat_34]
#   %convert_element_type_315 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_34, torch.float32), kwargs = {})
#   %var_mean_46 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_315, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf980,%buf981,%buf982
triton_red_fused__native_batch_norm_legit_functional_207 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_207', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 65536, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_207', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13171200, 'r0_': 0}, 'kernel_num_gb': 0.01260672, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_207(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 47040
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 960)
    x1 = xindex // 960
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_2 + 122880*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 47040, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_207.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_207.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.01260672
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ul/culkwsaoythpkzf4f4bx2fodmbmnkpksnxc7goafd2yjwav2wi23.py
# Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_147 => add_242, add_243, add_244, convert_element_type_315, mul_372, mul_373, mul_374, mul_375, mul_376, rsqrt_46, squeeze_138, squeeze_140, var_mean_46
# Graph fragment:
#   %buf980 : Tensor "f32[1, 960, 1, 1, 49][47040, 1, 47040, 47040, 960]cuda:0" = PlaceHolder[target=buf980]
#   %buf981 : Tensor "f32[1, 960, 1, 1, 49][47040, 1, 47040, 47040, 960]cuda:0" = PlaceHolder[target=buf981]
#   %buf982 : Tensor "f32[1, 960, 1, 1, 49][47040, 1, 47040, 47040, 960]cuda:0" = PlaceHolder[target=buf982]
#   %buf984 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=buf984]
#   %getitem_347 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_347]
#   %copy__139 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=copy__139]
#   %add_243 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=add_243]
#   %copy__140 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=copy__140]
#   %add_244 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=add_244]
#   %convert_element_type_315 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_34, torch.float32), kwargs = {})
#   %var_mean_46 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_315, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_242 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_346, 1e-05), kwargs = {})
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_242,), kwargs = {})
#   %squeeze_138 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_347, [0, 2, 3]), kwargs = {})
#   %mul_372 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_138, 0.1), kwargs = {})
#   %mul_373 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_381, 0.9), kwargs = {})
#   %add_243 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_372, %mul_373), kwargs = {})
#   %squeeze_140 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_346, [0, 2, 3]), kwargs = {})
#   %mul_374 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_140, 1.0001594642002871), kwargs = {})
#   %mul_375 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_374, 0.1), kwargs = {})
#   %mul_376 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_382, 0.9), kwargs = {})
#   %add_244 : Tensor "f32[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_375, %mul_376), kwargs = {})
#   %copy__139 : Tensor "f32[960][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_381, %add_243), kwargs = {})
#   %copy__140 : Tensor "f32[960][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_382, %add_244), kwargs = {})
#   return %getitem_347,%buf984,%rsqrt_46,%add_243,%buf1565,%add_244,%buf1568
triton_per_fused__native_batch_norm_legit_functional_copy__208 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__208', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 1024, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__208', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 602880, 'r0_': 0}, 'kernel_num_gb': 0.00058752, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__208(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 960
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 960*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 960*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 6272.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0001594642002871
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 960, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__208.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__208.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00058752
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nj/cnjsz6kvc4jhcfi6orklf3ligmo2a3oiroby3654mnzjsabuti37.py
# Topologically Sorted Source Nodes: [x_147, x_148], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_147 => add_245, convert_element_type_316, mul_371, mul_377, sub_46, unsqueeze_184, unsqueeze_185, unsqueeze_186, unsqueeze_187
#   x_148 => convert_element_type_317
# Graph fragment:
#   %cat_34 : Tensor "f16[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=cat_34]
#   %getitem_347 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=getitem_347]
#   %rsqrt_46 : Tensor "f32[1, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=rsqrt_46]
#   %primals_383 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_383]
#   %primals_384 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_384]
#   %sub_46 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_34, %getitem_347), kwargs = {})
#   %mul_371 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_46, %rsqrt_46), kwargs = {})
#   %unsqueeze_184 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_383, -1), kwargs = {})
#   %unsqueeze_185 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_184, -1), kwargs = {})
#   %mul_377 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_371, %unsqueeze_185), kwargs = {})
#   %unsqueeze_186 : Tensor "f32[960, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_384, -1), kwargs = {})
#   %unsqueeze_187 : Tensor "f32[960, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_186, -1), kwargs = {})
#   %add_245 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_377, %unsqueeze_187), kwargs = {})
#   %convert_element_type_316 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_245, torch.float16), kwargs = {})
#   %convert_element_type_317 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_316, torch.float32), kwargs = {})
#   return %convert_element_type_317
triton_poi_fused__native_batch_norm_legit_functional_silu_209 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_209', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_209', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 60226560}, 'kernel_num_gb': 0.03614208, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_209(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 960)
    tmp0 = tl.load(in_ptr0 + (x2), None).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_209.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_209.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03614208
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ha/chajjkgk7me7idteeo5stjfapodagurqva3xrzxixekaytwmqwwa.py
# Topologically Sorted Source Nodes: [x_148, x_se_48], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_148 => convert_element_type_318, mul_378, sigmoid_49
#   x_se_48 => mean_12
# Graph fragment:
#   %convert_element_type_317 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=convert_element_type_317]
#   %buf988 : Tensor "f32[128, 960, 1, 1][960, 1, 122880, 122880]cuda:0" = PlaceHolder[target=buf988]
#   %sigmoid_49 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_317,), kwargs = {})
#   %mul_378 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_317, %sigmoid_49), kwargs = {})
#   %convert_element_type_318 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_378, torch.float16), kwargs = {})
#   %mean_12 : Tensor "f16[128, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_318, [2, 3], True), kwargs = {})
#   return %buf988,%mean_12
triton_per_fused_mean_silu_210 = async_compile.triton('triton_per_fused_mean_silu_210', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 131072, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_mean_silu_210', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 24576000, 'r0_': 0}, 'kernel_num_gb': 0.02433024, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_mean_silu_210(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 122880
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_2 = r0_index
    x0 = (xindex % 960)
    x1 = xindex // 960
    x3 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 960*r0_2 + 47040*x1), r0_mask, other=0.0)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask, tmp5, 0)
    tmp8 = tl.sum(tmp7, 1)[:, None].to(tl.float32)
    tmp9 = 49.0
    tmp10 = (tmp8 / tmp9)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp11, None)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 122880, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_mean_silu_210.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_mean_silu_210.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.02433024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nb/cnb7su62hjbhkdrqdsjuocc5pnoqpsmfpnoaztjb3wjkxx2cgqkc.py
# Topologically Sorted Source Nodes: [x_se_49], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_49 => convert_element_type_320
# Graph fragment:
#   %primals_385 : Tensor "f32[80, 960, 1, 1][960, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_385]
#   %convert_element_type_320 : Tensor "f16[80, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_385, torch.float16), kwargs = {})
#   return %convert_element_type_320
triton_poi_fused__to_copy_211 = async_compile.triton('triton_poi_fused__to_copy_211', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_211', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 614400}, 'kernel_num_gb': 0.0004608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_211(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 76800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((80, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((80, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 76800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_211.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_211.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/jl/cjl2cagvgq2arte64fcsrjosgcg5ozy4nezeuxdysenrjoif6n5l.py
# Topologically Sorted Source Nodes: [x_se_49, x_se_50], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_49 => convert_element_type_319, convolution_124
#   x_se_50 => convert_element_type_321, convert_element_type_322, mul_379, sigmoid_50
# Graph fragment:
#   %buf991 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=buf991]
#   %primals_386 : Tensor "f32[80][1]cuda:0" = PlaceHolder[target=primals_386]
#   %convolution_124 : Tensor "f16[128, 80, 1, 1][80, 1, 80, 80]cuda:0" = PlaceHolder[target=convolution_124]
#   %convert_element_type_319 : Tensor "f16[80][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_386, torch.float16), kwargs = {})
#   %convolution_124 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_12, %convert_element_type_320, %convert_element_type_319, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_321 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_124, torch.float32), kwargs = {})
#   %sigmoid_50 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_321,), kwargs = {})
#   %mul_379 : Tensor "f32[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_321, %sigmoid_50), kwargs = {})
#   %convert_element_type_322 : Tensor "f16[128, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_379, torch.float16), kwargs = {})
#   return %convolution_124,%convert_element_type_322
triton_poi_fused__to_copy_convolution_silu_212 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_212', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_212', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 102720}, 'kernel_num_gb': 6.176e-05, 'kernel_flop': 19660800},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_212(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 10240
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 80)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((80,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 10240,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_212.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_212.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6.176e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3o/c3olofyrvqtdyelgdlhnlqxz3ruvksh5waixanqykwkatznxpppv.py
# Topologically Sorted Source Nodes: [x_se_51], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_51 => convert_element_type_324
# Graph fragment:
#   %primals_387 : Tensor "f32[960, 80, 1, 1][80, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_387]
#   %convert_element_type_324 : Tensor "f16[960, 80, 1, 1][80, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_387, torch.float16), kwargs = {})
#   return %convert_element_type_324
triton_poi_fused__to_copy_213 = async_compile.triton('triton_poi_fused__to_copy_213', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_213', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 614400}, 'kernel_num_gb': 0.0004608, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_213(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 76800
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((960, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((960, 80, 1, 1), (80, 1, 80, 80), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 76800,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_213.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_213.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.0004608
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/uz/cuziyvpgaib4yxxdx67dufvw2lq4ktdfeg5kja4gexgyax3meqf5.py
# Topologically Sorted Source Nodes: [x_se_51], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_51 => convert_element_type_323, convolution_125
# Graph fragment:
#   %buf995 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=buf995]
#   %primals_388 : Tensor "f32[960][1]cuda:0" = PlaceHolder[target=primals_388]
#   %convert_element_type_323 : Tensor "f16[960][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_388, torch.float16), kwargs = {})
#   %convolution_125 : Tensor "f16[128, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_322, %convert_element_type_324, %convert_element_type_323, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_125
triton_poi_fused__to_copy_convolution_214 = async_compile.triton('triton_poi_fused__to_copy_convolution_214', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_214', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 741120}, 'kernel_num_gb': 0.00049536, 'kernel_flop': 19660800},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_214(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 122880
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 960)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, None)


def get_args():
    arg_0 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((960,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 122880,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_214.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_214.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00049536
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fn/cfnpjkh5woppljxyxpzdr642cjp34cyk3gw736nzossjar3rw5ff.py
# Topologically Sorted Source Nodes: [x_148, sigmoid_12, x_149], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_12 => sigmoid_51
#   x_148 => convert_element_type_318, mul_378, sigmoid_49
#   x_149 => mul_380
# Graph fragment:
#   %convert_element_type_317 : Tensor "f32[128, 960, 7, 7][47040, 1, 6720, 960]cuda:0" = PlaceHolder[target=convert_element_type_317]
#   %convolution_125 : Tensor "f16[128, 960, 1, 1][960, 1, 960, 960]cuda:0" = PlaceHolder[target=convolution_125]
#   %sigmoid_49 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_317,), kwargs = {})
#   %mul_378 : Tensor "f32[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_317, %sigmoid_49), kwargs = {})
#   %convert_element_type_318 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_378, torch.float16), kwargs = {})
#   %sigmoid_51 : Tensor "f16[128, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_125,), kwargs = {})
#   %mul_380 : Tensor "f16[128, 960, 7, 7][47040, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_318, %sigmoid_51), kwargs = {})
#   return %mul_380
triton_poi_fused_mul_sigmoid_silu_215 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_215', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 8388608}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_215', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 48414720}, 'kernel_num_gb': 0.03637248, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_215(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6021120
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x3 = xindex
    x0 = (xindex % 960)
    x2 = xindex // 47040
    tmp0 = tl.load(in_ptr0 + (x3), None)
    tmp4 = tl.load(in_ptr1 + (x0 + 960*x2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (x3), tmp6, None)


def get_args():
    arg_0 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 960, 7, 7), (47040, 1, 6720, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 6021120,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_215.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_215.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.03637248
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nj/cnjteubikgxrmvtbqfeey5342pwfhvb2bpsvkxw27yr43yduegkh.py
# Topologically Sorted Source Nodes: [x_150], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_150 => convert_element_type_325
# Graph fragment:
#   %primals_389 : Tensor "f32[264, 960, 1, 1][960, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_389]
#   %convert_element_type_325 : Tensor "f16[264, 960, 1, 1][960, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_389, torch.float16), kwargs = {})
#   return %convert_element_type_325
triton_poi_fused__to_copy_216 = async_compile.triton('triton_poi_fused__to_copy_216', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_216', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 2027520}, 'kernel_num_gb': 0.00152064, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_216(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 253440
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((264, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((264, 960, 1, 1), (960, 1, 960, 960), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 253440,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_216.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_216.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.00152064
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/rj/crjywsqchvvyx7xtiuv7dwfgrhm6oc2lvf6hevxmfvgrsvyu4xx2.py
# Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_151 => convert_element_type_326, var_mean_47
# Graph fragment:
#   %convolution_126 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=convolution_126]
#   %convert_element_type_326 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_126, torch.float32), kwargs = {})
#   %var_mean_47 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_326, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf1000,%buf1001,%buf1002
triton_red_fused__native_batch_norm_legit_functional_217 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_217', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 16384, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_217', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3622080, 'r0_': 0}, 'kernel_num_gb': 0.003466848, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_217(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 12936
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 264)
    x1 = xindex // 264
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_2 + 33792*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 12936, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_217.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_217.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.003466848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/2b/c2bpvydsnmqymvolx5bphquvs3nolgfx2xkaf6jvjjgx3k2w3l4n.py
# Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_151 => add_247, add_248, add_249, convert_element_type_326, mul_382, mul_383, mul_384, mul_385, mul_386, rsqrt_47, squeeze_141, squeeze_143, var_mean_47
# Graph fragment:
#   %buf1000 : Tensor "f32[1, 264, 1, 1, 49][12936, 1, 12936, 12936, 264]cuda:0" = PlaceHolder[target=buf1000]
#   %buf1001 : Tensor "f32[1, 264, 1, 1, 49][12936, 1, 12936, 12936, 264]cuda:0" = PlaceHolder[target=buf1001]
#   %buf1002 : Tensor "f32[1, 264, 1, 1, 49][12936, 1, 12936, 12936, 264]cuda:0" = PlaceHolder[target=buf1002]
#   %buf1004 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=buf1004]
#   %getitem_349 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=getitem_349]
#   %copy__142 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=copy__142]
#   %add_248 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=add_248]
#   %copy__143 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=copy__143]
#   %add_249 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=add_249]
#   %convert_element_type_326 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_126, torch.float32), kwargs = {})
#   %var_mean_47 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_326, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_247 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_348, 1e-05), kwargs = {})
#   %rsqrt_47 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_247,), kwargs = {})
#   %squeeze_141 : Tensor "f32[264][1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_349, [0, 2, 3]), kwargs = {})
#   %mul_382 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_141, 0.1), kwargs = {})
#   %mul_383 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_391, 0.9), kwargs = {})
#   %add_248 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_382, %mul_383), kwargs = {})
#   %squeeze_143 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_348, [0, 2, 3]), kwargs = {})
#   %mul_384 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_143, 1.0001594642002871), kwargs = {})
#   %mul_385 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_384, 0.1), kwargs = {})
#   %mul_386 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_392, 0.9), kwargs = {})
#   %add_249 : Tensor "f32[264][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_385, %mul_386), kwargs = {})
#   %copy__142 : Tensor "f32[264][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_391, %add_248), kwargs = {})
#   %copy__143 : Tensor "f32[264][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_392, %add_249), kwargs = {})
#   return %getitem_349,%buf1004,%rsqrt_47,%add_248,%buf1573,%add_249,%buf1576
triton_per_fused__native_batch_norm_legit_functional_copy__218 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__218', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 512, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__218', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 167904, 'r0_': 0}, 'kernel_num_gb': 0.000162624, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__218(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 264
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 264*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 264*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 264*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 6272.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0001594642002871
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)
    tl.store(out_ptr1 + (x0), tmp14, xmask)


def get_args():
    arg_0 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_9 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, 264, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__218.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__218.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000162624
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7e/c7elqbjiwdgtsthe5o7xqbunl6ciwsi2piaaeub23ov7kersckqq.py
# Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_151 => add_247, add_250, convert_element_type_326, convert_element_type_327, mul_381, mul_387, rsqrt_47, sub_47, unsqueeze_188, unsqueeze_189, unsqueeze_190, unsqueeze_191, var_mean_47
# Graph fragment:
#   %convolution_126 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=convolution_126]
#   %getitem_349 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=getitem_349]
#   %buf1004 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=buf1004]
#   %primals_393 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_393]
#   %primals_394 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_394]
#   %convert_element_type_326 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_126, torch.float32), kwargs = {})
#   %var_mean_47 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_326, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_247 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_348, 1e-05), kwargs = {})
#   %rsqrt_47 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_247,), kwargs = {})
#   %sub_47 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_126, %getitem_349), kwargs = {})
#   %mul_381 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_47, %rsqrt_47), kwargs = {})
#   %unsqueeze_188 : Tensor "f32[264, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_393, -1), kwargs = {})
#   %unsqueeze_189 : Tensor "f32[264, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_188, -1), kwargs = {})
#   %mul_387 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_381, %unsqueeze_189), kwargs = {})
#   %unsqueeze_190 : Tensor "f32[264, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_394, -1), kwargs = {})
#   %unsqueeze_191 : Tensor "f32[264, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_190, -1), kwargs = {})
#   %add_250 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_387, %unsqueeze_191), kwargs = {})
#   %convert_element_type_327 : Tensor "f16[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_250, torch.float16), kwargs = {})
#   return %convert_element_type_327
triton_poi_fused__native_batch_norm_legit_functional_219 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_219', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_219', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 9939072}, 'kernel_num_gb': 0.006627456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_219(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 6272.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp15, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_219.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_219.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.006627456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7y/c7y3lvhpbumoj7skbrsqzitryb4malvzkgkghfg44zm7xnm3xfnq.py
# Topologically Sorted Source Nodes: [x_152], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_152 => convert_element_type_328
# Graph fragment:
#   %primals_395 : Tensor "f32[1584, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_395]
#   %convert_element_type_328 : Tensor "f16[1584, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_395, torch.float16), kwargs = {})
#   return %convert_element_type_328
triton_poi_fused__to_copy_220 = async_compile.triton('triton_poi_fused__to_copy_220', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 524288}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_220', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3345408}, 'kernel_num_gb': 0.002509056, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_220(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 418176
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1584, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1584, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 418176,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_220.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_220.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.002509056
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zc/czczpkxo5mpnyy3rzx2tdnnn6ucv6upj3rzzdqwes6ykjkzbgaj3.py
# Topologically Sorted Source Nodes: [x_153], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_153 => convert_element_type_329, var_mean_48
# Graph fragment:
#   %convolution_127 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convolution_127]
#   %convert_element_type_329 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_127, torch.float32), kwargs = {})
#   %var_mean_48 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_329, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf1010,%buf1011,%buf1012
triton_red_fused__native_batch_norm_legit_functional_221 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_221', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_221', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21732480, 'r0_': 0}, 'kernel_num_gb': 0.020801088, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_221(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 77616
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 1584)
    x1 = xindex // 1584
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 1584*r0_2 + 202752*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 77616, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_221.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_221.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.020801088
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/yn/cynqymadh2o6gjtobtpzravbtgdnrbjc4ekp6wvv6frkld45qkgz.py
# Topologically Sorted Source Nodes: [x_153], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_153 => add_252, add_253, add_254, convert_element_type_329, mul_389, mul_390, mul_391, mul_392, mul_393, rsqrt_48, squeeze_144, squeeze_146, var_mean_48
# Graph fragment:
#   %buf1010 : Tensor "f32[1, 1584, 1, 1, 49][77616, 1, 77616, 77616, 1584]cuda:0" = PlaceHolder[target=buf1010]
#   %buf1011 : Tensor "f32[1, 1584, 1, 1, 49][77616, 1, 77616, 77616, 1584]cuda:0" = PlaceHolder[target=buf1011]
#   %buf1012 : Tensor "f32[1, 1584, 1, 1, 49][77616, 1, 77616, 77616, 1584]cuda:0" = PlaceHolder[target=buf1012]
#   %buf1014 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=buf1014]
#   %getitem_351 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_351]
#   %copy__145 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=copy__145]
#   %add_253 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=add_253]
#   %copy__146 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=copy__146]
#   %add_254 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=add_254]
#   %convert_element_type_329 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_127, torch.float32), kwargs = {})
#   %var_mean_48 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_329, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_252 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_350, 1e-05), kwargs = {})
#   %rsqrt_48 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_252,), kwargs = {})
#   %squeeze_144 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_351, [0, 2, 3]), kwargs = {})
#   %mul_389 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_144, 0.1), kwargs = {})
#   %mul_390 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_397, 0.9), kwargs = {})
#   %add_253 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_389, %mul_390), kwargs = {})
#   %squeeze_146 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_350, [0, 2, 3]), kwargs = {})
#   %mul_391 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_146, 1.0001594642002871), kwargs = {})
#   %mul_392 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_391, 0.1), kwargs = {})
#   %mul_393 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_398, 0.9), kwargs = {})
#   %add_254 : Tensor "f32[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_392, %mul_393), kwargs = {})
#   %copy__145 : Tensor "f32[1584][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_397, %add_253), kwargs = {})
#   %copy__146 : Tensor "f32[1584][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_398, %add_254), kwargs = {})
#   return %getitem_351,%buf1014,%rsqrt_48,%add_253,%buf1581,%add_254,%buf1584
triton_per_fused__native_batch_norm_legit_functional_copy__222 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__222', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__222', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 994752, 'r0_': 0}, 'kernel_num_gb': 0.000969408, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__222(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1584
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1584*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 1584*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 1584*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 6272.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0001594642002871
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 1584, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__222.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__222.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000969408
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/d3/cd3fcsfmfocziua6qtvh2gyi2wrpelfmbzfaywmktsgb66tdyklc.py
# Topologically Sorted Source Nodes: [x_153, x_154], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_153 => add_255, convert_element_type_330, mul_388, mul_394, sub_48, unsqueeze_192, unsqueeze_193, unsqueeze_194, unsqueeze_195
#   x_154 => convert_element_type_331, convert_element_type_332, mul_395, sigmoid_52
# Graph fragment:
#   %convolution_127 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convolution_127]
#   %getitem_351 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_351]
#   %rsqrt_48 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_48]
#   %primals_399 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_399]
#   %primals_400 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_400]
#   %convert_element_type_331 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_331]
#   %sub_48 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_127, %getitem_351), kwargs = {})
#   %mul_388 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_48, %rsqrt_48), kwargs = {})
#   %unsqueeze_192 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_399, -1), kwargs = {})
#   %unsqueeze_193 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_192, -1), kwargs = {})
#   %mul_394 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_388, %unsqueeze_193), kwargs = {})
#   %unsqueeze_194 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_400, -1), kwargs = {})
#   %unsqueeze_195 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_194, -1), kwargs = {})
#   %add_255 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_394, %unsqueeze_195), kwargs = {})
#   %convert_element_type_330 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_255, torch.float16), kwargs = {})
#   %convert_element_type_331 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_330, torch.float32), kwargs = {})
#   %sigmoid_52 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_331,), kwargs = {})
#   %mul_395 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_331, %sigmoid_52), kwargs = {})
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_395, torch.float16), kwargs = {})
#   return %convert_element_type_331,%convert_element_type_332
triton_poi_fused__native_batch_norm_legit_functional_silu_223 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_223', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_223', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 39739392, 'x': 19895040}, 'kernel_num_gb': 0.039764736, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_223(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x1 = xindex
    y0 = yindex
    y2 = (yindex % 49)
    y3 = yindex // 49
    tmp0 = tl.load(in_ptr0 + (x1 + 1584*y0), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tmp12 = tl.sigmoid(tmp11)
    tmp13 = tmp11 * tmp12
    tmp14 = tmp13.to(tl.float32)
    tl.store(out_ptr1 + (y2 + 49*x1 + 77632*y3), tmp14, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_223.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_223.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.039764736
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/cr/ccrkqu4px5qcfdpkqocmmj22msfeug55h32c453zxrghzcbv6dzo.py
# Topologically Sorted Source Nodes: [conv2d_128], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_128 => convert_element_type_333
# Graph fragment:
#   %primals_401 : Tensor "f32[396, 1, 3, 3][9, 9, 3, 1]cuda:0" = PlaceHolder[target=primals_401]
#   %convert_element_type_333 : Tensor "f16[396, 1, 3, 3][9, 9, 3, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_401, torch.float16), kwargs = {})
#   return %convert_element_type_333
triton_poi_fused__to_copy_224 = async_compile.triton('triton_poi_fused__to_copy_224', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 4096}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_224', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 28512}, 'kernel_num_gb': 2.1384e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_224(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 3564
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((396, 1, 3, 3), (9, 1, 3, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 3564,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_224.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_224.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 2.1384e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/w7/cw75ga5gl6zn2nhj26ys2xhn4ldrcibgcct6iuc7x5kft6wrj2rb.py
# Topologically Sorted Source Nodes: [x_154, conv2d_128], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_128 => convolution_128, split_with_sizes_84
#   x_154 => convert_element_type_332, mul_395, sigmoid_52
# Graph fragment:
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_332]
#   %sigmoid_52 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_331,), kwargs = {})
#   %mul_395 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_331, %sigmoid_52), kwargs = {})
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_395, torch.float16), kwargs = {})
#   %split_with_sizes_84 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_332, [396, 396, 396, 396], 1), kwargs = {})
#   %convolution_128 : Tensor "f16[128, 396, 7, 7][19404, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_356, %convert_element_type_333, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 396), kwargs = {})
#   return %buf1020
triton_poi_fused_convolution_silu_split_with_sizes_225 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_225', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_225', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_225(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_225.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_225.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/fl/cflofs3hbwdpiczzjgfqj7ctp6tgyau7yroaf7jn47caax7fb63m.py
# Topologically Sorted Source Nodes: [conv2d_129], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_129 => convert_element_type_334
# Graph fragment:
#   %primals_402 : Tensor "f32[396, 1, 5, 5][25, 25, 5, 1]cuda:0" = PlaceHolder[target=primals_402]
#   %convert_element_type_334 : Tensor "f16[396, 1, 5, 5][25, 25, 5, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_402, torch.float16), kwargs = {})
#   return %convert_element_type_334
triton_poi_fused__to_copy_226 = async_compile.triton('triton_poi_fused__to_copy_226', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16384}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_226', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 79200}, 'kernel_num_gb': 5.94e-05, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_226(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9900
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((396, 1, 5, 5), (25, 1, 5, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 9900,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_226.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_226.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 5.94e-05
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kh/ckhje3wvss7ueqwgc4jts5y4hj7nfphjy2pkryuhhdd54t2o7xgr.py
# Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_129], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_128 => split_with_sizes_84
#   conv2d_129 => convolution_129
#   x_154 => convert_element_type_332, mul_395, sigmoid_52
# Graph fragment:
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_332]
#   %sigmoid_52 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_331,), kwargs = {})
#   %mul_395 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_331, %sigmoid_52), kwargs = {})
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_395, torch.float16), kwargs = {})
#   %split_with_sizes_84 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_332, [396, 396, 396, 396], 1), kwargs = {})
#   %convolution_129 : Tensor "f16[128, 396, 7, 7][19404, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_361, %convert_element_type_334, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 396), kwargs = {})
#   return %buf1023
triton_poi_fused_convolution_silu_split_with_sizes_227 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_227', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_227', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_227(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (19404 + x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_227.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_227.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gu/cgur4jfgyoyieolgzofnhsi62inmil7pa2jn43bxmdv4cweakgwx.py
# Topologically Sorted Source Nodes: [conv2d_130], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_130 => convert_element_type_335
# Graph fragment:
#   %primals_403 : Tensor "f32[396, 1, 7, 7][49, 49, 7, 1]cuda:0" = PlaceHolder[target=primals_403]
#   %convert_element_type_335 : Tensor "f16[396, 1, 7, 7][49, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_403, torch.float16), kwargs = {})
#   return %convert_element_type_335
triton_poi_fused__to_copy_228 = async_compile.triton('triton_poi_fused__to_copy_228', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_228', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 155232}, 'kernel_num_gb': 0.000116424, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_228(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 19404
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((396, 1, 7, 7), (49, 1, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 19404,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_228.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_228.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000116424
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/eu/ceukbf2obzy2hwg7uwro5dqmldhpewtueuaekzpuol6puoeuhmj4.py
# Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_130], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_128 => split_with_sizes_84
#   conv2d_130 => convolution_130
#   x_154 => convert_element_type_332, mul_395, sigmoid_52
# Graph fragment:
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_332]
#   %sigmoid_52 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_331,), kwargs = {})
#   %mul_395 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_331, %sigmoid_52), kwargs = {})
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_395, torch.float16), kwargs = {})
#   %split_with_sizes_84 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_332, [396, 396, 396, 396], 1), kwargs = {})
#   %convolution_130 : Tensor "f16[128, 396, 7, 7][19404, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_366, %convert_element_type_335, None, [1, 1], [3, 3], [1, 1], False, [0, 0], 396), kwargs = {})
#   return %buf1026
triton_poi_fused_convolution_silu_split_with_sizes_229 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_229', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_229', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_229(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (38808 + x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_229.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_229.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/vc/cvcohl6mixyxdhiqkbiwau22n4mnqxu4254vpbpkc5g5apvmaaef.py
# Topologically Sorted Source Nodes: [conv2d_131], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_131 => convert_element_type_336
# Graph fragment:
#   %primals_404 : Tensor "f32[396, 1, 9, 9][81, 81, 9, 1]cuda:0" = PlaceHolder[target=primals_404]
#   %convert_element_type_336 : Tensor "f16[396, 1, 9, 9][81, 81, 9, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_404, torch.float16), kwargs = {})
#   return %convert_element_type_336
triton_poi_fused__to_copy_230 = async_compile.triton('triton_poi_fused__to_copy_230', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_230', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 256608}, 'kernel_num_gb': 0.000192456, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_230(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 32076
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((396, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((396, 1, 9, 9), (81, 1, 9, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 32076,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_230.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_230.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000192456
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/i3/ci34ewp62rt4dlqds2yhnzf3j4wkged54x4mfkabfxwpa3huhsak.py
# Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_131], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_128 => split_with_sizes_84
#   conv2d_131 => convolution_131
#   x_154 => convert_element_type_332, mul_395, sigmoid_52
# Graph fragment:
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=convert_element_type_332]
#   %sigmoid_52 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_331,), kwargs = {})
#   %mul_395 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_331, %sigmoid_52), kwargs = {})
#   %convert_element_type_332 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_395, torch.float16), kwargs = {})
#   %split_with_sizes_84 : [num_users=4] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%convert_element_type_332, [396, 396, 396, 396], 1), kwargs = {})
#   %convolution_131 : Tensor "f16[128, 396, 7, 7][19404, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_371, %convert_element_type_336, None, [1, 1], [4, 4], [1, 1], False, [0, 0], 396), kwargs = {})
#   return %buf1029
triton_poi_fused_convolution_silu_split_with_sizes_231 = async_compile.triton('triton_poi_fused_convolution_silu_split_with_sizes_231', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 65536, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_silu_split_with_sizes_231', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 9934848, 'x': 4967424}, 'kernel_num_gb': 0.009934848, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_silu_split_with_sizes_231(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 50688
    xnumel = 49
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 396)
    y1 = yindex // 396
    tmp0 = tl.load(in_ptr0 + (58212 + x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 396*x2 + 19404*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 50688, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_silu_split_with_sizes_231.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_silu_split_with_sizes_231.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009934848
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/gt/cgtudnmav4fp3sfvaprisgtecsjcymwvwzrg5bajbl2ayqmowsiv.py
# Topologically Sorted Source Nodes: [x_155], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_155 => cat_35
# Graph fragment:
#   %convolution_128 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=convolution_128]
#   %convolution_129 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=convolution_129]
#   %convolution_130 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=convolution_130]
#   %convolution_131 : Tensor "f16[128, 396, 7, 7][19404, 1, 2772, 396]cuda:0" = PlaceHolder[target=convolution_131]
#   %cat_35 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_128, %convolution_129, %convolution_130, %convolution_131], 1), kwargs = {})
#   return %cat_35
triton_poi_fused_cat_232 = async_compile.triton('triton_poi_fused_cat_232', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_232', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 119218176}, 'kernel_num_gb': 0.039739392, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_232(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9934848
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 1584)
    x1 = xindex // 1584
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 396, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (396*x1 + (x0)), tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 792, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (396*x1 + ((-396) + x0)), tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 1188, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (396*x1 + ((-792) + x0)), tmp14 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 1584, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (396*x1 + ((-1188) + x0)), tmp16 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, xmask)


def get_args():
    arg_0 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_3 = rand_strided((128, 396, 7, 7), (19404, 1, 2772, 396), device='cuda:0', dtype=torch.float16)
    arg_4 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, 9934848,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_232.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_232.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.039739392
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/np/cnpx6pyvxpifgvnppagllvzhu7r7zalefka46jvme2mkrdgb4dhm.py
# Topologically Sorted Source Nodes: [x_156, x_157], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
# Source node to ATen node mapping:
#   x_156 => add_260, convert_element_type_338, mul_396, mul_402, sub_49, unsqueeze_196, unsqueeze_197, unsqueeze_198, unsqueeze_199
#   x_157 => convert_element_type_339
# Graph fragment:
#   %cat_35 : Tensor "f16[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=cat_35]
#   %getitem_373 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=getitem_373]
#   %rsqrt_49 : Tensor "f32[1, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=rsqrt_49]
#   %primals_408 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_408]
#   %primals_409 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_409]
#   %sub_49 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_35, %getitem_373), kwargs = {})
#   %mul_396 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_49, %rsqrt_49), kwargs = {})
#   %unsqueeze_196 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_408, -1), kwargs = {})
#   %unsqueeze_197 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_196, -1), kwargs = {})
#   %mul_402 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_396, %unsqueeze_197), kwargs = {})
#   %unsqueeze_198 : Tensor "f32[1584, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_409, -1), kwargs = {})
#   %unsqueeze_199 : Tensor "f32[1584, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_198, -1), kwargs = {})
#   %add_260 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_402, %unsqueeze_199), kwargs = {})
#   %convert_element_type_338 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_260, torch.float16), kwargs = {})
#   %convert_element_type_339 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_338, torch.float32), kwargs = {})
#   return %convert_element_type_339
triton_poi_fused__native_batch_norm_legit_functional_silu_233 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_silu_233', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 16777216}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_silu_233', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 99373824}, 'kernel_num_gb': 0.059634432, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_silu_233(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 9934848
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 1584)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 9934848,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_silu_233.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_silu_233.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.059634432
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/6o/c6okw4xm7ekxhhcciti67oyfos2oyak2bjm4lhljgvf4eus5wt4g.py
# Topologically Sorted Source Nodes: [x_157, x_se_52], Original ATen: [aten.silu, aten.mean]
# Source node to ATen node mapping:
#   x_157 => convert_element_type_340, mul_403, sigmoid_53
#   x_se_52 => mean_13
# Graph fragment:
#   %convert_element_type_339 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_339]
#   %buf1040 : Tensor "f32[128, 1584, 1, 1][1584, 1, 202752, 202752]cuda:0" = PlaceHolder[target=buf1040]
#   %sigmoid_53 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_339,), kwargs = {})
#   %mul_403 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_339, %sigmoid_53), kwargs = {})
#   %convert_element_type_340 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_403, torch.float16), kwargs = {})
#   %mean_13 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.mean.dim](args = (%convert_element_type_340, [2, 3], True), kwargs = {})
#   return %buf1040,%mean_13
triton_per_fused_mean_silu_234 = async_compile.triton('triton_per_fused_mean_silu_234', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 262144, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused_mean_silu_234', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 1, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 40550400, 'r0_': 0}, 'kernel_num_gb': 0.040144896, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused_mean_silu_234(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 202752
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_2 = r0_index
    x0 = (xindex % 1584)
    x1 = xindex // 1584
    x3 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1584*r0_2 + 77616*x1), r0_mask & xmask, other=0.0)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.broadcast_to(tmp4, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp8 = tl.sum(tmp7, 1)[:, None].to(tl.float32)
    tmp9 = 49.0
    tmp10 = (tmp8 / tmp9)
    tmp11 = tmp10.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp11, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 202752, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused_mean_silu_234.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused_mean_silu_234.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.040144896
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5r/c5rrbn5rn5n55qhhbif35njkxtweevpnpklkavjfeh2dwnvfah6p.py
# Topologically Sorted Source Nodes: [x_se_53], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_53 => convert_element_type_342
# Graph fragment:
#   %primals_410 : Tensor "f32[132, 1584, 1, 1][1584, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_410]
#   %convert_element_type_342 : Tensor "f16[132, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_410, torch.float16), kwargs = {})
#   return %convert_element_type_342
triton_poi_fused__to_copy_235 = async_compile.triton('triton_poi_fused__to_copy_235', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_235', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1672704}, 'kernel_num_gb': 0.001254528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_235(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 209088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((132, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((132, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 209088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_235.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_235.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001254528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/zz/czzutmin77p3mbr2xjpiozu57wond5weawouix5xjr3r3kjmsbjf.py
# Topologically Sorted Source Nodes: [x_se_53, x_se_54], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
# Source node to ATen node mapping:
#   x_se_53 => convert_element_type_341, convolution_132
#   x_se_54 => convert_element_type_343, convert_element_type_344, mul_404, sigmoid_54
# Graph fragment:
#   %buf1043 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0" = PlaceHolder[target=buf1043]
#   %primals_411 : Tensor "f32[132][1]cuda:0" = PlaceHolder[target=primals_411]
#   %convolution_132 : Tensor "f16[128, 132, 1, 1][132, 1, 132, 132]cuda:0" = PlaceHolder[target=convolution_132]
#   %convert_element_type_341 : Tensor "f16[132][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_411, torch.float16), kwargs = {})
#   %convolution_132 : Tensor "f16[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%mean_13, %convert_element_type_342, %convert_element_type_341, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   %convert_element_type_343 : Tensor "f32[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_132, torch.float32), kwargs = {})
#   %sigmoid_54 : Tensor "f32[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_343,), kwargs = {})
#   %mul_404 : Tensor "f32[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_343, %sigmoid_54), kwargs = {})
#   %convert_element_type_344 : Tensor "f16[128, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_404, torch.float16), kwargs = {})
#   return %convolution_132,%convert_element_type_344
triton_poi_fused__to_copy_convolution_silu_236 = async_compile.triton('triton_poi_fused__to_copy_convolution_silu_236', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 32768}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_silu_236', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 169488}, 'kernel_num_gb': 0.000101904, 'kernel_flop': 53526528},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_silu_236(in_out_ptr0, in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 16896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 132)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tmp4 = tmp3.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp4 * tmp5
    tmp7 = tmp6.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)
    tl.store(out_ptr0 + (x2), tmp7, xmask)


def get_args():
    arg_0 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((132,), (1,), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((128, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 16896,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_silu_236.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_silu_236.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000101904
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/aa/caahqpsto27liyjzgxepj37ny6k6dbzv46k7bw6kgvmny36ttzqf.py
# Topologically Sorted Source Nodes: [x_se_55], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_se_55 => convert_element_type_346
# Graph fragment:
#   %primals_412 : Tensor "f32[1584, 132, 1, 1][132, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_412]
#   %convert_element_type_346 : Tensor "f16[1584, 132, 1, 1][132, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_412, torch.float16), kwargs = {})
#   return %convert_element_type_346
triton_poi_fused__to_copy_237 = async_compile.triton('triton_poi_fused__to_copy_237', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_237', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1672704}, 'kernel_num_gb': 0.001254528, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_237(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 209088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1584, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1584, 132, 1, 1), (132, 1, 132, 132), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 209088,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_237.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_237.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.001254528
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ae/caexrvmy6y4v6c5677ooxzrk2folx7x532pzx65uswmsrb4xkebo.py
# Topologically Sorted Source Nodes: [x_se_55], Original ATen: [aten._to_copy, aten.convolution]
# Source node to ATen node mapping:
#   x_se_55 => convert_element_type_345, convolution_133
# Graph fragment:
#   %buf1047 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=buf1047]
#   %primals_413 : Tensor "f32[1584][1]cuda:0" = PlaceHolder[target=primals_413]
#   %convert_element_type_345 : Tensor "f16[1584][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_413, torch.float16), kwargs = {})
#   %convolution_133 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convert_element_type_344, %convert_element_type_346, %convert_element_type_345, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %convolution_133
triton_poi_fused__to_copy_convolution_238 = async_compile.triton('triton_poi_fused__to_copy_convolution_238', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 262144}, 
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_convolution_238', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 1222848}, 'kernel_num_gb': 0.000817344, 'kernel_flop': 53526528},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_convolution_238(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 202752
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 1584)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp1.to(tl.float32)
    tmp3 = tmp0 + tmp2
    tl.store(in_out_ptr0 + (x2), tmp3, xmask)


def get_args():
    arg_0 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1584,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, 202752,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_convolution_238.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_convolution_238.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000817344
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ao/caohwylgh74awfxtvgizj6v5cci7wxws7ktr2ykckudpyiqkhddk.py
# Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   sigmoid_13 => sigmoid_55
#   x_157 => convert_element_type_340, mul_403, sigmoid_53
#   x_158 => mul_405
# Graph fragment:
#   %convert_element_type_339 : Tensor "f32[128, 1584, 7, 7][77616, 1, 11088, 1584]cuda:0" = PlaceHolder[target=convert_element_type_339]
#   %convolution_133 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1584, 1584]cuda:0" = PlaceHolder[target=convolution_133]
#   %sigmoid_53 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_339,), kwargs = {})
#   %mul_403 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_339, %sigmoid_53), kwargs = {})
#   %convert_element_type_340 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_403, torch.float16), kwargs = {})
#   %sigmoid_55 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_133,), kwargs = {})
#   %mul_405 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_340, %sigmoid_55), kwargs = {})
#   return %mul_405
triton_poi_fused_mul_sigmoid_silu_239 = async_compile.triton('triton_poi_fused_mul_sigmoid_silu_239', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 8192, 'x': 2048}, tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_mul_sigmoid_silu_239', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 39739392, 'x': 40144896}, 'kernel_num_gb': 0.060014592, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_mul_sigmoid_silu_239(in_ptr0, in_ptr1, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 1584
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = yindex // 49
    y0 = (yindex % 49)
    tmp0 = tl.load(in_ptr0 + (x2 + 1584*y3), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr1 + (x2 + 1584*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.sigmoid(tmp0)
    tmp2 = tmp0 * tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tl.sigmoid(tmp4)
    tmp6 = tmp3 * tmp5
    tl.store(out_ptr0 + (y0 + 49*x2 + 77632*y1), tmp6, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77616, 1, 11088, 1584), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((128, 1584, 1, 1), (1584, 1, 1584, 1584), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 6272, 1584,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_mul_sigmoid_silu_239.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_mul_sigmoid_silu_239.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.060014592
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/7p/c7pvt2zmdyc7w3f32am67do6gkqyqjzeee7rw5mgm3zxiuvrttau.py
# Topologically Sorted Source Nodes: [conv2d_134], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   conv2d_134 => convert_element_type_347
# Graph fragment:
#   %primals_414 : Tensor "f32[132, 792, 1, 1][792, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_414]
#   %convert_element_type_347 : Tensor "f16[132, 792, 1, 1][792, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_414, torch.float16), kwargs = {})
#   return %convert_element_type_347
triton_poi_fused__to_copy_240 = async_compile.triton('triton_poi_fused__to_copy_240', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 131072}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_240', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 836352}, 'kernel_num_gb': 0.000627264, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_240(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 104544
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((132, 792, 1, 1), (792, 1, 792, 792), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 104544,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_240.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_240.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000627264
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/el/celyq6zjibho5hvqass7h5te7cjrqfnklmpxy3ez76wz73uokbws.py
# Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_134], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_134 => convolution_134
#   sigmoid_13 => sigmoid_55
#   split_36 => split_with_sizes_88
#   x_157 => convert_element_type_340, mul_403, sigmoid_53
#   x_158 => mul_405
# Graph fragment:
#   %mul_405 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=mul_405]
#   %sigmoid_53 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_339,), kwargs = {})
#   %mul_403 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_339, %sigmoid_53), kwargs = {})
#   %convert_element_type_340 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_403, torch.float16), kwargs = {})
#   %sigmoid_55 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_133,), kwargs = {})
#   %mul_405 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_340, %sigmoid_55), kwargs = {})
#   %split_with_sizes_88 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_405, [792, 792], 1), kwargs = {})
#   %convolution_134 : Tensor "f16[128, 132, 7, 7][6468, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_374, %convert_element_type_347, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf1051
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 19869696, 'x': 9934848}, 'kernel_num_gb': 0.019869696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 101376
    xnumel = 49
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 792)
    y1 = yindex // 792
    tmp0 = tl.load(in_ptr0 + (x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 792*x2 + 38808*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 101376, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019869696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/mf/cmf5s7ll453qbklmpeherb3lgphw5brs7sno3n22cx4jucjhwfej.py
# Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_135], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
# Source node to ATen node mapping:
#   conv2d_135 => convolution_135
#   sigmoid_13 => sigmoid_55
#   split_36 => split_with_sizes_88
#   x_157 => convert_element_type_340, mul_403, sigmoid_53
#   x_158 => mul_405
# Graph fragment:
#   %mul_405 : Tensor "f16[128, 1584, 7, 7][77632, 49, 7, 1]cuda:0" = PlaceHolder[target=mul_405]
#   %sigmoid_53 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_339,), kwargs = {})
#   %mul_403 : Tensor "f32[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_339, %sigmoid_53), kwargs = {})
#   %convert_element_type_340 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_403, torch.float16), kwargs = {})
#   %sigmoid_55 : Tensor "f16[128, 1584, 1, 1][1584, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convolution_133,), kwargs = {})
#   %mul_405 : Tensor "f16[128, 1584, 7, 7][77616, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_340, %sigmoid_55), kwargs = {})
#   %split_with_sizes_88 : [num_users=2] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%mul_405, [792, 792], 1), kwargs = {})
#   %convolution_135 : Tensor "f16[128, 132, 7, 7][6468, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%getitem_375, %convert_element_type_348, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
#   return %buf1054
triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242 = async_compile.triton('triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'y': 131072, 'x': 64}, tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ynumel': 'i32', 'xnumel': 'i32', 'YBLOCK': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid2DWithYZOverflow', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'y': 19869696, 'x': 9934848}, 'kernel_num_gb': 0.019869696, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 101376
    xnumel = 49
    yoffset = (tl.program_id(1) + tl.program_id(2) * tl.num_programs(1)) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[:, None]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[None, :]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = (yindex % 792)
    y1 = yindex // 792
    tmp0 = tl.load(in_ptr0 + (38808 + x2 + 49*y0 + 77632*y1), xmask & ymask, eviction_policy='evict_last').to(tl.float32)
    tl.store(out_ptr0 + (y0 + 792*x2 + 38808*y1), tmp0, xmask & ymask)


def get_args():
    arg_0 = rand_strided((128, 1584, 7, 7), (77632, 49, 7, 1), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 792, 7, 7), (38808, 1, 5544, 792), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 101376, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019869696
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/nd/cndx522rcl35mrg6lirqr24oq64cbwj4nvend7cll6tisafclyur.py
# Topologically Sorted Source Nodes: [x_159], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   x_159 => cat_36
# Graph fragment:
#   %convolution_134 : Tensor "f16[128, 132, 7, 7][6468, 1, 924, 132]cuda:0" = PlaceHolder[target=convolution_134]
#   %convolution_135 : Tensor "f16[128, 132, 7, 7][6468, 1, 924, 132]cuda:0" = PlaceHolder[target=convolution_135]
#   %cat_36 : Tensor "f16[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.cat.default](args = ([%convolution_134, %convolution_135], 1), kwargs = {})
#   return %cat_36
triton_poi_fused_cat_243 = async_compile.triton('triton_poi_fused_cat_243', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_243', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13246464}, 'kernel_num_gb': 0.006623232, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_243(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 264)
    x1 = xindex // 264
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 132, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (132*x1 + (x0)), tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 264, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tl.load(in_ptr1 + (132*x1 + ((-132) + x0)), tmp6 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tl.where(tmp4, tmp5, tmp9)
    tl.store(out_ptr0 + (x2), tmp10, xmask)


def get_args():
    arg_0 = rand_strided((128, 132, 7, 7), (6468, 1, 924, 132), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((128, 132, 7, 7), (6468, 1, 924, 132), device='cuda:0', dtype=torch.float16)
    arg_2 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_243.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_cat_243.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.006623232
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/h6/ch6zohlyc6omaihfkyfwsdkzihfqmbxqil4d5bedq6a6tx7ifos7.py
# Topologically Sorted Source Nodes: [x_160, x_161], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
# Source node to ATen node mapping:
#   x_160 => add_262, add_265, convert_element_type_349, convert_element_type_350, mul_406, mul_412, rsqrt_50, sub_50, unsqueeze_200, unsqueeze_201, unsqueeze_202, unsqueeze_203, var_mean_50
#   x_161 => add_266
# Graph fragment:
#   %cat_36 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=cat_36]
#   %getitem_377 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=getitem_377]
#   %buf1061 : Tensor "f32[1, 264, 1, 1][264, 1, 264, 264]cuda:0" = PlaceHolder[target=buf1061]
#   %primals_419 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_419]
#   %primals_420 : Tensor "f32[264][1]cuda:0" = PlaceHolder[target=primals_420]
#   %convert_element_type_327 : Tensor "f16[128, 264, 7, 7][12936, 1, 1848, 264]cuda:0" = PlaceHolder[target=convert_element_type_327]
#   %convert_element_type_349 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_36, torch.float32), kwargs = {})
#   %var_mean_50 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_349, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_262 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_376, 1e-05), kwargs = {})
#   %rsqrt_50 : Tensor "f32[1, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_262,), kwargs = {})
#   %sub_50 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%cat_36, %getitem_377), kwargs = {})
#   %mul_406 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_50, %rsqrt_50), kwargs = {})
#   %unsqueeze_200 : Tensor "f32[264, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_419, -1), kwargs = {})
#   %unsqueeze_201 : Tensor "f32[264, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_200, -1), kwargs = {})
#   %mul_412 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_406, %unsqueeze_201), kwargs = {})
#   %unsqueeze_202 : Tensor "f32[264, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_420, -1), kwargs = {})
#   %unsqueeze_203 : Tensor "f32[264, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_202, -1), kwargs = {})
#   %add_265 : Tensor "f32[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_412, %unsqueeze_203), kwargs = {})
#   %convert_element_type_350 : Tensor "f16[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_265, torch.float16), kwargs = {})
#   %add_266 : Tensor "f16[128, 264, 7, 7][12936, 49, 7, 1]cuda:0"[num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_350, %convert_element_type_327), kwargs = {})
#   return %add_266
triton_poi_fused__native_batch_norm_legit_functional_add_244 = async_compile.triton('triton_poi_fused__native_batch_norm_legit_functional_add_244', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__native_batch_norm_legit_functional_add_244', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 13250688}, 'kernel_num_gb': 0.009939072, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__native_batch_norm_legit_functional_add_244(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1655808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 264)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x2), xmask).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = 6272.0
    tmp6 = (tmp4 / tmp5)
    tmp7 = 1e-05
    tmp8 = tmp6 + tmp7
    tmp9 = libdevice.rsqrt(tmp8)
    tmp10 = tmp3 * tmp9
    tmp12 = tmp10 * tmp11
    tmp14 = tmp12 + tmp13
    tmp15 = tmp14.to(tl.float32)
    tmp17 = tmp15 + tmp16
    tl.store(out_ptr0 + (x2), tmp17, xmask)


def get_args():
    arg_0 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((264,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    arg_6 = rand_strided((128, 264, 7, 7), (12936, 1, 1848, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, 1655808,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__native_batch_norm_legit_functional_add_244.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__native_batch_norm_legit_functional_add_244.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009939072
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ae/caeuh4npa6rd55mtywxc3fku2etsgzfobpujhtiyi43wxv3fr2oh.py
# Topologically Sorted Source Nodes: [x_182], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_182 => convert_element_type_397
# Graph fragment:
#   %primals_473 : Tensor "f32[1536, 264, 1, 1][264, 1, 1, 1]cuda:0" = PlaceHolder[target=primals_473]
#   %convert_element_type_397 : Tensor "f16[1536, 264, 1, 1][264, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_473, torch.float16), kwargs = {})
#   return %convert_element_type_397
triton_poi_fused__to_copy_245 = async_compile.triton('triton_poi_fused__to_copy_245', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 524288}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_245', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 3244032}, 'kernel_num_gb': 0.002433024, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_245(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 405504
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), None)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, None)


def get_args():
    arg_0 = rand_strided((1536, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1536, 264, 1, 1), (264, 1, 264, 264), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 405504,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_245.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_245.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.002433024
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/64/c64lerreklanlyrmdscboll6vwnl4ei24u3tudojaenrc6gbgcet.py
# Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional]
# Source node to ATen node mapping:
#   x_183 => convert_element_type_398, var_mean_57
# Graph fragment:
#   %convolution_154 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convolution_154]
#   %convert_element_type_398 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_154, torch.float32), kwargs = {})
#   %var_mean_57 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_398, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   return %buf1181,%buf1182,%buf1183
triton_red_fused__native_batch_norm_legit_functional_246 = async_compile.triton('triton_red_fused__native_batch_norm_legit_functional_246', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.reduction(
    size_hints={'x': 131072, 'r0_': 128},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'out_ptr2': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr', 'R0_BLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_red_fused__native_batch_norm_legit_functional_246', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 3, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 21073920, 'r0_': 0}, 'kernel_num_gb': 0.020170752, 'kernel_flop': 0}
)
@triton.jit
def triton_red_fused__native_batch_norm_legit_functional_246(in_ptr0, out_ptr0, out_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):
    xnumel = 75264
    r0_numel = 128
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_base = tl.arange(0, R0_BLOCK)[None, :]
    rbase = r0_base
    x0 = (xindex % 1536)
    x1 = xindex // 1536
    tmp3_mean = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, R0_BLOCK], tl.float32)
    x3 = xindex
    for r0_offset in range(0, r0_numel, R0_BLOCK):
        r0_index = r0_offset + r0_base
        r0_mask = r0_index < r0_numel
        roffset = r0_offset
        rindex = r0_index
        r0_2 = r0_index
        tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_2 + 196608*x1), r0_mask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(r0_mask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(r0_mask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(r0_mask & xmask, tmp3_weight_next, tmp3_weight)
    tmp4, tmp5, tmp6 = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)
    tmp3 = tmp4[:, None]
    tmp7 = tmp5[:, None]
    tmp8 = tmp6[:, None]
    tl.store(out_ptr0 + (x3), tmp3, xmask)
    tl.store(out_ptr1 + (x3), tmp7, xmask)
    tl.store(out_ptr2 + (x3), tmp8, xmask)


def get_args():
    arg_0 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, 75264, 128,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_red_fused__native_batch_norm_legit_functional_246.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_red_fused__native_batch_norm_legit_functional_246.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.020170752
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/ad/cadzc373hdifyq77m6mlnjjnv4yiqvizmzy3neu6ahoictp6kv6v.py
# Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
# Source node to ATen node mapping:
#   x_183 => add_300, add_301, add_302, convert_element_type_398, mul_464, mul_465, mul_466, mul_467, mul_468, rsqrt_57, squeeze_171, squeeze_173, var_mean_57
# Graph fragment:
#   %buf1181 : Tensor "f32[1, 1536, 1, 1, 49][75264, 1, 75264, 75264, 1536]cuda:0" = PlaceHolder[target=buf1181]
#   %buf1182 : Tensor "f32[1, 1536, 1, 1, 49][75264, 1, 75264, 75264, 1536]cuda:0" = PlaceHolder[target=buf1182]
#   %buf1183 : Tensor "f32[1, 1536, 1, 1, 49][75264, 1, 75264, 75264, 1536]cuda:0" = PlaceHolder[target=buf1183]
#   %buf1185 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=buf1185]
#   %getitem_435 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=getitem_435]
#   %copy__172 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=copy__172]
#   %add_301 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=add_301]
#   %copy__173 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=copy__173]
#   %add_302 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=add_302]
#   %convert_element_type_398 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convolution_154, torch.float32), kwargs = {})
#   %var_mean_57 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_398, [0, 2, 3]), kwargs = {correction: 0, keepdim: True})
#   %add_300 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_434, 1e-05), kwargs = {})
#   %rsqrt_57 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=2] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_300,), kwargs = {})
#   %squeeze_171 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_435, [0, 2, 3]), kwargs = {})
#   %mul_464 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_171, 0.1), kwargs = {})
#   %mul_465 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_475, 0.9), kwargs = {})
#   %add_301 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_464, %mul_465), kwargs = {})
#   %squeeze_173 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.squeeze.dims](args = (%getitem_434, [0, 2, 3]), kwargs = {})
#   %mul_466 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_173, 1.0001594642002871), kwargs = {})
#   %mul_467 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_466, 0.1), kwargs = {})
#   %mul_468 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_476, 0.9), kwargs = {})
#   %add_302 : Tensor "f32[1536][1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_467, %mul_468), kwargs = {})
#   %copy__172 : Tensor "f32[1536][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_475, %add_301), kwargs = {})
#   %copy__173 : Tensor "f32[1536][1]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_476, %add_302), kwargs = {})
#   return %getitem_435,%buf1185,%rsqrt_57,%add_301,%buf1653,%add_302,%buf1656
triton_per_fused__native_batch_norm_legit_functional_copy__247 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_copy__247', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 2048, 'r0_': 64},
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr0': '*fp32', 'out_ptr2': '*fp32', 'out_ptr4': '*fp32', 'out_ptr6': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]], (7,): [['tt.divisibility', 16]], (8,): [['tt.divisibility', 16]], (9,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_copy__247', 'mutated_arg_names': ['in_ptr3', 'in_ptr4', 'out_ptr4', 'out_ptr6'], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 2, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 964608, 'r0_': 0}, 'kernel_num_gb': 0.000940032, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_copy__247(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr2, out_ptr4, out_ptr6, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 1536
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_1 = r0_index
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_1), r0_mask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + 1536*r0_1), r0_mask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (x0 + 1536*r0_1), r0_mask & xmask, other=0.0)
    tmp23 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])
    tmp4 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, R0_BLOCK])
    tmp7 = tl.where(r0_mask & xmask, tmp3, 0)
    tmp8 = tl.where(r0_mask & xmask, tmp4, 0)
    tmp9 = tl.where(r0_mask & xmask, tmp5, 0)
    tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
    tmp13 = tmp10[:, None]
    tmp14 = tmp11[:, None]
    tmp15 = tmp12[:, None]
    tmp16 = 6272.0
    tmp17 = (tmp14 / tmp16)
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = 0.1
    tmp22 = tmp13 * tmp21
    tmp24 = 0.9
    tmp25 = tmp23 * tmp24
    tmp26 = tmp22 + tmp25
    tmp27 = 1.0001594642002871
    tmp28 = tmp17 * tmp27
    tmp29 = tmp28 * tmp21
    tmp31 = tmp30 * tmp24
    tmp32 = tmp29 + tmp31
    tl.store(out_ptr2 + (x0), tmp20, xmask)
    tl.store(out_ptr4 + (x0), tmp26, xmask)
    tl.store(out_ptr6 + (x0), tmp32, xmask)
    tl.store(out_ptr0 + (x0), tmp13, xmask)


def get_args():
    arg_0 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_6 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_7 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_8 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, 1536, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_copy__247.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_copy__247.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.000940032
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/kx/ckx5t67uel5siuz6ix6is5lxfh2kkd3q3zokvticnloujhqqop33.py
# Topologically Sorted Source Nodes: [x_183, x_184, x_185], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.mean]
# Source node to ATen node mapping:
#   x_183 => add_303, convert_element_type_399, mul_463, mul_469, sub_57, unsqueeze_228, unsqueeze_229, unsqueeze_230, unsqueeze_231
#   x_184 => relu_6
#   x_185 => mean_16
# Graph fragment:
#   %convolution_154 : Tensor "f16[128, 1536, 7, 7][75264, 1, 10752, 1536]cuda:0" = PlaceHolder[target=convolution_154]
#   %getitem_435 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=getitem_435]
#   %rsqrt_57 : Tensor "f32[1, 1536, 1, 1][1536, 1, 1536, 1536]cuda:0" = PlaceHolder[target=rsqrt_57]
#   %primals_477 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=primals_477]
#   %primals_478 : Tensor "f32[1536][1]cuda:0" = PlaceHolder[target=primals_478]
#   %buf1188 : Tensor "f32[128, 1536, 1, 1][1536, 1, 196608, 196608]cuda:0" = PlaceHolder[target=buf1188]
#   %sub_57 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convolution_154, %getitem_435), kwargs = {})
#   %mul_463 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_57, %rsqrt_57), kwargs = {})
#   %unsqueeze_228 : Tensor "f32[1536, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_477, -1), kwargs = {})
#   %unsqueeze_229 : Tensor "f32[1536, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_228, -1), kwargs = {})
#   %mul_469 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_463, %unsqueeze_229), kwargs = {})
#   %unsqueeze_230 : Tensor "f32[1536, 1][1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%primals_478, -1), kwargs = {})
#   %unsqueeze_231 : Tensor "f32[1536, 1, 1][1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_230, -1), kwargs = {})
#   %add_303 : Tensor "f32[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_469, %unsqueeze_231), kwargs = {})
#   %convert_element_type_399 : Tensor "f16[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_303, torch.float16), kwargs = {})
#   %relu_6 : Tensor "f16[128, 1536, 7, 7][75264, 49, 7, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%convert_element_type_399,), kwargs = {})
#   %mean_16 : Tensor "f16[128, 1536, 1, 1][1536, 1, 1, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%relu_6, [-1, -2], True), kwargs = {})
#   return %buf1188,%mean_16
triton_per_fused__native_batch_norm_legit_functional_mean_relu_248 = async_compile.triton('triton_per_fused__native_batch_norm_legit_functional_mean_relu_248', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.persistent_reduction(
    size_hints={'x': 262144, 'r0_': 64},
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'out_ptr1': '*fp16', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]], (4,): [['tt.divisibility', 16]], (5,): [['tt.divisibility', 16]], (6,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__native_batch_norm_legit_functional_mean_relu_248', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': None, 'num_load': 5, 'num_reduction': 1, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 20078592, 'r0_': 0}, 'kernel_num_gb': 0.019685376, 'kernel_flop': 0}
)
@triton.jit
def triton_per_fused__native_batch_norm_legit_functional_mean_relu_248(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):
    xnumel = 196608
    r0_numel = 49
    R0_BLOCK: tl.constexpr = 64
    rnumel = r0_numel
    RBLOCK: tl.constexpr = R0_BLOCK
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)
    r0_index = tl.arange(0, R0_BLOCK)[None, :]
    r0_offset = 0
    r0_mask = r0_index < r0_numel
    roffset = r0_offset
    rindex = r0_index
    r0_2 = r0_index
    x0 = (xindex % 1536)
    x1 = xindex // 1536
    x3 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 1536*r0_2 + 75264*x1), r0_mask, other=0.0).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp1 = tmp0.to(tl.float32)
    tmp3 = tmp1 - tmp2
    tmp5 = tmp3 * tmp4
    tmp7 = tmp5 * tmp6
    tmp9 = tmp7 + tmp8
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tl.full([1, 1], 0, tl.int32)
    tmp12 = triton_helpers.maximum(tmp11, tmp10)
    tmp13 = tmp12.to(tl.float32)
    tmp14 = tl.broadcast_to(tmp13, [XBLOCK, R0_BLOCK])
    tmp16 = tl.where(r0_mask, tmp14, 0)
    tmp17 = tl.sum(tmp16, 1)[:, None].to(tl.float32)
    tmp18 = 49.0
    tmp19 = (tmp17 / tmp18)
    tmp20 = tmp19.to(tl.float32)
    tl.store(out_ptr1 + (x3), tmp20, None)


def get_args():
    arg_0 = rand_strided((128, 1536, 7, 7), (75264, 1, 10752, 1536), device='cuda:0', dtype=torch.float16)
    arg_1 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_2 = rand_strided((1, 1536, 1, 1), (1536, 1, 1536, 1536), device='cuda:0', dtype=torch.float32)
    arg_3 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_4 = rand_strided((1536,), (1,), device='cuda:0', dtype=torch.float32)
    arg_5 = rand_strided((128, 1536, 1, 1), (1536, 1, 196608, 196608), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, 196608, 49,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_per_fused__native_batch_norm_legit_functional_mean_relu_248.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_per_fused__native_batch_norm_legit_functional_mean_relu_248.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.019685376
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/at/cat5agiry74w5beml53sxdedxx7zhgd2chfl5xy27owidclcanlj.py
# Topologically Sorted Source Nodes: [x_187], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_187 => convert_element_type_401
# Graph fragment:
#   %primals_479 : Tensor "f32[1000, 1536][1536, 1]cuda:0" = PlaceHolder[target=primals_479]
#   %convert_element_type_401 : Tensor "f16[1000, 1536][1536, 1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_479, torch.float16), kwargs = {})
#   return %convert_element_type_401
triton_poi_fused__to_copy_249 = async_compile.triton('triton_poi_fused__to_copy_249', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 2097152}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_249', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 12288000}, 'kernel_num_gb': 0.009216, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_249(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1536000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), None)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, None)


def get_args():
    arg_0 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1536000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_249.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_249.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 0.009216
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/5u/c5uqnpw4gvcbhyyhzgb4xe24k4lng6gibo5dmedidmp4cdgnub4m.py
# Topologically Sorted Source Nodes: [x_187], Original ATen: [aten._to_copy]
# Source node to ATen node mapping:
#   x_187 => convert_element_type_400
# Graph fragment:
#   %primals_480 : Tensor "f32[1000][1]cuda:0" = PlaceHolder[target=primals_480]
#   %convert_element_type_400 : Tensor "f16[1000][1]cuda:0"[num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%primals_480, torch.float16), kwargs = {})
#   return %convert_element_type_400
triton_poi_fused__to_copy_250 = async_compile.triton('triton_poi_fused__to_copy_250', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1024}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp16', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_250', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 8000}, 'kernel_num_gb': 6e-06, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_250(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = tmp0.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp1, xmask)


def get_args():
    arg_0 = rand_strided((1000,), (1,), device='cuda:0', dtype=torch.float32)
    arg_1 = rand_strided((1000,), (1,), device='cuda:0', dtype=torch.float16)
    return arg_0, arg_1, 1000,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused__to_copy_250.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused__to_copy_250.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 6e-06
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


# kernel path: /tmp/torchinductor_jeromeku/3f/c3fi7a44o7kaxp3m77k35667c6fxwr25rbd5cgsjnxouirkx5bdl.py
# Topologically Sorted Source Nodes: [add_], Original ATen: [aten.add, aten.copy_]
# Source node to ATen node mapping:
#   add_ => add
# Graph fragment:
#   %copy_ : Tensor "i64[][]cuda:0" = PlaceHolder[target=copy_]
#   %add : Tensor "i64[][]cuda:0" = PlaceHolder[target=add]
#   %add : Tensor "i64[][]cuda:0"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%primals_3, 1), kwargs = {})
#   %copy_ : Tensor "i64[][]cuda:0"[num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%primals_3, %add), kwargs = {})
#   return %add,%buf1194
triton_poi_fused_add_copy__251 = async_compile.triton('triton_poi_fused_add_copy__251', '''
import triton
import triton.language as tl

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

from torch._dynamo.testing import rand_strided
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch

@triton_heuristics.pointwise(
    size_hints={'x': 1}, 
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*i64', 'out_ptr1': '*i64', 'xnumel': 'constexpr', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {'xnumel': 1}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
    inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_copy__251', 'mutated_arg_names': ['in_ptr0', 'out_ptr1'], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'BD5893E1E5403E2B9EB22973BBF403D5636604F148021905C86B2D099C4DE761', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'kernel_num_gb': 1.6e-08, 'kernel_flop': 0},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_copy__251(in_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    tmp0 = tl.load(in_ptr0 + (0))
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK])
    tmp2 = tl.full([1], 1, tl.int64)
    tmp3 = tmp1 + tmp2
    tl.store(out_ptr1 + (tl.full([XBLOCK], 0, tl.int32)), tmp3, None)


def get_args():
    arg_0 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    arg_1 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    return arg_0, arg_1, 1,


def call(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        stream0 = get_raw_stream(0)
        triton_poi_fused_add_copy__251.run(*args, stream=stream0)


def benchmark_all_configs(args):
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        return triton_poi_fused_add_copy__251.benchmark_all_configs(*args)


if __name__ == '__main__':
    from torch._inductor.runtime.benchmarking import benchmarker

    args = get_args()
    ms = benchmarker.benchmark_gpu(lambda: call(args), rep=40)
    num_gb = 1.6e-08
    gb_per_s = num_gb / (ms / 1e3)
    print(f"{ms:.3f}ms    {num_gb:.3f}GB    {gb_per_s:.2f}GB/s")
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, primals_75, primals_76, primals_77, primals_78, primals_79, primals_80, primals_81, primals_82, primals_83, primals_84, primals_85, primals_86, primals_87, primals_88, primals_89, primals_90, primals_91, primals_92, primals_93, primals_94, primals_95, primals_96, primals_97, primals_98, primals_99, primals_100, primals_101, primals_102, primals_103, primals_104, primals_105, primals_106, primals_107, primals_108, primals_109, primals_110, primals_111, primals_112, primals_113, primals_114, primals_115, primals_116, primals_117, primals_118, primals_119, primals_120, primals_121, primals_122, primals_123, primals_124, primals_125, primals_126, primals_127, primals_128, primals_129, primals_130, primals_131, primals_132, primals_133, primals_134, primals_135, primals_136, primals_137, primals_138, primals_139, primals_140, primals_141, primals_142, primals_143, primals_144, primals_145, primals_146, primals_147, primals_148, primals_149, primals_150, primals_151, primals_152, primals_153, primals_154, primals_155, primals_156, primals_157, primals_158, primals_159, primals_160, primals_161, primals_162, primals_163, primals_164, primals_165, primals_166, primals_167, primals_168, primals_169, primals_170, primals_171, primals_172, primals_173, primals_174, primals_175, primals_176, primals_177, primals_178, primals_179, primals_180, primals_181, primals_182, primals_183, primals_184, primals_185, primals_186, primals_187, primals_188, primals_189, primals_190, primals_191, primals_192, primals_193, primals_194, primals_195, primals_196, primals_197, primals_198, primals_199, primals_200, primals_201, primals_202, primals_203, primals_204, primals_205, primals_206, primals_207, primals_208, primals_209, primals_210, primals_211, primals_212, primals_213, primals_214, primals_215, primals_216, primals_217, primals_218, primals_219, primals_220, primals_221, primals_222, primals_223, primals_224, primals_225, primals_226, primals_227, primals_228, primals_229, primals_230, primals_231, primals_232, primals_233, primals_234, primals_235, primals_236, primals_237, primals_238, primals_239, primals_240, primals_241, primals_242, primals_243, primals_244, primals_245, primals_246, primals_247, primals_248, primals_249, primals_250, primals_251, primals_252, primals_253, primals_254, primals_255, primals_256, primals_257, primals_258, primals_259, primals_260, primals_261, primals_262, primals_263, primals_264, primals_265, primals_266, primals_267, primals_268, primals_269, primals_270, primals_271, primals_272, primals_273, primals_274, primals_275, primals_276, primals_277, primals_278, primals_279, primals_280, primals_281, primals_282, primals_283, primals_284, primals_285, primals_286, primals_287, primals_288, primals_289, primals_290, primals_291, primals_292, primals_293, primals_294, primals_295, primals_296, primals_297, primals_298, primals_299, primals_300, primals_301, primals_302, primals_303, primals_304, primals_305, primals_306, primals_307, primals_308, primals_309, primals_310, primals_311, primals_312, primals_313, primals_314, primals_315, primals_316, primals_317, primals_318, primals_319, primals_320, primals_321, primals_322, primals_323, primals_324, primals_325, primals_326, primals_327, primals_328, primals_329, primals_330, primals_331, primals_332, primals_333, primals_334, primals_335, primals_336, primals_337, primals_338, primals_339, primals_340, primals_341, primals_342, primals_343, primals_344, primals_345, primals_346, primals_347, primals_348, primals_349, primals_350, primals_351, primals_352, primals_353, primals_354, primals_355, primals_356, primals_357, primals_358, primals_359, primals_360, primals_361, primals_362, primals_363, primals_364, primals_365, primals_366, primals_367, primals_368, primals_369, primals_370, primals_371, primals_372, primals_373, primals_374, primals_375, primals_376, primals_377, primals_378, primals_379, primals_380, primals_381, primals_382, primals_383, primals_384, primals_385, primals_386, primals_387, primals_388, primals_389, primals_390, primals_391, primals_392, primals_393, primals_394, primals_395, primals_396, primals_397, primals_398, primals_399, primals_400, primals_401, primals_402, primals_403, primals_404, primals_405, primals_406, primals_407, primals_408, primals_409, primals_410, primals_411, primals_412, primals_413, primals_414, primals_415, primals_416, primals_417, primals_418, primals_419, primals_420, primals_421, primals_422, primals_423, primals_424, primals_425, primals_426, primals_427, primals_428, primals_429, primals_430, primals_431, primals_432, primals_433, primals_434, primals_435, primals_436, primals_437, primals_438, primals_439, primals_440, primals_441, primals_442, primals_443, primals_444, primals_445, primals_446, primals_447, primals_448, primals_449, primals_450, primals_451, primals_452, primals_453, primals_454, primals_455, primals_456, primals_457, primals_458, primals_459, primals_460, primals_461, primals_462, primals_463, primals_464, primals_465, primals_466, primals_467, primals_468, primals_469, primals_470, primals_471, primals_472, primals_473, primals_474, primals_475, primals_476, primals_477, primals_478, primals_479, primals_480 = args
        args.clear()
        assert_size_stride(primals_1, (32, 3, 3, 3), (27, 9, 3, 1))
        assert_size_stride(primals_2, (128, 3, 224, 224), (150528, 50176, 224, 1))
        assert_size_stride(primals_3, (), ())
        assert_size_stride(primals_4, (32, ), (1, ))
        assert_size_stride(primals_5, (32, ), (1, ))
        assert_size_stride(primals_6, (32, ), (1, ))
        assert_size_stride(primals_7, (32, ), (1, ))
        assert_size_stride(primals_8, (32, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_9, (), ())
        assert_size_stride(primals_10, (32, ), (1, ))
        assert_size_stride(primals_11, (32, ), (1, ))
        assert_size_stride(primals_12, (32, ), (1, ))
        assert_size_stride(primals_13, (32, ), (1, ))
        assert_size_stride(primals_14, (32, 32, 1, 1), (32, 1, 1, 1))
        assert_size_stride(primals_15, (), ())
        assert_size_stride(primals_16, (32, ), (1, ))
        assert_size_stride(primals_17, (32, ), (1, ))
        assert_size_stride(primals_18, (32, ), (1, ))
        assert_size_stride(primals_19, (32, ), (1, ))
        assert_size_stride(primals_20, (96, 16, 1, 1), (16, 1, 1, 1))
        assert_size_stride(primals_21, (96, 16, 1, 1), (16, 1, 1, 1))
        assert_size_stride(primals_22, (), ())
        assert_size_stride(primals_23, (192, ), (1, ))
        assert_size_stride(primals_24, (192, ), (1, ))
        assert_size_stride(primals_25, (192, ), (1, ))
        assert_size_stride(primals_26, (192, ), (1, ))
        assert_size_stride(primals_27, (64, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_28, (64, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_29, (64, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_30, (), ())
        assert_size_stride(primals_31, (192, ), (1, ))
        assert_size_stride(primals_32, (192, ), (1, ))
        assert_size_stride(primals_33, (192, ), (1, ))
        assert_size_stride(primals_34, (192, ), (1, ))
        assert_size_stride(primals_35, (20, 96, 1, 1), (96, 1, 1, 1))
        assert_size_stride(primals_36, (20, 96, 1, 1), (96, 1, 1, 1))
        assert_size_stride(primals_37, (), ())
        assert_size_stride(primals_38, (40, ), (1, ))
        assert_size_stride(primals_39, (40, ), (1, ))
        assert_size_stride(primals_40, (40, ), (1, ))
        assert_size_stride(primals_41, (40, ), (1, ))
        assert_size_stride(primals_42, (60, 20, 1, 1), (20, 1, 1, 1))
        assert_size_stride(primals_43, (60, 20, 1, 1), (20, 1, 1, 1))
        assert_size_stride(primals_44, (), ())
        assert_size_stride(primals_45, (120, ), (1, ))
        assert_size_stride(primals_46, (120, ), (1, ))
        assert_size_stride(primals_47, (120, ), (1, ))
        assert_size_stride(primals_48, (120, ), (1, ))
        assert_size_stride(primals_49, (120, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_50, (), ())
        assert_size_stride(primals_51, (120, ), (1, ))
        assert_size_stride(primals_52, (120, ), (1, ))
        assert_size_stride(primals_53, (120, ), (1, ))
        assert_size_stride(primals_54, (120, ), (1, ))
        assert_size_stride(primals_55, (20, 60, 1, 1), (60, 1, 1, 1))
        assert_size_stride(primals_56, (20, 60, 1, 1), (60, 1, 1, 1))
        assert_size_stride(primals_57, (), ())
        assert_size_stride(primals_58, (40, ), (1, ))
        assert_size_stride(primals_59, (40, ), (1, ))
        assert_size_stride(primals_60, (40, ), (1, ))
        assert_size_stride(primals_61, (40, ), (1, ))
        assert_size_stride(primals_62, (240, 40, 1, 1), (40, 1, 1, 1))
        assert_size_stride(primals_63, (), ())
        assert_size_stride(primals_64, (240, ), (1, ))
        assert_size_stride(primals_65, (240, ), (1, ))
        assert_size_stride(primals_66, (240, ), (1, ))
        assert_size_stride(primals_67, (240, ), (1, ))
        assert_size_stride(primals_68, (60, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_69, (60, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_70, (60, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_71, (60, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_72, (), ())
        assert_size_stride(primals_73, (240, ), (1, ))
        assert_size_stride(primals_74, (240, ), (1, ))
        assert_size_stride(primals_75, (240, ), (1, ))
        assert_size_stride(primals_76, (240, ), (1, ))
        assert_size_stride(primals_77, (20, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_78, (20, ), (1, ))
        assert_size_stride(primals_79, (240, 20, 1, 1), (20, 1, 1, 1))
        assert_size_stride(primals_80, (240, ), (1, ))
        assert_size_stride(primals_81, (56, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_82, (), ())
        assert_size_stride(primals_83, (56, ), (1, ))
        assert_size_stride(primals_84, (56, ), (1, ))
        assert_size_stride(primals_85, (56, ), (1, ))
        assert_size_stride(primals_86, (56, ), (1, ))
        assert_size_stride(primals_87, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_88, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_89, (), ())
        assert_size_stride(primals_90, (336, ), (1, ))
        assert_size_stride(primals_91, (336, ), (1, ))
        assert_size_stride(primals_92, (336, ), (1, ))
        assert_size_stride(primals_93, (336, ), (1, ))
        assert_size_stride(primals_94, (168, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_95, (168, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_96, (), ())
        assert_size_stride(primals_97, (336, ), (1, ))
        assert_size_stride(primals_98, (336, ), (1, ))
        assert_size_stride(primals_99, (336, ), (1, ))
        assert_size_stride(primals_100, (336, ), (1, ))
        assert_size_stride(primals_101, (28, 336, 1, 1), (336, 1, 1, 1))
        assert_size_stride(primals_102, (28, ), (1, ))
        assert_size_stride(primals_103, (336, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_104, (336, ), (1, ))
        assert_size_stride(primals_105, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_106, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_107, (), ())
        assert_size_stride(primals_108, (56, ), (1, ))
        assert_size_stride(primals_109, (56, ), (1, ))
        assert_size_stride(primals_110, (56, ), (1, ))
        assert_size_stride(primals_111, (56, ), (1, ))
        assert_size_stride(primals_112, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_113, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_114, (), ())
        assert_size_stride(primals_115, (336, ), (1, ))
        assert_size_stride(primals_116, (336, ), (1, ))
        assert_size_stride(primals_117, (336, ), (1, ))
        assert_size_stride(primals_118, (336, ), (1, ))
        assert_size_stride(primals_119, (168, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_120, (168, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_121, (), ())
        assert_size_stride(primals_122, (336, ), (1, ))
        assert_size_stride(primals_123, (336, ), (1, ))
        assert_size_stride(primals_124, (336, ), (1, ))
        assert_size_stride(primals_125, (336, ), (1, ))
        assert_size_stride(primals_126, (28, 336, 1, 1), (336, 1, 1, 1))
        assert_size_stride(primals_127, (28, ), (1, ))
        assert_size_stride(primals_128, (336, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_129, (336, ), (1, ))
        assert_size_stride(primals_130, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_131, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_132, (), ())
        assert_size_stride(primals_133, (56, ), (1, ))
        assert_size_stride(primals_134, (56, ), (1, ))
        assert_size_stride(primals_135, (56, ), (1, ))
        assert_size_stride(primals_136, (56, ), (1, ))
        assert_size_stride(primals_137, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_138, (168, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_139, (), ())
        assert_size_stride(primals_140, (336, ), (1, ))
        assert_size_stride(primals_141, (336, ), (1, ))
        assert_size_stride(primals_142, (336, ), (1, ))
        assert_size_stride(primals_143, (336, ), (1, ))
        assert_size_stride(primals_144, (168, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_145, (168, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_146, (), ())
        assert_size_stride(primals_147, (336, ), (1, ))
        assert_size_stride(primals_148, (336, ), (1, ))
        assert_size_stride(primals_149, (336, ), (1, ))
        assert_size_stride(primals_150, (336, ), (1, ))
        assert_size_stride(primals_151, (28, 336, 1, 1), (336, 1, 1, 1))
        assert_size_stride(primals_152, (28, ), (1, ))
        assert_size_stride(primals_153, (336, 28, 1, 1), (28, 1, 1, 1))
        assert_size_stride(primals_154, (336, ), (1, ))
        assert_size_stride(primals_155, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_156, (28, 168, 1, 1), (168, 1, 1, 1))
        assert_size_stride(primals_157, (), ())
        assert_size_stride(primals_158, (56, ), (1, ))
        assert_size_stride(primals_159, (56, ), (1, ))
        assert_size_stride(primals_160, (56, ), (1, ))
        assert_size_stride(primals_161, (56, ), (1, ))
        assert_size_stride(primals_162, (336, 56, 1, 1), (56, 1, 1, 1))
        assert_size_stride(primals_163, (), ())
        assert_size_stride(primals_164, (336, ), (1, ))
        assert_size_stride(primals_165, (336, ), (1, ))
        assert_size_stride(primals_166, (336, ), (1, ))
        assert_size_stride(primals_167, (336, ), (1, ))
        assert_size_stride(primals_168, (112, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_169, (112, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_170, (112, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_171, (), ())
        assert_size_stride(primals_172, (336, ), (1, ))
        assert_size_stride(primals_173, (336, ), (1, ))
        assert_size_stride(primals_174, (336, ), (1, ))
        assert_size_stride(primals_175, (336, ), (1, ))
        assert_size_stride(primals_176, (14, 336, 1, 1), (336, 1, 1, 1))
        assert_size_stride(primals_177, (14, ), (1, ))
        assert_size_stride(primals_178, (336, 14, 1, 1), (14, 1, 1, 1))
        assert_size_stride(primals_179, (336, ), (1, ))
        assert_size_stride(primals_180, (104, 336, 1, 1), (336, 1, 1, 1))
        assert_size_stride(primals_181, (), ())
        assert_size_stride(primals_182, (104, ), (1, ))
        assert_size_stride(primals_183, (104, ), (1, ))
        assert_size_stride(primals_184, (104, ), (1, ))
        assert_size_stride(primals_185, (104, ), (1, ))
        assert_size_stride(primals_186, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_187, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_188, (), ())
        assert_size_stride(primals_189, (624, ), (1, ))
        assert_size_stride(primals_190, (624, ), (1, ))
        assert_size_stride(primals_191, (624, ), (1, ))
        assert_size_stride(primals_192, (624, ), (1, ))
        assert_size_stride(primals_193, (156, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_194, (156, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_195, (156, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_196, (156, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_197, (), ())
        assert_size_stride(primals_198, (624, ), (1, ))
        assert_size_stride(primals_199, (624, ), (1, ))
        assert_size_stride(primals_200, (624, ), (1, ))
        assert_size_stride(primals_201, (624, ), (1, ))
        assert_size_stride(primals_202, (26, 624, 1, 1), (624, 1, 1, 1))
        assert_size_stride(primals_203, (26, ), (1, ))
        assert_size_stride(primals_204, (624, 26, 1, 1), (26, 1, 1, 1))
        assert_size_stride(primals_205, (624, ), (1, ))
        assert_size_stride(primals_206, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_207, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_208, (), ())
        assert_size_stride(primals_209, (104, ), (1, ))
        assert_size_stride(primals_210, (104, ), (1, ))
        assert_size_stride(primals_211, (104, ), (1, ))
        assert_size_stride(primals_212, (104, ), (1, ))
        assert_size_stride(primals_213, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_214, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_215, (), ())
        assert_size_stride(primals_216, (624, ), (1, ))
        assert_size_stride(primals_217, (624, ), (1, ))
        assert_size_stride(primals_218, (624, ), (1, ))
        assert_size_stride(primals_219, (624, ), (1, ))
        assert_size_stride(primals_220, (156, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_221, (156, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_222, (156, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_223, (156, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_224, (), ())
        assert_size_stride(primals_225, (624, ), (1, ))
        assert_size_stride(primals_226, (624, ), (1, ))
        assert_size_stride(primals_227, (624, ), (1, ))
        assert_size_stride(primals_228, (624, ), (1, ))
        assert_size_stride(primals_229, (26, 624, 1, 1), (624, 1, 1, 1))
        assert_size_stride(primals_230, (26, ), (1, ))
        assert_size_stride(primals_231, (624, 26, 1, 1), (26, 1, 1, 1))
        assert_size_stride(primals_232, (624, ), (1, ))
        assert_size_stride(primals_233, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_234, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_235, (), ())
        assert_size_stride(primals_236, (104, ), (1, ))
        assert_size_stride(primals_237, (104, ), (1, ))
        assert_size_stride(primals_238, (104, ), (1, ))
        assert_size_stride(primals_239, (104, ), (1, ))
        assert_size_stride(primals_240, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_241, (312, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_242, (), ())
        assert_size_stride(primals_243, (624, ), (1, ))
        assert_size_stride(primals_244, (624, ), (1, ))
        assert_size_stride(primals_245, (624, ), (1, ))
        assert_size_stride(primals_246, (624, ), (1, ))
        assert_size_stride(primals_247, (156, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_248, (156, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_249, (156, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_250, (156, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_251, (), ())
        assert_size_stride(primals_252, (624, ), (1, ))
        assert_size_stride(primals_253, (624, ), (1, ))
        assert_size_stride(primals_254, (624, ), (1, ))
        assert_size_stride(primals_255, (624, ), (1, ))
        assert_size_stride(primals_256, (26, 624, 1, 1), (624, 1, 1, 1))
        assert_size_stride(primals_257, (26, ), (1, ))
        assert_size_stride(primals_258, (624, 26, 1, 1), (26, 1, 1, 1))
        assert_size_stride(primals_259, (624, ), (1, ))
        assert_size_stride(primals_260, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_261, (52, 312, 1, 1), (312, 1, 1, 1))
        assert_size_stride(primals_262, (), ())
        assert_size_stride(primals_263, (104, ), (1, ))
        assert_size_stride(primals_264, (104, ), (1, ))
        assert_size_stride(primals_265, (104, ), (1, ))
        assert_size_stride(primals_266, (104, ), (1, ))
        assert_size_stride(primals_267, (624, 104, 1, 1), (104, 1, 1, 1))
        assert_size_stride(primals_268, (), ())
        assert_size_stride(primals_269, (624, ), (1, ))
        assert_size_stride(primals_270, (624, ), (1, ))
        assert_size_stride(primals_271, (624, ), (1, ))
        assert_size_stride(primals_272, (624, ), (1, ))
        assert_size_stride(primals_273, (624, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_274, (), ())
        assert_size_stride(primals_275, (624, ), (1, ))
        assert_size_stride(primals_276, (624, ), (1, ))
        assert_size_stride(primals_277, (624, ), (1, ))
        assert_size_stride(primals_278, (624, ), (1, ))
        assert_size_stride(primals_279, (52, 624, 1, 1), (624, 1, 1, 1))
        assert_size_stride(primals_280, (52, ), (1, ))
        assert_size_stride(primals_281, (624, 52, 1, 1), (52, 1, 1, 1))
        assert_size_stride(primals_282, (624, ), (1, ))
        assert_size_stride(primals_283, (160, 624, 1, 1), (624, 1, 1, 1))
        assert_size_stride(primals_284, (), ())
        assert_size_stride(primals_285, (160, ), (1, ))
        assert_size_stride(primals_286, (160, ), (1, ))
        assert_size_stride(primals_287, (160, ), (1, ))
        assert_size_stride(primals_288, (160, ), (1, ))
        assert_size_stride(primals_289, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_290, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_291, (), ())
        assert_size_stride(primals_292, (480, ), (1, ))
        assert_size_stride(primals_293, (480, ), (1, ))
        assert_size_stride(primals_294, (480, ), (1, ))
        assert_size_stride(primals_295, (480, ), (1, ))
        assert_size_stride(primals_296, (120, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_297, (120, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_298, (120, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_299, (120, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_300, (), ())
        assert_size_stride(primals_301, (480, ), (1, ))
        assert_size_stride(primals_302, (480, ), (1, ))
        assert_size_stride(primals_303, (480, ), (1, ))
        assert_size_stride(primals_304, (480, ), (1, ))
        assert_size_stride(primals_305, (80, 480, 1, 1), (480, 1, 1, 1))
        assert_size_stride(primals_306, (80, ), (1, ))
        assert_size_stride(primals_307, (480, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_308, (480, ), (1, ))
        assert_size_stride(primals_309, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_310, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_311, (), ())
        assert_size_stride(primals_312, (160, ), (1, ))
        assert_size_stride(primals_313, (160, ), (1, ))
        assert_size_stride(primals_314, (160, ), (1, ))
        assert_size_stride(primals_315, (160, ), (1, ))
        assert_size_stride(primals_316, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_317, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_318, (), ())
        assert_size_stride(primals_319, (480, ), (1, ))
        assert_size_stride(primals_320, (480, ), (1, ))
        assert_size_stride(primals_321, (480, ), (1, ))
        assert_size_stride(primals_322, (480, ), (1, ))
        assert_size_stride(primals_323, (120, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_324, (120, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_325, (120, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_326, (120, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_327, (), ())
        assert_size_stride(primals_328, (480, ), (1, ))
        assert_size_stride(primals_329, (480, ), (1, ))
        assert_size_stride(primals_330, (480, ), (1, ))
        assert_size_stride(primals_331, (480, ), (1, ))
        assert_size_stride(primals_332, (80, 480, 1, 1), (480, 1, 1, 1))
        assert_size_stride(primals_333, (80, ), (1, ))
        assert_size_stride(primals_334, (480, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_335, (480, ), (1, ))
        assert_size_stride(primals_336, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_337, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_338, (), ())
        assert_size_stride(primals_339, (160, ), (1, ))
        assert_size_stride(primals_340, (160, ), (1, ))
        assert_size_stride(primals_341, (160, ), (1, ))
        assert_size_stride(primals_342, (160, ), (1, ))
        assert_size_stride(primals_343, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_344, (240, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_345, (), ())
        assert_size_stride(primals_346, (480, ), (1, ))
        assert_size_stride(primals_347, (480, ), (1, ))
        assert_size_stride(primals_348, (480, ), (1, ))
        assert_size_stride(primals_349, (480, ), (1, ))
        assert_size_stride(primals_350, (120, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_351, (120, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_352, (120, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_353, (120, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_354, (), ())
        assert_size_stride(primals_355, (480, ), (1, ))
        assert_size_stride(primals_356, (480, ), (1, ))
        assert_size_stride(primals_357, (480, ), (1, ))
        assert_size_stride(primals_358, (480, ), (1, ))
        assert_size_stride(primals_359, (80, 480, 1, 1), (480, 1, 1, 1))
        assert_size_stride(primals_360, (80, ), (1, ))
        assert_size_stride(primals_361, (480, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_362, (480, ), (1, ))
        assert_size_stride(primals_363, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_364, (80, 240, 1, 1), (240, 1, 1, 1))
        assert_size_stride(primals_365, (), ())
        assert_size_stride(primals_366, (160, ), (1, ))
        assert_size_stride(primals_367, (160, ), (1, ))
        assert_size_stride(primals_368, (160, ), (1, ))
        assert_size_stride(primals_369, (160, ), (1, ))
        assert_size_stride(primals_370, (960, 160, 1, 1), (160, 1, 1, 1))
        assert_size_stride(primals_371, (), ())
        assert_size_stride(primals_372, (960, ), (1, ))
        assert_size_stride(primals_373, (960, ), (1, ))
        assert_size_stride(primals_374, (960, ), (1, ))
        assert_size_stride(primals_375, (960, ), (1, ))
        assert_size_stride(primals_376, (240, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_377, (240, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_378, (240, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_379, (240, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_380, (), ())
        assert_size_stride(primals_381, (960, ), (1, ))
        assert_size_stride(primals_382, (960, ), (1, ))
        assert_size_stride(primals_383, (960, ), (1, ))
        assert_size_stride(primals_384, (960, ), (1, ))
        assert_size_stride(primals_385, (80, 960, 1, 1), (960, 1, 1, 1))
        assert_size_stride(primals_386, (80, ), (1, ))
        assert_size_stride(primals_387, (960, 80, 1, 1), (80, 1, 1, 1))
        assert_size_stride(primals_388, (960, ), (1, ))
        assert_size_stride(primals_389, (264, 960, 1, 1), (960, 1, 1, 1))
        assert_size_stride(primals_390, (), ())
        assert_size_stride(primals_391, (264, ), (1, ))
        assert_size_stride(primals_392, (264, ), (1, ))
        assert_size_stride(primals_393, (264, ), (1, ))
        assert_size_stride(primals_394, (264, ), (1, ))
        assert_size_stride(primals_395, (1584, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(primals_396, (), ())
        assert_size_stride(primals_397, (1584, ), (1, ))
        assert_size_stride(primals_398, (1584, ), (1, ))
        assert_size_stride(primals_399, (1584, ), (1, ))
        assert_size_stride(primals_400, (1584, ), (1, ))
        assert_size_stride(primals_401, (396, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_402, (396, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_403, (396, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_404, (396, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_405, (), ())
        assert_size_stride(primals_406, (1584, ), (1, ))
        assert_size_stride(primals_407, (1584, ), (1, ))
        assert_size_stride(primals_408, (1584, ), (1, ))
        assert_size_stride(primals_409, (1584, ), (1, ))
        assert_size_stride(primals_410, (132, 1584, 1, 1), (1584, 1, 1, 1))
        assert_size_stride(primals_411, (132, ), (1, ))
        assert_size_stride(primals_412, (1584, 132, 1, 1), (132, 1, 1, 1))
        assert_size_stride(primals_413, (1584, ), (1, ))
        assert_size_stride(primals_414, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_415, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_416, (), ())
        assert_size_stride(primals_417, (264, ), (1, ))
        assert_size_stride(primals_418, (264, ), (1, ))
        assert_size_stride(primals_419, (264, ), (1, ))
        assert_size_stride(primals_420, (264, ), (1, ))
        assert_size_stride(primals_421, (1584, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(primals_422, (), ())
        assert_size_stride(primals_423, (1584, ), (1, ))
        assert_size_stride(primals_424, (1584, ), (1, ))
        assert_size_stride(primals_425, (1584, ), (1, ))
        assert_size_stride(primals_426, (1584, ), (1, ))
        assert_size_stride(primals_427, (396, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_428, (396, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_429, (396, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_430, (396, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_431, (), ())
        assert_size_stride(primals_432, (1584, ), (1, ))
        assert_size_stride(primals_433, (1584, ), (1, ))
        assert_size_stride(primals_434, (1584, ), (1, ))
        assert_size_stride(primals_435, (1584, ), (1, ))
        assert_size_stride(primals_436, (132, 1584, 1, 1), (1584, 1, 1, 1))
        assert_size_stride(primals_437, (132, ), (1, ))
        assert_size_stride(primals_438, (1584, 132, 1, 1), (132, 1, 1, 1))
        assert_size_stride(primals_439, (1584, ), (1, ))
        assert_size_stride(primals_440, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_441, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_442, (), ())
        assert_size_stride(primals_443, (264, ), (1, ))
        assert_size_stride(primals_444, (264, ), (1, ))
        assert_size_stride(primals_445, (264, ), (1, ))
        assert_size_stride(primals_446, (264, ), (1, ))
        assert_size_stride(primals_447, (1584, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(primals_448, (), ())
        assert_size_stride(primals_449, (1584, ), (1, ))
        assert_size_stride(primals_450, (1584, ), (1, ))
        assert_size_stride(primals_451, (1584, ), (1, ))
        assert_size_stride(primals_452, (1584, ), (1, ))
        assert_size_stride(primals_453, (396, 1, 3, 3), (9, 9, 3, 1))
        assert_size_stride(primals_454, (396, 1, 5, 5), (25, 25, 5, 1))
        assert_size_stride(primals_455, (396, 1, 7, 7), (49, 49, 7, 1))
        assert_size_stride(primals_456, (396, 1, 9, 9), (81, 81, 9, 1))
        assert_size_stride(primals_457, (), ())
        assert_size_stride(primals_458, (1584, ), (1, ))
        assert_size_stride(primals_459, (1584, ), (1, ))
        assert_size_stride(primals_460, (1584, ), (1, ))
        assert_size_stride(primals_461, (1584, ), (1, ))
        assert_size_stride(primals_462, (132, 1584, 1, 1), (1584, 1, 1, 1))
        assert_size_stride(primals_463, (132, ), (1, ))
        assert_size_stride(primals_464, (1584, 132, 1, 1), (132, 1, 1, 1))
        assert_size_stride(primals_465, (1584, ), (1, ))
        assert_size_stride(primals_466, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_467, (132, 792, 1, 1), (792, 1, 1, 1))
        assert_size_stride(primals_468, (), ())
        assert_size_stride(primals_469, (264, ), (1, ))
        assert_size_stride(primals_470, (264, ), (1, ))
        assert_size_stride(primals_471, (264, ), (1, ))
        assert_size_stride(primals_472, (264, ), (1, ))
        assert_size_stride(primals_473, (1536, 264, 1, 1), (264, 1, 1, 1))
        assert_size_stride(primals_474, (), ())
        assert_size_stride(primals_475, (1536, ), (1, ))
        assert_size_stride(primals_476, (1536, ), (1, ))
        assert_size_stride(primals_477, (1536, ), (1, ))
        assert_size_stride(primals_478, (1536, ), (1, ))
        assert_size_stride(primals_479, (1000, 1536), (1536, 1))
        assert_size_stride(primals_480, (1000, ), (1, ))
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((32, 3, 3, 3), (27, 1, 9, 3), torch.float16)
            # Topologically Sorted Source Nodes: [x], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_0.run(primals_1, buf0, 96, 9, stream=stream0)
            del primals_1
            buf1 = empty_strided_cuda((128, 3, 224, 224), (150528, 1, 672, 3), torch.float16)
            # Topologically Sorted Source Nodes: [x], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_1.run(primals_2, buf1, 384, 50176, stream=stream0)
            del primals_2
            # Topologically Sorted Source Nodes: [x], Original ATen: [aten.convolution]
            buf2 = extern_kernels.convolution(buf1, buf0, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf2, (128, 32, 112, 112), (401408, 1, 3584, 32), 'torch.ops.aten.convolution.default')
            buf3 = empty_strided_cuda((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), torch.float32)
            buf4 = empty_strided_cuda((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), torch.float32)
            buf5 = empty_strided_cuda((1, 32, 1, 1, 1024), (32768, 1, 32768, 32768, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_2.run(buf2, buf3, buf4, buf5, 32768, 1568, stream=stream0)
            buf6 = empty_strided_cuda((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), torch.float32)
            buf7 = empty_strided_cuda((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), torch.float32)
            buf8 = empty_strided_cuda((1, 32, 1, 1, 8), (256, 1, 256, 256, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_3.run(buf3, buf4, buf5, buf6, buf7, buf8, 256, 128, stream=stream0)
            buf9 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            buf10 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            buf12 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_1], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__4.run(buf6, buf7, buf8, primals_4, primals_5, buf9, buf10, buf12, primals_4, primals_5, 32, 8, stream=stream0)
            del primals_4
            del primals_5
            buf13 = empty_strided_cuda((128, 32, 112, 112), (401408, 1, 3584, 32), torch.float16)
            # Topologically Sorted Source Nodes: [x_1, x_2], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_5.run(buf2, buf9, buf10, primals_6, primals_7, buf13, 51380224, stream=stream0)
            del primals_7
            buf14 = empty_strided_cuda((32, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_6.run(primals_8, buf14, 288, stream=stream0)
            del primals_8
            # Topologically Sorted Source Nodes: [x_3], Original ATen: [aten.convolution]
            buf15 = extern_kernels.convolution(buf13, buf14, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=32, bias=None)
            assert_size_stride(buf15, (128, 32, 112, 112), (401408, 1, 3584, 32), 'torch.ops.aten.convolution.default')
            buf16 = buf5; del buf5  # reuse
            buf17 = buf4; del buf4  # reuse
            buf18 = buf3; del buf3  # reuse
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_2.run(buf15, buf16, buf17, buf18, 32768, 1568, stream=stream0)
            buf19 = buf8; del buf8  # reuse
            buf20 = buf7; del buf7  # reuse
            buf21 = buf6; del buf6  # reuse
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_3.run(buf16, buf17, buf18, buf19, buf20, buf21, 256, 128, stream=stream0)
            buf22 = buf10; del buf10  # reuse
            buf23 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            buf25 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_4], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__4.run(buf19, buf20, buf21, primals_10, primals_11, buf22, buf23, buf25, primals_10, primals_11, 32, 8, stream=stream0)
            del primals_10
            del primals_11
            buf26 = empty_strided_cuda((128, 32, 112, 112), (401408, 1, 3584, 32), torch.float16)
            # Topologically Sorted Source Nodes: [x_4, x_5], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_5.run(buf15, buf22, buf23, primals_12, primals_13, buf26, 51380224, stream=stream0)
            del primals_13
            buf27 = empty_strided_cuda((32, 32, 1, 1), (32, 1, 32, 32), torch.float16)
            # Topologically Sorted Source Nodes: [x_6], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_7.run(primals_14, buf27, 1024, stream=stream0)
            del primals_14
            # Topologically Sorted Source Nodes: [x_6], Original ATen: [aten.convolution]
            buf28 = extern_kernels.convolution(buf26, buf27, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf28, (128, 32, 112, 112), (401408, 1, 3584, 32), 'torch.ops.aten.convolution.default')
            buf29 = buf18; del buf18  # reuse
            buf30 = buf17; del buf17  # reuse
            buf31 = buf16; del buf16  # reuse
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_2.run(buf28, buf29, buf30, buf31, 32768, 1568, stream=stream0)
            buf32 = buf21; del buf21  # reuse
            buf33 = buf20; del buf20  # reuse
            buf34 = buf19; del buf19  # reuse
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_3.run(buf29, buf30, buf31, buf32, buf33, buf34, 256, 128, stream=stream0)
            del buf29
            del buf30
            del buf31
            buf35 = buf23; del buf23  # reuse
            buf36 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            buf38 = empty_strided_cuda((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
            # Topologically Sorted Source Nodes: [x_7], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__4.run(buf32, buf33, buf34, primals_16, primals_17, buf35, buf36, buf38, primals_16, primals_17, 32, 8, stream=stream0)
            del buf32
            del buf33
            del buf34
            del primals_16
            del primals_17
            buf39 = empty_strided_cuda((128, 32, 112, 112), (401408, 12544, 112, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_7, x_8], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_8.run(buf28, buf35, buf36, primals_18, primals_19, buf13, buf39, 1605632, 32, stream=stream0)
            del buf36
            del primals_19
            buf40 = empty_strided_cuda((96, 16, 1, 1), (16, 1, 16, 16), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_3], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_9.run(primals_20, buf40, 1536, stream=stream0)
            del primals_20
            buf41 = empty_strided_cuda((128, 16, 112, 112), (200704, 1, 1792, 16), torch.float16)
            # Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_3], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_10.run(buf39, buf41, 2048, 12544, stream=stream0)
            # Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_3], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
            buf42 = extern_kernels.convolution(buf41, buf40, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf42, (128, 96, 112, 112), (1204224, 1, 10752, 96), 'torch.ops.aten.convolution.default')
            del buf41
            buf43 = empty_strided_cuda((96, 16, 1, 1), (16, 1, 16, 16), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_4], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_9.run(primals_21, buf43, 1536, stream=stream0)
            del primals_21
            buf44 = empty_strided_cuda((128, 16, 112, 112), (200704, 1, 1792, 16), torch.float16)
            # Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_4], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_convolution_split_with_sizes_11.run(buf39, buf44, 2048, 12544, stream=stream0)
            # Topologically Sorted Source Nodes: [x_7, x_8, split, conv2d_4], Original ATen: [aten._native_batch_norm_legit_functional, aten.add, aten.split_with_sizes, aten.convolution]
            buf45 = extern_kernels.convolution(buf44, buf43, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf45, (128, 96, 112, 112), (1204224, 1, 10752, 96), 'torch.ops.aten.convolution.default')
            del buf44
            buf46 = empty_strided_cuda((128, 192, 112, 112), (2408448, 1, 21504, 192), torch.float16)
            # Topologically Sorted Source Nodes: [x_9], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_12.run(buf42, buf45, buf46, 308281344, stream=stream0)
            del buf42
            del buf45
            buf47 = empty_strided_cuda((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), torch.float32)
            buf48 = empty_strided_cuda((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), torch.float32)
            buf49 = empty_strided_cuda((1, 192, 1, 1, 784), (150528, 1, 150528, 150528, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_13.run(buf46, buf47, buf48, buf49, 150528, 2048, stream=stream0)
            buf50 = empty_strided_cuda((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), torch.float32)
            buf51 = empty_strided_cuda((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), torch.float32)
            buf52 = empty_strided_cuda((1, 192, 1, 1, 7), (1344, 1, 1344, 1344, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_14.run(buf47, buf48, buf49, buf50, buf51, buf52, 1344, 112, stream=stream0)
            del buf47
            del buf48
            del buf49
            buf53 = empty_strided_cuda((1, 192, 1, 1), (192, 1, 192, 192), torch.float32)
            buf56 = empty_strided_cuda((1, 192, 1, 1), (192, 1, 192, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_10], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__15.run(buf50, buf51, buf52, primals_23, primals_24, buf53, buf56, primals_23, primals_24, 192, 7, stream=stream0)
            del primals_23
            del primals_24
            buf57 = empty_strided_cuda((64, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_5], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_16.run(primals_27, buf57, 576, stream=stream0)
            del primals_27
            buf58 = empty_strided_cuda((128, 192, 112, 112), (2408448, 12544, 112, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_10, x_11], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_17.run(buf46, buf53, buf56, primals_25, primals_26, buf58, 24576, 12544, stream=stream0)
            buf59 = empty_strided_cuda((128, 64, 112, 112), (802816, 1, 7168, 64), torch.float16)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_18.run(buf58, buf59, 8192, 12544, stream=stream0)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf60 = extern_kernels.convolution(buf59, buf57, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=64, bias=None)
            assert_size_stride(buf60, (128, 64, 56, 56), (200704, 1, 3584, 64), 'torch.ops.aten.convolution.default')
            del buf59
            buf61 = empty_strided_cuda((64, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_6], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_19.run(primals_28, buf61, 1600, stream=stream0)
            del primals_28
            buf62 = empty_strided_cuda((128, 64, 112, 112), (802816, 1, 7168, 64), torch.float16)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_6], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_20.run(buf58, buf62, 8192, 12544, stream=stream0)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_6], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf63 = extern_kernels.convolution(buf62, buf61, stride=(2, 2), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=64, bias=None)
            assert_size_stride(buf63, (128, 64, 56, 56), (200704, 1, 3584, 64), 'torch.ops.aten.convolution.default')
            del buf62
            buf64 = empty_strided_cuda((64, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_7], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_21.run(primals_29, buf64, 3136, stream=stream0)
            del primals_29
            buf65 = empty_strided_cuda((128, 64, 112, 112), (802816, 1, 7168, 64), torch.float16)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_7], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_22.run(buf58, buf65, 8192, 12544, stream=stream0)
            # Topologically Sorted Source Nodes: [x_10, x_11, conv2d_5, conv2d_7], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf66 = extern_kernels.convolution(buf65, buf64, stride=(2, 2), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=64, bias=None)
            assert_size_stride(buf66, (128, 64, 56, 56), (200704, 1, 3584, 64), 'torch.ops.aten.convolution.default')
            del buf65
            buf67 = empty_strided_cuda((128, 192, 56, 56), (602112, 1, 10752, 192), torch.float16)
            # Topologically Sorted Source Nodes: [x_12], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_23.run(buf60, buf63, buf66, buf67, 77070336, stream=stream0)
            del buf60
            del buf63
            del buf66
            buf68 = empty_strided_cuda((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), torch.float32)
            buf69 = empty_strided_cuda((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), torch.float32)
            buf70 = empty_strided_cuda((1, 192, 1, 1, 512), (98304, 1, 98304, 98304, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_24.run(buf67, buf68, buf69, buf70, 98304, 784, stream=stream0)
            buf71 = empty_strided_cuda((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), torch.float32)
            buf72 = empty_strided_cuda((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), torch.float32)
            buf73 = empty_strided_cuda((1, 192, 1, 1, 4), (768, 1, 768, 768, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_25.run(buf68, buf69, buf70, buf71, buf72, buf73, 768, 128, stream=stream0)
            del buf68
            del buf69
            del buf70
            buf74 = empty_strided_cuda((1, 192, 1, 1), (192, 1, 192, 192), torch.float32)
            buf77 = empty_strided_cuda((1, 192, 1, 1), (192, 1, 192, 192), torch.float32)
            # Topologically Sorted Source Nodes: [x_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__26.run(buf71, buf72, buf73, primals_31, primals_32, buf74, buf77, primals_31, primals_32, 192, 4, stream=stream0)
            del buf71
            del buf72
            del buf73
            del primals_31
            del primals_32
            buf78 = empty_strided_cuda((20, 96, 1, 1), (96, 1, 96, 96), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_8], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_27.run(primals_35, buf78, 1920, stream=stream0)
            del primals_35
            buf79 = empty_strided_cuda((128, 192, 56, 56), (602112, 3136, 56, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_13, x_14], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_28.run(buf67, buf74, buf77, primals_33, primals_34, buf79, 24576, 3136, stream=stream0)
            buf80 = empty_strided_cuda((128, 96, 56, 56), (301056, 1, 5376, 96), torch.float16)
            # Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_29.run(buf79, buf80, 12288, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf81 = extern_kernels.convolution(buf80, buf78, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf81, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution.default')
            del buf80
            buf82 = empty_strided_cuda((20, 96, 1, 1), (96, 1, 96, 96), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_9], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_27.run(primals_36, buf82, 1920, stream=stream0)
            del primals_36
            buf83 = empty_strided_cuda((128, 96, 56, 56), (301056, 1, 5376, 96), torch.float16)
            # Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8, conv2d_9], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_30.run(buf79, buf83, 12288, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_13, x_14, conv2d_8, conv2d_9], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf84 = extern_kernels.convolution(buf83, buf82, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf84, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution.default')
            del buf83
            buf85 = empty_strided_cuda((128, 40, 56, 56), (125440, 1, 2240, 40), torch.float16)
            # Topologically Sorted Source Nodes: [x_15], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_31.run(buf81, buf84, buf85, 16056320, stream=stream0)
            del buf81
            del buf84
            buf86 = empty_strided_cuda((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), torch.float32)
            buf87 = empty_strided_cuda((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), torch.float32)
            buf88 = empty_strided_cuda((1, 40, 1, 1, 1024), (40960, 1, 40960, 40960, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_32.run(buf85, buf86, buf87, buf88, 40960, 392, stream=stream0)
            buf89 = empty_strided_cuda((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), torch.float32)
            buf90 = empty_strided_cuda((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), torch.float32)
            buf91 = empty_strided_cuda((1, 40, 1, 1, 8), (320, 1, 320, 320, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_33.run(buf86, buf87, buf88, buf89, buf90, buf91, 320, 128, stream=stream0)
            buf92 = empty_strided_cuda((1, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            buf93 = empty_strided_cuda((1, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            buf95 = empty_strided_cuda((1, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__34.run(buf89, buf90, buf91, primals_38, primals_39, buf92, buf93, buf95, primals_38, primals_39, 40, 8, stream=stream0)
            del primals_38
            del primals_39
            buf96 = empty_strided_cuda((128, 40, 56, 56), (125440, 1, 2240, 40), torch.float16)
            # Topologically Sorted Source Nodes: [x_16], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_35.run(buf85, buf92, buf93, primals_40, primals_41, buf96, 16056320, stream=stream0)
            del primals_41
            buf97 = empty_strided_cuda((128, 20, 56, 56), (62720, 3136, 56, 1), torch.float16)
            buf100 = empty_strided_cuda((128, 20, 56, 56), (62720, 1, 1120, 20), torch.float16)
            # Topologically Sorted Source Nodes: [split_3, conv2d_10], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_36.run(buf96, buf97, buf100, 2560, 3136, stream=stream0)
            buf98 = empty_strided_cuda((128, 20, 56, 56), (62720, 3136, 56, 1), torch.float16)
            buf103 = empty_strided_cuda((128, 20, 56, 56), (62720, 1, 1120, 20), torch.float16)
            # Topologically Sorted Source Nodes: [split_3, conv2d_11], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_37.run(buf96, buf98, buf103, 2560, 3136, stream=stream0)
            buf99 = empty_strided_cuda((60, 20, 1, 1), (20, 1, 20, 20), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_10], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_38.run(primals_42, buf99, 1200, stream=stream0)
            del primals_42
            # Topologically Sorted Source Nodes: [conv2d_10], Original ATen: [aten.convolution]
            buf101 = extern_kernels.convolution(buf100, buf99, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf101, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution.default')
            del buf100
            buf102 = empty_strided_cuda((60, 20, 1, 1), (20, 1, 20, 20), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_11], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_38.run(primals_43, buf102, 1200, stream=stream0)
            del primals_43
            # Topologically Sorted Source Nodes: [conv2d_11], Original ATen: [aten.convolution]
            buf104 = extern_kernels.convolution(buf103, buf102, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf104, (128, 60, 56, 56), (188160, 1, 3360, 60), 'torch.ops.aten.convolution.default')
            del buf103
            buf105 = empty_strided_cuda((128, 120, 56, 56), (376320, 1, 6720, 120), torch.float16)
            # Topologically Sorted Source Nodes: [x_17], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_39.run(buf101, buf104, buf105, 48168960, stream=stream0)
            del buf101
            del buf104
            buf106 = empty_strided_cuda((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), torch.float32)
            buf107 = empty_strided_cuda((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), torch.float32)
            buf108 = empty_strided_cuda((1, 120, 1, 1, 1024), (122880, 1, 122880, 122880, 120), torch.float32)
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_40.run(buf105, buf106, buf107, buf108, 122880, 392, stream=stream0)
            buf109 = empty_strided_cuda((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), torch.float32)
            buf110 = empty_strided_cuda((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), torch.float32)
            buf111 = empty_strided_cuda((1, 120, 1, 1, 8), (960, 1, 960, 960, 120), torch.float32)
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_41.run(buf106, buf107, buf108, buf109, buf110, buf111, 960, 128, stream=stream0)
            buf112 = empty_strided_cuda((1, 120, 1, 1), (120, 1, 120, 120), torch.float32)
            buf113 = empty_strided_cuda((1, 120, 1, 1), (120, 1, 120, 120), torch.float32)
            buf115 = empty_strided_cuda((1, 120, 1, 1), (120, 1, 120, 120), torch.float32)
            # Topologically Sorted Source Nodes: [x_18], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__42.run(buf109, buf110, buf111, primals_45, primals_46, buf112, buf113, buf115, primals_45, primals_46, 120, 8, stream=stream0)
            del primals_45
            del primals_46
            buf116 = empty_strided_cuda((128, 120, 56, 56), (376320, 1, 6720, 120), torch.float16)
            # Topologically Sorted Source Nodes: [x_18, x_19], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_43.run(buf105, buf112, buf113, primals_47, primals_48, buf116, 48168960, stream=stream0)
            del primals_48
            buf117 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_20], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_44.run(primals_49, buf117, 1080, stream=stream0)
            del primals_49
            # Topologically Sorted Source Nodes: [x_20], Original ATen: [aten.convolution]
            buf118 = extern_kernels.convolution(buf116, buf117, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf118, (128, 120, 56, 56), (376320, 1, 6720, 120), 'torch.ops.aten.convolution.default')
            buf119 = buf108; del buf108  # reuse
            buf120 = buf107; del buf107  # reuse
            buf121 = buf106; del buf106  # reuse
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_40.run(buf118, buf119, buf120, buf121, 122880, 392, stream=stream0)
            buf122 = buf111; del buf111  # reuse
            buf123 = buf110; del buf110  # reuse
            buf124 = buf109; del buf109  # reuse
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_41.run(buf119, buf120, buf121, buf122, buf123, buf124, 960, 128, stream=stream0)
            buf125 = buf113; del buf113  # reuse
            buf128 = empty_strided_cuda((1, 120, 1, 1), (120, 1, 120, 120), torch.float32)
            # Topologically Sorted Source Nodes: [x_21], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__45.run(buf122, buf123, buf124, primals_51, primals_52, buf125, buf128, primals_51, primals_52, 120, 8, stream=stream0)
            del primals_51
            del primals_52
            buf129 = empty_strided_cuda((20, 60, 1, 1), (60, 1, 60, 60), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_13], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_46.run(primals_55, buf129, 1200, stream=stream0)
            del primals_55
            buf130 = empty_strided_cuda((128, 120, 56, 56), (376320, 3136, 56, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_21, x_22], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_relu_47.run(buf118, buf125, buf128, primals_53, primals_54, buf130, 15360, 3136, stream=stream0)
            buf131 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_48.run(buf130, buf131, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf132 = extern_kernels.convolution(buf131, buf129, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf132, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution.default')
            del buf131
            buf133 = empty_strided_cuda((20, 60, 1, 1), (60, 1, 60, 60), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_14], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_46.run(primals_56, buf133, 1200, stream=stream0)
            del primals_56
            buf134 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13, conv2d_14], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_convolution_relu_split_with_sizes_49.run(buf130, buf134, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_21, x_22, conv2d_13, conv2d_14], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.split_with_sizes, aten.convolution]
            buf135 = extern_kernels.convolution(buf134, buf133, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf135, (128, 20, 56, 56), (62720, 1, 1120, 20), 'torch.ops.aten.convolution.default')
            del buf134
            buf136 = empty_strided_cuda((128, 40, 56, 56), (125440, 1, 2240, 40), torch.float16)
            # Topologically Sorted Source Nodes: [x_23], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_31.run(buf132, buf135, buf136, 16056320, stream=stream0)
            del buf132
            del buf135
            buf137 = buf88; del buf88  # reuse
            buf138 = buf87; del buf87  # reuse
            buf139 = buf86; del buf86  # reuse
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_32.run(buf136, buf137, buf138, buf139, 40960, 392, stream=stream0)
            buf140 = buf91; del buf91  # reuse
            buf141 = buf90; del buf90  # reuse
            buf142 = buf89; del buf89  # reuse
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_33.run(buf137, buf138, buf139, buf140, buf141, buf142, 320, 128, stream=stream0)
            del buf137
            del buf138
            del buf139
            buf143 = buf93; del buf93  # reuse
            buf144 = empty_strided_cuda((1, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            buf146 = empty_strided_cuda((1, 40, 1, 1), (40, 1, 40, 40), torch.float32)
            # Topologically Sorted Source Nodes: [x_24], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__34.run(buf140, buf141, buf142, primals_58, primals_59, buf143, buf144, buf146, primals_58, primals_59, 40, 8, stream=stream0)
            del primals_58
            del primals_59
            buf147 = buf96; del buf96  # reuse
            # Topologically Sorted Source Nodes: [x_24, x_25], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_50.run(buf147, buf136, buf143, buf144, primals_60, primals_61, 16056320, stream=stream0)
            del buf144
            del primals_61
            buf148 = empty_strided_cuda((240, 40, 1, 1), (40, 1, 40, 40), torch.float16)
            # Topologically Sorted Source Nodes: [x_26], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_51.run(primals_62, buf148, 9600, stream=stream0)
            del primals_62
            # Topologically Sorted Source Nodes: [x_26], Original ATen: [aten.convolution]
            buf149 = extern_kernels.convolution(buf147, buf148, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf149, (128, 240, 56, 56), (752640, 1, 13440, 240), 'torch.ops.aten.convolution.default')
            buf150 = reinterpret_tensor(buf121, (1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), 0); del buf121  # reuse
            buf151 = reinterpret_tensor(buf120, (1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), 0); del buf120  # reuse
            buf152 = reinterpret_tensor(buf119, (1, 240, 1, 1, 512), (122880, 1, 122880, 122880, 240), 0); del buf119  # reuse
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_52.run(buf149, buf150, buf151, buf152, 122880, 784, stream=stream0)
            buf153 = reinterpret_tensor(buf124, (1, 240, 1, 1, 4), (960, 1, 960, 960, 240), 0); del buf124  # reuse
            buf154 = reinterpret_tensor(buf123, (1, 240, 1, 1, 4), (960, 1, 960, 960, 240), 0); del buf123  # reuse
            buf155 = reinterpret_tensor(buf122, (1, 240, 1, 1, 4), (960, 1, 960, 960, 240), 0); del buf122  # reuse
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_53.run(buf150, buf151, buf152, buf153, buf154, buf155, 960, 128, stream=stream0)
            buf156 = empty_strided_cuda((1, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            buf159 = empty_strided_cuda((1, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [x_27], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__54.run(buf153, buf154, buf155, primals_64, primals_65, buf156, buf159, primals_64, primals_65, 240, 4, stream=stream0)
            del primals_64
            del primals_65
            buf162 = empty_strided_cuda((128, 240, 56, 56), (752640, 3136, 56, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_27, x_28], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_55.run(buf149, buf156, buf159, primals_66, primals_67, buf162, 401408, 240, stream=stream0)
            buf161 = empty_strided_cuda((60, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_16], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_56.run(primals_68, buf161, 540, stream=stream0)
            del primals_68
            buf163 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_57.run(buf162, buf163, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf164 = extern_kernels.convolution(buf163, buf161, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=60, bias=None)
            assert_size_stride(buf164, (128, 60, 28, 28), (47040, 1, 1680, 60), 'torch.ops.aten.convolution.default')
            del buf163
            buf165 = empty_strided_cuda((60, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_17], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_58.run(primals_69, buf165, 1500, stream=stream0)
            del primals_69
            buf166 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_17], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_59.run(buf162, buf166, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_17], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf167 = extern_kernels.convolution(buf166, buf165, stride=(2, 2), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=60, bias=None)
            assert_size_stride(buf167, (128, 60, 28, 28), (47040, 1, 1680, 60), 'torch.ops.aten.convolution.default')
            del buf166
            buf168 = empty_strided_cuda((60, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_18], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_60.run(primals_70, buf168, 2940, stream=stream0)
            del primals_70
            buf169 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_18], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_61.run(buf162, buf169, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_18], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf170 = extern_kernels.convolution(buf169, buf168, stride=(2, 2), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=60, bias=None)
            assert_size_stride(buf170, (128, 60, 28, 28), (47040, 1, 1680, 60), 'torch.ops.aten.convolution.default')
            del buf169
            buf171 = empty_strided_cuda((60, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_19], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_62.run(primals_71, buf171, 4860, stream=stream0)
            del primals_71
            buf172 = empty_strided_cuda((128, 60, 56, 56), (188160, 1, 3360, 60), torch.float16)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_19], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_63.run(buf162, buf172, 7680, 3136, stream=stream0)
            # Topologically Sorted Source Nodes: [x_28, conv2d_16, conv2d_19], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf173 = extern_kernels.convolution(buf172, buf171, stride=(2, 2), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=60, bias=None)
            assert_size_stride(buf173, (128, 60, 28, 28), (47040, 1, 1680, 60), 'torch.ops.aten.convolution.default')
            buf174 = reinterpret_tensor(buf172, (128, 240, 28, 28), (188160, 1, 6720, 240), 0); del buf172  # reuse
            # Topologically Sorted Source Nodes: [x_29], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_64.run(buf164, buf167, buf170, buf173, buf174, 24084480, stream=stream0)
            del buf164
            del buf167
            del buf170
            del buf173
            buf175 = buf152; del buf152  # reuse
            buf176 = buf151; del buf151  # reuse
            buf177 = buf150; del buf150  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_65.run(buf174, buf175, buf176, buf177, 122880, 196, stream=stream0)
            buf178 = buf155; del buf155  # reuse
            buf179 = buf154; del buf154  # reuse
            buf180 = buf153; del buf153  # reuse
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_53.run(buf175, buf176, buf177, buf178, buf179, buf180, 960, 128, stream=stream0)
            buf181 = empty_strided_cuda((1, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            buf184 = empty_strided_cuda((1, 240, 1, 1), (240, 1, 240, 240), torch.float32)
            # Topologically Sorted Source Nodes: [x_30], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__66.run(buf178, buf179, buf180, primals_73, primals_74, buf181, buf184, primals_73, primals_74, 240, 4, stream=stream0)
            del primals_73
            del primals_74
            buf185 = empty_strided_cuda((128, 240, 28, 28), (188160, 1, 6720, 240), torch.float32)
            # Topologically Sorted Source Nodes: [x_30, x_31], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_67.run(buf174, buf181, buf184, primals_75, primals_76, buf185, 24084480, stream=stream0)
            buf187 = empty_strided_cuda((128, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_31, x_se], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_68.run(buf185, buf187, 30720, 784, stream=stream0)
            buf188 = empty_strided_cuda((20, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_1], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_69.run(primals_77, buf188, 4800, stream=stream0)
            del primals_77
            # Topologically Sorted Source Nodes: [x_se_1], Original ATen: [aten._to_copy, aten.convolution]
            buf189 = extern_kernels.convolution(buf187, buf188, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf189, (128, 20, 1, 1), (20, 1, 20, 20), 'torch.ops.aten.convolution.default')
            buf190 = buf189; del buf189  # reuse
            buf191 = empty_strided_cuda((128, 20, 1, 1), (20, 1, 20, 20), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_1, x_se_2], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_70.run(buf190, primals_78, buf191, 2560, stream=stream0)
            del primals_78
            buf192 = empty_strided_cuda((240, 20, 1, 1), (20, 1, 20, 20), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_3], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_71.run(primals_79, buf192, 4800, stream=stream0)
            del primals_79
            # Topologically Sorted Source Nodes: [x_se_3], Original ATen: [aten._to_copy, aten.convolution]
            buf193 = extern_kernels.convolution(buf191, buf192, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf193, (128, 240, 1, 1), (240, 1, 240, 240), 'torch.ops.aten.convolution.default')
            buf194 = buf193; del buf193  # reuse
            # Topologically Sorted Source Nodes: [x_se_3], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_72.run(buf194, primals_80, 30720, stream=stream0)
            del primals_80
            buf195 = empty_strided_cuda((128, 240, 28, 28), (188160, 1, 6720, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_31, sigmoid, x_32], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_73.run(buf185, buf194, buf195, 24084480, stream=stream0)
            del buf185
            buf196 = empty_strided_cuda((56, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_33], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_74.run(primals_81, buf196, 13440, stream=stream0)
            del primals_81
            # Topologically Sorted Source Nodes: [x_33], Original ATen: [aten.convolution]
            buf197 = extern_kernels.convolution(buf195, buf196, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf197, (128, 56, 28, 28), (43904, 1, 1568, 56), 'torch.ops.aten.convolution.default')
            buf198 = empty_strided_cuda((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), torch.float32)
            buf199 = empty_strided_cuda((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), torch.float32)
            buf200 = empty_strided_cuda((1, 56, 1, 1, 784), (43904, 1, 43904, 43904, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_75.run(buf197, buf198, buf199, buf200, 43904, 128, stream=stream0)
            buf201 = empty_strided_cuda((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), torch.float32)
            buf202 = empty_strided_cuda((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), torch.float32)
            buf203 = empty_strided_cuda((1, 56, 1, 1, 7), (392, 1, 392, 392, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_76.run(buf198, buf199, buf200, buf201, buf202, buf203, 392, 112, stream=stream0)
            buf204 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            buf205 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            buf207 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__77.run(buf201, buf202, buf203, primals_83, primals_84, buf204, buf205, buf207, primals_83, primals_84, 56, 7, stream=stream0)
            del primals_83
            del primals_84
            buf208 = empty_strided_cuda((128, 56, 28, 28), (43904, 1, 1568, 56), torch.float16)
            # Topologically Sorted Source Nodes: [x_34], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_78.run(buf197, buf204, buf205, primals_85, primals_86, buf208, 5619712, stream=stream0)
            del primals_86
            buf209 = empty_strided_cuda((128, 28, 28, 28), (21952, 784, 28, 1), torch.float16)
            buf212 = empty_strided_cuda((128, 28, 28, 28), (21952, 1, 784, 28), torch.float16)
            # Topologically Sorted Source Nodes: [split_6, conv2d_23], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_79.run(buf208, buf209, buf212, 3584, 784, stream=stream0)
            buf210 = empty_strided_cuda((128, 28, 28, 28), (21952, 784, 28, 1), torch.float16)
            buf215 = empty_strided_cuda((128, 28, 28, 28), (21952, 1, 784, 28), torch.float16)
            # Topologically Sorted Source Nodes: [split_6, conv2d_24], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_80.run(buf208, buf210, buf215, 3584, 784, stream=stream0)
            buf211 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_23], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_87, buf211, 4704, stream=stream0)
            del primals_87
            # Topologically Sorted Source Nodes: [conv2d_23], Original ATen: [aten.convolution]
            buf213 = extern_kernels.convolution(buf212, buf211, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf213, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            buf214 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_24], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_88, buf214, 4704, stream=stream0)
            del primals_88
            # Topologically Sorted Source Nodes: [conv2d_24], Original ATen: [aten.convolution]
            buf216 = extern_kernels.convolution(buf215, buf214, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf216, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            buf217 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_35], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf213, buf216, buf217, 33718272, stream=stream0)
            del buf213
            del buf216
            buf218 = empty_strided_cuda((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), torch.float32)
            buf219 = empty_strided_cuda((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), torch.float32)
            buf220 = empty_strided_cuda((1, 336, 1, 1, 392), (131712, 1, 131712, 131712, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf217, buf218, buf219, buf220, 131712, 256, stream=stream0)
            buf221 = reinterpret_tensor(buf52, (1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), 0); del buf52  # reuse
            buf222 = reinterpret_tensor(buf51, (1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), 0); del buf51  # reuse
            buf223 = reinterpret_tensor(buf50, (1, 336, 1, 1, 4), (1344, 1, 1344, 1344, 336), 0); del buf50  # reuse
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf218, buf219, buf220, buf221, buf222, buf223, 1344, 98, stream=stream0)
            buf224 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf227 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_36], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf221, buf222, buf223, primals_90, primals_91, buf224, buf227, primals_90, primals_91, 336, 4, stream=stream0)
            del primals_90
            del primals_91
            buf230 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_36, x_37], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_86.run(buf217, buf224, buf227, primals_92, primals_93, buf230, 100352, 336, stream=stream0)
            buf229 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_25], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_87.run(primals_94, buf229, 1512, stream=stream0)
            del primals_94
            buf231 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_37, conv2d_25], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf230, buf231, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_37, conv2d_25], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf232 = extern_kernels.convolution(buf231, buf229, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf232, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf231
            buf233 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_26], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_89.run(primals_95, buf233, 4200, stream=stream0)
            del primals_95
            buf234 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_37, conv2d_25, conv2d_26], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf230, buf234, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_37, conv2d_25, conv2d_26], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf235 = extern_kernels.convolution(buf234, buf233, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf235, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf234
            buf236 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_38], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf232, buf235, buf236, 33718272, stream=stream0)
            del buf232
            del buf235
            buf237 = buf220; del buf220  # reuse
            buf238 = buf219; del buf219  # reuse
            buf239 = buf218; del buf218  # reuse
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf236, buf237, buf238, buf239, 131712, 256, stream=stream0)
            buf240 = buf223; del buf223  # reuse
            buf241 = buf222; del buf222  # reuse
            buf242 = buf221; del buf221  # reuse
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf237, buf238, buf239, buf240, buf241, buf242, 1344, 98, stream=stream0)
            buf243 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf246 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_39], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf240, buf241, buf242, primals_97, primals_98, buf243, buf246, primals_97, primals_98, 336, 4, stream=stream0)
            del primals_97
            del primals_98
            buf247 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_39, x_40], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_91.run(buf236, buf243, buf246, primals_99, primals_100, buf247, 33718272, stream=stream0)
            buf249 = empty_strided_cuda((128, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_40, x_se_4], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_92.run(buf247, buf249, 43008, 784, stream=stream0)
            buf250 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_5], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_93.run(primals_101, buf250, 9408, stream=stream0)
            del primals_101
            # Topologically Sorted Source Nodes: [x_se_5], Original ATen: [aten._to_copy, aten.convolution]
            buf251 = extern_kernels.convolution(buf249, buf250, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf251, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution.default')
            buf252 = buf251; del buf251  # reuse
            buf253 = empty_strided_cuda((128, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_5, x_se_6], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_94.run(buf252, primals_102, buf253, 3584, stream=stream0)
            del primals_102
            buf254 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_7], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_95.run(primals_103, buf254, 9408, stream=stream0)
            del primals_103
            # Topologically Sorted Source Nodes: [x_se_7], Original ATen: [aten._to_copy, aten.convolution]
            buf255 = extern_kernels.convolution(buf253, buf254, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf255, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution.default')
            buf256 = buf255; del buf255  # reuse
            # Topologically Sorted Source Nodes: [x_se_7], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_96.run(buf256, primals_104, 43008, stream=stream0)
            del primals_104
            buf257 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_97.run(buf247, buf256, buf257, 100352, 336, stream=stream0)
            del buf247
            buf258 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_29], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_105, buf258, 4704, stream=stream0)
            del primals_105
            buf259 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41, split_8, conv2d_29], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf257, buf259, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41, split_8, conv2d_29], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf260 = extern_kernels.convolution(buf259, buf258, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf260, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf259
            buf261 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_30], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_106, buf261, 4704, stream=stream0)
            del primals_106
            buf262 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41, split_8, conv2d_30], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf257, buf262, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_40, sigmoid_1, x_41, split_8, conv2d_30], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf263 = extern_kernels.convolution(buf262, buf261, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf263, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf262
            buf264 = empty_strided_cuda((128, 56, 28, 28), (43904, 1, 1568, 56), torch.float16)
            # Topologically Sorted Source Nodes: [x_42], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_99.run(buf260, buf263, buf264, 5619712, stream=stream0)
            buf265 = buf200; del buf200  # reuse
            buf266 = buf199; del buf199  # reuse
            buf267 = buf198; del buf198  # reuse
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_75.run(buf264, buf265, buf266, buf267, 43904, 128, stream=stream0)
            buf268 = buf203; del buf203  # reuse
            buf269 = buf202; del buf202  # reuse
            buf270 = buf201; del buf201  # reuse
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_76.run(buf265, buf266, buf267, buf268, buf269, buf270, 392, 112, stream=stream0)
            buf271 = buf205; del buf205  # reuse
            buf272 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            buf274 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_43], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__77.run(buf268, buf269, buf270, primals_108, primals_109, buf271, buf272, buf274, primals_108, primals_109, 56, 7, stream=stream0)
            del primals_108
            del primals_109
            buf275 = buf208; del buf208  # reuse
            # Topologically Sorted Source Nodes: [x_43, x_44], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_100.run(buf275, buf264, buf271, buf272, primals_110, primals_111, 5619712, stream=stream0)
            del primals_111
            buf276 = reinterpret_tensor(buf263, (128, 28, 28, 28), (21952, 784, 28, 1), 0); del buf263  # reuse
            buf279 = buf260; del buf260  # reuse
            # Topologically Sorted Source Nodes: [split_9, conv2d_31], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_79.run(buf275, buf276, buf279, 3584, 784, stream=stream0)
            buf277 = reinterpret_tensor(buf215, (128, 28, 28, 28), (21952, 784, 28, 1), 0); del buf215  # reuse
            buf282 = buf212; del buf212  # reuse
            # Topologically Sorted Source Nodes: [split_9, conv2d_32], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_80.run(buf275, buf277, buf282, 3584, 784, stream=stream0)
            buf278 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_31], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_112, buf278, 4704, stream=stream0)
            del primals_112
            # Topologically Sorted Source Nodes: [conv2d_31], Original ATen: [aten.convolution]
            buf280 = extern_kernels.convolution(buf279, buf278, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf280, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            buf281 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_32], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_113, buf281, 4704, stream=stream0)
            del primals_113
            # Topologically Sorted Source Nodes: [conv2d_32], Original ATen: [aten.convolution]
            buf283 = extern_kernels.convolution(buf282, buf281, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf283, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            buf284 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_45], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf280, buf283, buf284, 33718272, stream=stream0)
            del buf280
            del buf283
            buf285 = buf239; del buf239  # reuse
            buf286 = buf238; del buf238  # reuse
            buf287 = buf237; del buf237  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf284, buf285, buf286, buf287, 131712, 256, stream=stream0)
            buf288 = buf242; del buf242  # reuse
            buf289 = buf241; del buf241  # reuse
            buf290 = buf240; del buf240  # reuse
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf285, buf286, buf287, buf288, buf289, buf290, 1344, 98, stream=stream0)
            buf291 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf294 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_46], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf288, buf289, buf290, primals_115, primals_116, buf291, buf294, primals_115, primals_116, 336, 4, stream=stream0)
            del primals_115
            del primals_116
            buf297 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_46, x_47], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_86.run(buf284, buf291, buf294, primals_117, primals_118, buf297, 100352, 336, stream=stream0)
            buf296 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_33], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_87.run(primals_119, buf296, 1512, stream=stream0)
            del primals_119
            buf298 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_47, conv2d_33], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf297, buf298, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_47, conv2d_33], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf299 = extern_kernels.convolution(buf298, buf296, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf299, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf298
            buf300 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_34], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_89.run(primals_120, buf300, 4200, stream=stream0)
            del primals_120
            buf301 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_47, conv2d_33, conv2d_34], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf297, buf301, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_47, conv2d_33, conv2d_34], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf302 = extern_kernels.convolution(buf301, buf300, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf302, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf301
            buf303 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_48], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf299, buf302, buf303, 33718272, stream=stream0)
            del buf299
            del buf302
            buf304 = buf287; del buf287  # reuse
            buf305 = buf286; del buf286  # reuse
            buf306 = buf285; del buf285  # reuse
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf303, buf304, buf305, buf306, 131712, 256, stream=stream0)
            buf307 = buf290; del buf290  # reuse
            buf308 = buf289; del buf289  # reuse
            buf309 = buf288; del buf288  # reuse
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf304, buf305, buf306, buf307, buf308, buf309, 1344, 98, stream=stream0)
            buf310 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf313 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_49], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf307, buf308, buf309, primals_122, primals_123, buf310, buf313, primals_122, primals_123, 336, 4, stream=stream0)
            del primals_122
            del primals_123
            buf314 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_49, x_50], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_91.run(buf303, buf310, buf313, primals_124, primals_125, buf314, 33718272, stream=stream0)
            buf316 = empty_strided_cuda((128, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_50, x_se_8], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_92.run(buf314, buf316, 43008, 784, stream=stream0)
            buf317 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_9], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_93.run(primals_126, buf317, 9408, stream=stream0)
            del primals_126
            # Topologically Sorted Source Nodes: [x_se_9], Original ATen: [aten._to_copy, aten.convolution]
            buf318 = extern_kernels.convolution(buf316, buf317, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf318, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution.default')
            buf319 = buf318; del buf318  # reuse
            buf320 = empty_strided_cuda((128, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_9, x_se_10], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_94.run(buf319, primals_127, buf320, 3584, stream=stream0)
            del primals_127
            buf321 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_11], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_95.run(primals_128, buf321, 9408, stream=stream0)
            del primals_128
            # Topologically Sorted Source Nodes: [x_se_11], Original ATen: [aten._to_copy, aten.convolution]
            buf322 = extern_kernels.convolution(buf320, buf321, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf322, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution.default')
            buf323 = buf322; del buf322  # reuse
            # Topologically Sorted Source Nodes: [x_se_11], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_96.run(buf323, primals_129, 43008, stream=stream0)
            del primals_129
            buf324 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2, x_51], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_97.run(buf314, buf323, buf324, 100352, 336, stream=stream0)
            del buf314
            buf325 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_37], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_130, buf325, 4704, stream=stream0)
            del primals_130
            buf326 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2, x_51, split_11, conv2d_37], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf324, buf326, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2, x_51, split_11, conv2d_37], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf327 = extern_kernels.convolution(buf326, buf325, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf327, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf326
            buf328 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_38], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_131, buf328, 4704, stream=stream0)
            del primals_131
            buf329 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2, x_51, split_11, conv2d_38], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf324, buf329, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_50, sigmoid_2, x_51, split_11, conv2d_38], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf330 = extern_kernels.convolution(buf329, buf328, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf330, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf329
            buf331 = empty_strided_cuda((128, 56, 28, 28), (43904, 1, 1568, 56), torch.float16)
            # Topologically Sorted Source Nodes: [x_52], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_99.run(buf327, buf330, buf331, 5619712, stream=stream0)
            buf332 = buf267; del buf267  # reuse
            buf333 = buf266; del buf266  # reuse
            buf334 = buf265; del buf265  # reuse
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_75.run(buf331, buf332, buf333, buf334, 43904, 128, stream=stream0)
            buf335 = buf270; del buf270  # reuse
            buf336 = buf269; del buf269  # reuse
            buf337 = buf268; del buf268  # reuse
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_76.run(buf332, buf333, buf334, buf335, buf336, buf337, 392, 112, stream=stream0)
            buf338 = buf272; del buf272  # reuse
            buf339 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            buf341 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_53], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__77.run(buf335, buf336, buf337, primals_133, primals_134, buf338, buf339, buf341, primals_133, primals_134, 56, 7, stream=stream0)
            del primals_133
            del primals_134
            buf342 = buf275; del buf275  # reuse
            # Topologically Sorted Source Nodes: [x_53, x_54], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_100.run(buf342, buf331, buf338, buf339, primals_135, primals_136, 5619712, stream=stream0)
            del primals_136
            buf343 = reinterpret_tensor(buf330, (128, 28, 28, 28), (21952, 784, 28, 1), 0); del buf330  # reuse
            buf346 = buf327; del buf327  # reuse
            # Topologically Sorted Source Nodes: [split_12, conv2d_39], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_79.run(buf342, buf343, buf346, 3584, 784, stream=stream0)
            buf344 = reinterpret_tensor(buf282, (128, 28, 28, 28), (21952, 784, 28, 1), 0); del buf282  # reuse
            buf349 = buf279; del buf279  # reuse
            # Topologically Sorted Source Nodes: [split_12, conv2d_40], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_80.run(buf342, buf344, buf349, 3584, 784, stream=stream0)
            buf345 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_39], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_137, buf345, 4704, stream=stream0)
            del primals_137
            # Topologically Sorted Source Nodes: [conv2d_39], Original ATen: [aten.convolution]
            buf347 = extern_kernels.convolution(buf346, buf345, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf347, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf346
            buf348 = empty_strided_cuda((168, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_40], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_81.run(primals_138, buf348, 4704, stream=stream0)
            del primals_138
            # Topologically Sorted Source Nodes: [conv2d_40], Original ATen: [aten.convolution]
            buf350 = extern_kernels.convolution(buf349, buf348, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf350, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf349
            buf351 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_55], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf347, buf350, buf351, 33718272, stream=stream0)
            del buf347
            del buf350
            buf352 = buf306; del buf306  # reuse
            buf353 = buf305; del buf305  # reuse
            buf354 = buf304; del buf304  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf351, buf352, buf353, buf354, 131712, 256, stream=stream0)
            buf355 = buf309; del buf309  # reuse
            buf356 = buf308; del buf308  # reuse
            buf357 = buf307; del buf307  # reuse
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf352, buf353, buf354, buf355, buf356, buf357, 1344, 98, stream=stream0)
            buf358 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf361 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_56], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf355, buf356, buf357, primals_140, primals_141, buf358, buf361, primals_140, primals_141, 336, 4, stream=stream0)
            del primals_140
            del primals_141
            buf364 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_56, x_57], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_86.run(buf351, buf358, buf361, primals_142, primals_143, buf364, 100352, 336, stream=stream0)
            buf363 = empty_strided_cuda((168, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_41], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_87.run(primals_144, buf363, 1512, stream=stream0)
            del primals_144
            buf365 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_57, conv2d_41], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf364, buf365, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_57, conv2d_41], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf366 = extern_kernels.convolution(buf365, buf363, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf366, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf365
            buf367 = empty_strided_cuda((168, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_42], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_89.run(primals_145, buf367, 4200, stream=stream0)
            del primals_145
            buf368 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_57, conv2d_41, conv2d_42], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf364, buf368, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_57, conv2d_41, conv2d_42], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf369 = extern_kernels.convolution(buf368, buf367, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=168, bias=None)
            assert_size_stride(buf369, (128, 168, 28, 28), (131712, 1, 4704, 168), 'torch.ops.aten.convolution.default')
            del buf368
            buf370 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_58], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_82.run(buf366, buf369, buf370, 33718272, stream=stream0)
            del buf366
            del buf369
            buf371 = buf354; del buf354  # reuse
            buf372 = buf353; del buf353  # reuse
            buf373 = buf352; del buf352  # reuse
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf370, buf371, buf372, buf373, 131712, 256, stream=stream0)
            buf374 = buf357; del buf357  # reuse
            buf375 = buf356; del buf356  # reuse
            buf376 = buf355; del buf355  # reuse
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf371, buf372, buf373, buf374, buf375, buf376, 1344, 98, stream=stream0)
            buf377 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf380 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_59], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf374, buf375, buf376, primals_147, primals_148, buf377, buf380, primals_147, primals_148, 336, 4, stream=stream0)
            del primals_147
            del primals_148
            buf381 = empty_strided_cuda((128, 336, 28, 28), (263424, 1, 9408, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_59, x_60], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_91.run(buf370, buf377, buf380, primals_149, primals_150, buf381, 33718272, stream=stream0)
            buf383 = empty_strided_cuda((128, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_60, x_se_12], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_92.run(buf381, buf383, 43008, 784, stream=stream0)
            buf384 = empty_strided_cuda((28, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_13], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_93.run(primals_151, buf384, 9408, stream=stream0)
            del primals_151
            # Topologically Sorted Source Nodes: [x_se_13], Original ATen: [aten._to_copy, aten.convolution]
            buf385 = extern_kernels.convolution(buf383, buf384, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf385, (128, 28, 1, 1), (28, 1, 28, 28), 'torch.ops.aten.convolution.default')
            buf386 = buf385; del buf385  # reuse
            buf387 = empty_strided_cuda((128, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_13, x_se_14], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_94.run(buf386, primals_152, buf387, 3584, stream=stream0)
            del primals_152
            buf388 = empty_strided_cuda((336, 28, 1, 1), (28, 1, 28, 28), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_15], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_95.run(primals_153, buf388, 9408, stream=stream0)
            del primals_153
            # Topologically Sorted Source Nodes: [x_se_15], Original ATen: [aten._to_copy, aten.convolution]
            buf389 = extern_kernels.convolution(buf387, buf388, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf389, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution.default')
            buf390 = buf389; del buf389  # reuse
            # Topologically Sorted Source Nodes: [x_se_15], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_96.run(buf390, primals_154, 43008, stream=stream0)
            del primals_154
            buf391 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3, x_61], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_97.run(buf381, buf390, buf391, 100352, 336, stream=stream0)
            del buf381
            buf392 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_45], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_155, buf392, 4704, stream=stream0)
            del primals_155
            buf393 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3, x_61, split_14, conv2d_45], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_88.run(buf391, buf393, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3, x_61, split_14, conv2d_45], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf394 = extern_kernels.convolution(buf393, buf392, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf394, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf393
            buf395 = empty_strided_cuda((28, 168, 1, 1), (168, 1, 168, 168), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_46], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_98.run(primals_156, buf395, 4704, stream=stream0)
            del primals_156
            buf396 = empty_strided_cuda((128, 168, 28, 28), (131712, 1, 4704, 168), torch.float16)
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3, x_61, split_14, conv2d_46], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_90.run(buf391, buf396, 21504, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_60, sigmoid_3, x_61, split_14, conv2d_46], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf397 = extern_kernels.convolution(buf396, buf395, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf397, (128, 28, 28, 28), (21952, 1, 784, 28), 'torch.ops.aten.convolution.default')
            del buf396
            buf398 = empty_strided_cuda((128, 56, 28, 28), (43904, 1, 1568, 56), torch.float16)
            # Topologically Sorted Source Nodes: [x_62], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_99.run(buf394, buf397, buf398, 5619712, stream=stream0)
            del buf394
            del buf397
            buf399 = buf334; del buf334  # reuse
            buf400 = buf333; del buf333  # reuse
            buf401 = buf332; del buf332  # reuse
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_75.run(buf398, buf399, buf400, buf401, 43904, 128, stream=stream0)
            buf402 = buf337; del buf337  # reuse
            buf403 = buf336; del buf336  # reuse
            buf404 = buf335; del buf335  # reuse
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_76.run(buf399, buf400, buf401, buf402, buf403, buf404, 392, 112, stream=stream0)
            del buf399
            del buf400
            del buf401
            buf405 = buf339; del buf339  # reuse
            buf406 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            buf408 = empty_strided_cuda((1, 56, 1, 1), (56, 1, 56, 56), torch.float32)
            # Topologically Sorted Source Nodes: [x_63], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__77.run(buf402, buf403, buf404, primals_158, primals_159, buf405, buf406, buf408, primals_158, primals_159, 56, 7, stream=stream0)
            del buf402
            del buf403
            del buf404
            del primals_158
            del primals_159
            buf409 = buf342; del buf342  # reuse
            # Topologically Sorted Source Nodes: [x_63, x_64], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_100.run(buf409, buf398, buf405, buf406, primals_160, primals_161, 5619712, stream=stream0)
            del buf406
            del primals_161
            buf410 = empty_strided_cuda((336, 56, 1, 1), (56, 1, 56, 56), torch.float16)
            # Topologically Sorted Source Nodes: [x_65], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_101.run(primals_162, buf410, 18816, stream=stream0)
            del primals_162
            # Topologically Sorted Source Nodes: [x_65], Original ATen: [aten.convolution]
            buf411 = extern_kernels.convolution(buf409, buf410, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf411, (128, 336, 28, 28), (263424, 1, 9408, 336), 'torch.ops.aten.convolution.default')
            buf412 = buf373; del buf373  # reuse
            buf413 = buf372; del buf372  # reuse
            buf414 = buf371; del buf371  # reuse
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_83.run(buf411, buf412, buf413, buf414, 131712, 256, stream=stream0)
            buf415 = buf376; del buf376  # reuse
            buf416 = buf375; del buf375  # reuse
            buf417 = buf374; del buf374  # reuse
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_84.run(buf412, buf413, buf414, buf415, buf416, buf417, 1344, 98, stream=stream0)
            del buf412
            del buf413
            del buf414
            buf418 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf421 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_66], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__85.run(buf415, buf416, buf417, primals_164, primals_165, buf418, buf421, primals_164, primals_165, 336, 4, stream=stream0)
            del buf415
            del buf416
            del buf417
            del primals_164
            del primals_165
            buf424 = empty_strided_cuda((128, 336, 28, 28), (263424, 784, 28, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_66, x_67], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_86.run(buf411, buf418, buf421, primals_166, primals_167, buf424, 100352, 336, stream=stream0)
            buf423 = empty_strided_cuda((112, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_48], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_102.run(primals_168, buf423, 1008, stream=stream0)
            del primals_168
            buf425 = empty_strided_cuda((128, 112, 28, 28), (87808, 1, 3136, 112), torch.float16)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_103.run(buf424, buf425, 14336, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf426 = extern_kernels.convolution(buf425, buf423, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=112, bias=None)
            assert_size_stride(buf426, (128, 112, 14, 14), (21952, 1, 1568, 112), 'torch.ops.aten.convolution.default')
            del buf425
            buf427 = empty_strided_cuda((112, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_49], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_104.run(primals_169, buf427, 2800, stream=stream0)
            del primals_169
            buf428 = empty_strided_cuda((128, 112, 28, 28), (87808, 1, 3136, 112), torch.float16)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_49], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_105.run(buf424, buf428, 14336, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_49], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf429 = extern_kernels.convolution(buf428, buf427, stride=(2, 2), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=112, bias=None)
            assert_size_stride(buf429, (128, 112, 14, 14), (21952, 1, 1568, 112), 'torch.ops.aten.convolution.default')
            del buf428
            buf430 = empty_strided_cuda((112, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_50], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_106.run(primals_170, buf430, 5488, stream=stream0)
            del primals_170
            buf431 = empty_strided_cuda((128, 112, 28, 28), (87808, 1, 3136, 112), torch.float16)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_50], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_107.run(buf424, buf431, 14336, 784, stream=stream0)
            # Topologically Sorted Source Nodes: [x_67, conv2d_48, conv2d_50], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf432 = extern_kernels.convolution(buf431, buf430, stride=(2, 2), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=112, bias=None)
            assert_size_stride(buf432, (128, 112, 14, 14), (21952, 1, 1568, 112), 'torch.ops.aten.convolution.default')
            del buf431
            buf433 = empty_strided_cuda((128, 336, 14, 14), (65856, 1, 4704, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_68], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_108.run(buf426, buf429, buf432, buf433, 8429568, stream=stream0)
            del buf426
            del buf429
            del buf432
            buf434 = empty_strided_cuda((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), torch.float32)
            buf435 = empty_strided_cuda((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), torch.float32)
            buf436 = empty_strided_cuda((1, 336, 1, 1, 196), (65856, 1, 65856, 65856, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_109.run(buf433, buf434, buf435, buf436, 65856, 128, stream=stream0)
            buf437 = empty_strided_cuda((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), torch.float32)
            buf438 = empty_strided_cuda((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), torch.float32)
            buf439 = empty_strided_cuda((1, 336, 1, 1, 2), (672, 1, 672, 672, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_110.run(buf434, buf435, buf436, buf437, buf438, buf439, 672, 98, stream=stream0)
            del buf434
            del buf435
            del buf436
            buf440 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            buf443 = empty_strided_cuda((1, 336, 1, 1), (336, 1, 336, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_69], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__111.run(buf437, buf438, buf439, primals_172, primals_173, buf440, buf443, primals_172, primals_173, 336, 2, stream=stream0)
            del buf437
            del buf438
            del buf439
            del primals_172
            del primals_173
            buf444 = empty_strided_cuda((128, 336, 14, 14), (65856, 1, 4704, 336), torch.float32)
            # Topologically Sorted Source Nodes: [x_69, x_70], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_112.run(buf433, buf440, buf443, primals_174, primals_175, buf444, 8429568, stream=stream0)
            buf446 = empty_strided_cuda((128, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_70, x_se_16], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_113.run(buf444, buf446, 43008, 196, stream=stream0)
            buf447 = empty_strided_cuda((14, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_17], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_114.run(primals_176, buf447, 4704, stream=stream0)
            del primals_176
            # Topologically Sorted Source Nodes: [x_se_17], Original ATen: [aten._to_copy, aten.convolution]
            buf448 = extern_kernels.convolution(buf446, buf447, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf448, (128, 14, 1, 1), (14, 1, 14, 14), 'torch.ops.aten.convolution.default')
            buf449 = buf448; del buf448  # reuse
            buf450 = empty_strided_cuda((128, 14, 1, 1), (14, 1, 14, 14), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_17, x_se_18], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_115.run(buf449, primals_177, buf450, 1792, stream=stream0)
            del primals_177
            buf451 = empty_strided_cuda((336, 14, 1, 1), (14, 1, 14, 14), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_19], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_116.run(primals_178, buf451, 4704, stream=stream0)
            del primals_178
            # Topologically Sorted Source Nodes: [x_se_19], Original ATen: [aten._to_copy, aten.convolution]
            buf452 = extern_kernels.convolution(buf450, buf451, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf452, (128, 336, 1, 1), (336, 1, 336, 336), 'torch.ops.aten.convolution.default')
            buf453 = buf452; del buf452  # reuse
            # Topologically Sorted Source Nodes: [x_se_19], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_117.run(buf453, primals_179, 43008, stream=stream0)
            del primals_179
            buf454 = empty_strided_cuda((128, 336, 14, 14), (65856, 1, 4704, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_70, sigmoid_4, x_71], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_118.run(buf444, buf453, buf454, 8429568, stream=stream0)
            del buf444
            buf455 = empty_strided_cuda((104, 336, 1, 1), (336, 1, 336, 336), torch.float16)
            # Topologically Sorted Source Nodes: [x_72], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_119.run(primals_180, buf455, 34944, stream=stream0)
            del primals_180
            # Topologically Sorted Source Nodes: [x_72], Original ATen: [aten.convolution]
            buf456 = extern_kernels.convolution(buf454, buf455, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf456, (128, 104, 14, 14), (20384, 1, 1456, 104), 'torch.ops.aten.convolution.default')
            buf457 = empty_strided_cuda((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), torch.float32)
            buf458 = empty_strided_cuda((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), torch.float32)
            buf459 = empty_strided_cuda((1, 104, 1, 1, 196), (20384, 1, 20384, 20384, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_120.run(buf456, buf457, buf458, buf459, 20384, 128, stream=stream0)
            buf460 = empty_strided_cuda((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), torch.float32)
            buf461 = empty_strided_cuda((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), torch.float32)
            buf462 = empty_strided_cuda((1, 104, 1, 1, 2), (208, 1, 208, 208, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_121.run(buf457, buf458, buf459, buf460, buf461, buf462, 208, 98, stream=stream0)
            buf463 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            buf464 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            buf466 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__122.run(buf460, buf461, buf462, primals_182, primals_183, buf463, buf464, buf466, primals_182, primals_183, 104, 2, stream=stream0)
            del primals_182
            del primals_183
            buf467 = empty_strided_cuda((128, 104, 14, 14), (20384, 1, 1456, 104), torch.float16)
            # Topologically Sorted Source Nodes: [x_73], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_123.run(buf456, buf463, buf464, primals_184, primals_185, buf467, 2609152, stream=stream0)
            del primals_185
            buf468 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf471 = empty_strided_cuda((128, 52, 14, 14), (10192, 1, 728, 52), torch.float16)
            # Topologically Sorted Source Nodes: [split_16, conv2d_54], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_124.run(buf467, buf468, buf471, 6656, 196, stream=stream0)
            buf469 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf474 = empty_strided_cuda((128, 52, 14, 14), (10192, 1, 728, 52), torch.float16)
            # Topologically Sorted Source Nodes: [split_16, conv2d_55], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_125.run(buf467, buf469, buf474, 6656, 196, stream=stream0)
            buf470 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_54], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_186, buf470, 16224, stream=stream0)
            del primals_186
            # Topologically Sorted Source Nodes: [conv2d_54], Original ATen: [aten.convolution]
            buf472 = extern_kernels.convolution(buf471, buf470, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf472, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf471
            buf473 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_55], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_187, buf473, 16224, stream=stream0)
            del primals_187
            # Topologically Sorted Source Nodes: [conv2d_55], Original ATen: [aten.convolution]
            buf475 = extern_kernels.convolution(buf474, buf473, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf475, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf474
            buf476 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_74], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_127.run(buf472, buf475, buf476, 15654912, stream=stream0)
            del buf472
            del buf475
            buf477 = empty_strided_cuda((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), torch.float32)
            buf478 = empty_strided_cuda((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), torch.float32)
            buf479 = empty_strided_cuda((1, 624, 1, 1, 196), (122304, 1, 122304, 122304, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf476, buf477, buf478, buf479, 122304, 128, stream=stream0)
            buf480 = empty_strided_cuda((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), torch.float32)
            buf481 = empty_strided_cuda((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), torch.float32)
            buf482 = empty_strided_cuda((1, 624, 1, 1, 2), (1248, 1, 1248, 1248, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf477, buf478, buf479, buf480, buf481, buf482, 1248, 98, stream=stream0)
            buf483 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf486 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_75], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf480, buf481, buf482, primals_189, primals_190, buf483, buf486, primals_189, primals_190, 624, 2, stream=stream0)
            del primals_189
            del primals_190
            buf489 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_75, x_76], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_131.run(buf476, buf483, buf486, primals_191, primals_192, buf489, 25088, 624, stream=stream0)
            buf488 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_56], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_132.run(primals_193, buf488, 1404, stream=stream0)
            del primals_193
            buf490 = empty_strided_cuda((128, 156, 14, 14), (30576, 1, 2184, 156), torch.float16)
            # Topologically Sorted Source Nodes: [x_76, conv2d_56], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_133.run(buf489, buf490, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_76, conv2d_56], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf491 = extern_kernels.convolution(buf490, buf488, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf491, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf492 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_57], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_134.run(primals_194, buf492, 3900, stream=stream0)
            del primals_194
            buf493 = buf490; del buf490  # reuse
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_57], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_135.run(buf489, buf493, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_57], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf494 = extern_kernels.convolution(buf493, buf492, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf494, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf495 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_58], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_136.run(primals_195, buf495, 7644, stream=stream0)
            del primals_195
            buf496 = buf493; del buf493  # reuse
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_58], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_137.run(buf489, buf496, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_58], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf497 = extern_kernels.convolution(buf496, buf495, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf497, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf498 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_59], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_138.run(primals_196, buf498, 12636, stream=stream0)
            del primals_196
            buf499 = buf496; del buf496  # reuse
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_59], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_139.run(buf489, buf499, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_76, conv2d_56, conv2d_59], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf500 = extern_kernels.convolution(buf499, buf498, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf500, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            del buf499
            buf501 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_77], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_140.run(buf491, buf494, buf497, buf500, buf501, 15654912, stream=stream0)
            del buf491
            del buf494
            del buf497
            buf502 = buf479; del buf479  # reuse
            buf503 = buf478; del buf478  # reuse
            buf504 = buf477; del buf477  # reuse
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf501, buf502, buf503, buf504, 122304, 128, stream=stream0)
            buf505 = buf482; del buf482  # reuse
            buf506 = buf481; del buf481  # reuse
            buf507 = buf480; del buf480  # reuse
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf502, buf503, buf504, buf505, buf506, buf507, 1248, 98, stream=stream0)
            buf508 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf511 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_78], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf505, buf506, buf507, primals_198, primals_199, buf508, buf511, primals_198, primals_199, 624, 2, stream=stream0)
            del primals_198
            del primals_199
            buf512 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_78, x_79], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_141.run(buf501, buf508, buf511, primals_200, primals_201, buf512, 15654912, stream=stream0)
            buf514 = empty_strided_cuda((128, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_79, x_se_20], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_142.run(buf512, buf514, 79872, 196, stream=stream0)
            buf515 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_21], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_143.run(primals_202, buf515, 16224, stream=stream0)
            del primals_202
            # Topologically Sorted Source Nodes: [x_se_21], Original ATen: [aten._to_copy, aten.convolution]
            buf516 = extern_kernels.convolution(buf514, buf515, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf516, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution.default')
            buf517 = buf516; del buf516  # reuse
            buf518 = empty_strided_cuda((128, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_21, x_se_22], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_144.run(buf517, primals_203, buf518, 3328, stream=stream0)
            del primals_203
            buf519 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_23], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_145.run(primals_204, buf519, 16224, stream=stream0)
            del primals_204
            # Topologically Sorted Source Nodes: [x_se_23], Original ATen: [aten._to_copy, aten.convolution]
            buf520 = extern_kernels.convolution(buf518, buf519, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf520, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution.default')
            buf521 = buf520; del buf520  # reuse
            # Topologically Sorted Source Nodes: [x_se_23], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_146.run(buf521, primals_205, 79872, stream=stream0)
            del primals_205
            buf522 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_147.run(buf512, buf521, buf522, 25088, 624, stream=stream0)
            del buf512
            buf523 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_62], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_206, buf523, 16224, stream=stream0)
            del primals_206
            buf524 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_62], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149.run(buf522, buf524, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_62], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf525 = extern_kernels.convolution(buf524, buf523, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf525, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf524
            buf526 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_63], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_207, buf526, 16224, stream=stream0)
            del primals_207
            buf527 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_63], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150.run(buf522, buf527, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_79, sigmoid_5, x_80, split_18, conv2d_63], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf528 = extern_kernels.convolution(buf527, buf526, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf528, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf527
            buf529 = empty_strided_cuda((128, 104, 14, 14), (20384, 1, 1456, 104), torch.float16)
            # Topologically Sorted Source Nodes: [x_81], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_151.run(buf525, buf528, buf529, 2609152, stream=stream0)
            buf530 = buf459; del buf459  # reuse
            buf531 = buf458; del buf458  # reuse
            buf532 = buf457; del buf457  # reuse
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_120.run(buf529, buf530, buf531, buf532, 20384, 128, stream=stream0)
            buf533 = buf462; del buf462  # reuse
            buf534 = buf461; del buf461  # reuse
            buf535 = buf460; del buf460  # reuse
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_121.run(buf530, buf531, buf532, buf533, buf534, buf535, 208, 98, stream=stream0)
            buf536 = buf464; del buf464  # reuse
            buf537 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            buf539 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_82], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__122.run(buf533, buf534, buf535, primals_209, primals_210, buf536, buf537, buf539, primals_209, primals_210, 104, 2, stream=stream0)
            del primals_209
            del primals_210
            buf540 = buf467; del buf467  # reuse
            # Topologically Sorted Source Nodes: [x_82, x_83], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_152.run(buf540, buf529, buf536, buf537, primals_211, primals_212, 2609152, stream=stream0)
            del primals_212
            buf541 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf544 = buf528; del buf528  # reuse
            # Topologically Sorted Source Nodes: [split_19, conv2d_64], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_124.run(buf540, buf541, buf544, 6656, 196, stream=stream0)
            buf542 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf547 = buf525; del buf525  # reuse
            # Topologically Sorted Source Nodes: [split_19, conv2d_65], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_125.run(buf540, buf542, buf547, 6656, 196, stream=stream0)
            buf543 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_64], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_213, buf543, 16224, stream=stream0)
            del primals_213
            # Topologically Sorted Source Nodes: [conv2d_64], Original ATen: [aten.convolution]
            buf545 = extern_kernels.convolution(buf544, buf543, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf545, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf544
            buf546 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_65], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_214, buf546, 16224, stream=stream0)
            del primals_214
            # Topologically Sorted Source Nodes: [conv2d_65], Original ATen: [aten.convolution]
            buf548 = extern_kernels.convolution(buf547, buf546, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf548, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf547
            buf549 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_84], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_127.run(buf545, buf548, buf549, 15654912, stream=stream0)
            del buf545
            del buf548
            buf550 = buf504; del buf504  # reuse
            buf551 = buf503; del buf503  # reuse
            buf552 = buf502; del buf502  # reuse
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf549, buf550, buf551, buf552, 122304, 128, stream=stream0)
            buf553 = buf507; del buf507  # reuse
            buf554 = buf506; del buf506  # reuse
            buf555 = buf505; del buf505  # reuse
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf550, buf551, buf552, buf553, buf554, buf555, 1248, 98, stream=stream0)
            buf556 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf559 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_85], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf553, buf554, buf555, primals_216, primals_217, buf556, buf559, primals_216, primals_217, 624, 2, stream=stream0)
            del primals_216
            del primals_217
            buf562 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_85, x_86], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_131.run(buf549, buf556, buf559, primals_218, primals_219, buf562, 25088, 624, stream=stream0)
            buf561 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_66], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_132.run(primals_220, buf561, 1404, stream=stream0)
            del primals_220
            buf563 = buf500; del buf500  # reuse
            # Topologically Sorted Source Nodes: [x_86, conv2d_66], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_133.run(buf562, buf563, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_86, conv2d_66], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf564 = extern_kernels.convolution(buf563, buf561, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf564, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf565 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_67], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_134.run(primals_221, buf565, 3900, stream=stream0)
            del primals_221
            buf566 = buf563; del buf563  # reuse
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_67], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_135.run(buf562, buf566, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_67], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf567 = extern_kernels.convolution(buf566, buf565, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf567, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf568 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_68], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_136.run(primals_222, buf568, 7644, stream=stream0)
            del primals_222
            buf569 = buf566; del buf566  # reuse
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_68], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_137.run(buf562, buf569, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_68], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf570 = extern_kernels.convolution(buf569, buf568, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf570, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf571 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_69], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_138.run(primals_223, buf571, 12636, stream=stream0)
            del primals_223
            buf572 = buf569; del buf569  # reuse
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_69], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_139.run(buf562, buf572, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_86, conv2d_66, conv2d_69], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf573 = extern_kernels.convolution(buf572, buf571, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf573, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            del buf572
            buf574 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_87], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_140.run(buf564, buf567, buf570, buf573, buf574, 15654912, stream=stream0)
            del buf564
            del buf567
            del buf570
            buf575 = buf552; del buf552  # reuse
            buf576 = buf551; del buf551  # reuse
            buf577 = buf550; del buf550  # reuse
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf574, buf575, buf576, buf577, 122304, 128, stream=stream0)
            buf578 = buf555; del buf555  # reuse
            buf579 = buf554; del buf554  # reuse
            buf580 = buf553; del buf553  # reuse
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf575, buf576, buf577, buf578, buf579, buf580, 1248, 98, stream=stream0)
            buf581 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf584 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_88], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf578, buf579, buf580, primals_225, primals_226, buf581, buf584, primals_225, primals_226, 624, 2, stream=stream0)
            del primals_225
            del primals_226
            buf585 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_88, x_89], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_141.run(buf574, buf581, buf584, primals_227, primals_228, buf585, 15654912, stream=stream0)
            buf587 = empty_strided_cuda((128, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_89, x_se_24], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_142.run(buf585, buf587, 79872, 196, stream=stream0)
            buf588 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_25], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_143.run(primals_229, buf588, 16224, stream=stream0)
            del primals_229
            # Topologically Sorted Source Nodes: [x_se_25], Original ATen: [aten._to_copy, aten.convolution]
            buf589 = extern_kernels.convolution(buf587, buf588, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf589, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution.default')
            buf590 = buf589; del buf589  # reuse
            buf591 = empty_strided_cuda((128, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_25, x_se_26], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_144.run(buf590, primals_230, buf591, 3328, stream=stream0)
            del primals_230
            buf592 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_27], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_145.run(primals_231, buf592, 16224, stream=stream0)
            del primals_231
            # Topologically Sorted Source Nodes: [x_se_27], Original ATen: [aten._to_copy, aten.convolution]
            buf593 = extern_kernels.convolution(buf591, buf592, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf593, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution.default')
            buf594 = buf593; del buf593  # reuse
            # Topologically Sorted Source Nodes: [x_se_27], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_146.run(buf594, primals_232, 79872, stream=stream0)
            del primals_232
            buf595 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6, x_90], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_147.run(buf585, buf594, buf595, 25088, 624, stream=stream0)
            del buf585
            buf596 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_72], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_233, buf596, 16224, stream=stream0)
            del primals_233
            buf597 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6, x_90, split_21, conv2d_72], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149.run(buf595, buf597, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6, x_90, split_21, conv2d_72], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf598 = extern_kernels.convolution(buf597, buf596, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf598, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf597
            buf599 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_73], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_234, buf599, 16224, stream=stream0)
            del primals_234
            buf600 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6, x_90, split_21, conv2d_73], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150.run(buf595, buf600, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_89, sigmoid_6, x_90, split_21, conv2d_73], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf601 = extern_kernels.convolution(buf600, buf599, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf601, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf600
            buf602 = empty_strided_cuda((128, 104, 14, 14), (20384, 1, 1456, 104), torch.float16)
            # Topologically Sorted Source Nodes: [x_91], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_151.run(buf598, buf601, buf602, 2609152, stream=stream0)
            buf603 = buf532; del buf532  # reuse
            buf604 = buf531; del buf531  # reuse
            buf605 = buf530; del buf530  # reuse
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_120.run(buf602, buf603, buf604, buf605, 20384, 128, stream=stream0)
            buf606 = buf535; del buf535  # reuse
            buf607 = buf534; del buf534  # reuse
            buf608 = buf533; del buf533  # reuse
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_121.run(buf603, buf604, buf605, buf606, buf607, buf608, 208, 98, stream=stream0)
            buf609 = buf537; del buf537  # reuse
            buf610 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            buf612 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_92], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__122.run(buf606, buf607, buf608, primals_236, primals_237, buf609, buf610, buf612, primals_236, primals_237, 104, 2, stream=stream0)
            del primals_236
            del primals_237
            buf613 = buf540; del buf540  # reuse
            # Topologically Sorted Source Nodes: [x_92, x_93], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_152.run(buf613, buf602, buf609, buf610, primals_238, primals_239, 2609152, stream=stream0)
            del primals_239
            buf614 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf617 = buf601; del buf601  # reuse
            # Topologically Sorted Source Nodes: [split_22, conv2d_74], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_124.run(buf613, buf614, buf617, 6656, 196, stream=stream0)
            buf615 = empty_strided_cuda((128, 52, 14, 14), (10240, 196, 14, 1), torch.float16)
            buf620 = buf598; del buf598  # reuse
            # Topologically Sorted Source Nodes: [split_22, conv2d_75], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_125.run(buf613, buf615, buf620, 6656, 196, stream=stream0)
            buf616 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_74], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_240, buf616, 16224, stream=stream0)
            del primals_240
            # Topologically Sorted Source Nodes: [conv2d_74], Original ATen: [aten.convolution]
            buf618 = extern_kernels.convolution(buf617, buf616, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf618, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf617
            buf619 = empty_strided_cuda((312, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_75], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_126.run(primals_241, buf619, 16224, stream=stream0)
            del primals_241
            # Topologically Sorted Source Nodes: [conv2d_75], Original ATen: [aten.convolution]
            buf621 = extern_kernels.convolution(buf620, buf619, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf621, (128, 312, 14, 14), (61152, 1, 4368, 312), 'torch.ops.aten.convolution.default')
            del buf620
            buf622 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_94], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_127.run(buf618, buf621, buf622, 15654912, stream=stream0)
            del buf618
            del buf621
            buf623 = buf577; del buf577  # reuse
            buf624 = buf576; del buf576  # reuse
            buf625 = buf575; del buf575  # reuse
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf622, buf623, buf624, buf625, 122304, 128, stream=stream0)
            buf626 = buf580; del buf580  # reuse
            buf627 = buf579; del buf579  # reuse
            buf628 = buf578; del buf578  # reuse
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf623, buf624, buf625, buf626, buf627, buf628, 1248, 98, stream=stream0)
            buf629 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf632 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_95], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf626, buf627, buf628, primals_243, primals_244, buf629, buf632, primals_243, primals_244, 624, 2, stream=stream0)
            del primals_243
            del primals_244
            buf635 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_95, x_96], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_131.run(buf622, buf629, buf632, primals_245, primals_246, buf635, 25088, 624, stream=stream0)
            buf634 = empty_strided_cuda((156, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_76], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_132.run(primals_247, buf634, 1404, stream=stream0)
            del primals_247
            buf636 = buf573; del buf573  # reuse
            # Topologically Sorted Source Nodes: [x_96, conv2d_76], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_133.run(buf635, buf636, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_96, conv2d_76], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf637 = extern_kernels.convolution(buf636, buf634, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf637, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf638 = empty_strided_cuda((156, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_77], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_134.run(primals_248, buf638, 3900, stream=stream0)
            del primals_248
            buf639 = buf636; del buf636  # reuse
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_77], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_135.run(buf635, buf639, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_77], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf640 = extern_kernels.convolution(buf639, buf638, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf640, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf641 = empty_strided_cuda((156, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_78], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_136.run(primals_249, buf641, 7644, stream=stream0)
            del primals_249
            buf642 = buf639; del buf639  # reuse
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_78], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_137.run(buf635, buf642, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_78], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf643 = extern_kernels.convolution(buf642, buf641, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf643, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            buf644 = empty_strided_cuda((156, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_79], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_138.run(primals_250, buf644, 12636, stream=stream0)
            del primals_250
            buf645 = buf642; del buf642  # reuse
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_79], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_139.run(buf635, buf645, 19968, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_96, conv2d_76, conv2d_79], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf646 = extern_kernels.convolution(buf645, buf644, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=156, bias=None)
            assert_size_stride(buf646, (128, 156, 14, 14), (30576, 1, 2184, 156), 'torch.ops.aten.convolution.default')
            del buf645
            buf647 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_97], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_140.run(buf637, buf640, buf643, buf646, buf647, 15654912, stream=stream0)
            del buf637
            del buf640
            del buf643
            del buf646
            buf648 = buf625; del buf625  # reuse
            buf649 = buf624; del buf624  # reuse
            buf650 = buf623; del buf623  # reuse
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf647, buf648, buf649, buf650, 122304, 128, stream=stream0)
            buf651 = buf628; del buf628  # reuse
            buf652 = buf627; del buf627  # reuse
            buf653 = buf626; del buf626  # reuse
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf648, buf649, buf650, buf651, buf652, buf653, 1248, 98, stream=stream0)
            buf654 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf657 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_98], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf651, buf652, buf653, primals_252, primals_253, buf654, buf657, primals_252, primals_253, 624, 2, stream=stream0)
            del primals_252
            del primals_253
            buf658 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_98, x_99], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_141.run(buf647, buf654, buf657, primals_254, primals_255, buf658, 15654912, stream=stream0)
            buf660 = empty_strided_cuda((128, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_99, x_se_28], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_142.run(buf658, buf660, 79872, 196, stream=stream0)
            buf661 = empty_strided_cuda((26, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_29], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_143.run(primals_256, buf661, 16224, stream=stream0)
            del primals_256
            # Topologically Sorted Source Nodes: [x_se_29], Original ATen: [aten._to_copy, aten.convolution]
            buf662 = extern_kernels.convolution(buf660, buf661, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf662, (128, 26, 1, 1), (26, 1, 26, 26), 'torch.ops.aten.convolution.default')
            buf663 = buf662; del buf662  # reuse
            buf664 = empty_strided_cuda((128, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_29, x_se_30], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_144.run(buf663, primals_257, buf664, 3328, stream=stream0)
            del primals_257
            buf665 = empty_strided_cuda((624, 26, 1, 1), (26, 1, 26, 26), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_31], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_145.run(primals_258, buf665, 16224, stream=stream0)
            del primals_258
            # Topologically Sorted Source Nodes: [x_se_31], Original ATen: [aten._to_copy, aten.convolution]
            buf666 = extern_kernels.convolution(buf664, buf665, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf666, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution.default')
            buf667 = buf666; del buf666  # reuse
            # Topologically Sorted Source Nodes: [x_se_31], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_146.run(buf667, primals_259, 79872, stream=stream0)
            del primals_259
            buf668 = empty_strided_cuda((128, 624, 14, 14), (122304, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7, x_100], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_147.run(buf658, buf667, buf668, 25088, 624, stream=stream0)
            del buf658
            buf669 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_82], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_260, buf669, 16224, stream=stream0)
            del primals_260
            buf670 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7, x_100, split_24, conv2d_82], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_149.run(buf668, buf670, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7, x_100, split_24, conv2d_82], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf671 = extern_kernels.convolution(buf670, buf669, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf671, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf670
            buf672 = empty_strided_cuda((52, 312, 1, 1), (312, 1, 312, 312), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_83], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_148.run(primals_261, buf672, 16224, stream=stream0)
            del primals_261
            buf673 = empty_strided_cuda((128, 312, 14, 14), (61152, 1, 4368, 312), torch.float16)
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7, x_100, split_24, conv2d_83], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_150.run(buf668, buf673, 39936, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_99, sigmoid_7, x_100, split_24, conv2d_83], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf674 = extern_kernels.convolution(buf673, buf672, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf674, (128, 52, 14, 14), (10192, 1, 728, 52), 'torch.ops.aten.convolution.default')
            del buf673
            buf675 = empty_strided_cuda((128, 104, 14, 14), (20384, 1, 1456, 104), torch.float16)
            # Topologically Sorted Source Nodes: [x_101], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_151.run(buf671, buf674, buf675, 2609152, stream=stream0)
            del buf671
            del buf674
            buf676 = buf605; del buf605  # reuse
            buf677 = buf604; del buf604  # reuse
            buf678 = buf603; del buf603  # reuse
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_120.run(buf675, buf676, buf677, buf678, 20384, 128, stream=stream0)
            buf679 = buf608; del buf608  # reuse
            buf680 = buf607; del buf607  # reuse
            buf681 = buf606; del buf606  # reuse
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_121.run(buf676, buf677, buf678, buf679, buf680, buf681, 208, 98, stream=stream0)
            del buf676
            del buf677
            del buf678
            buf682 = buf610; del buf610  # reuse
            buf683 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            buf685 = empty_strided_cuda((1, 104, 1, 1), (104, 1, 104, 104), torch.float32)
            # Topologically Sorted Source Nodes: [x_102], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__122.run(buf679, buf680, buf681, primals_263, primals_264, buf682, buf683, buf685, primals_263, primals_264, 104, 2, stream=stream0)
            del buf679
            del buf680
            del buf681
            del primals_263
            del primals_264
            buf686 = buf613; del buf613  # reuse
            # Topologically Sorted Source Nodes: [x_102, x_103], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_152.run(buf686, buf675, buf682, buf683, primals_265, primals_266, 2609152, stream=stream0)
            del buf683
            del primals_266
            buf687 = empty_strided_cuda((624, 104, 1, 1), (104, 1, 104, 104), torch.float16)
            # Topologically Sorted Source Nodes: [x_104], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_153.run(primals_267, buf687, 64896, stream=stream0)
            del primals_267
            # Topologically Sorted Source Nodes: [x_104], Original ATen: [aten.convolution]
            buf688 = extern_kernels.convolution(buf686, buf687, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf688, (128, 624, 14, 14), (122304, 1, 8736, 624), 'torch.ops.aten.convolution.default')
            buf689 = buf650; del buf650  # reuse
            buf690 = buf649; del buf649  # reuse
            buf691 = buf648; del buf648  # reuse
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf688, buf689, buf690, buf691, 122304, 128, stream=stream0)
            buf692 = buf653; del buf653  # reuse
            buf693 = buf652; del buf652  # reuse
            buf694 = buf651; del buf651  # reuse
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf689, buf690, buf691, buf692, buf693, buf694, 1248, 98, stream=stream0)
            buf695 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf698 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_105], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf692, buf693, buf694, primals_269, primals_270, buf695, buf698, primals_269, primals_270, 624, 2, stream=stream0)
            del primals_269
            del primals_270
            buf700 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_105, x_106], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_154.run(buf688, buf695, buf698, primals_271, primals_272, buf700, 15654912, stream=stream0)
            buf701 = empty_strided_cuda((624, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_107], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_155.run(primals_273, buf701, 5616, stream=stream0)
            del primals_273
            # Topologically Sorted Source Nodes: [x_107], Original ATen: [aten.convolution]
            buf702 = extern_kernels.convolution(buf700, buf701, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=624, bias=None)
            assert_size_stride(buf702, (128, 624, 14, 14), (122304, 1, 8736, 624), 'torch.ops.aten.convolution.default')
            buf703 = buf691; del buf691  # reuse
            buf704 = buf690; del buf690  # reuse
            buf705 = buf689; del buf689  # reuse
            # Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_128.run(buf702, buf703, buf704, buf705, 122304, 128, stream=stream0)
            buf706 = buf694; del buf694  # reuse
            buf707 = buf693; del buf693  # reuse
            buf708 = buf692; del buf692  # reuse
            # Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_129.run(buf703, buf704, buf705, buf706, buf707, buf708, 1248, 98, stream=stream0)
            del buf703
            del buf704
            del buf705
            buf709 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            buf712 = empty_strided_cuda((1, 624, 1, 1), (624, 1, 624, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_108], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__130.run(buf706, buf707, buf708, primals_275, primals_276, buf709, buf712, primals_275, primals_276, 624, 2, stream=stream0)
            del buf706
            del buf707
            del buf708
            del primals_275
            del primals_276
            buf713 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float32)
            # Topologically Sorted Source Nodes: [x_108, x_109], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_141.run(buf702, buf709, buf712, primals_277, primals_278, buf713, 15654912, stream=stream0)
            buf715 = empty_strided_cuda((128, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_109, x_se_32], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_142.run(buf713, buf715, 79872, 196, stream=stream0)
            buf716 = empty_strided_cuda((52, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_33], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_156.run(primals_279, buf716, 32448, stream=stream0)
            del primals_279
            # Topologically Sorted Source Nodes: [x_se_33], Original ATen: [aten._to_copy, aten.convolution]
            buf717 = extern_kernels.convolution(buf715, buf716, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf717, (128, 52, 1, 1), (52, 1, 52, 52), 'torch.ops.aten.convolution.default')
            buf718 = buf717; del buf717  # reuse
            buf719 = empty_strided_cuda((128, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_33, x_se_34], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_157.run(buf718, primals_280, buf719, 6656, stream=stream0)
            del primals_280
            buf720 = empty_strided_cuda((624, 52, 1, 1), (52, 1, 52, 52), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_35], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_158.run(primals_281, buf720, 32448, stream=stream0)
            del primals_281
            # Topologically Sorted Source Nodes: [x_se_35], Original ATen: [aten._to_copy, aten.convolution]
            buf721 = extern_kernels.convolution(buf719, buf720, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf721, (128, 624, 1, 1), (624, 1, 624, 624), 'torch.ops.aten.convolution.default')
            buf722 = buf721; del buf721  # reuse
            # Topologically Sorted Source Nodes: [x_se_35], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_159.run(buf722, primals_282, 79872, stream=stream0)
            del primals_282
            buf723 = empty_strided_cuda((128, 624, 14, 14), (122304, 1, 8736, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_109, sigmoid_8, x_110], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_160.run(buf713, buf722, buf723, 15654912, stream=stream0)
            del buf713
            buf724 = empty_strided_cuda((160, 624, 1, 1), (624, 1, 624, 624), torch.float16)
            # Topologically Sorted Source Nodes: [x_111], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_161.run(primals_283, buf724, 99840, stream=stream0)
            del primals_283
            # Topologically Sorted Source Nodes: [x_111], Original ATen: [aten.convolution]
            buf725 = extern_kernels.convolution(buf723, buf724, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf725, (128, 160, 14, 14), (31360, 1, 2240, 160), 'torch.ops.aten.convolution.default')
            buf726 = empty_strided_cuda((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), torch.float32)
            buf727 = empty_strided_cuda((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), torch.float32)
            buf728 = empty_strided_cuda((1, 160, 1, 1, 196), (31360, 1, 31360, 31360, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_162.run(buf725, buf726, buf727, buf728, 31360, 128, stream=stream0)
            buf729 = reinterpret_tensor(buf142, (1, 160, 1, 1, 2), (320, 1, 320, 320, 160), 0); del buf142  # reuse
            buf730 = reinterpret_tensor(buf141, (1, 160, 1, 1, 2), (320, 1, 320, 320, 160), 0); del buf141  # reuse
            buf731 = reinterpret_tensor(buf140, (1, 160, 1, 1, 2), (320, 1, 320, 320, 160), 0); del buf140  # reuse
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_163.run(buf726, buf727, buf728, buf729, buf730, buf731, 320, 98, stream=stream0)
            buf732 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            buf733 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            buf735 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__164.run(buf729, buf730, buf731, primals_285, primals_286, buf732, buf733, buf735, primals_285, primals_286, 160, 2, stream=stream0)
            del primals_285
            del primals_286
            buf736 = empty_strided_cuda((128, 160, 14, 14), (31360, 1, 2240, 160), torch.float16)
            # Topologically Sorted Source Nodes: [x_112], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_165.run(buf725, buf732, buf733, primals_287, primals_288, buf736, 4014080, stream=stream0)
            del primals_288
            buf737 = empty_strided_cuda((128, 80, 14, 14), (15680, 196, 14, 1), torch.float16)
            buf740 = empty_strided_cuda((128, 80, 14, 14), (15680, 1, 1120, 80), torch.float16)
            # Topologically Sorted Source Nodes: [split_25, conv2d_89], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_166.run(buf736, buf737, buf740, 10240, 196, stream=stream0)
            buf738 = empty_strided_cuda((128, 80, 14, 14), (15680, 196, 14, 1), torch.float16)
            buf743 = empty_strided_cuda((128, 80, 14, 14), (15680, 1, 1120, 80), torch.float16)
            # Topologically Sorted Source Nodes: [split_25, conv2d_90], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_167.run(buf736, buf738, buf743, 10240, 196, stream=stream0)
            buf739 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_89], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_289, buf739, 19200, stream=stream0)
            del primals_289
            # Topologically Sorted Source Nodes: [conv2d_89], Original ATen: [aten.convolution]
            buf741 = extern_kernels.convolution(buf740, buf739, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf741, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            buf742 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_90], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_290, buf742, 19200, stream=stream0)
            del primals_290
            # Topologically Sorted Source Nodes: [conv2d_90], Original ATen: [aten.convolution]
            buf744 = extern_kernels.convolution(buf743, buf742, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf744, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            buf745 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_113], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_169.run(buf741, buf744, buf745, 12042240, stream=stream0)
            del buf741
            del buf744
            buf746 = empty_strided_cuda((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), torch.float32)
            buf747 = empty_strided_cuda((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), torch.float32)
            buf748 = empty_strided_cuda((1, 480, 1, 1, 196), (94080, 1, 94080, 94080, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf745, buf746, buf747, buf748, 94080, 128, stream=stream0)
            buf749 = reinterpret_tensor(buf180, (1, 480, 1, 1, 2), (960, 1, 960, 960, 480), 0); del buf180  # reuse
            buf750 = reinterpret_tensor(buf179, (1, 480, 1, 1, 2), (960, 1, 960, 960, 480), 0); del buf179  # reuse
            buf751 = reinterpret_tensor(buf178, (1, 480, 1, 1, 2), (960, 1, 960, 960, 480), 0); del buf178  # reuse
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf746, buf747, buf748, buf749, buf750, buf751, 960, 98, stream=stream0)
            buf752 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf755 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_114], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf749, buf750, buf751, primals_292, primals_293, buf752, buf755, primals_292, primals_293, 480, 2, stream=stream0)
            del primals_292
            del primals_293
            buf758 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_114, x_115], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_173.run(buf745, buf752, buf755, primals_294, primals_295, buf758, 25088, 480, stream=stream0)
            buf757 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_91], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_44.run(primals_296, buf757, 1080, stream=stream0)
            del primals_296
            buf759 = empty_strided_cuda((128, 120, 14, 14), (23520, 1, 1680, 120), torch.float16)
            # Topologically Sorted Source Nodes: [x_115, conv2d_91], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_174.run(buf758, buf759, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_115, conv2d_91], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf760 = extern_kernels.convolution(buf759, buf757, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf760, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf761 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_92], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_175.run(primals_297, buf761, 3000, stream=stream0)
            del primals_297
            buf762 = buf759; del buf759  # reuse
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_92], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_176.run(buf758, buf762, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_92], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf763 = extern_kernels.convolution(buf762, buf761, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf763, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf764 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_93], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_177.run(primals_298, buf764, 5880, stream=stream0)
            del primals_298
            buf765 = buf762; del buf762  # reuse
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_93], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_178.run(buf758, buf765, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_93], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf766 = extern_kernels.convolution(buf765, buf764, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf766, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf767 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_94], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_179.run(primals_299, buf767, 9720, stream=stream0)
            del primals_299
            buf768 = buf765; del buf765  # reuse
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_94], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_180.run(buf758, buf768, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_115, conv2d_91, conv2d_94], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf769 = extern_kernels.convolution(buf768, buf767, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf769, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            del buf768
            buf770 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_116], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_181.run(buf760, buf763, buf766, buf769, buf770, 12042240, stream=stream0)
            del buf760
            del buf763
            del buf766
            buf771 = buf748; del buf748  # reuse
            buf772 = buf747; del buf747  # reuse
            buf773 = buf746; del buf746  # reuse
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf770, buf771, buf772, buf773, 94080, 128, stream=stream0)
            buf774 = buf751; del buf751  # reuse
            buf775 = buf750; del buf750  # reuse
            buf776 = buf749; del buf749  # reuse
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf771, buf772, buf773, buf774, buf775, buf776, 960, 98, stream=stream0)
            buf777 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf780 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_117], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf774, buf775, buf776, primals_301, primals_302, buf777, buf780, primals_301, primals_302, 480, 2, stream=stream0)
            del primals_301
            del primals_302
            buf781 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_117, x_118], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_182.run(buf770, buf777, buf780, primals_303, primals_304, buf781, 12042240, stream=stream0)
            buf783 = empty_strided_cuda((128, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_118, x_se_36], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_183.run(buf781, buf783, 61440, 196, stream=stream0)
            buf784 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_37], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_184.run(primals_305, buf784, 38400, stream=stream0)
            del primals_305
            # Topologically Sorted Source Nodes: [x_se_37], Original ATen: [aten._to_copy, aten.convolution]
            buf785 = extern_kernels.convolution(buf783, buf784, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf785, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution.default')
            buf786 = buf785; del buf785  # reuse
            buf787 = empty_strided_cuda((128, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_37, x_se_38], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_185.run(buf786, primals_306, buf787, 10240, stream=stream0)
            del primals_306
            buf788 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_39], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_186.run(primals_307, buf788, 38400, stream=stream0)
            del primals_307
            # Topologically Sorted Source Nodes: [x_se_39], Original ATen: [aten._to_copy, aten.convolution]
            buf789 = extern_kernels.convolution(buf787, buf788, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf789, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution.default')
            buf790 = buf789; del buf789  # reuse
            # Topologically Sorted Source Nodes: [x_se_39], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_187.run(buf790, primals_308, 61440, stream=stream0)
            del primals_308
            buf791 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_188.run(buf781, buf790, buf791, 25088, 480, stream=stream0)
            del buf781
            buf792 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_97], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_309, buf792, 19200, stream=stream0)
            del primals_309
            buf793 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_97], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190.run(buf791, buf793, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_97], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf794 = extern_kernels.convolution(buf793, buf792, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf794, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf793
            buf795 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_98], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_310, buf795, 19200, stream=stream0)
            del primals_310
            buf796 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_98], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191.run(buf791, buf796, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_118, sigmoid_9, x_119, split_27, conv2d_98], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf797 = extern_kernels.convolution(buf796, buf795, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf797, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf796
            buf798 = empty_strided_cuda((128, 160, 14, 14), (31360, 1, 2240, 160), torch.float16)
            # Topologically Sorted Source Nodes: [x_120], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_192.run(buf794, buf797, buf798, 4014080, stream=stream0)
            buf799 = buf728; del buf728  # reuse
            buf800 = buf727; del buf727  # reuse
            buf801 = buf726; del buf726  # reuse
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_162.run(buf798, buf799, buf800, buf801, 31360, 128, stream=stream0)
            buf802 = buf731; del buf731  # reuse
            buf803 = buf730; del buf730  # reuse
            buf804 = buf729; del buf729  # reuse
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_163.run(buf799, buf800, buf801, buf802, buf803, buf804, 320, 98, stream=stream0)
            buf805 = buf733; del buf733  # reuse
            buf806 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            buf808 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_121], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__164.run(buf802, buf803, buf804, primals_312, primals_313, buf805, buf806, buf808, primals_312, primals_313, 160, 2, stream=stream0)
            del primals_312
            del primals_313
            buf809 = buf736; del buf736  # reuse
            # Topologically Sorted Source Nodes: [x_121, x_122], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_193.run(buf809, buf798, buf805, buf806, primals_314, primals_315, 4014080, stream=stream0)
            del primals_315
            buf810 = reinterpret_tensor(buf797, (128, 80, 14, 14), (15680, 196, 14, 1), 0); del buf797  # reuse
            buf813 = buf794; del buf794  # reuse
            # Topologically Sorted Source Nodes: [split_28, conv2d_99], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_166.run(buf809, buf810, buf813, 10240, 196, stream=stream0)
            buf811 = reinterpret_tensor(buf743, (128, 80, 14, 14), (15680, 196, 14, 1), 0); del buf743  # reuse
            buf816 = buf740; del buf740  # reuse
            # Topologically Sorted Source Nodes: [split_28, conv2d_100], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_167.run(buf809, buf811, buf816, 10240, 196, stream=stream0)
            buf812 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_99], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_316, buf812, 19200, stream=stream0)
            del primals_316
            # Topologically Sorted Source Nodes: [conv2d_99], Original ATen: [aten.convolution]
            buf814 = extern_kernels.convolution(buf813, buf812, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf814, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            buf815 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_100], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_317, buf815, 19200, stream=stream0)
            del primals_317
            # Topologically Sorted Source Nodes: [conv2d_100], Original ATen: [aten.convolution]
            buf817 = extern_kernels.convolution(buf816, buf815, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf817, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            buf818 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_123], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_169.run(buf814, buf817, buf818, 12042240, stream=stream0)
            del buf814
            del buf817
            buf819 = buf773; del buf773  # reuse
            buf820 = buf772; del buf772  # reuse
            buf821 = buf771; del buf771  # reuse
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf818, buf819, buf820, buf821, 94080, 128, stream=stream0)
            buf822 = buf776; del buf776  # reuse
            buf823 = buf775; del buf775  # reuse
            buf824 = buf774; del buf774  # reuse
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf819, buf820, buf821, buf822, buf823, buf824, 960, 98, stream=stream0)
            buf825 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf828 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_124], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf822, buf823, buf824, primals_319, primals_320, buf825, buf828, primals_319, primals_320, 480, 2, stream=stream0)
            del primals_319
            del primals_320
            buf831 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_124, x_125], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_173.run(buf818, buf825, buf828, primals_321, primals_322, buf831, 25088, 480, stream=stream0)
            buf830 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_101], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_44.run(primals_323, buf830, 1080, stream=stream0)
            del primals_323
            buf832 = buf769; del buf769  # reuse
            # Topologically Sorted Source Nodes: [x_125, conv2d_101], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_174.run(buf831, buf832, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_125, conv2d_101], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf833 = extern_kernels.convolution(buf832, buf830, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf833, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf834 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_102], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_175.run(primals_324, buf834, 3000, stream=stream0)
            del primals_324
            buf835 = buf832; del buf832  # reuse
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_102], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_176.run(buf831, buf835, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_102], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf836 = extern_kernels.convolution(buf835, buf834, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf836, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf837 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_103], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_177.run(primals_325, buf837, 5880, stream=stream0)
            del primals_325
            buf838 = buf835; del buf835  # reuse
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_103], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_178.run(buf831, buf838, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_103], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf839 = extern_kernels.convolution(buf838, buf837, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf839, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf840 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_104], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_179.run(primals_326, buf840, 9720, stream=stream0)
            del primals_326
            buf841 = buf838; del buf838  # reuse
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_104], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_180.run(buf831, buf841, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_125, conv2d_101, conv2d_104], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf842 = extern_kernels.convolution(buf841, buf840, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf842, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            del buf841
            buf843 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_126], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_181.run(buf833, buf836, buf839, buf842, buf843, 12042240, stream=stream0)
            del buf833
            del buf836
            del buf839
            buf844 = buf821; del buf821  # reuse
            buf845 = buf820; del buf820  # reuse
            buf846 = buf819; del buf819  # reuse
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf843, buf844, buf845, buf846, 94080, 128, stream=stream0)
            buf847 = buf824; del buf824  # reuse
            buf848 = buf823; del buf823  # reuse
            buf849 = buf822; del buf822  # reuse
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf844, buf845, buf846, buf847, buf848, buf849, 960, 98, stream=stream0)
            buf850 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf853 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_127], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf847, buf848, buf849, primals_328, primals_329, buf850, buf853, primals_328, primals_329, 480, 2, stream=stream0)
            del primals_328
            del primals_329
            buf854 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_127, x_128], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_182.run(buf843, buf850, buf853, primals_330, primals_331, buf854, 12042240, stream=stream0)
            buf856 = empty_strided_cuda((128, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_128, x_se_40], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_183.run(buf854, buf856, 61440, 196, stream=stream0)
            buf857 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_41], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_184.run(primals_332, buf857, 38400, stream=stream0)
            del primals_332
            # Topologically Sorted Source Nodes: [x_se_41], Original ATen: [aten._to_copy, aten.convolution]
            buf858 = extern_kernels.convolution(buf856, buf857, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf858, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution.default')
            buf859 = buf858; del buf858  # reuse
            buf860 = empty_strided_cuda((128, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_41, x_se_42], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_185.run(buf859, primals_333, buf860, 10240, stream=stream0)
            del primals_333
            buf861 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_43], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_186.run(primals_334, buf861, 38400, stream=stream0)
            del primals_334
            # Topologically Sorted Source Nodes: [x_se_43], Original ATen: [aten._to_copy, aten.convolution]
            buf862 = extern_kernels.convolution(buf860, buf861, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf862, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution.default')
            buf863 = buf862; del buf862  # reuse
            # Topologically Sorted Source Nodes: [x_se_43], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_187.run(buf863, primals_335, 61440, stream=stream0)
            del primals_335
            buf864 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10, x_129], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_188.run(buf854, buf863, buf864, 25088, 480, stream=stream0)
            del buf854
            buf865 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_107], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_336, buf865, 19200, stream=stream0)
            del primals_336
            buf866 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10, x_129, split_30, conv2d_107], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190.run(buf864, buf866, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10, x_129, split_30, conv2d_107], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf867 = extern_kernels.convolution(buf866, buf865, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf867, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf866
            buf868 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_108], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_337, buf868, 19200, stream=stream0)
            del primals_337
            buf869 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10, x_129, split_30, conv2d_108], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191.run(buf864, buf869, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_128, sigmoid_10, x_129, split_30, conv2d_108], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf870 = extern_kernels.convolution(buf869, buf868, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf870, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf869
            buf871 = empty_strided_cuda((128, 160, 14, 14), (31360, 1, 2240, 160), torch.float16)
            # Topologically Sorted Source Nodes: [x_130], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_192.run(buf867, buf870, buf871, 4014080, stream=stream0)
            buf872 = buf801; del buf801  # reuse
            buf873 = buf800; del buf800  # reuse
            buf874 = buf799; del buf799  # reuse
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_162.run(buf871, buf872, buf873, buf874, 31360, 128, stream=stream0)
            buf875 = buf804; del buf804  # reuse
            buf876 = buf803; del buf803  # reuse
            buf877 = buf802; del buf802  # reuse
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_163.run(buf872, buf873, buf874, buf875, buf876, buf877, 320, 98, stream=stream0)
            buf878 = buf806; del buf806  # reuse
            buf879 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            buf881 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_131], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__164.run(buf875, buf876, buf877, primals_339, primals_340, buf878, buf879, buf881, primals_339, primals_340, 160, 2, stream=stream0)
            del primals_339
            del primals_340
            buf882 = buf809; del buf809  # reuse
            # Topologically Sorted Source Nodes: [x_131, x_132], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_193.run(buf882, buf871, buf878, buf879, primals_341, primals_342, 4014080, stream=stream0)
            del primals_342
            buf883 = reinterpret_tensor(buf870, (128, 80, 14, 14), (15680, 196, 14, 1), 0); del buf870  # reuse
            buf886 = buf867; del buf867  # reuse
            # Topologically Sorted Source Nodes: [split_31, conv2d_109], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_166.run(buf882, buf883, buf886, 10240, 196, stream=stream0)
            buf884 = reinterpret_tensor(buf816, (128, 80, 14, 14), (15680, 196, 14, 1), 0); del buf816  # reuse
            buf889 = buf813; del buf813  # reuse
            # Topologically Sorted Source Nodes: [split_31, conv2d_110], Original ATen: [aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_split_with_sizes_167.run(buf882, buf884, buf889, 10240, 196, stream=stream0)
            buf885 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_109], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_343, buf885, 19200, stream=stream0)
            del primals_343
            # Topologically Sorted Source Nodes: [conv2d_109], Original ATen: [aten.convolution]
            buf887 = extern_kernels.convolution(buf886, buf885, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf887, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            del buf886
            buf888 = empty_strided_cuda((240, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_110], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_168.run(primals_344, buf888, 19200, stream=stream0)
            del primals_344
            # Topologically Sorted Source Nodes: [conv2d_110], Original ATen: [aten.convolution]
            buf890 = extern_kernels.convolution(buf889, buf888, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf890, (128, 240, 14, 14), (47040, 1, 3360, 240), 'torch.ops.aten.convolution.default')
            del buf889
            buf891 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_133], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_169.run(buf887, buf890, buf891, 12042240, stream=stream0)
            del buf887
            del buf890
            buf892 = buf846; del buf846  # reuse
            buf893 = buf845; del buf845  # reuse
            buf894 = buf844; del buf844  # reuse
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf891, buf892, buf893, buf894, 94080, 128, stream=stream0)
            buf895 = buf849; del buf849  # reuse
            buf896 = buf848; del buf848  # reuse
            buf897 = buf847; del buf847  # reuse
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf892, buf893, buf894, buf895, buf896, buf897, 960, 98, stream=stream0)
            buf898 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf901 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_134], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf895, buf896, buf897, primals_346, primals_347, buf898, buf901, primals_346, primals_347, 480, 2, stream=stream0)
            del primals_346
            del primals_347
            buf904 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_134, x_135], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_173.run(buf891, buf898, buf901, primals_348, primals_349, buf904, 25088, 480, stream=stream0)
            buf903 = empty_strided_cuda((120, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_111], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_44.run(primals_350, buf903, 1080, stream=stream0)
            del primals_350
            buf905 = buf842; del buf842  # reuse
            # Topologically Sorted Source Nodes: [x_135, conv2d_111], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_174.run(buf904, buf905, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_135, conv2d_111], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf906 = extern_kernels.convolution(buf905, buf903, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf906, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf907 = empty_strided_cuda((120, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_112], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_175.run(primals_351, buf907, 3000, stream=stream0)
            del primals_351
            buf908 = buf905; del buf905  # reuse
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_112], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_176.run(buf904, buf908, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_112], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf909 = extern_kernels.convolution(buf908, buf907, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf909, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf910 = empty_strided_cuda((120, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_113], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_177.run(primals_352, buf910, 5880, stream=stream0)
            del primals_352
            buf911 = buf908; del buf908  # reuse
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_113], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_178.run(buf904, buf911, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_113], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf912 = extern_kernels.convolution(buf911, buf910, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf912, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            buf913 = empty_strided_cuda((120, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_114], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_179.run(primals_353, buf913, 9720, stream=stream0)
            del primals_353
            buf914 = buf911; del buf911  # reuse
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_114], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_180.run(buf904, buf914, 15360, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_135, conv2d_111, conv2d_114], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf915 = extern_kernels.convolution(buf914, buf913, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=120, bias=None)
            assert_size_stride(buf915, (128, 120, 14, 14), (23520, 1, 1680, 120), 'torch.ops.aten.convolution.default')
            del buf914
            buf916 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_136], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_181.run(buf906, buf909, buf912, buf915, buf916, 12042240, stream=stream0)
            del buf906
            del buf909
            del buf912
            del buf915
            buf917 = buf894; del buf894  # reuse
            buf918 = buf893; del buf893  # reuse
            buf919 = buf892; del buf892  # reuse
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_170.run(buf916, buf917, buf918, buf919, 94080, 128, stream=stream0)
            buf920 = buf897; del buf897  # reuse
            buf921 = buf896; del buf896  # reuse
            buf922 = buf895; del buf895  # reuse
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_171.run(buf917, buf918, buf919, buf920, buf921, buf922, 960, 98, stream=stream0)
            del buf917
            del buf918
            del buf919
            buf923 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            buf926 = empty_strided_cuda((1, 480, 1, 1), (480, 1, 480, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_137], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__172.run(buf920, buf921, buf922, primals_355, primals_356, buf923, buf926, primals_355, primals_356, 480, 2, stream=stream0)
            del primals_355
            del primals_356
            buf927 = empty_strided_cuda((128, 480, 14, 14), (94080, 1, 6720, 480), torch.float32)
            # Topologically Sorted Source Nodes: [x_137, x_138], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_182.run(buf916, buf923, buf926, primals_357, primals_358, buf927, 12042240, stream=stream0)
            buf929 = empty_strided_cuda((128, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_138, x_se_44], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_red_fused_mean_silu_183.run(buf927, buf929, 61440, 196, stream=stream0)
            buf930 = empty_strided_cuda((80, 480, 1, 1), (480, 1, 480, 480), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_45], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_184.run(primals_359, buf930, 38400, stream=stream0)
            del primals_359
            # Topologically Sorted Source Nodes: [x_se_45], Original ATen: [aten._to_copy, aten.convolution]
            buf931 = extern_kernels.convolution(buf929, buf930, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf931, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution.default')
            buf932 = buf931; del buf931  # reuse
            buf933 = empty_strided_cuda((128, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_45, x_se_46], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_185.run(buf932, primals_360, buf933, 10240, stream=stream0)
            del primals_360
            buf934 = empty_strided_cuda((480, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_47], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_186.run(primals_361, buf934, 38400, stream=stream0)
            del primals_361
            # Topologically Sorted Source Nodes: [x_se_47], Original ATen: [aten._to_copy, aten.convolution]
            buf935 = extern_kernels.convolution(buf933, buf934, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf935, (128, 480, 1, 1), (480, 1, 480, 480), 'torch.ops.aten.convolution.default')
            buf936 = buf935; del buf935  # reuse
            # Topologically Sorted Source Nodes: [x_se_47], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_187.run(buf936, primals_362, 61440, stream=stream0)
            del primals_362
            buf937 = empty_strided_cuda((128, 480, 14, 14), (94080, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11, x_139], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_188.run(buf927, buf936, buf937, 25088, 480, stream=stream0)
            del buf927
            buf938 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_117], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_363, buf938, 19200, stream=stream0)
            del primals_363
            buf939 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11, x_139, split_33, conv2d_117], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_190.run(buf937, buf939, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11, x_139, split_33, conv2d_117], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf940 = extern_kernels.convolution(buf939, buf938, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf940, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf939
            buf941 = empty_strided_cuda((80, 240, 1, 1), (240, 1, 240, 240), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_118], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_189.run(primals_364, buf941, 19200, stream=stream0)
            del primals_364
            buf942 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11, x_139, split_33, conv2d_118], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_191.run(buf937, buf942, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_138, sigmoid_11, x_139, split_33, conv2d_118], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf943 = extern_kernels.convolution(buf942, buf941, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf943, (128, 80, 14, 14), (15680, 1, 1120, 80), 'torch.ops.aten.convolution.default')
            del buf942
            buf944 = empty_strided_cuda((128, 160, 14, 14), (31360, 1, 2240, 160), torch.float16)
            # Topologically Sorted Source Nodes: [x_140], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_192.run(buf940, buf943, buf944, 4014080, stream=stream0)
            del buf940
            del buf943
            buf945 = buf874; del buf874  # reuse
            buf946 = buf873; del buf873  # reuse
            buf947 = buf872; del buf872  # reuse
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_162.run(buf944, buf945, buf946, buf947, 31360, 128, stream=stream0)
            buf948 = buf877; del buf877  # reuse
            buf949 = buf876; del buf876  # reuse
            buf950 = buf875; del buf875  # reuse
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_163.run(buf945, buf946, buf947, buf948, buf949, buf950, 320, 98, stream=stream0)
            del buf945
            del buf946
            del buf947
            buf951 = buf879; del buf879  # reuse
            buf952 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            buf954 = empty_strided_cuda((1, 160, 1, 1), (160, 1, 160, 160), torch.float32)
            # Topologically Sorted Source Nodes: [x_141], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__164.run(buf948, buf949, buf950, primals_366, primals_367, buf951, buf952, buf954, primals_366, primals_367, 160, 2, stream=stream0)
            del buf948
            del buf949
            del buf950
            del primals_366
            del primals_367
            buf955 = buf882; del buf882  # reuse
            # Topologically Sorted Source Nodes: [x_141, x_142], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_193.run(buf955, buf944, buf951, buf952, primals_368, primals_369, 4014080, stream=stream0)
            del buf952
            del primals_369
            buf956 = empty_strided_cuda((960, 160, 1, 1), (160, 1, 160, 160), torch.float16)
            # Topologically Sorted Source Nodes: [x_143], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_194.run(primals_370, buf956, 153600, stream=stream0)
            del primals_370
            # Topologically Sorted Source Nodes: [x_143], Original ATen: [aten.convolution]
            buf957 = extern_kernels.convolution(buf955, buf956, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf957, (128, 960, 14, 14), (188160, 1, 13440, 960), 'torch.ops.aten.convolution.default')
            buf958 = reinterpret_tensor(buf177, (1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), 0); del buf177  # reuse
            buf959 = reinterpret_tensor(buf176, (1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), 0); del buf176  # reuse
            buf960 = reinterpret_tensor(buf175, (1, 960, 1, 1, 128), (122880, 1, 122880, 122880, 960), 0); del buf175  # reuse
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_195.run(buf957, buf958, buf959, buf960, 122880, 196, stream=stream0)
            buf961 = reinterpret_tensor(buf922, (1, 960, 1, 1), (960, 1, 960, 960), 0); del buf922  # reuse
            buf964 = reinterpret_tensor(buf921, (1, 960, 1, 1), (960, 1, 960, 960), 0); del buf921  # reuse
            # Topologically Sorted Source Nodes: [x_144], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_copy__196.run(buf958, buf959, buf960, primals_372, primals_373, buf961, buf964, primals_372, primals_373, 960, 128, stream=stream0)
            del buf958
            del buf959
            del buf960
            del primals_372
            del primals_373
            buf967 = empty_strided_cuda((128, 960, 14, 14), (188160, 196, 14, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_144, x_145], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_197.run(buf957, buf961, buf964, primals_374, primals_375, buf967, 25088, 960, stream=stream0)
            buf966 = empty_strided_cuda((240, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_120], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_198.run(primals_376, buf966, 2160, stream=stream0)
            del primals_376
            buf968 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_199.run(buf967, buf968, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf969 = extern_kernels.convolution(buf968, buf966, stride=(2, 2), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=240, bias=None)
            assert_size_stride(buf969, (128, 240, 7, 7), (11760, 1, 1680, 240), 'torch.ops.aten.convolution.default')
            del buf968
            buf970 = empty_strided_cuda((240, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_121], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_200.run(primals_377, buf970, 6000, stream=stream0)
            del primals_377
            buf971 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_121], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_201.run(buf967, buf971, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_121], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf972 = extern_kernels.convolution(buf971, buf970, stride=(2, 2), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=240, bias=None)
            assert_size_stride(buf972, (128, 240, 7, 7), (11760, 1, 1680, 240), 'torch.ops.aten.convolution.default')
            del buf971
            buf973 = empty_strided_cuda((240, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_122], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_202.run(primals_378, buf973, 11760, stream=stream0)
            del primals_378
            buf974 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_122], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_203.run(buf967, buf974, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_122], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf975 = extern_kernels.convolution(buf974, buf973, stride=(2, 2), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=240, bias=None)
            assert_size_stride(buf975, (128, 240, 7, 7), (11760, 1, 1680, 240), 'torch.ops.aten.convolution.default')
            del buf974
            buf976 = empty_strided_cuda((240, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_123], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_204.run(primals_379, buf976, 19440, stream=stream0)
            del primals_379
            buf977 = empty_strided_cuda((128, 240, 14, 14), (47040, 1, 3360, 240), torch.float16)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_123], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_205.run(buf967, buf977, 30720, 196, stream=stream0)
            # Topologically Sorted Source Nodes: [x_145, conv2d_120, conv2d_123], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf978 = extern_kernels.convolution(buf977, buf976, stride=(2, 2), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=240, bias=None)
            assert_size_stride(buf978, (128, 240, 7, 7), (11760, 1, 1680, 240), 'torch.ops.aten.convolution.default')
            buf979 = reinterpret_tensor(buf977, (128, 960, 7, 7), (47040, 1, 6720, 960), 0); del buf977  # reuse
            # Topologically Sorted Source Nodes: [x_146], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_206.run(buf969, buf972, buf975, buf978, buf979, 6021120, stream=stream0)
            del buf969
            del buf972
            del buf975
            del buf978
            buf980 = empty_strided_cuda((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), torch.float32)
            buf981 = empty_strided_cuda((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), torch.float32)
            buf982 = empty_strided_cuda((1, 960, 1, 1, 49), (47040, 1, 47040, 47040, 960), torch.float32)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_207.run(buf979, buf980, buf981, buf982, 47040, 128, stream=stream0)
            buf983 = reinterpret_tensor(buf920, (1, 960, 1, 1), (960, 1, 960, 960), 0); del buf920  # reuse
            buf986 = empty_strided_cuda((1, 960, 1, 1), (960, 1, 960, 960), torch.float32)
            # Topologically Sorted Source Nodes: [x_147], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__208.run(buf980, buf981, buf982, primals_381, primals_382, buf983, buf986, primals_381, primals_382, 960, 49, stream=stream0)
            del buf980
            del buf981
            del buf982
            del primals_381
            del primals_382
            buf987 = empty_strided_cuda((128, 960, 7, 7), (47040, 1, 6720, 960), torch.float32)
            # Topologically Sorted Source Nodes: [x_147, x_148], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_209.run(buf979, buf983, buf986, primals_383, primals_384, buf987, 6021120, stream=stream0)
            buf989 = empty_strided_cuda((128, 960, 1, 1), (960, 1, 960, 960), torch.float16)
            # Topologically Sorted Source Nodes: [x_148, x_se_48], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_per_fused_mean_silu_210.run(buf987, buf989, 122880, 49, stream=stream0)
            buf990 = empty_strided_cuda((80, 960, 1, 1), (960, 1, 960, 960), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_49], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_211.run(primals_385, buf990, 76800, stream=stream0)
            del primals_385
            # Topologically Sorted Source Nodes: [x_se_49], Original ATen: [aten._to_copy, aten.convolution]
            buf991 = extern_kernels.convolution(buf989, buf990, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf991, (128, 80, 1, 1), (80, 1, 80, 80), 'torch.ops.aten.convolution.default')
            buf992 = buf991; del buf991  # reuse
            buf993 = empty_strided_cuda((128, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_49, x_se_50], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_212.run(buf992, primals_386, buf993, 10240, stream=stream0)
            del primals_386
            buf994 = empty_strided_cuda((960, 80, 1, 1), (80, 1, 80, 80), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_51], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_213.run(primals_387, buf994, 76800, stream=stream0)
            del primals_387
            # Topologically Sorted Source Nodes: [x_se_51], Original ATen: [aten._to_copy, aten.convolution]
            buf995 = extern_kernels.convolution(buf993, buf994, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf995, (128, 960, 1, 1), (960, 1, 960, 960), 'torch.ops.aten.convolution.default')
            buf996 = buf995; del buf995  # reuse
            # Topologically Sorted Source Nodes: [x_se_51], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_214.run(buf996, primals_388, 122880, stream=stream0)
            del primals_388
            buf997 = empty_strided_cuda((128, 960, 7, 7), (47040, 1, 6720, 960), torch.float16)
            # Topologically Sorted Source Nodes: [x_148, sigmoid_12, x_149], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_215.run(buf987, buf996, buf997, 6021120, stream=stream0)
            del buf987
            buf998 = empty_strided_cuda((264, 960, 1, 1), (960, 1, 960, 960), torch.float16)
            # Topologically Sorted Source Nodes: [x_150], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_216.run(primals_389, buf998, 253440, stream=stream0)
            del primals_389
            # Topologically Sorted Source Nodes: [x_150], Original ATen: [aten.convolution]
            buf999 = extern_kernels.convolution(buf997, buf998, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf999, (128, 264, 7, 7), (12936, 1, 1848, 264), 'torch.ops.aten.convolution.default')
            buf1000 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            buf1001 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            buf1002 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_217.run(buf999, buf1000, buf1001, buf1002, 12936, 128, stream=stream0)
            buf1003 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1004 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1006 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__218.run(buf1000, buf1001, buf1002, primals_391, primals_392, buf1003, buf1004, buf1006, primals_391, primals_392, 264, 49, stream=stream0)
            del primals_391
            del primals_392
            buf1007 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_151], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_219.run(buf999, buf1003, buf1004, primals_393, primals_394, buf1007, 1655808, stream=stream0)
            del primals_394
            buf1008 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_152], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_220.run(primals_395, buf1008, 418176, stream=stream0)
            del primals_395
            # Topologically Sorted Source Nodes: [x_152], Original ATen: [aten.convolution]
            buf1009 = extern_kernels.convolution(buf1007, buf1008, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1009, (128, 1584, 7, 7), (77616, 1, 11088, 1584), 'torch.ops.aten.convolution.default')
            buf1010 = empty_strided_cuda((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), torch.float32)
            buf1011 = empty_strided_cuda((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), torch.float32)
            buf1012 = empty_strided_cuda((1, 1584, 1, 1, 49), (77616, 1, 77616, 77616, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1009, buf1010, buf1011, buf1012, 77616, 128, stream=stream0)
            buf1013 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1016 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_153], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1010, buf1011, buf1012, primals_397, primals_398, buf1013, buf1016, primals_397, primals_398, 1584, 49, stream=stream0)
            del primals_397
            del primals_398
            buf1019 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_153, x_154], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_223.run(buf1009, buf1013, buf1016, primals_399, primals_400, buf1019, 6272, 1584, stream=stream0)
            buf1018 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_128], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_224.run(primals_401, buf1018, 3564, stream=stream0)
            del primals_401
            buf1020 = empty_strided_cuda((128, 396, 7, 7), (19404, 1, 2772, 396), torch.float16)
            # Topologically Sorted Source Nodes: [x_154, conv2d_128], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_225.run(buf1019, buf1020, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_154, conv2d_128], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1021 = extern_kernels.convolution(buf1020, buf1018, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1021, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1022 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_129], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(primals_402, buf1022, 9900, stream=stream0)
            del primals_402
            buf1023 = buf1020; del buf1020  # reuse
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_129], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_227.run(buf1019, buf1023, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_129], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1024 = extern_kernels.convolution(buf1023, buf1022, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1024, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1025 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_130], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(primals_403, buf1025, 19404, stream=stream0)
            del primals_403
            buf1026 = buf1023; del buf1023  # reuse
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_130], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_229.run(buf1019, buf1026, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_130], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1027 = extern_kernels.convolution(buf1026, buf1025, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1027, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1028 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_131], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_230.run(primals_404, buf1028, 32076, stream=stream0)
            del primals_404
            buf1029 = buf1026; del buf1026  # reuse
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_131], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_231.run(buf1019, buf1029, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_154, conv2d_128, conv2d_131], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1030 = extern_kernels.convolution(buf1029, buf1028, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1030, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            del buf1029
            buf1031 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_155], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_232.run(buf1021, buf1024, buf1027, buf1030, buf1031, 9934848, stream=stream0)
            del buf1021
            del buf1024
            del buf1027
            buf1032 = buf1012; del buf1012  # reuse
            buf1033 = buf1011; del buf1011  # reuse
            buf1034 = buf1010; del buf1010  # reuse
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1031, buf1032, buf1033, buf1034, 77616, 128, stream=stream0)
            buf1035 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1038 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_156], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1032, buf1033, buf1034, primals_406, primals_407, buf1035, buf1038, primals_406, primals_407, 1584, 49, stream=stream0)
            del primals_406
            del primals_407
            buf1039 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_156, x_157], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_233.run(buf1031, buf1035, buf1038, primals_408, primals_409, buf1039, 9934848, stream=stream0)
            buf1041 = empty_strided_cuda((128, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_157, x_se_52], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_per_fused_mean_silu_234.run(buf1039, buf1041, 202752, 49, stream=stream0)
            buf1042 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_53], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_235.run(primals_410, buf1042, 209088, stream=stream0)
            del primals_410
            # Topologically Sorted Source Nodes: [x_se_53], Original ATen: [aten._to_copy, aten.convolution]
            buf1043 = extern_kernels.convolution(buf1041, buf1042, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1043, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution.default')
            buf1044 = buf1043; del buf1043  # reuse
            buf1045 = empty_strided_cuda((128, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_53, x_se_54], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_236.run(buf1044, primals_411, buf1045, 16896, stream=stream0)
            del primals_411
            buf1046 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_55], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_237.run(primals_412, buf1046, 209088, stream=stream0)
            del primals_412
            # Topologically Sorted Source Nodes: [x_se_55], Original ATen: [aten._to_copy, aten.convolution]
            buf1047 = extern_kernels.convolution(buf1045, buf1046, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1047, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution.default')
            buf1048 = buf1047; del buf1047  # reuse
            # Topologically Sorted Source Nodes: [x_se_55], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_238.run(buf1048, primals_413, 202752, stream=stream0)
            del primals_413
            buf1049 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_239.run(buf1039, buf1048, buf1049, 6272, 1584, stream=stream0)
            del buf1039
            buf1050 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_134], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_414, buf1050, 104544, stream=stream0)
            del primals_414
            buf1051 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_134], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241.run(buf1049, buf1051, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_134], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1052 = extern_kernels.convolution(buf1051, buf1050, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1052, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1051
            buf1053 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_135], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_415, buf1053, 104544, stream=stream0)
            del primals_415
            buf1054 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_135], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242.run(buf1049, buf1054, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_157, sigmoid_13, x_158, split_36, conv2d_135], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1055 = extern_kernels.convolution(buf1054, buf1053, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1055, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1054
            buf1056 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_159], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_243.run(buf1052, buf1055, buf1056, 1655808, stream=stream0)
            del buf1052
            del buf1055
            buf1057 = buf1002; del buf1002  # reuse
            buf1058 = buf1001; del buf1001  # reuse
            buf1059 = buf1000; del buf1000  # reuse
            # Topologically Sorted Source Nodes: [x_160], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_217.run(buf1056, buf1057, buf1058, buf1059, 12936, 128, stream=stream0)
            buf1060 = buf1004; del buf1004  # reuse
            buf1061 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1063 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_160], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__218.run(buf1057, buf1058, buf1059, primals_417, primals_418, buf1060, buf1061, buf1063, primals_417, primals_418, 264, 49, stream=stream0)
            del primals_417
            del primals_418
            buf1064 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_160, x_161], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_244.run(buf1056, buf1060, buf1061, primals_419, primals_420, buf1007, buf1064, 1655808, stream=stream0)
            del primals_420
            buf1065 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_162], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_220.run(primals_421, buf1065, 418176, stream=stream0)
            del primals_421
            # Topologically Sorted Source Nodes: [x_162], Original ATen: [aten.convolution]
            buf1066 = extern_kernels.convolution(buf1064, buf1065, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1066, (128, 1584, 7, 7), (77616, 1, 11088, 1584), 'torch.ops.aten.convolution.default')
            buf1067 = buf1034; del buf1034  # reuse
            buf1068 = buf1033; del buf1033  # reuse
            buf1069 = buf1032; del buf1032  # reuse
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1066, buf1067, buf1068, buf1069, 77616, 128, stream=stream0)
            buf1070 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1073 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_163], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1067, buf1068, buf1069, primals_423, primals_424, buf1070, buf1073, primals_423, primals_424, 1584, 49, stream=stream0)
            del primals_423
            del primals_424
            buf1076 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_163, x_164], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_223.run(buf1066, buf1070, buf1073, primals_425, primals_426, buf1076, 6272, 1584, stream=stream0)
            buf1075 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_137], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_224.run(primals_427, buf1075, 3564, stream=stream0)
            del primals_427
            buf1077 = buf1030; del buf1030  # reuse
            # Topologically Sorted Source Nodes: [x_164, conv2d_137], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_225.run(buf1076, buf1077, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_164, conv2d_137], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1078 = extern_kernels.convolution(buf1077, buf1075, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1078, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1079 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_138], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(primals_428, buf1079, 9900, stream=stream0)
            del primals_428
            buf1080 = buf1077; del buf1077  # reuse
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_138], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_227.run(buf1076, buf1080, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_138], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1081 = extern_kernels.convolution(buf1080, buf1079, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1081, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1082 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_139], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(primals_429, buf1082, 19404, stream=stream0)
            del primals_429
            buf1083 = buf1080; del buf1080  # reuse
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_139], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_229.run(buf1076, buf1083, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_139], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1084 = extern_kernels.convolution(buf1083, buf1082, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1084, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1085 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_140], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_230.run(primals_430, buf1085, 32076, stream=stream0)
            del primals_430
            buf1086 = buf1083; del buf1083  # reuse
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_140], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_231.run(buf1076, buf1086, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_164, conv2d_137, conv2d_140], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1087 = extern_kernels.convolution(buf1086, buf1085, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1087, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            del buf1086
            buf1088 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_165], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_232.run(buf1078, buf1081, buf1084, buf1087, buf1088, 9934848, stream=stream0)
            del buf1078
            del buf1081
            del buf1084
            buf1089 = buf1069; del buf1069  # reuse
            buf1090 = buf1068; del buf1068  # reuse
            buf1091 = buf1067; del buf1067  # reuse
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1088, buf1089, buf1090, buf1091, 77616, 128, stream=stream0)
            buf1092 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1095 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_166], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1089, buf1090, buf1091, primals_432, primals_433, buf1092, buf1095, primals_432, primals_433, 1584, 49, stream=stream0)
            del primals_432
            del primals_433
            buf1096 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_166, x_167], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_233.run(buf1088, buf1092, buf1095, primals_434, primals_435, buf1096, 9934848, stream=stream0)
            buf1098 = empty_strided_cuda((128, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_167, x_se_56], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_per_fused_mean_silu_234.run(buf1096, buf1098, 202752, 49, stream=stream0)
            buf1099 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_57], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_235.run(primals_436, buf1099, 209088, stream=stream0)
            del primals_436
            # Topologically Sorted Source Nodes: [x_se_57], Original ATen: [aten._to_copy, aten.convolution]
            buf1100 = extern_kernels.convolution(buf1098, buf1099, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1100, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution.default')
            buf1101 = buf1100; del buf1100  # reuse
            buf1102 = empty_strided_cuda((128, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_57, x_se_58], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_236.run(buf1101, primals_437, buf1102, 16896, stream=stream0)
            del primals_437
            buf1103 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_59], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_237.run(primals_438, buf1103, 209088, stream=stream0)
            del primals_438
            # Topologically Sorted Source Nodes: [x_se_59], Original ATen: [aten._to_copy, aten.convolution]
            buf1104 = extern_kernels.convolution(buf1102, buf1103, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1104, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution.default')
            buf1105 = buf1104; del buf1104  # reuse
            # Topologically Sorted Source Nodes: [x_se_59], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_238.run(buf1105, primals_439, 202752, stream=stream0)
            del primals_439
            buf1106 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14, x_168], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_239.run(buf1096, buf1105, buf1106, 6272, 1584, stream=stream0)
            del buf1096
            buf1107 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_143], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_440, buf1107, 104544, stream=stream0)
            del primals_440
            buf1108 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14, x_168, split_38, conv2d_143], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241.run(buf1106, buf1108, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14, x_168, split_38, conv2d_143], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1109 = extern_kernels.convolution(buf1108, buf1107, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1109, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1108
            buf1110 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_144], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_441, buf1110, 104544, stream=stream0)
            del primals_441
            buf1111 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14, x_168, split_38, conv2d_144], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242.run(buf1106, buf1111, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_167, sigmoid_14, x_168, split_38, conv2d_144], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1112 = extern_kernels.convolution(buf1111, buf1110, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1112, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1111
            buf1113 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_169], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_243.run(buf1109, buf1112, buf1113, 1655808, stream=stream0)
            del buf1109
            del buf1112
            buf1114 = buf1059; del buf1059  # reuse
            buf1115 = buf1058; del buf1058  # reuse
            buf1116 = buf1057; del buf1057  # reuse
            # Topologically Sorted Source Nodes: [x_170], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_217.run(buf1113, buf1114, buf1115, buf1116, 12936, 128, stream=stream0)
            buf1117 = buf1061; del buf1061  # reuse
            buf1118 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1120 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_170], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__218.run(buf1114, buf1115, buf1116, primals_443, primals_444, buf1117, buf1118, buf1120, primals_443, primals_444, 264, 49, stream=stream0)
            del buf1114
            del buf1115
            del buf1116
            del primals_443
            del primals_444
            buf1121 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_170, x_171], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_244.run(buf1113, buf1117, buf1118, primals_445, primals_446, buf1064, buf1121, 1655808, stream=stream0)
            del buf1118
            del primals_446
            buf1122 = empty_strided_cuda((1584, 264, 1, 1), (264, 1, 264, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_172], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_220.run(primals_447, buf1122, 418176, stream=stream0)
            del primals_447
            # Topologically Sorted Source Nodes: [x_172], Original ATen: [aten.convolution]
            buf1123 = extern_kernels.convolution(buf1121, buf1122, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1123, (128, 1584, 7, 7), (77616, 1, 11088, 1584), 'torch.ops.aten.convolution.default')
            buf1124 = buf1091; del buf1091  # reuse
            buf1125 = buf1090; del buf1090  # reuse
            buf1126 = buf1089; del buf1089  # reuse
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1123, buf1124, buf1125, buf1126, 77616, 128, stream=stream0)
            buf1127 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1130 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_173], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1124, buf1125, buf1126, primals_449, primals_450, buf1127, buf1130, primals_449, primals_450, 1584, 49, stream=stream0)
            del primals_449
            del primals_450
            buf1133 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_173, x_174], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_223.run(buf1123, buf1127, buf1130, primals_451, primals_452, buf1133, 6272, 1584, stream=stream0)
            buf1132 = empty_strided_cuda((396, 1, 3, 3), (9, 1, 3, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_146], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_224.run(primals_453, buf1132, 3564, stream=stream0)
            del primals_453
            buf1134 = buf1087; del buf1087  # reuse
            # Topologically Sorted Source Nodes: [x_174, conv2d_146], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_225.run(buf1133, buf1134, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_174, conv2d_146], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1135 = extern_kernels.convolution(buf1134, buf1132, stride=(1, 1), padding=(1, 1), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1135, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1136 = empty_strided_cuda((396, 1, 5, 5), (25, 1, 5, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_147], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_226.run(primals_454, buf1136, 9900, stream=stream0)
            del primals_454
            buf1137 = buf1134; del buf1134  # reuse
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_147], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_227.run(buf1133, buf1137, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_147], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1138 = extern_kernels.convolution(buf1137, buf1136, stride=(1, 1), padding=(2, 2), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1138, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1139 = empty_strided_cuda((396, 1, 7, 7), (49, 1, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_148], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_228.run(primals_455, buf1139, 19404, stream=stream0)
            del primals_455
            buf1140 = buf1137; del buf1137  # reuse
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_148], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_229.run(buf1133, buf1140, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_148], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1141 = extern_kernels.convolution(buf1140, buf1139, stride=(1, 1), padding=(3, 3), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1141, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            buf1142 = empty_strided_cuda((396, 1, 9, 9), (81, 1, 9, 1), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_149], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_230.run(primals_456, buf1142, 32076, stream=stream0)
            del primals_456
            buf1143 = buf1140; del buf1140  # reuse
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_149], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_silu_split_with_sizes_231.run(buf1133, buf1143, 50688, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_174, conv2d_146, conv2d_149], Original ATen: [aten.silu, aten.split_with_sizes, aten.convolution]
            buf1144 = extern_kernels.convolution(buf1143, buf1142, stride=(1, 1), padding=(4, 4), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=396, bias=None)
            assert_size_stride(buf1144, (128, 396, 7, 7), (19404, 1, 2772, 396), 'torch.ops.aten.convolution.default')
            del buf1143
            buf1145 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_175], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_232.run(buf1135, buf1138, buf1141, buf1144, buf1145, 9934848, stream=stream0)
            del buf1135
            del buf1138
            del buf1141
            del buf1144
            buf1146 = buf1126; del buf1126  # reuse
            buf1147 = buf1125; del buf1125  # reuse
            buf1148 = buf1124; del buf1124  # reuse
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_221.run(buf1145, buf1146, buf1147, buf1148, 77616, 128, stream=stream0)
            buf1149 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            buf1152 = empty_strided_cuda((1, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_176], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__222.run(buf1146, buf1147, buf1148, primals_458, primals_459, buf1149, buf1152, primals_458, primals_459, 1584, 49, stream=stream0)
            del buf1146
            del buf1147
            del buf1148
            del primals_458
            del primals_459
            buf1153 = empty_strided_cuda((128, 1584, 7, 7), (77616, 1, 11088, 1584), torch.float32)
            # Topologically Sorted Source Nodes: [x_176, x_177], Original ATen: [aten._native_batch_norm_legit_functional, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_silu_233.run(buf1145, buf1149, buf1152, primals_460, primals_461, buf1153, 9934848, stream=stream0)
            buf1155 = empty_strided_cuda((128, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_177, x_se_60], Original ATen: [aten.silu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_per_fused_mean_silu_234.run(buf1153, buf1155, 202752, 49, stream=stream0)
            buf1156 = empty_strided_cuda((132, 1584, 1, 1), (1584, 1, 1584, 1584), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_61], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_235.run(primals_462, buf1156, 209088, stream=stream0)
            del primals_462
            # Topologically Sorted Source Nodes: [x_se_61], Original ATen: [aten._to_copy, aten.convolution]
            buf1157 = extern_kernels.convolution(buf1155, buf1156, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1157, (128, 132, 1, 1), (132, 1, 132, 132), 'torch.ops.aten.convolution.default')
            buf1158 = buf1157; del buf1157  # reuse
            buf1159 = empty_strided_cuda((128, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_61, x_se_62], Original ATen: [aten._to_copy, aten.convolution, aten.silu]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_silu_236.run(buf1158, primals_463, buf1159, 16896, stream=stream0)
            del primals_463
            buf1160 = empty_strided_cuda((1584, 132, 1, 1), (132, 1, 132, 132), torch.float16)
            # Topologically Sorted Source Nodes: [x_se_63], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_237.run(primals_464, buf1160, 209088, stream=stream0)
            del primals_464
            # Topologically Sorted Source Nodes: [x_se_63], Original ATen: [aten._to_copy, aten.convolution]
            buf1161 = extern_kernels.convolution(buf1159, buf1160, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1161, (128, 1584, 1, 1), (1584, 1, 1584, 1584), 'torch.ops.aten.convolution.default')
            buf1162 = buf1161; del buf1161  # reuse
            # Topologically Sorted Source Nodes: [x_se_63], Original ATen: [aten._to_copy, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_convolution_238.run(buf1162, primals_465, 202752, stream=stream0)
            del primals_465
            buf1163 = empty_strided_cuda((128, 1584, 7, 7), (77632, 49, 7, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15, x_178], Original ATen: [aten.silu, aten.sigmoid, aten.mul]
            stream0 = get_raw_stream(0)
            triton_poi_fused_mul_sigmoid_silu_239.run(buf1153, buf1162, buf1163, 6272, 1584, stream=stream0)
            del buf1153
            buf1164 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_152], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_466, buf1164, 104544, stream=stream0)
            del primals_466
            buf1165 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15, x_178, split_40, conv2d_152], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_241.run(buf1163, buf1165, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15, x_178, split_40, conv2d_152], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1166 = extern_kernels.convolution(buf1165, buf1164, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1166, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1165
            buf1167 = empty_strided_cuda((132, 792, 1, 1), (792, 1, 792, 792), torch.float16)
            # Topologically Sorted Source Nodes: [conv2d_153], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_240.run(primals_467, buf1167, 104544, stream=stream0)
            del primals_467
            buf1168 = empty_strided_cuda((128, 792, 7, 7), (38808, 1, 5544, 792), torch.float16)
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15, x_178, split_40, conv2d_153], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            stream0 = get_raw_stream(0)
            triton_poi_fused_convolution_mul_sigmoid_silu_split_with_sizes_242.run(buf1163, buf1168, 101376, 49, stream=stream0)
            # Topologically Sorted Source Nodes: [x_177, sigmoid_15, x_178, split_40, conv2d_153], Original ATen: [aten.silu, aten.sigmoid, aten.mul, aten.split_with_sizes, aten.convolution]
            buf1169 = extern_kernels.convolution(buf1168, buf1167, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1169, (128, 132, 7, 7), (6468, 1, 924, 132), 'torch.ops.aten.convolution.default')
            del buf1168
            buf1170 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_179], Original ATen: [aten.cat]
            stream0 = get_raw_stream(0)
            triton_poi_fused_cat_243.run(buf1166, buf1169, buf1170, 1655808, stream=stream0)
            del buf1166
            del buf1169
            buf1171 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            buf1172 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            buf1173 = empty_strided_cuda((1, 264, 1, 1, 49), (12936, 1, 12936, 12936, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_180], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_217.run(buf1170, buf1171, buf1172, buf1173, 12936, 128, stream=stream0)
            buf1174 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1175 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            buf1177 = empty_strided_cuda((1, 264, 1, 1), (264, 1, 264, 264), torch.float32)
            # Topologically Sorted Source Nodes: [x_180], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__218.run(buf1171, buf1172, buf1173, primals_469, primals_470, buf1174, buf1175, buf1177, primals_469, primals_470, 264, 49, stream=stream0)
            del buf1171
            del buf1172
            del buf1173
            del primals_469
            del primals_470
            buf1178 = empty_strided_cuda((128, 264, 7, 7), (12936, 1, 1848, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_180, x_181], Original ATen: [aten._native_batch_norm_legit_functional, aten.add]
            stream0 = get_raw_stream(0)
            triton_poi_fused__native_batch_norm_legit_functional_add_244.run(buf1170, buf1174, buf1175, primals_471, primals_472, buf1121, buf1178, 1655808, stream=stream0)
            del buf1175
            del primals_472
            buf1179 = empty_strided_cuda((1536, 264, 1, 1), (264, 1, 264, 264), torch.float16)
            # Topologically Sorted Source Nodes: [x_182], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_245.run(primals_473, buf1179, 405504, stream=stream0)
            del primals_473
            # Topologically Sorted Source Nodes: [x_182], Original ATen: [aten.convolution]
            buf1180 = extern_kernels.convolution(buf1178, buf1179, stride=(1, 1), padding=(0, 0), dilation=(1, 1), transposed=False, output_padding=(0, 0), groups=1, bias=None)
            assert_size_stride(buf1180, (128, 1536, 7, 7), (75264, 1, 10752, 1536), 'torch.ops.aten.convolution.default')
            buf1181 = empty_strided_cuda((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), torch.float32)
            buf1182 = empty_strided_cuda((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), torch.float32)
            buf1183 = empty_strided_cuda((1, 1536, 1, 1, 49), (75264, 1, 75264, 75264, 1536), torch.float32)
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional]
            stream0 = get_raw_stream(0)
            triton_red_fused__native_batch_norm_legit_functional_246.run(buf1180, buf1181, buf1182, buf1183, 75264, 128, stream=stream0)
            buf1184 = empty_strided_cuda((1, 1536, 1, 1), (1536, 1, 1536, 1536), torch.float32)
            buf1187 = empty_strided_cuda((1, 1536, 1, 1), (1536, 1, 1536, 1536), torch.float32)
            # Topologically Sorted Source Nodes: [x_183], Original ATen: [aten._native_batch_norm_legit_functional, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_copy__247.run(buf1181, buf1182, buf1183, primals_475, primals_476, buf1184, buf1187, primals_475, primals_476, 1536, 49, stream=stream0)
            del buf1181
            del buf1182
            del buf1183
            del primals_475
            del primals_476
            buf1189 = empty_strided_cuda((128, 1536, 1, 1), (1536, 1, 196608, 196608), torch.float16)
            # Topologically Sorted Source Nodes: [x_183, x_184, x_185], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.mean]
            stream0 = get_raw_stream(0)
            triton_per_fused__native_batch_norm_legit_functional_mean_relu_248.run(buf1180, buf1184, buf1187, primals_477, primals_478, buf1189, 196608, 49, stream=stream0)
            buf1190 = empty_strided_cuda((1000, 1536), (1536, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_187], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_249.run(primals_479, buf1190, 1536000, stream=stream0)
            del primals_479
            buf1191 = empty_strided_cuda((1000, ), (1, ), torch.float16)
            # Topologically Sorted Source Nodes: [x_187], Original ATen: [aten._to_copy]
            stream0 = get_raw_stream(0)
            triton_poi_fused__to_copy_250.run(primals_480, buf1191, 1000, stream=stream0)
            del primals_480
            buf1192 = empty_strided_cuda((128, 1000), (1000, 1), torch.float16)
            # Topologically Sorted Source Nodes: [x_183, x_184, x_185, x_186, x_187], Original ATen: [aten._native_batch_norm_legit_functional, aten.relu, aten.mean, aten.view, aten._to_copy, aten.t, aten.addmm]
            extern_kernels.addmm(buf1191, reinterpret_tensor(buf1189, (128, 1536), (1536, 1), 0), reinterpret_tensor(buf1190, (1536, 1000), (1, 1536), 0), alpha=1, beta=1, out=buf1192)
            del buf1191
            # Topologically Sorted Source Nodes: [add_], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_3, primals_3, 1, stream=stream0)
            del primals_3
            # Topologically Sorted Source Nodes: [add__1], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_9, primals_9, 1, stream=stream0)
            del primals_9
            # Topologically Sorted Source Nodes: [add__2], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_15, primals_15, 1, stream=stream0)
            del primals_15
            # Topologically Sorted Source Nodes: [add__3], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_22, primals_22, 1, stream=stream0)
            del primals_22
            # Topologically Sorted Source Nodes: [add__4], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_30, primals_30, 1, stream=stream0)
            del primals_30
            # Topologically Sorted Source Nodes: [add__5], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_37, primals_37, 1, stream=stream0)
            del primals_37
            # Topologically Sorted Source Nodes: [add__6], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_44, primals_44, 1, stream=stream0)
            del primals_44
            # Topologically Sorted Source Nodes: [add__7], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_50, primals_50, 1, stream=stream0)
            del primals_50
            # Topologically Sorted Source Nodes: [add__8], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_57, primals_57, 1, stream=stream0)
            del primals_57
            # Topologically Sorted Source Nodes: [add__9], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_63, primals_63, 1, stream=stream0)
            del primals_63
            # Topologically Sorted Source Nodes: [add__10], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_72, primals_72, 1, stream=stream0)
            del primals_72
            # Topologically Sorted Source Nodes: [add__11], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_82, primals_82, 1, stream=stream0)
            del primals_82
            # Topologically Sorted Source Nodes: [add__12], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_89, primals_89, 1, stream=stream0)
            del primals_89
            # Topologically Sorted Source Nodes: [add__13], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_96, primals_96, 1, stream=stream0)
            del primals_96
            # Topologically Sorted Source Nodes: [add__14], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_107, primals_107, 1, stream=stream0)
            del primals_107
            # Topologically Sorted Source Nodes: [add__15], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_114, primals_114, 1, stream=stream0)
            del primals_114
            # Topologically Sorted Source Nodes: [add__16], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_121, primals_121, 1, stream=stream0)
            del primals_121
            # Topologically Sorted Source Nodes: [add__17], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_132, primals_132, 1, stream=stream0)
            del primals_132
            # Topologically Sorted Source Nodes: [add__18], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_139, primals_139, 1, stream=stream0)
            del primals_139
            # Topologically Sorted Source Nodes: [add__19], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_146, primals_146, 1, stream=stream0)
            del primals_146
            # Topologically Sorted Source Nodes: [add__20], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_157, primals_157, 1, stream=stream0)
            del primals_157
            # Topologically Sorted Source Nodes: [add__21], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_163, primals_163, 1, stream=stream0)
            del primals_163
            # Topologically Sorted Source Nodes: [add__22], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_171, primals_171, 1, stream=stream0)
            del primals_171
            # Topologically Sorted Source Nodes: [add__23], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_181, primals_181, 1, stream=stream0)
            del primals_181
            # Topologically Sorted Source Nodes: [add__24], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_188, primals_188, 1, stream=stream0)
            del primals_188
            # Topologically Sorted Source Nodes: [add__25], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_197, primals_197, 1, stream=stream0)
            del primals_197
            # Topologically Sorted Source Nodes: [add__26], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_208, primals_208, 1, stream=stream0)
            del primals_208
            # Topologically Sorted Source Nodes: [add__27], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_215, primals_215, 1, stream=stream0)
            del primals_215
            # Topologically Sorted Source Nodes: [add__28], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_224, primals_224, 1, stream=stream0)
            del primals_224
            # Topologically Sorted Source Nodes: [add__29], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_235, primals_235, 1, stream=stream0)
            del primals_235
            # Topologically Sorted Source Nodes: [add__30], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_242, primals_242, 1, stream=stream0)
            del primals_242
            # Topologically Sorted Source Nodes: [add__31], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_251, primals_251, 1, stream=stream0)
            del primals_251
            # Topologically Sorted Source Nodes: [add__32], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_262, primals_262, 1, stream=stream0)
            del primals_262
            # Topologically Sorted Source Nodes: [add__33], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_268, primals_268, 1, stream=stream0)
            del primals_268
            # Topologically Sorted Source Nodes: [add__34], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_274, primals_274, 1, stream=stream0)
            del primals_274
            # Topologically Sorted Source Nodes: [add__35], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_284, primals_284, 1, stream=stream0)
            del primals_284
            # Topologically Sorted Source Nodes: [add__36], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_291, primals_291, 1, stream=stream0)
            del primals_291
            # Topologically Sorted Source Nodes: [add__37], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_300, primals_300, 1, stream=stream0)
            del primals_300
            # Topologically Sorted Source Nodes: [add__38], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_311, primals_311, 1, stream=stream0)
            del primals_311
            # Topologically Sorted Source Nodes: [add__39], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_318, primals_318, 1, stream=stream0)
            del primals_318
            # Topologically Sorted Source Nodes: [add__40], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_327, primals_327, 1, stream=stream0)
            del primals_327
            # Topologically Sorted Source Nodes: [add__41], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_338, primals_338, 1, stream=stream0)
            del primals_338
            # Topologically Sorted Source Nodes: [add__42], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_345, primals_345, 1, stream=stream0)
            del primals_345
            # Topologically Sorted Source Nodes: [add__43], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_354, primals_354, 1, stream=stream0)
            del primals_354
            # Topologically Sorted Source Nodes: [add__44], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_365, primals_365, 1, stream=stream0)
            del primals_365
            # Topologically Sorted Source Nodes: [add__45], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_371, primals_371, 1, stream=stream0)
            del primals_371
            # Topologically Sorted Source Nodes: [add__46], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_380, primals_380, 1, stream=stream0)
            del primals_380
            # Topologically Sorted Source Nodes: [add__47], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_390, primals_390, 1, stream=stream0)
            del primals_390
            # Topologically Sorted Source Nodes: [add__48], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_396, primals_396, 1, stream=stream0)
            del primals_396
            # Topologically Sorted Source Nodes: [add__49], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_405, primals_405, 1, stream=stream0)
            del primals_405
            # Topologically Sorted Source Nodes: [add__50], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_416, primals_416, 1, stream=stream0)
            del primals_416
            # Topologically Sorted Source Nodes: [add__51], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_422, primals_422, 1, stream=stream0)
            del primals_422
            # Topologically Sorted Source Nodes: [add__52], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_431, primals_431, 1, stream=stream0)
            del primals_431
            # Topologically Sorted Source Nodes: [add__53], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_442, primals_442, 1, stream=stream0)
            del primals_442
            # Topologically Sorted Source Nodes: [add__54], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_448, primals_448, 1, stream=stream0)
            del primals_448
            # Topologically Sorted Source Nodes: [add__55], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_457, primals_457, 1, stream=stream0)
            del primals_457
            # Topologically Sorted Source Nodes: [add__56], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_468, primals_468, 1, stream=stream0)
            del primals_468
            # Topologically Sorted Source Nodes: [add__57], Original ATen: [aten.add, aten.copy_]
            stream0 = get_raw_stream(0)
            triton_poi_fused_add_copy__251.run(primals_474, primals_474, 1, stream=stream0)
            del primals_474
        return (buf1192, primals_6, primals_12, primals_18, primals_25, primals_26, primals_33, primals_34, primals_40, primals_47, primals_53, primals_54, primals_60, primals_66, primals_67, primals_75, primals_76, primals_85, primals_92, primals_93, primals_99, primals_100, primals_110, primals_117, primals_118, primals_124, primals_125, primals_135, primals_142, primals_143, primals_149, primals_150, primals_160, primals_166, primals_167, primals_174, primals_175, primals_184, primals_191, primals_192, primals_200, primals_201, primals_211, primals_218, primals_219, primals_227, primals_228, primals_238, primals_245, primals_246, primals_254, primals_255, primals_265, primals_271, primals_272, primals_277, primals_278, primals_287, primals_294, primals_295, primals_303, primals_304, primals_314, primals_321, primals_322, primals_330, primals_331, primals_341, primals_348, primals_349, primals_357, primals_358, primals_368, primals_374, primals_375, primals_383, primals_384, primals_393, primals_399, primals_400, primals_408, primals_409, primals_419, primals_425, primals_426, primals_434, primals_435, primals_445, primals_451, primals_452, primals_460, primals_461, primals_471, primals_477, primals_478, buf0, buf1, buf2, reinterpret_tensor(buf12, (32, ), (1, ), 0), buf13, buf14, buf15, reinterpret_tensor(buf25, (32, ), (1, ), 0), buf26, buf27, buf28, reinterpret_tensor(buf38, (32, ), (1, ), 0), reinterpret_tensor(buf39, (128, 16, 112, 112), (401408, 12544, 112, 1), 0), reinterpret_tensor(buf39, (128, 16, 112, 112), (401408, 12544, 112, 1), 200704), buf40, buf43, buf46, buf53, buf56, buf57, reinterpret_tensor(buf58, (128, 64, 112, 112), (2408448, 12544, 112, 1), 0), buf61, reinterpret_tensor(buf58, (128, 64, 112, 112), (2408448, 12544, 112, 1), 802816), buf64, reinterpret_tensor(buf58, (128, 64, 112, 112), (2408448, 12544, 112, 1), 1605632), buf67, buf74, buf77, buf78, reinterpret_tensor(buf79, (128, 96, 56, 56), (602112, 3136, 56, 1), 0), buf82, reinterpret_tensor(buf79, (128, 96, 56, 56), (602112, 3136, 56, 1), 301056), buf85, reinterpret_tensor(buf95, (40, ), (1, ), 0), buf97, buf98, buf99, buf102, buf105, reinterpret_tensor(buf115, (120, ), (1, ), 0), buf116, buf117, buf118, buf125, buf128, buf129, reinterpret_tensor(buf130, (128, 60, 56, 56), (376320, 3136, 56, 1), 0), buf133, reinterpret_tensor(buf130, (128, 60, 56, 56), (376320, 3136, 56, 1), 188160), buf136, reinterpret_tensor(buf146, (40, ), (1, ), 0), buf147, buf148, buf149, buf156, buf159, buf161, reinterpret_tensor(buf162, (128, 60, 56, 56), (752640, 3136, 56, 1), 0), buf165, reinterpret_tensor(buf162, (128, 60, 56, 56), (752640, 3136, 56, 1), 188160), buf168, reinterpret_tensor(buf162, (128, 60, 56, 56), (752640, 3136, 56, 1), 376320), buf171, reinterpret_tensor(buf162, (128, 60, 56, 56), (752640, 3136, 56, 1), 564480), buf174, buf181, buf184, buf187, buf188, buf190, buf191, buf192, buf194, buf195, buf196, buf197, reinterpret_tensor(buf207, (56, ), (1, ), 0), buf209, buf210, buf211, buf214, buf217, buf224, buf227, buf229, reinterpret_tensor(buf230, (128, 168, 28, 28), (263424, 784, 28, 1), 0), buf233, reinterpret_tensor(buf230, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf236, buf243, buf246, buf249, buf250, buf252, buf253, buf254, buf256, reinterpret_tensor(buf257, (128, 168, 28, 28), (263424, 784, 28, 1), 0), reinterpret_tensor(buf257, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf258, buf261, buf264, reinterpret_tensor(buf274, (56, ), (1, ), 0), buf276, buf277, buf278, buf281, buf284, buf291, buf294, buf296, reinterpret_tensor(buf297, (128, 168, 28, 28), (263424, 784, 28, 1), 0), buf300, reinterpret_tensor(buf297, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf303, buf310, buf313, buf316, buf317, buf319, buf320, buf321, buf323, reinterpret_tensor(buf324, (128, 168, 28, 28), (263424, 784, 28, 1), 0), reinterpret_tensor(buf324, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf325, buf328, buf331, reinterpret_tensor(buf341, (56, ), (1, ), 0), buf343, buf344, buf345, buf348, buf351, buf358, buf361, buf363, reinterpret_tensor(buf364, (128, 168, 28, 28), (263424, 784, 28, 1), 0), buf367, reinterpret_tensor(buf364, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf370, buf377, buf380, buf383, buf384, buf386, buf387, buf388, buf390, reinterpret_tensor(buf391, (128, 168, 28, 28), (263424, 784, 28, 1), 0), reinterpret_tensor(buf391, (128, 168, 28, 28), (263424, 784, 28, 1), 131712), buf392, buf395, buf398, reinterpret_tensor(buf408, (56, ), (1, ), 0), buf409, buf410, buf411, buf418, buf421, buf423, reinterpret_tensor(buf424, (128, 112, 28, 28), (263424, 784, 28, 1), 0), buf427, reinterpret_tensor(buf424, (128, 112, 28, 28), (263424, 784, 28, 1), 87808), buf430, reinterpret_tensor(buf424, (128, 112, 28, 28), (263424, 784, 28, 1), 175616), buf433, buf440, buf443, buf446, buf447, buf449, buf450, buf451, buf453, buf454, buf455, buf456, reinterpret_tensor(buf466, (104, ), (1, ), 0), buf468, buf469, buf470, buf473, buf476, buf483, buf486, buf488, reinterpret_tensor(buf489, (128, 156, 14, 14), (122304, 196, 14, 1), 0), buf492, reinterpret_tensor(buf489, (128, 156, 14, 14), (122304, 196, 14, 1), 30576), buf495, reinterpret_tensor(buf489, (128, 156, 14, 14), (122304, 196, 14, 1), 61152), buf498, reinterpret_tensor(buf489, (128, 156, 14, 14), (122304, 196, 14, 1), 91728), buf501, buf508, buf511, buf514, buf515, buf517, buf518, buf519, buf521, reinterpret_tensor(buf522, (128, 312, 14, 14), (122304, 196, 14, 1), 0), reinterpret_tensor(buf522, (128, 312, 14, 14), (122304, 196, 14, 1), 61152), buf523, buf526, buf529, reinterpret_tensor(buf539, (104, ), (1, ), 0), buf541, buf542, buf543, buf546, buf549, buf556, buf559, buf561, reinterpret_tensor(buf562, (128, 156, 14, 14), (122304, 196, 14, 1), 0), buf565, reinterpret_tensor(buf562, (128, 156, 14, 14), (122304, 196, 14, 1), 30576), buf568, reinterpret_tensor(buf562, (128, 156, 14, 14), (122304, 196, 14, 1), 61152), buf571, reinterpret_tensor(buf562, (128, 156, 14, 14), (122304, 196, 14, 1), 91728), buf574, buf581, buf584, buf587, buf588, buf590, buf591, buf592, buf594, reinterpret_tensor(buf595, (128, 312, 14, 14), (122304, 196, 14, 1), 0), reinterpret_tensor(buf595, (128, 312, 14, 14), (122304, 196, 14, 1), 61152), buf596, buf599, buf602, reinterpret_tensor(buf612, (104, ), (1, ), 0), buf614, buf615, buf616, buf619, buf622, buf629, buf632, buf634, reinterpret_tensor(buf635, (128, 156, 14, 14), (122304, 196, 14, 1), 0), buf638, reinterpret_tensor(buf635, (128, 156, 14, 14), (122304, 196, 14, 1), 30576), buf641, reinterpret_tensor(buf635, (128, 156, 14, 14), (122304, 196, 14, 1), 61152), buf644, reinterpret_tensor(buf635, (128, 156, 14, 14), (122304, 196, 14, 1), 91728), buf647, buf654, buf657, buf660, buf661, buf663, buf664, buf665, buf667, reinterpret_tensor(buf668, (128, 312, 14, 14), (122304, 196, 14, 1), 0), reinterpret_tensor(buf668, (128, 312, 14, 14), (122304, 196, 14, 1), 61152), buf669, buf672, buf675, reinterpret_tensor(buf685, (104, ), (1, ), 0), buf686, buf687, buf688, buf695, buf698, buf700, buf701, buf702, buf709, buf712, buf715, buf716, buf718, buf719, buf720, buf722, buf723, buf724, buf725, reinterpret_tensor(buf735, (160, ), (1, ), 0), buf737, buf738, buf739, buf742, buf745, buf752, buf755, buf757, reinterpret_tensor(buf758, (128, 120, 14, 14), (94080, 196, 14, 1), 0), buf761, reinterpret_tensor(buf758, (128, 120, 14, 14), (94080, 196, 14, 1), 23520), buf764, reinterpret_tensor(buf758, (128, 120, 14, 14), (94080, 196, 14, 1), 47040), buf767, reinterpret_tensor(buf758, (128, 120, 14, 14), (94080, 196, 14, 1), 70560), buf770, buf777, buf780, buf783, buf784, buf786, buf787, buf788, buf790, reinterpret_tensor(buf791, (128, 240, 14, 14), (94080, 196, 14, 1), 0), reinterpret_tensor(buf791, (128, 240, 14, 14), (94080, 196, 14, 1), 47040), buf792, buf795, buf798, reinterpret_tensor(buf808, (160, ), (1, ), 0), buf810, buf811, buf812, buf815, buf818, buf825, buf828, buf830, reinterpret_tensor(buf831, (128, 120, 14, 14), (94080, 196, 14, 1), 0), buf834, reinterpret_tensor(buf831, (128, 120, 14, 14), (94080, 196, 14, 1), 23520), buf837, reinterpret_tensor(buf831, (128, 120, 14, 14), (94080, 196, 14, 1), 47040), buf840, reinterpret_tensor(buf831, (128, 120, 14, 14), (94080, 196, 14, 1), 70560), buf843, buf850, buf853, buf856, buf857, buf859, buf860, buf861, buf863, reinterpret_tensor(buf864, (128, 240, 14, 14), (94080, 196, 14, 1), 0), reinterpret_tensor(buf864, (128, 240, 14, 14), (94080, 196, 14, 1), 47040), buf865, buf868, buf871, reinterpret_tensor(buf881, (160, ), (1, ), 0), buf883, buf884, buf885, buf888, buf891, buf898, buf901, buf903, reinterpret_tensor(buf904, (128, 120, 14, 14), (94080, 196, 14, 1), 0), buf907, reinterpret_tensor(buf904, (128, 120, 14, 14), (94080, 196, 14, 1), 23520), buf910, reinterpret_tensor(buf904, (128, 120, 14, 14), (94080, 196, 14, 1), 47040), buf913, reinterpret_tensor(buf904, (128, 120, 14, 14), (94080, 196, 14, 1), 70560), buf916, buf923, buf926, buf929, buf930, buf932, buf933, buf934, buf936, reinterpret_tensor(buf937, (128, 240, 14, 14), (94080, 196, 14, 1), 0), reinterpret_tensor(buf937, (128, 240, 14, 14), (94080, 196, 14, 1), 47040), buf938, buf941, buf944, reinterpret_tensor(buf954, (160, ), (1, ), 0), buf955, buf956, buf957, buf961, buf964, buf966, reinterpret_tensor(buf967, (128, 240, 14, 14), (188160, 196, 14, 1), 0), buf970, reinterpret_tensor(buf967, (128, 240, 14, 14), (188160, 196, 14, 1), 47040), buf973, reinterpret_tensor(buf967, (128, 240, 14, 14), (188160, 196, 14, 1), 94080), buf976, reinterpret_tensor(buf967, (128, 240, 14, 14), (188160, 196, 14, 1), 141120), buf979, buf983, buf986, buf989, buf990, buf992, buf993, buf994, buf996, buf997, buf998, buf999, reinterpret_tensor(buf1006, (264, ), (1, ), 0), buf1007, buf1008, buf1009, buf1013, buf1016, buf1018, reinterpret_tensor(buf1019, (128, 396, 7, 7), (77632, 49, 7, 1), 0), buf1022, reinterpret_tensor(buf1019, (128, 396, 7, 7), (77632, 49, 7, 1), 19404), buf1025, reinterpret_tensor(buf1019, (128, 396, 7, 7), (77632, 49, 7, 1), 38808), buf1028, reinterpret_tensor(buf1019, (128, 396, 7, 7), (77632, 49, 7, 1), 58212), buf1031, buf1035, buf1038, buf1041, buf1042, buf1044, buf1045, buf1046, buf1048, reinterpret_tensor(buf1049, (128, 792, 7, 7), (77632, 49, 7, 1), 0), reinterpret_tensor(buf1049, (128, 792, 7, 7), (77632, 49, 7, 1), 38808), buf1050, buf1053, buf1056, reinterpret_tensor(buf1063, (264, ), (1, ), 0), buf1064, buf1065, buf1066, buf1070, buf1073, buf1075, reinterpret_tensor(buf1076, (128, 396, 7, 7), (77632, 49, 7, 1), 0), buf1079, reinterpret_tensor(buf1076, (128, 396, 7, 7), (77632, 49, 7, 1), 19404), buf1082, reinterpret_tensor(buf1076, (128, 396, 7, 7), (77632, 49, 7, 1), 38808), buf1085, reinterpret_tensor(buf1076, (128, 396, 7, 7), (77632, 49, 7, 1), 58212), buf1088, buf1092, buf1095, buf1098, buf1099, buf1101, buf1102, buf1103, buf1105, reinterpret_tensor(buf1106, (128, 792, 7, 7), (77632, 49, 7, 1), 0), reinterpret_tensor(buf1106, (128, 792, 7, 7), (77632, 49, 7, 1), 38808), buf1107, buf1110, buf1113, reinterpret_tensor(buf1120, (264, ), (1, ), 0), buf1121, buf1122, buf1123, buf1127, buf1130, buf1132, reinterpret_tensor(buf1133, (128, 396, 7, 7), (77632, 49, 7, 1), 0), buf1136, reinterpret_tensor(buf1133, (128, 396, 7, 7), (77632, 49, 7, 1), 19404), buf1139, reinterpret_tensor(buf1133, (128, 396, 7, 7), (77632, 49, 7, 1), 38808), buf1142, reinterpret_tensor(buf1133, (128, 396, 7, 7), (77632, 49, 7, 1), 58212), buf1145, buf1149, buf1152, buf1155, buf1156, buf1158, buf1159, buf1160, buf1162, reinterpret_tensor(buf1163, (128, 792, 7, 7), (77632, 49, 7, 1), 0), reinterpret_tensor(buf1163, (128, 792, 7, 7), (77632, 49, 7, 1), 38808), buf1164, buf1167, buf1170, reinterpret_tensor(buf1177, (264, ), (1, ), 0), buf1178, buf1179, buf1180, buf1184, buf1187, reinterpret_tensor(buf1189, (128, 1536), (1536, 1), 0), buf1190, reinterpret_tensor(buf1174, (1, 264, 1, 1), (264, 1, 1, 1), 0), reinterpret_tensor(buf1117, (1, 264, 1, 1), (264, 1, 1, 1), 0), reinterpret_tensor(buf1060, (1, 264, 1, 1), (264, 1, 1, 1), 0), reinterpret_tensor(buf1003, (1, 264, 1, 1), (264, 1, 1, 1), 0), reinterpret_tensor(buf951, (1, 160, 1, 1), (160, 1, 1, 1), 0), reinterpret_tensor(buf878, (1, 160, 1, 1), (160, 1, 1, 1), 0), reinterpret_tensor(buf805, (1, 160, 1, 1), (160, 1, 1, 1), 0), reinterpret_tensor(buf732, (1, 160, 1, 1), (160, 1, 1, 1), 0), reinterpret_tensor(buf682, (1, 104, 1, 1), (104, 1, 1, 1), 0), reinterpret_tensor(buf609, (1, 104, 1, 1), (104, 1, 1, 1), 0), reinterpret_tensor(buf536, (1, 104, 1, 1), (104, 1, 1, 1), 0), reinterpret_tensor(buf463, (1, 104, 1, 1), (104, 1, 1, 1), 0), reinterpret_tensor(buf405, (1, 56, 1, 1), (56, 1, 1, 1), 0), reinterpret_tensor(buf338, (1, 56, 1, 1), (56, 1, 1, 1), 0), reinterpret_tensor(buf271, (1, 56, 1, 1), (56, 1, 1, 1), 0), reinterpret_tensor(buf204, (1, 56, 1, 1), (56, 1, 1, 1), 0), reinterpret_tensor(buf143, (1, 40, 1, 1), (40, 1, 1, 1), 0), reinterpret_tensor(buf112, (1, 120, 1, 1), (120, 1, 1, 1), 0), reinterpret_tensor(buf92, (1, 40, 1, 1), (40, 1, 1, 1), 0), reinterpret_tensor(buf35, (1, 32, 1, 1), (32, 1, 1, 1), 0), reinterpret_tensor(buf22, (1, 32, 1, 1), (32, 1, 1, 1), 0), reinterpret_tensor(buf9, (1, 32, 1, 1), (32, 1, 1, 1), 0), )

runner = Runner(partitions=[])
call = runner.call
recursively_apply_fns = runner.recursively_apply_fns


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((32, 3, 3, 3), (27, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((128, 3, 224, 224), (150528, 50176, 224, 1), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_4 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_6 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_7 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_8 = rand_strided((32, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_9 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_10 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_11 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_12 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_13 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_14 = rand_strided((32, 32, 1, 1), (32, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_15 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_16 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_17 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_18 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_19 = rand_strided((32, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_20 = rand_strided((96, 16, 1, 1), (16, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_21 = rand_strided((96, 16, 1, 1), (16, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_22 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_23 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_24 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_25 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_26 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_27 = rand_strided((64, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_28 = rand_strided((64, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_29 = rand_strided((64, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_30 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_31 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_32 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_33 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_34 = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_35 = rand_strided((20, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_36 = rand_strided((20, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_37 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_38 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_39 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_40 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_41 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_42 = rand_strided((60, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_43 = rand_strided((60, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_44 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_45 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_46 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_47 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_48 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_49 = rand_strided((120, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_50 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_51 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_52 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_53 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_54 = rand_strided((120, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_55 = rand_strided((20, 60, 1, 1), (60, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_56 = rand_strided((20, 60, 1, 1), (60, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_57 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_58 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_59 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_60 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_61 = rand_strided((40, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_62 = rand_strided((240, 40, 1, 1), (40, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_63 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_64 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_65 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_66 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_67 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_68 = rand_strided((60, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_69 = rand_strided((60, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_70 = rand_strided((60, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_71 = rand_strided((60, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_72 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_73 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_74 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_75 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_76 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_77 = rand_strided((20, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_78 = rand_strided((20, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_79 = rand_strided((240, 20, 1, 1), (20, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_80 = rand_strided((240, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_81 = rand_strided((56, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_82 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_83 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_84 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_85 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_86 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_87 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_88 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_89 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_90 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_91 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_92 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_93 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_94 = rand_strided((168, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_95 = rand_strided((168, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_96 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_97 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_98 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_99 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_100 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_101 = rand_strided((28, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_102 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_103 = rand_strided((336, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_104 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_105 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_106 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_107 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_108 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_109 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_110 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_111 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_112 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_113 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_114 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_115 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_116 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_117 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_118 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_119 = rand_strided((168, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_120 = rand_strided((168, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_121 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_122 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_123 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_124 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_125 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_126 = rand_strided((28, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_127 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_128 = rand_strided((336, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_129 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_130 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_131 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_132 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_133 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_134 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_135 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_136 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_137 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_138 = rand_strided((168, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_139 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_140 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_141 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_142 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_143 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_144 = rand_strided((168, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_145 = rand_strided((168, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_146 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_147 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_148 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_149 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_150 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_151 = rand_strided((28, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_152 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_153 = rand_strided((336, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_154 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_155 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_156 = rand_strided((28, 168, 1, 1), (168, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_157 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_158 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_159 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_160 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_161 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_162 = rand_strided((336, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_163 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_164 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_165 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_166 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_167 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_168 = rand_strided((112, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_169 = rand_strided((112, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_170 = rand_strided((112, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_171 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_172 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_173 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_174 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_175 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_176 = rand_strided((14, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_177 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_178 = rand_strided((336, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_179 = rand_strided((336, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_180 = rand_strided((104, 336, 1, 1), (336, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_181 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_182 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_183 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_184 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_185 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_186 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_187 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_188 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_189 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_190 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_191 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_192 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_193 = rand_strided((156, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_194 = rand_strided((156, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_195 = rand_strided((156, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_196 = rand_strided((156, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_197 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_198 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_199 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_200 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_201 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_202 = rand_strided((26, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_203 = rand_strided((26, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_204 = rand_strided((624, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_205 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_206 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_207 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_208 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_209 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_210 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_211 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_212 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_213 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_214 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_215 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_216 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_217 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_218 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_219 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_220 = rand_strided((156, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_221 = rand_strided((156, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_222 = rand_strided((156, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_223 = rand_strided((156, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_224 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_225 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_226 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_227 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_228 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_229 = rand_strided((26, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_230 = rand_strided((26, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_231 = rand_strided((624, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_232 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_233 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_234 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_235 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_236 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_237 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_238 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_239 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_240 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_241 = rand_strided((312, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_242 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_243 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_244 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_245 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_246 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_247 = rand_strided((156, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_248 = rand_strided((156, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_249 = rand_strided((156, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_250 = rand_strided((156, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_251 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_252 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_253 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_254 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_255 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_256 = rand_strided((26, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_257 = rand_strided((26, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_258 = rand_strided((624, 26, 1, 1), (26, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_259 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_260 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_261 = rand_strided((52, 312, 1, 1), (312, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_262 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_263 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_264 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_265 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_266 = rand_strided((104, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_267 = rand_strided((624, 104, 1, 1), (104, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_268 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_269 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_270 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_271 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_272 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_273 = rand_strided((624, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_274 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_275 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_276 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_277 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_278 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_279 = rand_strided((52, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_280 = rand_strided((52, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_281 = rand_strided((624, 52, 1, 1), (52, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_282 = rand_strided((624, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_283 = rand_strided((160, 624, 1, 1), (624, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_284 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_285 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_286 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_287 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_288 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_289 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_290 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_291 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_292 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_293 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_294 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_295 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_296 = rand_strided((120, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_297 = rand_strided((120, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_298 = rand_strided((120, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_299 = rand_strided((120, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_300 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_301 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_302 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_303 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_304 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_305 = rand_strided((80, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_306 = rand_strided((80, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_307 = rand_strided((480, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_308 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_309 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_310 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_311 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_312 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_313 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_314 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_315 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_316 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_317 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_318 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_319 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_320 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_321 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_322 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_323 = rand_strided((120, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_324 = rand_strided((120, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_325 = rand_strided((120, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_326 = rand_strided((120, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_327 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_328 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_329 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_330 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_331 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_332 = rand_strided((80, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_333 = rand_strided((80, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_334 = rand_strided((480, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_335 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_336 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_337 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_338 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_339 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_340 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_341 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_342 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_343 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_344 = rand_strided((240, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_345 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_346 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_347 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_348 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_349 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_350 = rand_strided((120, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_351 = rand_strided((120, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_352 = rand_strided((120, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_353 = rand_strided((120, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_354 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_355 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_356 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_357 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_358 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_359 = rand_strided((80, 480, 1, 1), (480, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_360 = rand_strided((80, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_361 = rand_strided((480, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_362 = rand_strided((480, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_363 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_364 = rand_strided((80, 240, 1, 1), (240, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_365 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_366 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_367 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_368 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_369 = rand_strided((160, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_370 = rand_strided((960, 160, 1, 1), (160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_371 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_372 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_373 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_374 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_375 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_376 = rand_strided((240, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_377 = rand_strided((240, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_378 = rand_strided((240, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_379 = rand_strided((240, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_380 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_381 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_382 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_383 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_384 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_385 = rand_strided((80, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_386 = rand_strided((80, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_387 = rand_strided((960, 80, 1, 1), (80, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_388 = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_389 = rand_strided((264, 960, 1, 1), (960, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_390 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_391 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_392 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_393 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_394 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_395 = rand_strided((1584, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_396 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_397 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_398 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_399 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_400 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_401 = rand_strided((396, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_402 = rand_strided((396, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_403 = rand_strided((396, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_404 = rand_strided((396, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_405 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_406 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_407 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_408 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_409 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_410 = rand_strided((132, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_411 = rand_strided((132, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_412 = rand_strided((1584, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_413 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_414 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_415 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_416 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_417 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_418 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_419 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_420 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_421 = rand_strided((1584, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_422 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_423 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_424 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_425 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_426 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_427 = rand_strided((396, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_428 = rand_strided((396, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_429 = rand_strided((396, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_430 = rand_strided((396, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_431 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_432 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_433 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_434 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_435 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_436 = rand_strided((132, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_437 = rand_strided((132, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_438 = rand_strided((1584, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_439 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_440 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_441 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_442 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_443 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_444 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_445 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_446 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_447 = rand_strided((1584, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_448 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_449 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_450 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_451 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_452 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_453 = rand_strided((396, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_454 = rand_strided((396, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_455 = rand_strided((396, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_456 = rand_strided((396, 1, 9, 9), (81, 81, 9, 1), device='cuda:0', dtype=torch.float32)
    primals_457 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_458 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_459 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_460 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_461 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_462 = rand_strided((132, 1584, 1, 1), (1584, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_463 = rand_strided((132, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_464 = rand_strided((1584, 132, 1, 1), (132, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_465 = rand_strided((1584, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_466 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_467 = rand_strided((132, 792, 1, 1), (792, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_468 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_469 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_470 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_471 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_472 = rand_strided((264, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_473 = rand_strided((1536, 264, 1, 1), (264, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_474 = rand_strided((), (), device='cuda:0', dtype=torch.int64)
    primals_475 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_476 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_477 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_478 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_479 = rand_strided((1000, 1536), (1536, 1), device='cuda:0', dtype=torch.float32)
    primals_480 = rand_strided((1000, ), (1, ), device='cuda:0', dtype=torch.float32)
    fn = lambda: call([primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, primals_75, primals_76, primals_77, primals_78, primals_79, primals_80, primals_81, primals_82, primals_83, primals_84, primals_85, primals_86, primals_87, primals_88, primals_89, primals_90, primals_91, primals_92, primals_93, primals_94, primals_95, primals_96, primals_97, primals_98, primals_99, primals_100, primals_101, primals_102, primals_103, primals_104, primals_105, primals_106, primals_107, primals_108, primals_109, primals_110, primals_111, primals_112, primals_113, primals_114, primals_115, primals_116, primals_117, primals_118, primals_119, primals_120, primals_121, primals_122, primals_123, primals_124, primals_125, primals_126, primals_127, primals_128, primals_129, primals_130, primals_131, primals_132, primals_133, primals_134, primals_135, primals_136, primals_137, primals_138, primals_139, primals_140, primals_141, primals_142, primals_143, primals_144, primals_145, primals_146, primals_147, primals_148, primals_149, primals_150, primals_151, primals_152, primals_153, primals_154, primals_155, primals_156, primals_157, primals_158, primals_159, primals_160, primals_161, primals_162, primals_163, primals_164, primals_165, primals_166, primals_167, primals_168, primals_169, primals_170, primals_171, primals_172, primals_173, primals_174, primals_175, primals_176, primals_177, primals_178, primals_179, primals_180, primals_181, primals_182, primals_183, primals_184, primals_185, primals_186, primals_187, primals_188, primals_189, primals_190, primals_191, primals_192, primals_193, primals_194, primals_195, primals_196, primals_197, primals_198, primals_199, primals_200, primals_201, primals_202, primals_203, primals_204, primals_205, primals_206, primals_207, primals_208, primals_209, primals_210, primals_211, primals_212, primals_213, primals_214, primals_215, primals_216, primals_217, primals_218, primals_219, primals_220, primals_221, primals_222, primals_223, primals_224, primals_225, primals_226, primals_227, primals_228, primals_229, primals_230, primals_231, primals_232, primals_233, primals_234, primals_235, primals_236, primals_237, primals_238, primals_239, primals_240, primals_241, primals_242, primals_243, primals_244, primals_245, primals_246, primals_247, primals_248, primals_249, primals_250, primals_251, primals_252, primals_253, primals_254, primals_255, primals_256, primals_257, primals_258, primals_259, primals_260, primals_261, primals_262, primals_263, primals_264, primals_265, primals_266, primals_267, primals_268, primals_269, primals_270, primals_271, primals_272, primals_273, primals_274, primals_275, primals_276, primals_277, primals_278, primals_279, primals_280, primals_281, primals_282, primals_283, primals_284, primals_285, primals_286, primals_287, primals_288, primals_289, primals_290, primals_291, primals_292, primals_293, primals_294, primals_295, primals_296, primals_297, primals_298, primals_299, primals_300, primals_301, primals_302, primals_303, primals_304, primals_305, primals_306, primals_307, primals_308, primals_309, primals_310, primals_311, primals_312, primals_313, primals_314, primals_315, primals_316, primals_317, primals_318, primals_319, primals_320, primals_321, primals_322, primals_323, primals_324, primals_325, primals_326, primals_327, primals_328, primals_329, primals_330, primals_331, primals_332, primals_333, primals_334, primals_335, primals_336, primals_337, primals_338, primals_339, primals_340, primals_341, primals_342, primals_343, primals_344, primals_345, primals_346, primals_347, primals_348, primals_349, primals_350, primals_351, primals_352, primals_353, primals_354, primals_355, primals_356, primals_357, primals_358, primals_359, primals_360, primals_361, primals_362, primals_363, primals_364, primals_365, primals_366, primals_367, primals_368, primals_369, primals_370, primals_371, primals_372, primals_373, primals_374, primals_375, primals_376, primals_377, primals_378, primals_379, primals_380, primals_381, primals_382, primals_383, primals_384, primals_385, primals_386, primals_387, primals_388, primals_389, primals_390, primals_391, primals_392, primals_393, primals_394, primals_395, primals_396, primals_397, primals_398, primals_399, primals_400, primals_401, primals_402, primals_403, primals_404, primals_405, primals_406, primals_407, primals_408, primals_409, primals_410, primals_411, primals_412, primals_413, primals_414, primals_415, primals_416, primals_417, primals_418, primals_419, primals_420, primals_421, primals_422, primals_423, primals_424, primals_425, primals_426, primals_427, primals_428, primals_429, primals_430, primals_431, primals_432, primals_433, primals_434, primals_435, primals_436, primals_437, primals_438, primals_439, primals_440, primals_441, primals_442, primals_443, primals_444, primals_445, primals_446, primals_447, primals_448, primals_449, primals_450, primals_451, primals_452, primals_453, primals_454, primals_455, primals_456, primals_457, primals_458, primals_459, primals_460, primals_461, primals_462, primals_463, primals_464, primals_465, primals_466, primals_467, primals_468, primals_469, primals_470, primals_471, primals_472, primals_473, primals_474, primals_475, primals_476, primals_477, primals_478, primals_479, primals_480])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('mixnet_l', benchmark_compiled_module)
